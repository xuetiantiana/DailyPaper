{
  "data": [
    {
      "id": "2507.00066",
      "abstract": "Human reliability remains a critical concern in safety-critical domains such as nuclear power, where operational failures are often linked to human error. While conventional human reliability analysis (HRA) methods have been widely adopted, they rely heavily on expert judgment for identifying human failure events (HFEs) and assigning performance influencing factors (PIFs). This reliance introduces challenges related to reproducibility, subjectivity, and limited integration of interface-level data. In particular, current approaches lack the capacity to rigorously assess how human-machine interface design contributes to operator performance variability and error susceptibility. To address these limitations, this study proposes a framework for risk-informed human failure event identification and interface-induced risk assessment driven by AutoGraph (InSight-R). By linking empirical behavioral data to the interface-embedded knowledge graph (IE-KG) constructed by the automated graph-based execution framework (AutoGraph), the InSight-R framework enables automated HFE identification based on both error-prone and time-deviated operational paths. Furthermore, we discuss the relationship between designer-user conflicts and human error. The results demonstrate that InSight-R not only enhances the objectivity and interpretability of HFE identification but also provides a scalable pathway toward dynamic, real-time human reliability assessment in digitalized control environments. This framework offers actionable insights for interface design optimization and contributes to the advancement of mechanism-driven HRA methodologies.",
      "authors": [
        "Xingyu Xiao",
        "Jiejuan Tong",
        "Peng Chen",
        "Jun Sun",
        "Zhe Sui",
        "Jingang Liang",
        "Hongru Zhao",
        "Jun Zhao",
        "Haitao Wang"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-28T02:04:06+00:00",
          "link": "https://arxiv.org/abs/2507.00066v1",
          "size": "9735kb",
          "version": "v1"
        }
      ],
      "title": "InSight-R: A Framework for Risk-informed Human Failure Event Identification and Interface-Induced Risk Assessment Driven by AutoGraph",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00066",
        "HTML": "https://arxiv.org/html/2507.00066v1",
        "PDF": "https://arxiv.org/pdf/2507.00066"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper deals with human failure event identification and risk assessment in safety-critical environments. Creativity is not mentioned or relevant to the study's focus on reliability and error assessment."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00161",
      "abstract": "Political polarization undermines democratic civic education by exacerbating identity-based resistance to opposing viewpoints. Emerging AI technologies offer new opportunities to advance interventions that reduce polarization and promote political open-mindedness. We examined novel design strategies that leverage adaptive and emotionally-responsive civic narratives that may sustain students' emotional engagement in stories, and in turn, promote perspective-taking toward members of political out-groups. Drawing on theories from political psychology and narratology, we investigate how affective computing techniques can support three storytelling mechanisms: transportation into a story world, identification with characters, and interaction with the storyteller. Using a design-based research (DBR) approach, we iteratively developed and refined an AI-mediated Digital Civic Storytelling (AI-DCS) platform. Our prototype integrates facial emotion recognition and attention tracking to assess users' affective and attentional states in real time. Narrative content is organized around pre-structured story outlines, with beat-by-beat language adaptation implemented via GPT-4, personalizing linguistic tone to sustain students' emotional engagement in stories that center political perspectives different from their own. Our work offers a foundation for AI-supported, emotionally-sensitive strategies that address affective polarization while preserving learner autonomy. We conclude with implications for civic education interventions, algorithmic literacy, and HCI challenges associated with AI dialogue management and affect-adaptive learning environments.",
      "authors": [
        "Christopher M. Wegemer",
        "Edward Halim",
        "Jeff Burke"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T18:11:12+00:00",
          "link": "https://arxiv.org/abs/2507.00161v1",
          "size": "931kb",
          "version": "v1"
        }
      ],
      "title": "Designing an Adaptive Storytelling Platform to Promote Civic Education in Politically Polarized Learning Environments",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00161",
        "PDF": "https://arxiv.org/pdf/2507.00161"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses designing an adaptive storytelling platform for civic education focusing on emotional and political engagement, which incorporates creative narrative techniques such as transportation into story worlds and interaction with storytelling\u2014indicating a creative dimension as a secondary theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00198",
      "abstract": "We investigate methods for placing labels in AR environments that have visually cluttered scenes. As the number of items increases in a scene within the user' FOV, it is challenging to effectively place labels based on existing label placement guidelines. To address this issue, we implemented three label placement techniques for in-view objects for AR applications. We specifically target a scenario, where various items of different types are scattered within the user's field of view, and multiple items of the same type are situated close together. We evaluate three placement techniques for three target tasks. Our study shows that using a label to spatially group the same types of items is beneficial for identifying, comparing, and summarizing data.",
      "authors": [
        "Ji Hwan Park",
        "Braden Roper",
        "Amirhossein Arezoumand",
        "Tien Tran"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T19:07:34+00:00",
          "link": "https://arxiv.org/abs/2507.00198v1",
          "size": "1164kb",
          "version": "v1"
        }
      ],
      "title": "Exploring AR Label Placements in Visually Cluttered Scenarios",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00198",
        "HTML": "https://arxiv.org/html/2507.00198v1",
        "PDF": "https://arxiv.org/pdf/2507.00198"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper focuses on techniques for label placement in AR environments, which does not relate to creativity, as it addresses visualization efficiency rather than creative processes or outcomes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00202",
      "abstract": "Purpose: Little research has explored the communication needs of autistic adults and how their needs differ from those of other disabled populations. Augmentative and Alternative Communication (AAC) can support these communication needs, but more guidance is needed on how to design AAC to support this population.\n  Materials and Methods: We conducted an online, asynchronous, text-based focus group with five autistic adults to explore their social communication and community engagement and how AAC can help support them.\n  Results and Conclusion: Our analysis of the participant responses found that 1) participants' emotional experiences impacted the communication methods they used, 2) speaking autistic adults can benefit from AAC use, and 3) autistic shutdown creates dynamic communication needs. We present implications for future AAC design: supporting communication in times of shutdown, indicating communication ability to communication partners, and a need to better understand the fear of using AAC. These implications can inform the design for future AAC systems. We also provide themes for future autism research: exploring the impact of a late diagnosis, gaining a better understanding of the communication needs during autistic shutdown, and expanding research to include the social and environmental factors that impact communication. Finally, we provide guidance on how future online focus groups can be run in an accessible manner.",
      "authors": [
        "Blade Frisch",
        "Betts Peters",
        "Keith Vertanen"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T19:10:47+00:00",
          "link": "https://arxiv.org/abs/2507.00202v1",
          "size": "210kb",
          "version": "v1"
        }
      ],
      "title": "Examining the Social Communication and Community Engagement of Autistic Adults through an Asynchronous Focus Group",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00202",
        "HTML": "https://arxiv.org/html/2507.00202v1",
        "PDF": "https://arxiv.org/pdf/2507.00202"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "While the primary focus is on social communication and AAC design for autistic adults, the paper touches on creative design approaches for AAC, indicating creativity as a secondary theme in designing user support tools."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00271",
      "abstract": "While recent research highlights the potential of social robots to support mood regulation, little is known about how prospective users view their integration into everyday life. To explore this, we conducted an exploratory case study that used a speculative robot concept \"Mora\" to provoke reflection and facilitate meaningful discussion about using social robots to manage subtle, day-to-day emotional experiences. We focused on the \"Sunday Blues,\" a common dip in mood that occurs at the end of the weekend, as a relatable context in which to explore individuals' insights. Using a video prototype and a co-constructing stories method, we engaged 15 participants in imagining interactions with Mora and discussing their expectations, doubts, and concerns. The study surfaced a range of nuanced reflections around the attributes of social robots like empathy, intervention effectiveness, and ethical boundaries, which we translated into design considerations for future research and development in human-robot interaction.",
      "authors": [
        "Zhuochao Peng",
        "Jiaxin Xu",
        "Jun Hu",
        "Haian Xue",
        "Laurens A. G. Kolks",
        "Pieter M. A. Desmet"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T21:27:59+00:00",
          "link": "https://arxiv.org/abs/2507.00271v1",
          "size": "2298kb",
          "version": "v1"
        }
      ],
      "title": "User Concerns Regarding Social Robots for Mood Regulation: A Case Study on the \"Sunday Blues\"",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00271",
        "HTML": "https://arxiv.org/html/2507.00271v1",
        "PDF": "https://arxiv.org/pdf/2507.00271"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on mood regulation using social robots, emphasizing empathy and ethical boundaries rather than creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00286",
      "abstract": "Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to interpret and manage visual content in their daily lives. While such tools can enhance the accessibility of visual content and so enable greater user independence, they also introduce complex challenges around visual privacy. In this paper, we investigate the current practices and future design preferences of blind and low vision individuals through an interview study with 21 participants. Our findings reveal a range of current practices with GenAI that balance privacy, efficiency, and emotional agency, with users accounting for privacy risks across six key scenarios, such as self-presentation, indoor/outdoor spatial privacy, social sharing, and handling professional content. Our findings reveal design preferences, including on-device processing, zero-retention guarantees, sensitive content redaction, privacy-aware appearance indicators, and multimodal tactile mirrored interaction methods. We conclude with actionable design recommendations to support user-centered visual privacy through GenAI, expanding the notion of privacy and responsible handling of others data.",
      "authors": [
        "Tanusree Sharma",
        "Yu-Yun Tseng",
        "Lotus Zhang",
        "Ayae Ide",
        "Kelly Avery Mack",
        "Leah Findlater",
        "Danna Gurari",
        "Yang Wang"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Emerging Technologies (cs.ET)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T21:55:21+00:00",
          "link": "https://arxiv.org/abs/2507.00286v1",
          "size": "3283kb",
          "version": "v1"
        }
      ],
      "title": "Visual Privacy Management with Generative AI for Blind and Low-Vision People",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00286",
        "HTML": "https://arxiv.org/html/2507.00286v1",
        "PDF": "https://arxiv.org/pdf/2507.00286"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses visual privacy management for blind and low-vision individuals using Generative AI, with no focus on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00299",
      "abstract": "TikTok, the social media platform that is popular among children and adolescents, offers a more restrictive \"Under 13 Experience\" exclusively for young users in the US, also known as TikTok's \"Kids Mode\". While prior research has studied various aspects of TikTok's regular mode, including privacy and personalization, TikTok's Kids Mode remains understudied, and there is a lack of transparency regarding its content curation and its safety and privacy protections for children. In this paper, (i) we propose an auditing methodology to comprehensively investigate TikTok's Kids Mode and (ii) we apply it to characterize the platform's content curation and determine the prevalence of child-directed content, based on regulations in the Children's Online Privacy Protection Act (COPPA). We find that 83% of videos observed on the \"For You\" page in Kids Mode are actually not child-directed, and even inappropriate content was found. The platform also lacks critical features, namely parental controls and accessibility settings. Our findings have important design and regulatory implications, as children may be incentivized to use TikTok's regular mode instead of Kids Mode, where they are known to be exposed to further safety and privacy risks.",
      "authors": [
        "Olivia Figueira",
        "Pranathi Chamarthi",
        "Tu Le",
        "Athina Markopoulou"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T22:31:31+00:00",
          "link": "https://arxiv.org/abs/2507.00299v1",
          "size": "2335kb",
          "version": "v1"
        }
      ],
      "title": "When Kids Mode Isn't For Kids: Investigating TikTok's \"Under 13 Experience\"",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00299",
        "HTML": "https://arxiv.org/html/2507.00299v1",
        "PDF": "https://arxiv.org/pdf/2507.00299"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The research investigates TikTok's Kids Mode regarding safety, privacy, and content curation, with no direct connection to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00305",
      "abstract": "Patients with amyotrophic lateral sclerosis (ALS) in the completely locked-in state (CLIS) can lose all reliable motor control and are left without any means of communication. It remains unknown whether non-invasive electroencephalogram (EEG) based brain-computer interfaces (BCIs) can support volitional communication in CLIS. Here, we show that a CLIS patient was able to operate an EEG-based BCI across multiple online sessions to respond to both general knowledge and personally relevant assistive questions. The patient delivered \"Yes\"/\"No\" responses by volitionally modulating alpha and beta band power at different channels, guided by real-time auditory feedback from the BCI. The patient communicated assistive needs above chance in all sessions, achieving a perfect score in the final session. Performance on general knowledge questions varied across sessions, with two sessions showing accurate and above-chance responses, while the first and last sessions remained at chance level. The patient also showed consistent modulation patterns over time. These findings suggest that non-invasive BCIs may offer a potential pathway for restoring basic communication in CLIS.",
      "authors": [
        "Deland Liu",
        "Frigyes Samuel Racz",
        "Zoe Lalji and Jose del R. Millan"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Neurons and Cognition (q-bio.NC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T22:46:13+00:00",
          "link": "https://arxiv.org/abs/2507.00305v1",
          "size": "5563kb",
          "version": "v1"
        }
      ],
      "title": "EEG-Based Auditory BCI for Communication in a Completely Locked-In Patient Using Volitional Frequency Band Modulation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00305",
        "HTML": "https://arxiv.org/html/2507.00305v1",
        "PDF": "https://arxiv.org/pdf/2507.00305"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper is about using EEG-based BCIs for communication in locked-in patients, focusing on communication technology rather than creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00333",
      "abstract": "Marksmanship practices are required in various professions, including police, military personnel, hunters, as well as sports shooters, such as Olympic shooting, biathlon, and modern pentathlon. The current form of training and coaching is mostly based on repetition, where the coach does not see through the eyes of the shooter, and analysis is limited to stance and accuracy post-session. In this study, we present a shooting visualization system and evaluate its perceived effectiveness for both novice and expert shooters. To achieve this, five composite visualizations were developed using first-person shooting video recordings enriched with overlaid metrics and graphical summaries. These views were evaluated with 10 participants (5 expert marksmen, 5 novices) through a mixed-methods study including shot-count and aiming interpretation tasks, pairwise preference comparisons, and semi-structured interviews. The results show that a dashboard-style composite view, combining raw video with a polar plot and selected graphs, was preferred in 9 of 10 cases and supported understanding across skill levels. The insights gained from this design study point to the broader value of integrating first-person video with visual analytics for coaching, and we suggest directions for applying this approach to other precision-based sports.",
      "authors": [
        "Emin Zerman",
        "Jonas Carlsson",
        "M{\\aa}rten Sj\\\"ostr\\\"om"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Graphics (cs.GR)",
        "Image and Video Processing (eess.IV)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T00:16:41+00:00",
          "link": "https://arxiv.org/abs/2507.00333v1",
          "size": "3934kb",
          "version": "v1"
        }
      ],
      "title": "Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00333",
        "HTML": "https://arxiv.org/html/2507.00333v1",
        "PDF": "https://arxiv.org/pdf/2507.00333"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The study designs visualizations for marksmanship training, focusing on effectiveness for training, not on creative applications or creativity in design."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00513",
      "abstract": "The integration of various AI tools creates a complex socio-technical environment where employee-customer interactions form the core of work practices. This study investigates how customer service representatives (CSRs) at the power grid service customer service call center perceive AI assistance in their interactions with customers. Through a field visit and semi-structured interviews with 13 CSRs, we found that AI can alleviate some traditional burdens during the call (e.g., typing and memorizing) but also introduces new burdens (e.g., earning, compliance, psychological burdens). This research contributes to a more nuanced understanding of AI integration in organizational settings and highlights the efforts and burdens undertaken by CSRs to adapt to the updated system.",
      "authors": [
        "Kai Qin",
        "Kexin Du",
        "Yimeng Chen",
        "Yueyan Liu",
        "Jie Cai",
        "Zhiqiang Nie",
        "Nan Gao",
        "Guohui Wei",
        "Shengzhu Wang",
        "and Chun Yu"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T07:27:34+00:00",
          "link": "https://arxiv.org/abs/2507.00513v1",
          "size": "2136kb",
          "version": "v1"
        }
      ],
      "title": "Customer Service Representative's Perception of the AI Assistant in an Organization's Call Center",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00513",
        "HTML": "https://arxiv.org/html/2507.00513v1",
        "PDF": "https://arxiv.org/pdf/2507.00513"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The research investigates customer service representatives' perception of AI assistance in a call center. The focus is on socio-technical dynamics and adaptation to AI, without addressing creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00596",
      "abstract": "Privacy is a highly subjective concept and perceived variably by different individuals. Previous research on quantifying user-perceived privacy has primarily relied on questionnaires. Furthermore, applying user-perceived privacy to optimise the parameters of privacy-preserving techniques (PPT) remains insufficiently explored. To address these limitations, we introduce Gaze3P -- the first dataset specifically designed to facilitate systematic investigations into user-perceived privacy. Our dataset comprises gaze data from 100 participants and 1,000 stimuli, encompassing a range of private and safe attributes. With Gaze3P, we train a machine learning model to implicitly and dynamically predict perceived privacy from human eye gaze. Through comprehensive experiments, we show that the resulting models achieve high accuracy. Finally, we illustrate how predicted privacy can be used to optimise the parameters of differentially private mechanisms, thereby enhancing their alignment with user expectations.",
      "authors": [
        "Mayar Elfares",
        "Pascal Reisert",
        "Ralf K\\\"usters",
        "Andreas Bulling"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T09:26:38+00:00",
          "link": "https://arxiv.org/abs/2507.00596v1",
          "size": "12648kb",
          "version": "v1"
        }
      ],
      "title": "Gaze3P: Gaze-Based Prediction of User-Perceived Privacy",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00596",
        "HTML": "https://arxiv.org/html/2507.00596v1",
        "PDF": "https://arxiv.org/pdf/2507.00596"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This study introduces a dataset for predicting user-perceived privacy based on gaze data. It deals with privacy prediction and optimization of privacy techniques, having no linkage to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00657",
      "abstract": "We investigate how Large Language Models (LLMs) behave when simulating political discourse on social media. Leveraging 21 million interactions on X during the 2024 U.S. presidential election, we construct LLM agents based on 1,186 real users, prompting them to reply to politically salient tweets under controlled conditions. Agents are initialized either with minimal ideological cues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one comparisons with human replies. We evaluate three model families (Gemini, Mistral, and DeepSeek) across linguistic style, ideological consistency, and toxicity. We find that richer contextualization improves internal consistency but also amplifies polarization, stylized signals, and harmful language. We observe an emergent distortion that we call \"generation exaggeration\": a systematic amplification of salient traits beyond empirical baselines. Our analysis shows that LLMs do not emulate users, they reconstruct them. Their outputs, indeed, reflect internal optimization dynamics more than observed behavior, introducing structural biases that compromise their reliability as social proxies. This challenges their use in content moderation, deliberative simulations, and policy modeling.",
      "authors": [
        "Jacopo Nudo",
        "Mario Edoardo Pandolfo",
        "Edoardo Loru",
        "Mattia Samory",
        "Matteo Cinelli and Walter Quattrociocchi"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Social and Information Networks (cs.SI)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T10:54:51+00:00",
          "link": "https://arxiv.org/abs/2507.00657v1",
          "size": "17729kb",
          "version": "v1"
        }
      ],
      "title": "Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00657",
        "HTML": "https://arxiv.org/html/2507.00657v1",
        "PDF": "https://arxiv.org/pdf/2507.00657"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "While the paper primarily examines LLM behavior in political discourse, it touches upon the generation of stylized language, which might be loosely connected to creative expression. However, creativity is not a core focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00775",
      "abstract": "We present a systematic review on tasks, interactions, and visualization widgets (refer to tangible entities that are used to accomplish data exploration tasks through specific interactions) in the context of tangible data exploration. Tangible widgets have been shown to reduce cognitive load, enable more natural interactions, and support the completion of complex data exploration tasks. Yet, the field lacks a structured understanding of how task types, interaction methods, and widget designs are coordinated, limiting the ability to identify recurring design patterns and opportunities for innovation. To address this gap, we conduct a systematic review to analyze existing work and characterize the current design of data exploration tasks, interactions, and tangible visualization widgets. We next reflect based on our findings and propose a research agenda to inform the development of a future widget design toolkit for tangible data exploration. Our systematic review and supplemental materials are available at physicalviswidget.github.io and osf.io/vjw5e.",
      "authors": [
        "Haonan Yao",
        "Lingyun Yu",
        "Lijie Yao"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T14:17:58+00:00",
          "link": "https://arxiv.org/abs/2507.00775v1",
          "size": "2943kb",
          "version": "v1"
        }
      ],
      "title": "Designing Visualization Widgets for Tangible Data Exploration: A Systematic Review",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00775",
        "HTML": "https://arxiv.org/html/2507.00775v1",
        "PDF": "https://arxiv.org/pdf/2507.00775"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses designing visualization widgets for data exploration, which can be related to creativity in terms of design innovation and user interaction but does not focus directly on creativity itself."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00821",
      "abstract": "Designers have ample opportunities to impact the healthcare domain. However, hospitals are often closed ecosystems that pose challenges in engaging clinical stakeholders, developing domain knowledge, and accessing relevant systems and data. In this paper, we introduce a making-oriented approach to help designers understand the intricacies of their target healthcare context. Using Remote Patient Monitoring (RPM) as a case study, we explore how manually crafting synthetic datasets based on real-world observations enables designers to learn about complex data-driven healthcare systems. Our process involves observing and modeling the real-world RPM context, crafting synthetic datasets, and iteratively prototyping a simplified RPM system that balances contextual richness and intentional abstraction. Through this iterative process of sensemaking through making, designers can still develop context familiarity when direct access to the actual healthcare system is limited. Our approach emphasizes the value of hands-on interaction with data structures to support designers in understanding opaque healthcare systems.",
      "authors": [
        "Mihnea Stefan Calota",
        "Wessel Nieuwenhuys",
        "Janet Yi-Ching Huang",
        "Lin-Lin Chen",
        "Mathias Funk"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T14:52:07+00:00",
          "link": "https://arxiv.org/abs/2507.00821v1",
          "size": "478kb",
          "version": "v1"
        }
      ],
      "title": "Sensemaking Through Making: Developing Clinical Domain Knowledge by Crafting Synthetic Datasets and Prototyping System Architectures",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00821",
        "PDF": "https://arxiv.org/pdf/2507.00821"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "Exploring sensemaking through making involves some aspects of creativity in understanding and prototyping complex systems, but creativity is not the main focus of the study."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00881",
      "abstract": "Traditional instance-based model analysis focuses mainly on misclassified instances. However, this approach overlooks the varying difficulty associated with different instances. Ideally, a robust model should recognize and reflect the challenges presented by intrinsically difficult instances. It is also valuable to investigate whether the difficulty perceived by the model aligns with that perceived by humans. To address this, we propose incorporating instance difficulty into the deep neural network evaluation process, specifically for supervised classification tasks on image data. Specifically, we consider difficulty measures from three perspectives -- data, model, and human -- to facilitate comprehensive evaluation and comparison. Additionally, we develop an interactive visual tool, DifficultyEyes, to support the identification of instances of interest based on various difficulty patterns and to aid in analyzing potential data or model issues. Case studies demonstrate the effectiveness of our approach.",
      "authors": [
        "Linhao Meng",
        "Stef van den Elzen",
        "Anna Vilanova"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T15:50:03+00:00",
          "link": "https://arxiv.org/abs/2507.00881v1",
          "size": "2087kb",
          "version": "v1"
        }
      ],
      "title": "Towards Difficulty-Aware Analysis of Deep Neural Networks",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00881",
        "HTML": "https://arxiv.org/html/2507.00881v1",
        "PDF": "https://arxiv.org/pdf/2507.00881"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper introduces difficulty-aware analysis for deep neural networks and mentions a visual tool, DifficultyEyes, which could support user creativity in diagnosing model and data issues. However, creativity is not the main focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00963",
      "abstract": "As social robots increasingly enter dementia care, concerns about deception, intentional or not, are gaining attention. Yet, how robotic design cues might elicit misleading perceptions in people with dementia, and how these perceptions arise, remains insufficiently understood. In this scoping review, we examined 26 empirical studies on interactions between people with dementia and physical social robots. We identify four key design cue categories that may influence deceptive impressions: cues resembling physiological signs (e.g., simulated breathing), social intentions (e.g., playful movement), familiar beings (e.g., animal-like form and sound), and, to a lesser extent, cues that reveal artificiality. Thematic analysis of user responses reveals that people with dementia often attribute biological, social, and mental capacities to robots, dynamically shifting between awareness and illusion. These findings underscore the fluctuating nature of ontological perception in dementia contexts. Existing definitions of robotic deception often rest on philosophical or behaviorist premises, but rarely engage with the cognitive mechanisms involved. We propose an empirically grounded definition: robotic deception occurs when Type 1 (automatic, heuristic) processing dominates over Type 2 (deliberative, analytic) reasoning, leading to misinterpretation of a robot's artificial nature. This dual-process perspective highlights the ethical complexity of social robots in dementia care and calls for design approaches that are not only engaging, but also epistemically respectful.",
      "authors": [
        "Fan Wang",
        "Giulia Perugia",
        "Yuan Feng and Wijnand IJsselsteijn"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T17:12:40+00:00",
          "link": "https://arxiv.org/abs/2507.00963v1",
          "size": "1026kb",
          "version": "v1"
        }
      ],
      "title": "Social Robots for People with Dementia: A Literature Review on Deception from Design to Perception",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00963",
        "HTML": "https://arxiv.org/html/2507.00963v1",
        "PDF": "https://arxiv.org/pdf/2507.00963"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper reviews the ethical and perceptual aspects of social robots in dementia care, focusing on deception and cognitive perceptions. There is no mention of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01017",
      "abstract": "Human error remains a dominant risk driver in safety-critical sectors such as nuclear power, aviation, and healthcare, where seemingly minor mistakes can cascade into catastrophic outcomes. Although decades of research have produced a rich repertoire of mitigation techniques, persistent limitations: scarce high-quality data, algorithmic opacity, and residual reliance on expert judgment, continue to constrain progress. This review synthesizes recent advances at the intersection of risk-informed decision making, human reliability assessment (HRA), artificial intelligence (AI), and cognitive science to clarify how their convergence can curb human-error risk. We first categorize the principal forms of human error observed in complex sociotechnical environments and outline their quantitative impact on system reliability. Next, we examine risk-informed frameworks that embed HRA within probabilistic and data-driven methodologies, highlighting successes and gaps. We then survey cognitive and human-performance models, detailing how mechanistic accounts of perception, memory, and decision-making enrich error prediction and complement HRA metrics. Building on these foundations, we critically assess AI-enabled techniques for real-time error detection, operator-state estimation, and AI-augmented HRA workflows. Across these strands, a recurring insight emerges: integrating cognitive models with AI-based analytics inside risk-informed HRA pipelines markedly enhances predictive fidelity, yet doing so demands richer datasets, transparent algorithms, and rigorous validation. Finally, we identify promising research directions, coupling resilience engineering concepts with grounded theory, operationalizing the iceberg model of incident causation, and establishing cross-domain data consortia, to foster a multidisciplinary paradigm that elevates human reliability in high-stakes systems.",
      "authors": [
        "Xingyu Xiao",
        "Hongxu Zhu",
        "Jingang Liang",
        "Jiejuan Tong",
        "Haitao Wang"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-10T11:37:06+00:00",
          "link": "https://arxiv.org/abs/2507.01017v1",
          "size": "5350kb",
          "version": "v1"
        }
      ],
      "title": "A Comprehensive Review of Human Error in Risk-Informed Decision Making: Integrating Human Reliability Assessment, Artificial Intelligence, and Human Performance Models",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01017",
        "HTML": "https://arxiv.org/html/2507.01017v1",
        "PDF": "https://arxiv.org/pdf/2507.01017"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper reviews human error in risk-informed decision making and integration with AI. It does not relate to creativity, focusing instead on error risk and reliability."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00008",
      "abstract": "Grounding natural language queries in graphical user interfaces (GUIs) poses unique challenges due to the diversity of visual elements, spatial clutter, and the ambiguity of language. In this paper, we introduce DiMo-GUI, a training-free framework for GUI grounding that leverages two core strategies: dynamic visual grounding and modality-aware optimization. Instead of treating the GUI as a monolithic image, our method splits the input into textual elements and iconic elements, allowing the model to reason over each modality independently using general-purpose vision-language models. When predictions are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by generating candidate focal regions centered on the model's initial predictions and incrementally zooms into subregions to refine the grounding result. This hierarchical refinement process helps disambiguate visually crowded layouts without the need for additional training or annotations. We evaluate our approach on standard GUI grounding benchmarks and demonstrate consistent improvements over baseline inference pipelines, highlighting the effectiveness of combining modality separation with region-focused reasoning.",
      "authors": [
        "Hang Wu",
        "Hongkai Chen",
        "Yujun Cai",
        "Chang Liu",
        "Qingwen Ye",
        "Ming-Hsuan Yang",
        "Yiwei Wang"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-12T03:13:21+00:00",
          "link": "https://arxiv.org/abs/2507.00008v1",
          "size": "1588kb",
          "version": "v1"
        }
      ],
      "title": "DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00008",
        "HTML": "https://arxiv.org/html/2507.00008v1",
        "PDF": "https://arxiv.org/pdf/2507.00008"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper centers around GUI grounding and modality-aware visual reasoning, focusing on improving model predictions in graphical user interfaces. Creativity is not mentioned or implied as part of the study."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00050",
      "abstract": "Human Activity Recognition (HAR), which uses data from Inertial Measurement Unit (IMU) sensors, has many practical applications in healthcare and assisted living environments. However, its use in real-world scenarios has been limited by the lack of comprehensive IMU-based HAR datasets that cover a wide range of activities and the lack of transparency in existing HAR models. Zero-shot HAR (ZS-HAR) overcomes the data limitations, but current models struggle to explain their decisions, making them less transparent. This paper introduces a novel IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity Recognition Network (SEZ-HARN). It can recognize activities not encountered during training and provide skeleton videos to explain its decision-making process. We evaluate the effectiveness of the proposed SEZ-HARN on four benchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its performance against three state-of-the-art black-box ZS-HAR models. The experiment results demonstrate that SEZ-HARN produces realistic and understandable explanations while achieving competitive Zero-shot recognition accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\\% of the best-performing black-box model on PAMAP2 while maintaining comparable performance on the other three datasets.",
      "authors": [
        "Devin Y. De Silva",
        "Sandareka Wickramanayake",
        "Dulani Meedeniya",
        "Sanka Rasnayaka"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-25T02:10:34+00:00",
          "link": "https://arxiv.org/abs/2507.00050v1",
          "size": "4008kb",
          "version": "v1"
        }
      ],
      "title": "SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00050",
        "HTML": "https://arxiv.org/html/2507.00050v1",
        "PDF": "https://arxiv.org/pdf/2507.00050"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper introduces a self-explainable zero-shot recognition network for Human Activity Recognition. The focus is on transparency and accuracy in activity recognition rather than creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00055",
      "abstract": "Voice interfaces integral to the human-computer interaction systems can benefit from speech emotion recognition (SER) to customize responses based on user emotions. Since humans convey emotions through multi-modal audio-visual cues, developing SER systems using both the modalities is beneficial. However, collecting a vast amount of labeled data for their development is expensive. This paper proposes a knowledge distillation framework called LightweightSER (LiSER) that leverages unlabeled audio-visual data for SER, using large teacher models built on advanced speech and face representation models. LiSER transfers knowledge regarding speech emotions and facial expressions from the teacher models to lightweight student models. Experiments conducted on two benchmark datasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence on extensive labeled datasets for SER tasks.",
      "authors": [
        "Varsha Pendyala",
        "Pedro Morgado and William Sethares"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Human-Computer Interaction (cs.HC)",
        "Multimedia (cs.MM)",
        "Audio and Speech Processing (eess.AS)",
        "Image and Video Processing (eess.IV)",
        "Signal Processing (eess.SP)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-26T04:13:47+00:00",
          "link": "https://arxiv.org/abs/2507.00055v1",
          "size": "728kb",
          "version": "v1"
        }
      ],
      "title": "Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00055",
        "HTML": "https://arxiv.org/html/2507.00055v1",
        "PDF": "https://arxiv.org/pdf/2507.00055"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The research deals with speech emotion recognition using knowledge distillation techniques and does not discuss or relate to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00224",
      "abstract": "Interactive and spatially aware technologies are transforming educational frameworks, particularly in K-12 settings where hands-on exploration fosters deeper conceptual understanding. However, during collaborative tasks, existing systems often lack the ability to accurately capture real-world interactions between students and physical objects. This issue could be addressed with automatic 6D pose estimation, i.e., estimation of an object's position and orientation in 3D space from RGB images or videos. For collaborative groups that interact with physical objects, 6D pose estimates allow AI systems to relate objects and entities. As part of this work, we introduce FiboSB, a novel and challenging 6D pose video dataset featuring groups of three participants solving an interactive task featuring small hand-held cubes and a weight scale. This setup poses unique challenges for 6D pose because groups are holistically recorded from a distance in order to capture all participants -- this, coupled with the small size of the cubes, makes 6D pose estimation inherently non-trivial. We evaluated four state-of-the-art 6D pose estimation methods on FiboSB, exposing the limitations of current algorithms on collaborative group work. An error analysis of these methods reveals that the 6D pose methods' object detection modules fail. We address this by fine-tuning YOLO11-x for FiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results, and analysis of YOLO11-x errors presented here lay the groundwork for leveraging the estimation of 6D poses in difficult collaborative contexts.",
      "authors": [
        "Changsoo Jung",
        "Sheikh Mannan",
        "Jack Fitzgerald",
        "Nathaniel Blanchard"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T19:51:28+00:00",
          "link": "https://arxiv.org/abs/2507.00224v1",
          "size": "563kb",
          "version": "v1"
        }
      ],
      "title": "Computer Vision for Objects used in Group Work: Challenges and Opportunities",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00224",
        "HTML": "https://arxiv.org/html/2507.00224v1",
        "PDF": "https://arxiv.org/pdf/2507.00224"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on 6D pose estimation for educational frameworks and group work environments, concerning technical challenges rather than creativity, hence not relating clearly to creative processes or evaluation."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00253",
      "abstract": "Enabling robots to understand human gaze target is a crucial step to allow capabilities in downstream tasks, for example, attention estimation and movement anticipation in real-world human-robot interactions. Prior works have addressed the in-frame target localization problem with data-driven approaches by carefully removing out-of-frame samples. Vision-based gaze estimation methods, such as OpenFace, do not effectively absorb background information in images and cannot predict gaze target in situations where subjects look away from the camera. In this work, we propose a system to address the problem of 360-degree gaze target estimation from an image in generalized visual scenes. The system, named GazeTarget360, integrates conditional inference engines of an eye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion decoder. Cross validation results show that GazeTarget360 can produce accurate and reliable gaze target predictions in unseen scenarios. This makes a first-of-its-kind system to predict gaze targets from realistic camera footage which is highly efficient and deployable. Our source code is made publicly available at: https://github.com/zdai257/DisengageNet.",
      "authors": [
        "Zhuangzhuang Dai",
        "Vincent Gbouna Zakka",
        "Luis J. Manso",
        "and Chen Li"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T20:44:40+00:00",
          "link": "https://arxiv.org/abs/2507.00253v1",
          "size": "5133kb",
          "version": "v1"
        }
      ],
      "title": "GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00253",
        "HTML": "https://arxiv.org/html/2507.00253v1",
        "PDF": "https://arxiv.org/pdf/2507.00253"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper deals with gaze target estimation for robot perception, focusing on technical aspects of gaze understanding and prediction, without addressing creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00456",
      "abstract": "This study investigates Shiksha copilot, an AI-assisted lesson planning tool deployed in government schools across Karnataka, India. The system combined LLMs and human expertise through a structured process in which English and Kannada lesson plans were co-created by curators and AI; teachers then further customized these curated plans for their classrooms using their own expertise alongside AI support. Drawing on a large-scale mixed-methods study involving 1,043 teachers and 23 curators, we examine how educators collaborate with AI to generate context-sensitive lesson plans, assess the quality of AI-generated content, and analyze shifts in teaching practices within multilingual, low-resource environments. Our findings show that teachers used Shiksha copilot both to meet administrative documentation needs and to support their teaching. The tool eased bureaucratic workload, reduced lesson planning time, and lowered teaching-related stress, while promoting a shift toward activity-based pedagogy. However, systemic challenges such as staffing shortages and administrative demands constrained broader pedagogical change. We frame these findings through the lenses of teacher-AI collaboration and communities of practice to examine the effective integration of AI tools in teaching. Finally, we propose design directions for future teacher-centered EdTech, particularly in multilingual and Global South contexts.",
      "authors": [
        "Deepak Varuvel Dennison",
        "Bakhtawar Ahtisham",
        "Kavyansh Chourasia",
        "Nirmit Arora",
        "Rahul Singh",
        "Rene F. Kizilcec",
        "Akshay Nambi",
        "Tanuja Ganu",
        "Aditya Vashistha"
      ],
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T06:14:25+00:00",
          "link": "https://arxiv.org/abs/2507.00456v1",
          "size": "925kb",
          "version": "v1"
        }
      ],
      "title": "Teacher-AI Collaboration for Curating and Customizing Lesson Plans in Low-Resource Schools",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00456",
        "HTML": "https://arxiv.org/html/2507.00456v1",
        "PDF": "https://arxiv.org/pdf/2507.00456"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses AI-assisted lesson planning and highlights the use of AI in co-creating lesson plans, which involves some aspects of creativity through activity-based pedagogy. However, creativity is not the primary focus, but rather a secondary theme related to teaching practices."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00481",
      "abstract": "Although software engineering research has focused on optimizing processes and technology, there is a growing recognition that human factors, particularly teamwork, also significantly impact optimization. Recent research suggests that developer personality has a strong influence on teamwork. In fact, personality considerations may have a greater impact on software development than processes and tools. This paper aims to design a study that measures the impact of HEXACO personality traits on the Teamwork Quality (TWQ) of software teams. A preliminary data collection (n=54) was conducted for this purpose. The analysis showed that several personality traits, as well as their composition, had a significant impact on TWQ. Additionally, other variables, such as the proportion of women and age distribution, also affected TWQ. The study's initial results demonstrate the usefulness and validity of the study design. The results also suggest several opportunities to improve teamwork in IT organizations and avenues for further research.",
      "authors": [
        "Philipp M. Z\\\"ahl",
        "Sabine Theis",
        "Martin R. Wolf"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Software Engineering (cs.SE)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T06:56:48+00:00",
          "link": "https://arxiv.org/abs/2507.00481v1",
          "size": "155kb",
          "version": "v1"
        }
      ],
      "title": "The Influence of HEXACO Personality Traits on the Teamwork Quality in Software Teams -- A Preliminary Research Approach",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00481",
        "HTML": "https://arxiv.org/html/2507.00481v1",
        "PDF": "https://arxiv.org/pdf/2507.00481"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The study focuses on the influence of personality traits on teamwork quality in software teams, with no direct connection to creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00543",
      "abstract": "Despite growing interest in using large language models (LLMs) to automate annotation, their effectiveness in complex, nuanced, and multi-dimensional labelling tasks remains relatively underexplored. This study focuses on annotation for the search clarification task, leveraging a high-quality, multi-dimensional dataset that includes five distinct fine-grained annotation subtasks. Although LLMs have shown impressive capabilities in general settings, our study reveals that even state-of-the-art models struggle to replicate human-level performance in subjective or fine-grained evaluation tasks. Through a systematic assessment, we demonstrate that LLM predictions are often inconsistent, poorly calibrated, and highly sensitive to prompt variations. To address these limitations, we propose a simple yet effective human-in-the-loop (HITL) workflow that uses confidence thresholds and inter-model disagreement to selectively involve human review. Our findings show that this lightweight intervention significantly improves annotation reliability while reducing human effort by up to 45%, offering a relatively scalable and cost-effective yet accurate path forward for deploying LLMs in real-world evaluation settings.",
      "authors": [
        "Leila Tavakoli",
        "Hamed Zamani"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Information Retrieval (cs.IR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T08:04:58+00:00",
          "link": "https://arxiv.org/abs/2507.00543v1",
          "size": "2500kb",
          "version": "v1"
        }
      ],
      "title": "Reliable Annotations with Less Effort: Evaluating LLM-Human Collaboration in Search Clarifications",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00543",
        "PDF": "https://arxiv.org/pdf/2507.00543"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper examines the use of LLMs for search clarification task annotations. It is concerned with annotation reliability and human-in-the-loop workflows, with no mention of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00635",
      "abstract": "Ophthalmic surgical robots offer superior stability and precision by reducing the natural hand tremors of human surgeons, enabling delicate operations in confined surgical spaces. Despite the advancements in developing vision- and force-based control methods for surgical robots, preoperative navigation remains heavily reliant on manual operation, limiting the consistency and increasing the uncertainty. Existing eye gaze estimation techniques in the surgery, whether traditional or deep learning-based, face challenges including dependence on additional sensors, occlusion issues in surgical environments, and the requirement for facial detection. To address these limitations, this study proposes an innovative eye localization and tracking method that combines machine learning with traditional algorithms, eliminating the requirements of landmarks and maintaining stable iris detection and gaze estimation under varying lighting and shadow conditions. Extensive real-world experiment results show that our proposed method has an average estimation error of 0.58 degrees for eye orientation estimation and 2.08-degree average control error for the robotic arm's movement based on the calculated orientation.",
      "authors": [
        "Tinghe Hong",
        "Shenlin Cai",
        "Boyang Li",
        "Kai Huang"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T10:28:40+00:00",
          "link": "https://arxiv.org/abs/2507.00635v1",
          "size": "1093kb",
          "version": "v1"
        }
      ],
      "title": "Stable Tracking of Eye Gaze Direction During Ophthalmic Surgery",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00635",
        "HTML": "https://arxiv.org/html/2507.00635v1",
        "PDF": "https://arxiv.org/pdf/2507.00635"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on eye gaze tracking in surgical robots, with no mention of creativity or related topics."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00792",
      "abstract": "Generating accurate and realistic virtual human movements in real-time is of high importance for a variety of applications in computer graphics, interactive virtual environments, robotics, and biomechanics. This paper introduces a novel real-time inverse kinematics (IK) solver specifically designed for realistic human-like movement generation. Leveraging the automatic differentiation and just-in-time compilation of TensorFlow, the proposed solver efficiently handles complex articulated human skeletons with high degrees of freedom. By treating forward and inverse kinematics as differentiable operations, our method effectively addresses common challenges such as error accumulation and complicated joint limits in multi-constrained problems, which are critical for realistic human motion modeling. We demonstrate the solver's effectiveness on the SMPLX human skeleton model, evaluating its performance against widely used iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK, and the nonlinear optimization algorithm IPOPT. Our experiments cover both simple end-effector tasks and sophisticated, multi-constrained problems with realistic joint limits. Results indicate that our IK solver achieves real-time performance, exhibiting rapid convergence, minimal computational overhead per iteration, and improved success rates compared to existing methods. The project code is available at https://github.com/hvoss-techfak/TF-JAX-IK",
      "authors": [
        "Hendric Voss",
        "Stefan Kopp"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T14:26:30+00:00",
          "link": "https://arxiv.org/abs/2507.00792v1",
          "size": "1055kb",
          "version": "v1"
        }
      ],
      "title": "Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00792",
        "HTML": "https://arxiv.org/html/2507.00792v1",
        "PDF": "https://arxiv.org/pdf/2507.00792"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper is concerned with real-time inverse kinematics for virtual human characters, focusing on technical and computational aspects without addressing creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00875",
      "abstract": "Multi-agent systems empowered by large language models (LLMs) have demonstrated remarkable capabilities in a wide range of downstream applications, including machine translation. However, the potential of LLMs in translating Hong Kong legal judgments remains uncertain due to challenges such as intricate legal terminology, culturally embedded nuances, and strict linguistic structures. In this work, we introduce TransLaw, a novel multi-agent framework implemented for real-world Hong Kong case law translation. It employs three specialized agents, namely, Translator, Annotator, and Proofreader, to collaboratively produce translations for high accuracy in legal meaning, appropriateness in style, and adequate coherence and cohesion in structure. This framework supports customizable LLM configurations and achieves tremendous cost reduction compared to professional human translation services. We evaluated its performance using 13 open-source and commercial LLMs as agents and obtained interesting findings, including that it surpasses GPT-4o in legal semantic accuracy, structural coherence, and stylistic fidelity, yet trails human experts in contextualizing complex terminology and stylistic naturalness. Our platform website is available at CityUHK, and our bilingual judgment corpus used for the evaluation is available at Hugging Face.",
      "authors": [
        "Xi Xuan",
        "King-kui Sin",
        "Yufei Zhou",
        "Chunyu Kit"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)",
        "Multiagent Systems (cs.MA)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T15:39:26+00:00",
          "link": "https://arxiv.org/abs/2507.00875v1",
          "size": "2894kb",
          "version": "v1"
        }
      ],
      "title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00875",
        "HTML": "https://arxiv.org/html/2507.00875v1",
        "PDF": "https://arxiv.org/pdf/2507.00875"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on translation of legal judgments using multi-agent systems and large language models. It does not address creativity as a topic, with no mention of creative processes or systems."
      },
      "source": "arXiv"
    },
    {
      "id": "2504.09296",
      "abstract": "Engaging with AI assistants to gather essential information in a timely manner is becoming increasingly common. Traditional activation methods, like wake words such as Hey Siri, Ok Google, and Hey Alexa, are constrained by technical challenges such as false activations, recognition errors, and discomfort in public settings. Similarly, activating AI systems via physical buttons imposes strict interactive limitations as it demands particular physical actions, which hinders fluid and spontaneous communication with AI. Our approach employs eye-tracking technology within AR glasses to discern a user's intention to engage with the AI assistant. By sustaining eye contact on a virtual AI avatar for a specific time, users can initiate an interaction silently and without using their hands. Preliminary user feedback suggests that this technique is relatively intuitive, natural, and less obtrusive, highlighting its potential for integrating AI assistants fluidly into everyday interactions.",
      "authors": [
        "Zhang Qing",
        "Rekimoto Jun"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-04-12T17:39:19+00:00",
          "link": "https://arxiv.org/abs/2504.09296v1",
          "size": "8034kb",
          "version": "v1"
        },
        {
          "date": "2025-07-01T08:30:17+00:00",
          "link": "https://arxiv.org/abs/2504.09296v2",
          "size": "2487kb",
          "version": "v2"
        }
      ],
      "title": "Look and Talk: Seamless AI Assistant Interaction with Gaze-Triggered Activation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2504.09296",
        "HTML": "https://arxiv.org/html/2504.09296v2",
        "PDF": "https://arxiv.org/pdf/2504.09296"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper explores AI assistant interaction through gaze-triggered activation, not addressing creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.16571",
      "abstract": "Prior natural language datasets for data visualization have focused on tasks such as visualization literacy assessment, insight generation, and visualization generation from natural language instructions. These studies often rely on controlled setups with purpose-built visualizations and artificially constructed questions. As a result, they tend to prioritize the interpretation of visualizations, focusing on decoding visualizations rather than understanding their encoding. In this paper, we present a new dataset and methodology for probing visualization design rationale through natural language. We leverage a unique source of real-world visualizations and natural language narratives: literate visualization notebooks created by students as part of a data visualization course. These notebooks combine visual artifacts with design exposition, in which students make explicit the rationale behind their design decisions. We also use large language models (LLMs) to generate and categorize question-answer-rationale triples from the narratives and articulations in the notebooks. We then carefully validate the triples and curate a dataset that captures and distills the visualization design choices and corresponding rationales of the students.",
      "authors": [
        "Maeve Hutchinson",
        "Radu Jianu",
        "Aidan Slingsby",
        "Jo Wood and Pranava Madhyastha"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-19T19:52:53+00:00",
          "link": "https://arxiv.org/abs/2506.16571v1",
          "size": "22329kb",
          "version": "v1"
        },
        {
          "date": "2025-07-01T17:51:47+00:00",
          "link": "https://arxiv.org/abs/2506.16571v2",
          "size": "5898kb",
          "version": "v2"
        }
      ],
      "title": "Capturing Visualization Design Rationale",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.16571",
        "HTML": "https://arxiv.org/html/2506.16571v2",
        "PDF": "https://arxiv.org/pdf/2506.16571"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The work deals with visualization design rationale, without any direct relevance to creativity as a theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.21319",
      "abstract": "Current multimodal large language models (MLLMs), while effective in natural image understanding, struggle with visualization understanding due to their inability to decode the data-to-visual mapping and extract structured information. To address these challenges, we propose SimVec, a compact and structured vector format that encodes chart elements, including mark types, positions, and sizes. Then, we present a new visualization dataset, which consists of bitmap images of charts, their corresponding SimVec representations, and data-centric question-answering pairs, each accompanied by explanatory chain-of-thought sentences. We fine-tune state-of-the-art MLLMs using our dataset. The experimental results show that fine-tuning leads to substantial improvements in data-centric reasoning tasks compared to their zero-shot versions. SimVec also enables MLLMs to accurately and compactly reconstruct chart structures from images. Our dataset and code are available at: https://github.com/VIDA-Lab/MLLM4VIS.",
      "authors": [
        "Can Liu and Chunlin Da and Xiaoxiao Long and Yuxiao Yang and Yu Zhang and Yong Wang"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-26T14:35:59+00:00",
          "link": "https://arxiv.org/abs/2506.21319v1",
          "size": "5645kb",
          "version": "v1"
        },
        {
          "date": "2025-07-01T10:11:25+00:00",
          "link": "https://arxiv.org/abs/2506.21319v2",
          "size": "5583kb",
          "version": "v2"
        }
      ],
      "title": "A Dataset for Enhancing MLLMs in Visualization Understanding and Reconstruction",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.21319",
        "HTML": "https://arxiv.org/html/2506.21319v2",
        "PDF": "https://arxiv.org/pdf/2506.21319"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on visualization understanding and reconstruction using MLLMs, with no mention or exploration of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22968",
      "abstract": "AI is flattening culture. Evaluations of \"culture\" are showing the myriad ways in which large AI models are homogenizing language and culture, averaging out rich linguistic differences into generic expressions. I call this phenomenon \"softmaxing culture,'' and it is one of the fundamental challenges facing AI evaluations today. Efforts to improve and strengthen evaluations of culture are central to the project of cultural alignment in large AI systems. This position paper argues that machine learning (ML) and human-computer interaction (HCI) approaches to evaluation are limited. I propose two key conceptual shifts. First, instead of asking \"what is culture?\" at the start of system evaluations, I propose beginning with the question: \"when is culture?\" Second, while I acknowledge the philosophical claim that cultural universals exist, the challenge is not simply to describe them, but to situate them in relation to their particulars. Taken together, these conceptual shifts invite evaluation approaches that move beyond technical requirements toward perspectives that are more responsive to the complexities of culture.",
      "authors": [
        "Daniel Mwesigwa"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-28T17:59:17+00:00",
          "link": "https://arxiv.org/abs/2506.22968v1",
          "size": "13kb",
          "version": "v1"
        },
        {
          "date": "2025-07-01T10:45:21+00:00",
          "link": "https://arxiv.org/abs/2506.22968v2",
          "size": "13kb",
          "version": "v2"
        }
      ],
      "title": "Against 'softmaxing' culture",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22968",
        "HTML": "https://arxiv.org/html/2506.22968v2",
        "PDF": "https://arxiv.org/pdf/2506.22968"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "Although the paper's main focus is on cultural evaluation and critique, it tangentially touches on creativity by addressing AI's impact on linguistic and cultural diversity, which can influence creative expression."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23116",
      "abstract": "User experience (UX) practices have evolved in stages and are entering a transformative phase (UX 3.0), driven by AI technologies and shifting user needs. Human-centered AI (HCAI) experiences are emerging, necessitating new UX approaches to support UX practices in the AI era. We propose a UX 3.0 paradigm framework to respond and guide UX practices in developing HCAI systems.",
      "authors": [
        "Wei Xu"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-29T06:45:14+00:00",
          "link": "https://arxiv.org/abs/2506.23116v1",
          "size": "406kb",
          "version": "v1"
        },
        {
          "date": "2025-07-01T04:32:53+00:00",
          "link": "https://arxiv.org/abs/2506.23116v2",
          "size": "407kb",
          "version": "v2"
        }
      ],
      "title": "A User Experience 3.0 (UX 3.0) Paradigm Framework: Designing for Human-Centered AI Experiences",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23116",
        "PDF": "https://arxiv.org/pdf/2506.23116"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper discusses a new paradigm in UX design driven by AI, focusing on human-centered experiences which inherently involve creativity and creative design in human-computer interactions."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23458",
      "abstract": "Portable and wearable consumer-grade electroencephalography (EEG) devices, like Muse headbands, offer unprecedented mobility for daily brain-computer interface (BCI) applications, including cognitive load detection. However, the exacerbated non-stationarity in portable EEG signals constrains data fidelity and decoding accuracy, creating a fundamental trade-off between portability and performance. To mitigate such limitation, we propose MuseCogNet (Muse-based Cognitive Network), a unified joint learning framework integrating self-supervised and supervised training paradigms. In particular, we introduce an EEG-grounded self-supervised reconstruction loss based on average pooling to capture robust neurophysiological patterns, while cross-entropy loss refines task-specific cognitive discriminants. This joint learning framework resembles the bottom-up and top-down attention in humans, enabling MuseCogNet to significantly outperform state-of-the-art methods on a publicly available Muse dataset and establish an implementable pathway for neurocognitive monitoring in ecological settings.",
      "authors": [
        "Xiaoxiao Yang",
        "Chao Feng",
        "Jiancheng Chen"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T01:42:31+00:00",
          "link": "https://arxiv.org/abs/2506.23458v1",
          "size": "73kb",
          "version": "v1"
        },
        {
          "date": "2025-07-01T01:40:27+00:00",
          "link": "https://arxiv.org/abs/2506.23458v2",
          "size": "73kb",
          "version": "v2"
        }
      ],
      "title": "Neuro-Informed Joint Learning Enhances Cognitive Workload Decoding in Portable BCIs",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23458",
        "HTML": "https://arxiv.org/html/2506.23458v2",
        "PDF": "https://arxiv.org/pdf/2506.23458"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This research deals with cognitive workload decoding using portable BCIs, concentrating on technical advancements rather than creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23815",
      "abstract": "The influence of Artificial Intelligence (AI), and specifically Large Language Models (LLM), on education is continuously increasing. These models are frequently used by students, giving rise to the question whether current forms of assessment are still a valid way to evaluate student performance and comprehension. The theoretical framework developed in this paper is grounded in Constructive Alignment (CA) theory and Bloom's taxonomy for defining learning objectives. We argue that AI influences learning objectives of different Bloom levels in a different way, and assessment has to be adopted accordingly. Furthermore, in line with Bloom's vision, formative and summative assessment should be aligned on whether the use of AI is permitted or not.\n  Although lecturers tend to agree that education and assessment need to be adapted to the presence of AI, a strong bias exists on the extent to which lecturers want to allow for AI in assessment. This bias is caused by a lecturer's familiarity with AI and specifically whether they use it themselves. To avoid this bias, we propose structured guidelines on a university or faculty level, to foster alignment among the staff. Besides that, we argue that teaching staff should be trained on the capabilities and limitations of AI tools. In this way, they are better able to adapt their assessment methods.",
      "authors": [
        "Patrick Stokkink"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T13:02:01+00:00",
          "link": "https://arxiv.org/abs/2506.23815v1",
          "size": "241kb",
          "version": "v1"
        },
        {
          "date": "2025-07-01T07:51:20+00:00",
          "link": "https://arxiv.org/abs/2506.23815v2",
          "size": "241kb",
          "version": "v2"
        }
      ],
      "title": "The Impact of AI on Educational Assessment: A Framework for Constructive Alignment",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23815",
        "PDF": "https://arxiv.org/pdf/2506.23815"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses the impact of AI on educational assessment, implying a need for creative adaptations in teaching methods and assessments, but creativity is not the main focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23952",
      "abstract": "AI systems increasingly support human decision-making across domains of professional, skill-based, and personal activity. While previous work has examined how AI might affect human autonomy globally, the effects of AI on domain-specific autonomy -- the capacity for self-governed action within defined realms of skill or expertise -- remain understudied. We analyze how AI decision-support systems affect two key components of domain-specific autonomy: skilled competence (the ability to make informed judgments within one's domain) and authentic value-formation (the capacity to form genuine domain-relevant values and preferences). By engaging with prior investigations and analyzing empirical cases across medical, financial, and educational domains, we demonstrate how the absence of reliable failure indicators and the potential for unconscious value shifts can erode domain-specific autonomy both immediately and over time. We then develop a constructive framework for autonomy-preserving AI support systems. We propose specific socio-technical design patterns -- including careful role specification, implementation of defeater mechanisms, and support for reflective practice -- that can help maintain domain-specific autonomy while leveraging AI capabilities. This framework provides concrete guidance for developing AI systems that enhance rather than diminish human agency within specialized domains of action.",
      "authors": [
        "Stefan Buijsman",
        "Sarah Carter",
        "Juan Pablo Berm\\'udez"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)",
        "General Economics (econ.GN)",
        "Economics (q-fin.EC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T15:20:10+00:00",
          "link": "https://arxiv.org/abs/2506.23952v1",
          "size": "457kb",
          "version": "v1"
        },
        {
          "date": "2025-07-01T05:46:26+00:00",
          "link": "https://arxiv.org/abs/2506.23952v2",
          "size": "457kb",
          "version": "v2"
        }
      ],
      "title": "Autonomy by Design: Preserving Human Autonomy in AI Decision-Support",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23952",
        "PDF": "https://arxiv.org/pdf/2506.23952"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on preserving human autonomy within AI decision-support systems, analyzing how such systems impact decision-making and expertise. It does not specifically address creativity as a theme or design goal."
      },
      "source": "arXiv"
    },
    {
      "id": "2411.04037",
      "abstract": "In today's online environments, users encounter harm and abuse on a daily basis. Therefore, content moderation is crucial to ensure their safety and well-being. However, the effectiveness of many moderation interventions is still uncertain. Here, we apply a causal inference approach to shed light on the effectiveness of The Great Ban, a massive social media deplatforming intervention on Reddit. We analyze 53M comments shared by nearly 34K users, providing in-depth results on both the intended and unintended consequences of the ban. Our causal analyses reveal that 15.6% of the moderated users abandoned the platform while the remaining ones decreased their overall toxicity by 4.1%. Nonetheless, a small subset of users exhibited marked increases in both the intensity and volume of toxic behavior, particularly among those whose activity levels changed after the intervention. However, these reactions were not accompanied by greater activity or engagement, suggesting that even the most toxic users maintained a limited overall impact. Our findings bring to light new insights on the effectiveness of deplatforming moderation interventions. Furthermore, they also contribute to informing future content moderation strategies and regulations.",
      "authors": [
        "Lorenzo Cima",
        "Benedetta Tessa",
        "Stefano Cresci",
        "Amaury Trujillo",
        "Marco Avvenuti"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-11-06T16:34:59+00:00",
          "link": "https://arxiv.org/abs/2411.04037v1",
          "size": "28724kb",
          "version": "v1"
        },
        {
          "date": "2024-11-07T08:26:32+00:00",
          "link": "https://arxiv.org/abs/2411.04037v2",
          "size": "28724kb",
          "version": "v2"
        },
        {
          "date": "2024-12-02T12:51:48+00:00",
          "link": "https://arxiv.org/abs/2411.04037v3",
          "size": "9581kb",
          "version": "v3"
        },
        {
          "date": "2025-07-01T14:22:24+00:00",
          "link": "https://arxiv.org/abs/2411.04037v4",
          "size": "3373kb",
          "version": "v4"
        }
      ],
      "title": "Investigating the heterogenous effects of a massive content moderation intervention via Difference-in-Differences",
      "links": {
        "Abstract": "https://arxiv.org/abs/2411.04037",
        "HTML": "https://arxiv.org/html/2411.04037v4",
        "PDF": "https://arxiv.org/pdf/2411.04037"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The study investigates content moderation effects and does not relate to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2501.07713",
      "abstract": "Reliable detection and segmentation of human hands are critical for enhancing safety and facilitating advanced interactions in human-robot collaboration. Current research predominantly evaluates hand segmentation under in-distribution (ID) data, which reflects the training data of deep learning (DL) models. However, this approach fails to address out-of-distribution (OOD) scenarios that often arise in real-world human-robot interactions. In this study, we present a novel approach by evaluating the performance of pre-trained DL models under both ID data and more challenging OOD scenarios. To mimic realistic industrial scenarios, we designed a diverse dataset featuring simple and cluttered backgrounds with industrial tools, varying numbers of hands (0 to 4), and hands with and without gloves. For OOD scenarios, we incorporated unique and rare conditions such as finger-crossing gestures and motion blur from fast-moving hands, addressing both epistemic and aleatoric uncertainties. To ensure multiple point of views (PoVs), we utilized both egocentric cameras, mounted on the operator's head, and static cameras to capture RGB images of human-robot interactions. This approach allowed us to account for multiple camera perspectives while also evaluating the performance of models trained on existing egocentric datasets as well as static-camera datasets. For segmentation, we used a deep ensemble model composed of UNet and RefineNet as base learners. Performance evaluation was conducted using segmentation metrics and uncertainty quantification via predictive entropy. Results revealed that models trained on industrial datasets outperformed those trained on non-industrial datasets, highlighting the importance of context-specific training. Although all models struggled with OOD scenarios, those trained on industrial datasets demonstrated significantly better generalization.",
      "authors": [
        "Reza Jalayer",
        "Yuxin Chen",
        "Masoud Jalayer",
        "Carlotta Orsenigo",
        "Masayoshi Tomizuka"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "2025-01-13T21:52:46+00:00",
          "link": "https://arxiv.org/abs/2501.07713v1",
          "size": "7981kb",
          "version": "v1"
        }
      ],
      "title": "Testing Human-Hand Segmentation on In-Distribution and Out-of-Distribution Data in Human-Robot Interactions Using a Deep Ensemble Model",
      "links": {
        "Abstract": "https://arxiv.org/abs/2501.07713",
        "HTML": "https://arxiv.org/html/2501.07713",
        "PDF": "https://arxiv.org/pdf/2501.07713"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "Focus is on human-hand segmentation in human-robot interaction, which is not connected to creativity."
      },
      "tasks": [
        "Hand Segmentation",
        "Segmentation",
        "Uncertainty Quantification"
      ],
      "source": "arXiv"
    },
    {
      "id": "2409.20218",
      "abstract": "For humans and robots to form an effective human-robot team (HRT) there must be sufficient trust between team members throughout a mission. We analyze data from an HRT experiment focused on trust dynamics in teams of one human and two robots, where trust was manipulated by robots becoming temporarily unresponsive. Whole-body movement tracking was achieved using ultrasound beacons, alongside communications and performance logs from a human-robot interface. We find evidence that synchronization between time series of human-robot movement, within a certain spatial proximity, is correlated with changes in self-reported trust. This suggests that the interplay of proxemics and kinesics, i.e. moving together through space, where implicit communication via coordination can occur, could play a role in building and maintaining trust in human-robot teams. Thus, quantitative indicators of coordination dynamics between team members could be used to predict trust over time and also provide early warning signals of the need for timely trust repair if trust is damaged. Hence, we aim to develop the metrology of trust in mobile human-robot teams.",
      "authors": [
        "Nicola Webb",
        "Sanja Milivojevic",
        "Mehdi Sobhani",
        "Zachary R. Madin",
        "James C. Ward",
        "Sagir Yusuf",
        "Chris Baber",
        "Edmund R. Hunt"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-09-30T11:56:22+00:00",
          "link": "https://arxiv.org/abs/2409.20218v1",
          "size": "2719kb",
          "version": "v1"
        }
      ],
      "title": "Co-Movement and Trust Development in Human-Robot Teams",
      "links": {
        "Abstract": "https://arxiv.org/abs/2409.20218",
        "HTML": "https://arxiv.org/html/2409.20218",
        "PDF": "https://arxiv.org/pdf/2409.20218"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper primarily focuses on trust development in human-robot teams through movement synchrony, without discussing creativity."
      },
      "source": "arXiv"
    }
  ],
  "subjects": [
    "Computer Vision and Pattern Recognition (cs.CV)",
    "Multimedia (cs.MM)",
    "Computation and Language (cs.CL)",
    "Image and Video Processing (eess.IV)",
    "Cryptography and Security (cs.CR)",
    "Robotics (cs.RO)",
    "Information Retrieval (cs.IR)",
    "Artificial Intelligence (cs.AI)",
    "Neurons and Cognition (q-bio.NC)",
    "Emerging Technologies (cs.ET)",
    "Signal Processing (eess.SP)",
    "Audio and Speech Processing (eess.AS)",
    "Social and Information Networks (cs.SI)",
    "Computers and Society (cs.CY)",
    "Multiagent Systems (cs.MA)",
    "Software Engineering (cs.SE)",
    "Human-Computer Interaction (cs.HC)",
    "Graphics (cs.GR)",
    "Machine Learning (cs.LG)",
    "General Economics (econ.GN)",
    "Economics (q-fin.EC)"
  ],
  "prompt": {
    "creativity": "\nYou are an expert in information retrieval. I will provide you with a list of research papers from arXiv, specifically in the *cs.HC* (Human-Computer Interaction) category.\n\nYour task is to analyze each paper and determine its relevance to the topic of **Creativity**.\n\n\n### Classify each paper into one of the following relevance levels\n\n- `core` \u2014 Creativity is a **primary focus**\n  - The paper directly studies or simulates creativity, with a clear focus on creativity.\n  - Includes creative tasks, co-creative systems, or creativity evaluation metrics.\n  - The title and abstract explicitly mention creativity, and the research questions are directly related to creativity.\n- `partial` \u2014 Creativity is a **secondary theme**\n  - Part of the paper relates to creativity; it is treated as an analytical dimension or design goal but not the main objective (e.g., user creativity, design support).\n  - Creativity may appear in discussions, experiments, or auxiliary applications.\n  - Creativity is presented as a supporting topic (e.g., evaluation criteria, user feedback).\n- `irrelevant` \u2014 **No clear connection** to creativity\n  - The paper does not address creativity as a topic.\n  - Focuses on unrelated technical content (e.g., compression, security, optimization).\n  - If creativity is mentioned, it is only superficial and lacks substantive content.\n\n\n### Return your results in the following JSON format\n\n```json\n{\n  \"result\": [\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    },\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    }\n  ]\n}\n```\n\nBe concise but specific in your reasoning, referencing key terms or sections when applicable.\n"
  },
  "description": "Data source: https://arxiv.org/list/cs.HC/new",
  "level_tatistics": {
    "irrelevant": 29,
    "partial": 9,
    "core": 1
  },
  "arxiv_update_date": "2025-07-02",
  "updated_at": "2025-07-02 10:06:45"
}