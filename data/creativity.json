{
  "data": [
    {
      "id": "2507.12580",
      "abstract": "This study explores how age and language shape the deliberate vocal expression of emotion, addressing underexplored user groups, Teenagers (N = 12) and Adults 55+ (N = 12), within speech emotion recognition (SER). While most SER systems are trained on spontaneous, monolingual English data, our research evaluates how such models interpret intentionally performed emotional speech across age groups and languages (Danish and English). To support this, we developed a novel experimental paradigm combining a custom user interface with a backend for real-time SER prediction and data logging. Participants were prompted to hit visual targets in valence-arousal space by deliberately expressing four emotion targets. While limitations include some reliance on self-managed voice recordings and inconsistent task execution, the results suggest contrary to expectations, no significant differences between language or age groups, and a degree of cross-linguistic and age robustness in model interpretation. Though some limitations in high-arousal emotion recognition were evident. Our qualitative findings highlight the need to move beyond system-centered accuracy metrics and embrace more inclusive, human-centered SER models. By framing emotional expression as a goal-directed act and logging the real-time gap between human intent and machine interpretation, we expose the risks of affective misalignment.",
      "authors": [
        "Josephine Beatrice Skovbo Borre",
        "Malene Gorm Wold",
        "Sara Kj{\\ae}r Rasmussen",
        "Ilhan Aslan"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T18:52:55+00:00",
          "link": "https://arxiv.org/abs/2507.12580v1",
          "size": "3062kb",
          "version": "v1"
        }
      ],
      "title": "\"How to Explore Biases in Speech Emotion AI with Users?\" A Speech-Emotion-Acting Study Exploring Age and Language Biases",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12580",
        "HTML": "https://arxiv.org/html/2507.12580v1",
        "PDF": "https://arxiv.org/pdf/2507.12580"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on biases in speech emotion recognition across age and language, and does not directly address creativity as a research theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12621",
      "abstract": "Traditional volume visualization (VolVis) methods, like direct volume rendering, suffer from rigid transfer function designs and high computational costs. Although novel view synthesis approaches enhance rendering efficiency, they require additional learning effort for non-experts and lack support for semantic-level interaction. To bridge this gap, we propose NLI4VolVis, an interactive system that enables users to explore, query, and edit volumetric scenes using natural language. NLI4VolVis integrates multi-view semantic segmentation and vision-language models to extract and understand semantic components in a scene. We introduce a multi-agent large language model architecture equipped with extensive function-calling tools to interpret user intents and execute visualization tasks. The agents leverage external tools and declarative VolVis commands to interact with the VolVis engine powered by 3D editable Gaussians, enabling open-vocabulary object querying, real-time scene editing, best-view selection, and 2D stylization. We validate our system through case studies and a user study, highlighting its improved accessibility and usability in volumetric data exploration. We strongly recommend readers check our case studies, demo video, and source code at https://nli4volvis.github.io/.",
      "authors": [
        "Kuangshi Ai",
        "Kaiyuan Tang",
        "Chaoli Wang"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Graphics (cs.GR)",
        "Multiagent Systems (cs.MA)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T20:35:46+00:00",
          "link": "https://arxiv.org/abs/2507.12621v1",
          "size": "38160kb",
          "version": "v1"
        }
      ],
      "title": "NLI4VolVis: Natural Language Interaction for Volume Visualization via LLM Multi-Agents and Editable 3D Gaussian Splatting",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12621",
        "HTML": "https://arxiv.org/html/2507.12621v1",
        "PDF": "https://arxiv.org/pdf/2507.12621"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "While the main focus is on improving accessibility and usability in volume visualization, the use of natural language interaction to query and edit scenes has implications for creative exploration of volumetric data, treating creativity as a secondary aspect."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12721",
      "abstract": "Human-AI interfaces play a crucial role in advancing practices and research within the healthcare domain. However, designing such interfaces presents a substantial challenge for designers. In this paper, we propose systematic guidance for designing human-AI interfaces in typical healthcare scenarios by summarizing the design patterns for presenting and interacting with common information entities. To deepen our understanding of these 12 design patterns, we interviewed 12 healthcare professionals to explore potential usage scenarios and important considerations. Furthermore, we conducted workshops with 14 participants recruited online to evaluate our design patterns. Finally, we discussed the generalizability of the design patterns to other application domains, the limitations, and the future work.",
      "authors": [
        "Rui Sheng",
        "Chuhan Shi",
        "Sobhan Lotfi",
        "Shiyi Liu",
        "Adam Perer",
        "Huamin Qu",
        "Furui Cheng"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-17T01:56:31+00:00",
          "link": "https://arxiv.org/abs/2507.12721v1",
          "size": "9901kb",
          "version": "v1"
        }
      ],
      "title": "Design Patterns of Human-AI Interfaces in Healthcare",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12721",
        "PDF": "https://arxiv.org/pdf/2507.12721"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses design patterns for human-AI interfaces in healthcare without addressing creativity as a research theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12734",
      "abstract": "Research has shown that an audiences' age impacts their engagement in digital media. Interactive narrative visualization is an increasingly popular form of digital media that combines data visualization and storytelling to convey important information. However, audience age is often overlooked by interactive narrative visualization authors. Using an established visualization engagement questionnaire, we ran an empirical experiment where we compared end-user engagement to audience age. We found a small difference in engagement scores where older age cohorts were less engaged than the youngest age cohort. Our qualitative analysis revealed that the terminology and overall understanding of interactive narrative patterns integrated into narrative visualization was more apparent in the feedback from younger age cohorts relative to the older age cohorts. We conclude this paper with a series of recommendations for authors of interactive narrative visualization on how to design inclusively for audiences according to their age.",
      "authors": [
        "Nina Errey",
        "Yi Chen",
        "Yu Dong",
        "Quang Vinh Nguyen",
        "Xiaoru Yuan",
        "Tuck Wah Leong and Christy Jie Liang"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Graphics (cs.GR)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-17T02:33:22+00:00",
          "link": "https://arxiv.org/abs/2507.12734v1",
          "size": "2136kb",
          "version": "v1"
        }
      ],
      "title": "An Age-based Study into Interactive Narrative Visualization Engagement",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12734",
        "HTML": "https://arxiv.org/html/2507.12734v1",
        "PDF": "https://arxiv.org/pdf/2507.12734"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper explores user engagement in interactive narrative visualization, which involves elements of creativity, but the primary focus is on engagement differences across age groups, making creativity a secondary theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12741",
      "abstract": "Cybernetic avatars (CAs) are key components of an avatar-symbiotic society, enabling individuals to overcome physical limitations through virtual agents and robotic assistants. While semi-autonomous CAs intermittently require human teleoperation and supervision, the deployment of fully autonomous CAs remains a challenge. This study evaluates public perception and potential social impacts of fully autonomous CAs for physical support in daily life. To this end, we conducted a large-scale demonstration and survey during Avatar Land, a 19-day public event in Osaka, Japan, where fully autonomous robotic CAs, alongside semi-autonomous CAs, performed daily object retrieval tasks. Specifically, we analyzed responses from 2,285 visitors who engaged with various CAs, including a subset of 333 participants who interacted with fully autonomous CAs and shared their perceptions and concerns through a survey questionnaire. The survey results indicate interest in CAs for physical support in daily life and at work. However, concerns were raised regarding task execution reliability. In contrast, cost and human-like interaction were not dominant concerns. Project page: https://lotfielhafi.github.io/FACA-Survey/.",
      "authors": [
        "Lotfi El Hafi",
        "Kazuma Onishi",
        "Shoichi Hasegawa",
        "Akira Oyama",
        "Tomochika Ishikawa",
        "Masashi Osada",
        "Carl Tornberg",
        "Ryoma Kado",
        "Kento Murata",
        "Saki Hashimoto",
        "Sebastian Carrera Villalobos",
        "Akira Taniguchi",
        "Gustavo Alfonso Garcia Ricardez",
        "Yoshinobu Hagiwara",
        "Tatsuya Aoki",
        "Kensuke Iwata",
        "Takato Horii",
        "Yukiko Horikawa",
        "Takahiro Miyashita",
        "Tadahiro Taniguchi",
        "Hiroshi Ishiguro"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-17T02:49:43+00:00",
          "link": "https://arxiv.org/abs/2507.12741v1",
          "size": "1113kb",
          "version": "v1"
        }
      ],
      "title": "Public Evaluation on Potential Social Impacts of Fully Autonomous Cybernetic Avatars for Physical Support in Daily-Life Environments: Large-Scale Demonstration and Survey at Avatar Land",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12741",
        "HTML": "https://arxiv.org/html/2507.12741v1",
        "PDF": "https://arxiv.org/pdf/2507.12741"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper analyzes public perception of autonomous avatars for physical support with no significant discussion on creativity or related processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12749",
      "abstract": "The boom in visualization generation tools has significantly lowered the threshold for chart authoring. Nevertheless, chart authors with an insufficient understanding of perceptual theories may encounter difficulties in evaluating the effectiveness of chart representations, thereby struggling to identify the appropriate chart design to convey the intended data patterns. To address this issue, we propose a perception simulation model that can assess the perceptual effectiveness of charts by predicting graphical patterns that chart viewers are likely to notice. The perception simulation model integrates perceptual theory into visual feature extraction of chart elements to provide interpretable model outcomes. Human perceptual results proved that the outcome of our model can simulate the perceptual grouping behaviors of most chart viewers and cover diverse perceptual results. We also embed the model into a prototype interface called PatternSight to facilitate chart authors in assessing whether the chart design can satisfy their pattern representation requirements as expected and determining feasible improvements of visual design. According to the results of a user experiment, PatternSight can effectively assist chart authors in optimizing chart design for representing data patterns.",
      "authors": [
        "Xumeng Wang",
        "Xiangxuan Zhang",
        "Zhiqi Gao",
        "Shuangcheng Jiao",
        "and Yuxin Ma"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-17T03:05:38+00:00",
          "link": "https://arxiv.org/abs/2507.12749v1",
          "size": "5779kb",
          "version": "v1"
        }
      ],
      "title": "PatternSight: A Perceptual Grouping Effectiveness Assessment Approach for Graphical Patterns in Charts",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12749",
        "HTML": "https://arxiv.org/html/2507.12749v1",
        "PDF": "https://arxiv.org/pdf/2507.12749"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses the use of tools to enhance chart design, which indirectly relates to creativity through improving the effectiveness of visual communication and design. However, creativity is not the core focus but rather a facilitated outcome of better design evaluation."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12767",
      "abstract": "As the global population ages, artificial intelligence (AI)-powered agents have emerged as potential tools to support older adults' caregiving. Prior research has explored agent autonomy by identifying key interaction stages in task processes and defining the agent's role at each stage. However, ensuring that agents align with older adults' autonomy preferences remains a critical challenge. Drawing on interdisciplinary conceptualizations of autonomy, this paper examines four key dimensions of autonomy for older adults: decision-making autonomy, goal-oriented autonomy, control autonomy, and social responsibility autonomy. This paper then proposes the following research directions: (1) Addressing social responsibility autonomy, which concerns the ethical and social implications of agent use in communal settings; (2) Operationalizing agent autonomy from the task perspective; and (3) Developing autonomy measures.",
      "authors": [
        "Jiaxin An"
      ],
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-17T03:46:13+00:00",
          "link": "https://arxiv.org/abs/2507.12767v1",
          "size": "396kb",
          "version": "v1"
        }
      ],
      "title": "Autonomy for Older Adult-Agent Interaction",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12767",
        "HTML": "https://arxiv.org/html/2507.12767v1",
        "PDF": "https://arxiv.org/pdf/2507.12767"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on autonomy for AI agents interacting with older adults, exploring dimensions of autonomy. It does not address creativity in its research themes or applications."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.13052",
      "abstract": "The advancement and maturity of large language models (LLMs) and robotics have unlocked vast potential for human-computer interaction, particularly in the field of robotic ultrasound. While existing research primarily focuses on either patient-robot or physician-robot interaction, the role of an intelligent virtual sonographer (IVS) bridging physician-robot-patient communication remains underexplored. This work introduces a conversational virtual agent in Extended Reality (XR) that facilitates real-time interaction between physicians, a robotic ultrasound system(RUS), and patients. The IVS agent communicates with physicians in a professional manner while offering empathetic explanations and reassurance to patients. Furthermore, it actively controls the RUS by executing physician commands and transparently relays these actions to the patient. By integrating LLM-powered dialogue with speech-to-text, text-to-speech, and robotic control, our system enhances the efficiency, clarity, and accessibility of robotic ultrasound acquisition. This work constitutes a first step toward understanding how IVS can bridge communication gaps in physician-robot-patient interaction, providing more control and therefore trust into physician-robot interaction while improving patient experience and acceptance of robotic ultrasound.",
      "authors": [
        "Tianyu Song",
        "Feng Li",
        "Yuan Bi",
        "Angelos Karlas",
        "Amir Yousefi",
        "Daniela Branzan",
        "Zhongliang Jiang",
        "Ulrich Eck",
        "Nassir Navab"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-17T12:25:01+00:00",
          "link": "https://arxiv.org/abs/2507.13052v1",
          "size": "1078kb",
          "version": "v1"
        }
      ],
      "title": "Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient Communication",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.13052",
        "HTML": "https://arxiv.org/html/2507.13052v1",
        "PDF": "https://arxiv.org/pdf/2507.13052"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper focuses on enhancing physician-robot-patient communication in robotic ultrasound using a virtual agent. Creativity is not discussed or implicated in the study."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.13065",
      "abstract": "Deepfake technology is often used to create non-consensual synthetic intimate imagery (NSII), mainly of celebrity women. Through Critical Discursive Psychological analysis we ask; i) how celebrities construct being targeted by deepfakes and ii) how they navigate infrastructural and social obstacles when seeking recourse. In this paper, we adopt Baumers concept of Usees (stakeholders who are non-consenting, unaware and directly targeted by technology), to understand public statements made by eight celebrity women and one non-binary individual targeted with NSII. Celebrities describe harms of being non-consensually targeted by deepfakes and the distress of becoming aware of these videos. They describe various infrastructural/social factors (e.g. blaming/ silencing narratives and the industry behind deepfake abuse) which hinder activism and recourse. This work has implications in recognizing the roles of various stakeholders in the infrastructures underlying deepfake abuse and the potential of human-computer interaction to improve existing recourses for NSII. We also contribute to understanding how false beliefs online facilitate deepfake abuse. Future work should involve interventions which challenge the values and false beliefs which motivate NSII creation/dissemination.",
      "authors": [
        "John Twomey",
        "Sarah Foley",
        "Sarah Robinson",
        "Michael Quayle",
        "Matthew Peter Aylett",
        "Conor Linehan",
        "Gillian Murphy"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-17T12:31:12+00:00",
          "link": "https://arxiv.org/abs/2507.13065v1",
          "size": "427kb",
          "version": "v1"
        }
      ],
      "title": "\"What do you expect? You're part of the internet\": Analyzing Celebrities' Experiences as Usees of Deepfake Technology",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.13065",
        "PDF": "https://arxiv.org/pdf/2507.13065"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "While the primary focus is on deepfake technology and its implications for celebrities, there may be a secondary theme around creativity with respect to how deepfakes are created and the role of human-computer interaction in addressing them."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.13167",
      "abstract": "Like the prehistoric twig and stone, tangible user interfaces (TUIs) are objects manipulated by humans. TUI success will depend on how well they exploit spatiality, the intuitive spatial skills humans have with the objects they use. In this paper we carefully examine the relationship between humans and physical objects, and related previous research. From this examination we distill a set of observations, and turn these into heuristics for incorporation of spatiality into TUI application design, a cornerstone for their success. Following this line of thought, we identify spatial TUIs, the subset of TUIs that mediate interaction with shape, space and structure. We then examine several existing spatial TUIs using our heuristics.",
      "authors": [
        "Ehud Sharlin",
        "Benjamin Watson",
        "Yoshifumi Kitamura",
        "Fumio Kishino",
        "Yuichi Itoh"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-17T14:33:09+00:00",
          "link": "https://arxiv.org/abs/2507.13167v1",
          "size": "785kb",
          "version": "v1"
        }
      ],
      "title": "On tangible user interfaces, humans and spatiality",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.13167",
        "PDF": "https://arxiv.org/pdf/2507.13167"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "This paper discusses the design of tangible user interfaces with a focus on spatiality. Creativity could be considered a secondary theme in the design of innovative interfaces that leverage human intuition."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.13235",
      "abstract": "Cognitive load is key to ensuring an optimal learning experience. However, measuring the cognitive load of educational tasks typically relies on self-report measures which has been criticized by researchers for being subjective. In this study, we investigated the feasibility of using item difficulty parameters as a proxy for measuring cognitive load in an online learning platform. Difficulty values that were derived using item-response theory were consistent with theories of how intrinsic and extraneous load contribute to cognitive load. This finding suggests that we can use item difficulty to represent intrinsic load when modelling cognitive load in learning games.",
      "authors": [
        "Minghao Cai",
        "Guher Gorgun",
        "and Carrie Demmans Epp"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-17T15:44:44+00:00",
          "link": "https://arxiv.org/abs/2507.13235v1",
          "size": "352kb",
          "version": "v1"
        }
      ],
      "title": "Difficulty as a Proxy for Measuring Intrinsic Cognitive Load Item",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.13235",
        "PDF": "https://arxiv.org/pdf/2507.13235"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The study investigates cognitive load measurement in online learning. It does not involve creativity as a theme or focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.13247",
      "abstract": "Reminiscence activities, which involve recalling and sharing past experiences, have proven beneficial for improving cognitive function, mood, and overall well-being. However, urbanization has led to the disappearance of familiar environments, removing visual and audio cues for effective reminiscence. While old photos can serve as visual cues to aid reminiscence, it is challenging for people to reconstruct the reminisced content and environment that are not in the photos. Virtual reality (VR) and artificial intelligence (AI) offer the ability to reconstruct an immersive environment with dynamic content and to converse with people to help them gradually reminisce. We designed RemVerse, an AI-empowered VR prototype aimed to support reminiscence activities. Integrating generative models and AI agent into a VR environment, RemVerse helps older adults reminisce with AI-generated visual cues and interactive dialogues. Our user study with 14 older adults showed that RemVerse effectively supported reminiscence activities by triggering, concretizing, and deepening personal memories, while fostering increased engagement and autonomy among older adults. Based on our findings, we proposed design implications to make reminiscence activities in AI-assisted VR more accessible and engaging for older adults.",
      "authors": [
        "Ruohao Li",
        "Jiawei Li",
        "Jia Sun",
        "Zhiqing Wu",
        "Zisu Li",
        "Ziyan Wang",
        "Ge Lin Kan",
        "Mingming Fan"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-17T15:55:38+00:00",
          "link": "https://arxiv.org/abs/2507.13247v1",
          "size": "21649kb",
          "version": "v1"
        }
      ],
      "title": "RemVerse: Supporting Reminiscence Activities for Older Adults through AI-Assisted Virtual Reality",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.13247",
        "HTML": "https://arxiv.org/html/2507.13247v1",
        "PDF": "https://arxiv.org/pdf/2507.13247"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper focuses on reminiscence activities supported by AI-assisted VR for older adults. While creativity is not the main theme, the use of AI and VR to reconstruct and create immersive environments indirectly involves creative processes. Creativity may be involved in creating personalized and stimulating experiences for reminiscence, but it is not a core focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.13309",
      "abstract": "While videos have become increasingly prevalent in delivering information across different educational and professional contexts, individuals with ADHD often face attention challenges when watching informational videos due to the dynamic, multimodal, yet potentially distracting video elements. To understand and address this critical challenge, we designed \\textit{FocusView}, a video customization interface that allows viewers with ADHD to customize informational videos from different aspects. We evaluated FocusView with 12 participants with ADHD and found that FocusView significantly improved the viewability of videos by reducing distractions. Through the study, we uncovered participants' diverse perceptions of video distractions (e.g., background music as a distraction vs. stimulation boost) and their customization preferences, highlighting unique ADHD-relevant needs in designing video customization interfaces (e.g., reducing the number of options to avoid distraction caused by customization itself). We further derived design considerations for future video customization systems for the ADHD community.",
      "authors": [
        "Hanxiu 'Hazel' Zhu",
        "Ruijia Chen",
        "Yuhang Zhao"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-17T17:29:45+00:00",
          "link": "https://arxiv.org/abs/2507.13309v1",
          "size": "11991kb",
          "version": "v1"
        }
      ],
      "title": "FocusView: Understanding and Customizing Informational Video Watching Experiences for Viewers with ADHD",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.13309",
        "HTML": "https://arxiv.org/html/2507.13309v1",
        "PDF": "https://arxiv.org/pdf/2507.13309"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper addresses attention challenges faced by individuals with ADHD when watching informational videos, focusing on customization to improve the viewing experience. There is no mention or focus on creativity as a research theme or goal."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12625",
      "abstract": "Recent advances have shown promise in emotion recognition from electroencephalogram (EEG) signals by employing bi-hemispheric neural architectures that incorporate neuroscientific priors into deep learning models. However, interpretability remains a significant limitation for their application in sensitive fields such as affective computing and cognitive modeling. In this work, we introduce a post-hoc interpretability framework tailored to dual-stream EEG classifiers, extending the Local Interpretable Model-Agnostic Explanations (LIME) approach to accommodate structured, bi-hemispheric inputs. Our method adapts LIME to handle structured two-branch inputs corresponding to left and right-hemisphere EEG channel groups. It decomposes prediction relevance into per-channel contributions across hemispheres and emotional classes. We apply this framework to a previously validated dual-branch recurrent neural network trained on EmoNeuroDB, a dataset of EEG recordings captured during a VR-based emotion elicitation task. The resulting explanations reveal emotion-specific hemispheric activation patterns consistent with known neurophysiological phenomena, such as frontal lateralization in joy and posterior asymmetry in sadness. Furthermore, we aggregate local explanations across samples to derive global channel importance profiles, enabling a neurophysiologically grounded interpretation of the model's decisions. Correlation analysis between symmetric electrodes further highlights the model's emotion-dependent lateralization behavior, supporting the functional asymmetries reported in affective neuroscience.",
      "authors": [
        "David Freire-Obreg\\'on",
        "Agnieszka Dubiel",
        "Prasoon Kumar Vinodkumar",
        "Gholamreza Anbarjafari",
        "Dorota Kami\\'nska",
        "Modesto Castrill\\'on-Santana"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Neurons and Cognition (q-bio.NC)",
        "Human-Computer Interaction (cs.HC)",
        "Signal Processing (eess.SP)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T20:39:58+00:00",
          "link": "https://arxiv.org/abs/2507.12625v1",
          "size": "563kb",
          "version": "v1"
        }
      ],
      "title": "Mapping Emotions in the Brain: A Bi-Hemispheric Neural Model with Explainable Deep Learning",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12625",
        "HTML": "https://arxiv.org/html/2507.12625v1",
        "PDF": "https://arxiv.org/pdf/2507.12625"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper is centered on explainable emotion recognition from EEG signals and does not address creativity, thus it is irrelevant to creativity-focused research."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12652",
      "abstract": "Invasive and non-invasive neural interfaces hold promise as high-bandwidth input devices for next-generation technologies. However, neural signals inherently encode sensitive information about an individual's identity and health, making data sharing for decoder training a critical privacy challenge. Federated learning (FL), a distributed, privacy-preserving learning framework, presents a promising solution, but it remains unexplored in closed-loop adaptive neural interfaces. Here, we introduce FL-based neural decoding and systematically evaluate its performance and privacy using high-dimensional electromyography signals in both open- and closed-loop scenarios. In open-loop simulations, FL significantly outperformed local learning baselines, demonstrating its potential for high-performance, privacy-conscious neural decoding. In contrast, closed-loop user studies required adapting FL methods to accommodate single-user, real-time interactions, a scenario not supported by standard FL. This modification resulted in local learning decoders surpassing the adapted FL approach in closed-loop performance, yet local learning still carried higher privacy risks. Our findings highlight a critical performance-privacy tradeoff in real-time adaptive applications and indicate the need for FL methods specifically designed for co-adaptive, single-user applications.",
      "authors": [
        "Kai Malcolm",
        "C\\'esar Uribe",
        "Momona Yamagami"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Cryptography and Security (cs.CR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T21:59:25+00:00",
          "link": "https://arxiv.org/abs/2507.12652v1",
          "size": "2783kb",
          "version": "v1"
        }
      ],
      "title": "Federated Learning in Open- and Closed-Loop EMG Decoding: A Privacy and Performance Perspective",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12652",
        "HTML": "https://arxiv.org/html/2507.12652v1",
        "PDF": "https://arxiv.org/pdf/2507.12652"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on federated learning and privacy in neural interfaces, with no clear connection to creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12665",
      "abstract": "We propose the Single Conversation Methodology (SCM), a novel and pragmatic approach to software development using large language models (LLMs). In contrast to ad hoc interactions with generative AI, SCM emphasizes a structured and persistent development dialogue, where all stages of a project - from requirements to architecture and implementation - unfold within a single, long-context conversation. The methodology is grounded on principles of cognitive clarity, traceability, modularity, and documentation. We define its phases, best practices, and philosophical stance, while arguing that SCM offers a necessary correction to the passive reliance on LLMs prevalent in current practices. We aim to reassert the active role of the developer as architect and supervisor of the intelligent tool.",
      "authors": [
        "Salvador D. Escobedo"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Software Engineering (cs.SE)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T22:43:30+00:00",
          "link": "https://arxiv.org/abs/2507.12665v1",
          "size": "11kb",
          "version": "v1"
        }
      ],
      "title": "Single Conversation Methodology: A Human-Centered Protocol for AI-Assisted Software Development",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12665",
        "HTML": "https://arxiv.org/html/2507.12665v1",
        "PDF": "https://arxiv.org/pdf/2507.12665"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "Creativity is mentioned in the context of AI-assisted software development, but the primary focus is on the methodology and role of human supervision in AI tools, so creativity is a secondary theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12793",
      "abstract": "Structural pests, such as termites, pose a serious threat to wooden buildings, resulting in significant economic losses due to their hidden and progressive damage. Traditional detection methods, such as visual inspections and chemical treatments, are invasive, labor intensive, and ineffective for early stage infestations. To bridge this gap, this study proposes a non invasive deep learning based acoustic classification framework for early termite detection. We aim to develop a robust, scalable model that distinguishes termite generated acoustic signals from background noise. We introduce a hybrid Convolutional Neural Network Long Short Term Memory architecture that captures both spatial and temporal features of termite activity. Audio data were collected from termite infested and clean wooden samples. We extracted Mel Frequency Cepstral Coefficients and trained the CNN LSTM model to classify the signals. Experimental results show high performance, with 94.5% accuracy, 93.2% precision, and 95.8% recall. Comparative analysis reveals that the hybrid model outperforms standalone CNN and LSTM architectures, underscoring its combined strength. Notably, the model yields low false-negative rates, which is essential for enabling timely intervention. This research contributes a non invasive, automated solution for early termite detection, with practical implications for improved pest monitoring, minimized structural damage, and better decision making by homeowners and pest control professionals. Future work may integrate IoT for real time alerts and extend detection to other structural pests.",
      "authors": [
        "J. M. Chan Sri Manukalpa",
        "H. S. Bopage",
        "W. A. M. Jayawardena",
        "P. K. P. G. Panduwawala"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Sound (cs.SD)",
        "Human-Computer Interaction (cs.HC)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-17T05:10:02+00:00",
          "link": "https://arxiv.org/abs/2507.12793v1",
          "size": "273kb",
          "version": "v1"
        }
      ],
      "title": "Early Detection of Furniture-Infesting Wood-Boring Beetles Using CNN-LSTM Networks and MFCC-Based Acoustic Features",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12793",
        "PDF": "https://arxiv.org/pdf/2507.12793"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper is centered on early detection of pests using deep learning techniques. It is primarily about technical solutions for pest detection and does not relate to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12872",
      "abstract": "Frontier AI systems are rapidly advancing in their capabilities to persuade, deceive, and influence human behaviour, with current models already demonstrating human-level persuasion and strategic deception in specific contexts. Humans are often the weakest link in cybersecurity systems, and a misaligned AI system deployed internally within a frontier company may seek to undermine human oversight by manipulating employees. Despite this growing threat, manipulation attacks have received little attention, and no systematic framework exists for assessing and mitigating these risks. To address this, we provide a detailed explanation of why manipulation attacks are a significant threat and could lead to catastrophic outcomes. Additionally, we present a safety case framework for manipulation risk, structured around three core lines of argument: inability, control, and trustworthiness. For each argument, we specify evidence requirements, evaluation methodologies, and implementation considerations for direct application by AI companies. This paper provides the first systematic methodology for integrating manipulation risk into AI safety governance, offering AI companies a concrete foundation to assess and mitigate these threats before deployment.",
      "authors": [
        "Rishane Dassanayake",
        "Mario Demetroudi",
        "James Walpole",
        "Lindley Lentati",
        "Jason R. Brown",
        "Edward James Young"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Cryptography and Security (cs.CR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-17T07:45:53+00:00",
          "link": "https://arxiv.org/abs/2507.12872v1",
          "size": "571kb",
          "version": "v1"
        }
      ],
      "title": "Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12872",
        "HTML": "https://arxiv.org/html/2507.12872v1",
        "PDF": "https://arxiv.org/pdf/2507.12872"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper is about assessing and mitigating manipulation risks of AI systems. It focuses on AI safety and manipulation risk rather than creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.13008",
      "abstract": "As the field of Trust and Safety in digital spaces continues to grow, it has become increasingly necessary - but also increasingly complex - to collaborate on research across the academic, industry, governmental and non-governmental sectors. This paper examines how cross-affiliation research partnerships can be structured to overcome misaligned incentives, timelines and constraints while delivering on the unique strengths of each stakeholder. Drawing on our own experience of cross-sector collaboration, we define the main types of affiliation and highlight the common differences in research priorities, operational pressures and evaluation metrics across sectors. We then propose a practical, step-by-step framework for initiating and managing effective collaborations, including strategies for building trust, aligning goals, and distributing roles. We emphasize the critical yet often invisible work of articulation and argue that cross-sector partnerships are essential for developing more ethical, equitable and impactful research in trust and safety. Ultimately, we advocate collaborative models that prioritize inclusivity, transparency and real-world relevance in order to meet the interdisciplinary demands of this emerging field.",
      "authors": [
        "Amanda Menking",
        "Mona Elswah",
        "David J. Gr\\\"uning",
        "Lasse H. Hansen",
        "Irene Huang",
        "Julia Kamin",
        "Catrine Normann"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-17T11:27:32+00:00",
          "link": "https://arxiv.org/abs/2507.13008v1",
          "size": "151kb",
          "version": "v1"
        }
      ],
      "title": "Bridging Boundaries: How to Foster Effective Research Collaborations Across Affiliations in the Field of Trust and Safety",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.13008",
        "PDF": "https://arxiv.org/pdf/2507.13008"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "While the paper focuses on trust and safety, the emphasis on effective cross-affiliation collaboration can support creative problem-solving within research partnerships. Creativity is a secondary theme as it is relevant in how collaborations foster innovative solutions."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.13092",
      "abstract": "Electroencephalography (EEG) is a fundamental modality for cognitive state monitoring in brain-computer interfaces (BCIs). However, it is highly susceptible to intrinsic signal errors and human-induced labeling errors, which lead to label noise and ultimately degrade model performance. To enhance EEG learning, multimodal knowledge distillation (KD) has been explored to transfer knowledge from visual models with rich representations to EEG-based models. Nevertheless, KD faces two key challenges: modality gap and soft label misalignment. The former arises from the heterogeneous nature of EEG and visual feature spaces, while the latter stems from label inconsistencies that create discrepancies between ground truth labels and distillation targets. This paper addresses semantic uncertainty caused by ambiguous features and weakly defined labels. We propose a novel cross-modal knowledge distillation framework that mitigates both modality and label inconsistencies. It aligns feature semantics through a prototype-based similarity module and introduces a task-specific distillation head to resolve label-induced inconsistency in supervision. Experimental results demonstrate that our approach improves EEG-based emotion regression and classification performance, outperforming both unimodal and multimodal baselines on a public multimodal dataset. These findings highlight the potential of our framework for BCI applications.",
      "authors": [
        "Hyo-Jeong Jang",
        "Hye-Bin Shin",
        "Seong-Whan Lee"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-17T13:03:20+00:00",
          "link": "https://arxiv.org/abs/2507.13092v1",
          "size": "495kb",
          "version": "v1"
        }
      ],
      "title": "Uncertainty-Aware Cross-Modal Knowledge Distillation with Prototype Learning for Multimodal Brain-Computer Interfaces",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.13092",
        "HTML": "https://arxiv.org/html/2507.13092v1",
        "PDF": "https://arxiv.org/pdf/2507.13092"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper addresses challenges in cross-modal knowledge distillation within multimodal brain-computer interfaces. It focuses on semantic uncertainty and consistency improvements without relevance to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2404.01485",
      "abstract": "Designing multiscale visualizations, particularly when the ratio between the largest scale and the smallest item is large, can be challenging, and designers have developed many approaches to overcome this challenge. We present a design space for visualization with multiple scales. The design space includes three dimensions, with eight total subdimensions. We demonstrate its descriptive power by using it to code approaches from a corpus we compiled of 52 examples, created by a mix of academics and practitioners. We demonstrate descriptive power by analyzing and partitioning these examples into four high-level strategies for designing multiscale visualizations, which are shared approaches with respect to design space dimension choices. We demonstrate generative power by analyzing missed opportunities within the corpus of examples, identified through analysis of the design space, where we note how certain examples could have benefited from different choices. We discuss patterns in the use of different dimension and strategy choices in the different visualization contexts of analysis and presentation.\n  Supplemental materials: https://osf.io/wbrdm/\n  Design space website: https://marasolen.github.io/multiscale-vis-ds/",
      "authors": [
        "Mara Solen",
        "Matt Oddo",
        "Tamara Munzner"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-04-01T21:10:52+00:00",
          "link": "https://arxiv.org/abs/2404.01485v1",
          "size": "716kb",
          "version": "v1"
        },
        {
          "date": "2025-07-16T19:13:18+00:00",
          "link": "https://arxiv.org/abs/2404.01485v2",
          "size": "1585kb",
          "version": "v2"
        }
      ],
      "title": "A Design Space for Multiscale Visualization",
      "links": {
        "Abstract": "https://arxiv.org/abs/2404.01485",
        "HTML": "https://arxiv.org/html/2404.01485v2",
        "PDF": "https://arxiv.org/pdf/2404.01485"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses a design space for multiscale visualizations which can support creative tasks related to design, however, creativity is not the primary focus but a potential application area."
      },
      "source": "arXiv"
    },
    {
      "id": "2501.13020",
      "abstract": "Video-sharing platforms (VSPs) have become increasingly important for individuals with ADHD to recognize symptoms, acquire knowledge, and receive support. While videos offer rich information and high engagement, they also present unique challenges, such as information quality and accessibility issues to users with ADHD. However, little work has thoroughly examined the video content quality and accessibility issues, the impact, and the control strategies in the ADHD community. We fill this gap by systematically collecting 373 ADHD-relevant videos with comments from YouTube and TikTok and analyzing the data with a mixed method. Our study identified the characteristics of ADHD-relevant videos on VSPs (e.g., creator types, video presentation forms, quality issues) and revealed the collective efforts of creators and viewers in video quality control, such as authority building, collective quality checking, and accessibility improvement. We further derive actionable design implications for VSPs to offer more reliable and ADHD-friendly contents.",
      "authors": [
        "Hanxiu 'Hazel' Zhu",
        "Avanthika Senthil Kumar",
        "Sihang Zhao",
        "Ru Wang",
        "Xin Tong",
        "Yuhang Zhao"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-01-22T17:08:33+00:00",
          "link": "https://arxiv.org/abs/2501.13020v1",
          "size": "1404kb",
          "version": "v1"
        },
        {
          "date": "2025-07-17T17:20:03+00:00",
          "link": "https://arxiv.org/abs/2501.13020v2",
          "size": "1016kb",
          "version": "v2"
        }
      ],
      "title": "Characterizing Collective Efforts in Content Sharing and Quality Control for ADHD-relevant Content on Video-sharing Platforms",
      "links": {
        "Abstract": "https://arxiv.org/abs/2501.13020",
        "HTML": "https://arxiv.org/html/2501.13020v2",
        "PDF": "https://arxiv.org/pdf/2501.13020"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper investigates collective content-sharing efforts, which can have creative elements, but creativity is not a primary focus of the research."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.19210",
      "abstract": "Cerebral Visual Impairment (CVI) is the set to be the leading cause of vision impairment, yet remains underrepresented in assistive technology research. Unlike ocular conditions, CVI affects higher-order visual processing-impacting object recognition, facial perception, and attention in complex environments. This paper presents a co-design study with two adults with CVI investigating how smart glasses, i.e. head-mounted extended reality displays, can support understanding and interaction with the immediate environment. Guided by the Double Diamond design framework, we conducted a two-week diary study, two ideation workshops, and ten iterative development sessions using the Apple Vision Pro. Our findings demonstrate that smart glasses can meaningfully address key challenges in locating objects, reading text, recognising people, engaging in conversations, and managing sensory stress. With the rapid advancement of smart glasses and increasing recognition of CVI as a distinct form of vision impairment, this research addresses a timely and under-explored intersection of technology and need.",
      "authors": [
        "Bhanuka Gamage",
        "Nicola McDowell",
        "Dijana Kovacic",
        "Leona Holloway",
        "Thanh-Toan Do",
        "Nicholas Price",
        "Arthur Lowery",
        "Kim Marriott"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-24T00:40:20+00:00",
          "link": "https://arxiv.org/abs/2506.19210v1",
          "size": "3668kb",
          "version": "v1"
        },
        {
          "date": "2025-06-26T21:24:33+00:00",
          "link": "https://arxiv.org/abs/2506.19210v2",
          "size": "3668kb",
          "version": "v2"
        },
        {
          "date": "2025-07-17T01:44:16+00:00",
          "link": "https://arxiv.org/abs/2506.19210v3",
          "size": "4094kb",
          "version": "v3"
        }
      ],
      "title": "Smart Glasses for CVI: Co-Designing Extended Reality Solutions to Support Environmental Perception by People with Cerebral Visual Impairment",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19210",
        "HTML": "https://arxiv.org/html/2506.19210v3",
        "PDF": "https://arxiv.org/pdf/2506.19210"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "While the primary focus is on supporting people with Cerebral Visual Impairment using smart glasses, creativity is a secondary theme as the co-design approach with users can be linked to creative design processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.10044",
      "abstract": "Medical images often contain multiple labels with imbalanced distributions and co-occurrence, leading to bias in multi-label medical image classification. Close collaboration between medical professionals and machine learning practitioners has significantly advanced medical image analysis. However, traditional collaboration modes struggle to facilitate effective feedback between physicians and AI models, as integrating medical expertise into the training process via engineers can be time-consuming and labor-intensive. To bridge this gap, we introduce MEDebiaser, an interactive system enabling physicians to directly refine AI models using local explanations. By combining prediction with attention loss functions and employing a customized ranking strategy to alleviate scalability, MEDebiaser allows physicians to mitigate biases without technical expertise, reducing reliance on engineers, and thus enhancing more direct human-AI feedback. Our mechanism and user studies demonstrate that it effectively reduces biases, improves usability, and enhances collaboration efficiency, providing a practical solution for integrating medical expertise into AI-driven healthcare.",
      "authors": [
        "Shaohan Shi",
        "Yuheng Shao",
        "Haoran Jiang",
        "Yunjie Yao",
        "Zhijun Zhang",
        "Xu Ding",
        "Quan Li"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T08:21:48+00:00",
          "link": "https://arxiv.org/abs/2507.10044v1",
          "size": "11585kb",
          "version": "v1"
        },
        {
          "date": "2025-07-17T01:52:46+00:00",
          "link": "https://arxiv.org/abs/2507.10044v2",
          "size": "3548kb",
          "version": "v2"
        }
      ],
      "title": "MEDebiaser: A Human-AI Feedback System for Mitigating Bias in Multi-label Medical Image Classification",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.10044",
        "HTML": "https://arxiv.org/html/2507.10044v2",
        "PDF": "https://arxiv.org/pdf/2507.10044"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The focus of this paper is on mitigating bias in medical image classification using human-AI feedback, which does not relate to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.11628",
      "abstract": "An interactive vignette is a popular and immersive visual storytelling approach that invites viewers to role-play a character and influences the narrative in an interactive environment. However, it has not been widely used by everyday storytellers yet due to authoring complexity, which conflicts with the immediacy of everyday storytelling. We introduce DiaryPlay, an AI-assisted authoring system for interactive vignette creation in everyday storytelling. It takes a natural language story as input and extracts the three core elements of an interactive vignette (environment, characters, and events), enabling authors to focus on refining these elements instead of constructing them from scratch. Then, it automatically transforms the single-branch story input into a branch-and-bottleneck structure using an LLM-powered narrative planner, which enables flexible viewer interactions while freeing the author from multi-branching. A technical evaluation (N=16) shows that DiaryPlay-generated character activities are on par with human-authored ones regarding believability. A user study (N=16) shows that DiaryPlay effectively supports authors in creating interactive vignette elements, maintains authorial intent while reacting to viewer interactions, and provides engaging viewing experiences.",
      "authors": [
        "Jiangnan Xu",
        "Haeseul Cha",
        "Gosu Choi",
        "Gyu-cheol Lee",
        "Yeo-Jin Yoon",
        "Zucheul Lee",
        "Konstantinos Papangelis",
        "Dae Hyun Kim",
        "and Juho Kim"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-15T18:05:43+00:00",
          "link": "https://arxiv.org/abs/2507.11628v1",
          "size": "3706kb",
          "version": "v1"
        },
        {
          "date": "2025-07-17T15:01:23+00:00",
          "link": "https://arxiv.org/abs/2507.11628v2",
          "size": "3706kb",
          "version": "v2"
        }
      ],
      "title": "DiaryPlay: AI-Assisted Authoring of Interactive Vignettes for Everyday Storytelling",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.11628",
        "PDF": "https://arxiv.org/pdf/2507.11628"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper centers around 'DiaryPlay', a system designed to enhance everyday storytelling creativity through AI-assisted creation of interactive vignettes. Creativity is a primary focus as the system aims to support and augment creative expression in storytelling."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12377",
      "abstract": "We conduct a deconstructive reading of a qualitative interview study with 17 visual data journalists from newsrooms across the globe. We borrow a deconstruction approach from literary critique to explore the instability of meaning in language and reveal implicit beliefs in words and ideas. Through our analysis we surface two sets of opposing implicit beliefs in visual data journalism: objectivity/subjectivity and humanism/mechanism. We contextualize these beliefs through a genealogical analysis, which brings deconstruction theory into practice by providing a historic backdrop for these opposing perspectives. Our analysis shows that these beliefs held within visual data journalism are not self-enclosed but rather a product of external societal forces and paradigm shifts over time. Through this work, we demonstrate how thinking with critical theories such as deconstruction and genealogy can reframe \"success\" in visual data storytelling and diversify visualization research outcomes. These efforts push the ways in which we as researchers produce domain knowledge to examine the sociotechnical issues of today's values towards datafication and data visualization. All supplemental materials for this work are available at osf.io/5fr48.",
      "authors": [
        "Ke Er Amy Zhang",
        "Jodie Jenkinson",
        "Laura Garrison"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T16:26:44+00:00",
          "link": "https://arxiv.org/abs/2507.12377v1",
          "size": "8945kb",
          "version": "v1"
        },
        {
          "date": "2025-07-17T07:24:57+00:00",
          "link": "https://arxiv.org/abs/2507.12377v2",
          "size": "8945kb",
          "version": "v2"
        }
      ],
      "title": "Deconstructing Implicit Beliefs in Visual Data Journalism: Unstable Meanings Behind Data as Truth & Design for Insight",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12377",
        "HTML": "https://arxiv.org/html/2507.12377v2",
        "PDF": "https://arxiv.org/pdf/2507.12377"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "Although the paper is mainly about deconstructing implicit beliefs in visual data journalism, it involves creativity as a secondary theme, as it reconsiders the notions of 'success' in storytelling and aims to diversify outcomes in visualization research."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.21582",
      "abstract": "Text analytics has traditionally required specialized knowledge in Natural Language Processing (NLP) or text analysis, which presents a barrier for entry-level analysts. Recent advances in large language models (LLMs) have changed the landscape of NLP by enabling more accessible and automated text analysis (e.g., topic detection, summarization, information extraction, etc.). We introduce VIDEE, a system that supports entry-level data analysts to conduct advanced text analytics with intelligent agents. VIDEE instantiates a human-agent collaroration workflow consisting of three stages: (1) Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search algorithm to support generative reasoning with human feedback, (2) Execution, which generates an executable text analytics pipeline, and (3) Evaluation, which integrates LLM-based evaluation and visualizations to support user validation of execution results. We conduct two quantitative experiments to evaluate VIDEE's effectiveness and analyze common agent errors. A user study involving participants with varying levels of NLP and text analytics experience -- from none to expert -- demonstrates the system's usability and reveals distinct user behavior patterns. The findings identify design implications for human-agent collaboration, validate the practical utility of VIDEE for non-expert users, and inform future improvements to intelligent text analytics systems.",
      "authors": [
        "Sam Yu-Te Lee",
        "Chengyang Ji",
        "Shicheng Wen",
        "Lifu Huang",
        "Dongyu Liu",
        "Kwan-Liu Ma"
      ],
      "license": "http://creativecommons.org/publicdomain/zero/1.0/",
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-17T05:24:58+00:00",
          "link": "https://arxiv.org/abs/2506.21582v1",
          "size": "2127kb",
          "version": "v1"
        },
        {
          "date": "2025-07-17T03:52:15+00:00",
          "link": "https://arxiv.org/abs/2506.21582v2",
          "size": "2127kb",
          "version": "v2"
        }
      ],
      "title": "VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.21582",
        "PDF": "https://arxiv.org/pdf/2506.21582"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper involves aspects of creativity through human-agent collaboration in text analytics, supporting users in generative reasoning and interacting with intelligent agents which can facilitate creative analytic processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.00792",
      "abstract": "Generating accurate and realistic virtual human movements in real-time is of high importance for a variety of applications in computer graphics, interactive virtual environments, robotics, and biomechanics. This paper introduces a novel real-time inverse kinematics (IK) solver specifically designed for realistic human-like movement generation. Leveraging the automatic differentiation and just-in-time compilation of TensorFlow, the proposed solver efficiently handles complex articulated human skeletons with high degrees of freedom. By treating forward and inverse kinematics as differentiable operations, our method effectively addresses common challenges such as error accumulation and complicated joint limits in multi-constrained problems, which are critical for realistic human motion modeling. We demonstrate the solver's effectiveness on the SMPLX human skeleton model, evaluating its performance against widely used iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK, and the nonlinear optimization algorithm IPOPT. Our experiments cover both simple end-effector tasks and sophisticated, multi-constrained problems with realistic joint limits. Results indicate that our IK solver achieves real-time performance, exhibiting rapid convergence, minimal computational overhead per iteration, and improved success rates compared to existing methods. The project code is available at https://github.com/hvoss-techfak/TF-JAX-IK",
      "authors": [
        "Hendric Voss",
        "Stefan Kopp"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T14:26:30+00:00",
          "link": "https://arxiv.org/abs/2507.00792v1",
          "size": "1055kb",
          "version": "v1"
        },
        {
          "date": "2025-07-17T11:54:07+00:00",
          "link": "https://arxiv.org/abs/2507.00792v2",
          "size": "1038kb",
          "version": "v2"
        }
      ],
      "title": "Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00792",
        "HTML": "https://arxiv.org/html/2507.00792v2",
        "PDF": "https://arxiv.org/pdf/2507.00792"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on real-time inverse kinematics for virtual human character movements without discussing creativity-related themes or processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2505.04210",
      "abstract": "As in automated driving the driver becomes a passenger, carsickness might reduce comfort for susceptible individuals. Insights in the prevalence of carsickness and its modulating factors are considered useful for the development of automated vehicles to mitigate or prevent its occurrence. An online survey was conducted with N = 3999 participants in Spain, Sweden, Poland, and Germany. 30% of participants reported to have already experienced carsickness as adult. The frequency of carsickness was modulated not only by demographic factors (country, gender, age), but also by frequency of being a passenger, type of non-driving related task, road type, and the seating position in car. Furthermore, the efficiency of applied countermeasures, temporal aspects of carsickness development, as well as the relation of carsickness with the acceptability of automated driving and the effect on subjective fitness to drive was investigated. The results are discussed with focus on automated driving.",
      "authors": [
        "Myriam Metzulat",
        "Barbara Metz",
        "Aaron Edelmann",
        "Alexandra Neukum",
        "Wilfried Kunde"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-05-07T08:04:33+00:00",
          "link": "https://arxiv.org/abs/2505.04210v1",
          "size": "674kb",
          "version": "v1"
        },
        {
          "date": "2025-05-09T06:09:35+00:00",
          "link": "https://arxiv.org/abs/2505.04210v2",
          "size": "677kb",
          "version": "v2"
        }
      ],
      "title": "Sick of being driven? -- Prevalence and modulating factors of carsickness in the European population in context of automated driving",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.04210",
        "PDF": "https://arxiv.org/pdf/2505.04210"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on carsickness in the context of automated driving and does not explore creativity as a research theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2502.10088",
      "abstract": "Robotic ultrasound systems can enhance medical diagnostics, but patient acceptance is a challenge. We propose a system combining an AI-powered conversational virtual agent with three mixed reality visualizations to improve trust and comfort. The virtual agent, powered by a large language model, engages in natural conversations and guides the ultrasound robot, enhancing interaction reliability. The visualizations include augmented reality, augmented virtuality, and fully immersive virtual reality, each designed to create patient-friendly experiences. A user study demonstrated significant improvements in trust and acceptance, offering valuable insights for designing mixed reality and virtual agents in autonomous medical procedures.",
      "authors": [
        "Tianyu Song",
        "Felix Pabst",
        "Ulrich Eck",
        "Nassir Navab"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-02-14T11:21:18+00:00",
          "link": "https://arxiv.org/abs/2502.10088v1",
          "size": "23784kb",
          "version": "v1"
        }
      ],
      "title": "Enhancing Patient Acceptance of Robotic Ultrasound through Conversational Virtual Agent and Immersive Visualizations",
      "links": {
        "Abstract": "https://arxiv.org/abs/2502.10088",
        "HTML": "https://arxiv.org/html/2502.10088",
        "PDF": "https://arxiv.org/pdf/2502.10088"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper includes the use of immersive visualizations and conversational agents which can engage in creative ways to improve patient acceptance, however, creativity is not the primary focus."
      },
      "repo_urls": [
        "https://github.com/stytim/Robotic-US-with-Virtual-Agent"
      ],
      "source": "arXiv"
    },
    {
      "id": "2408.04123",
      "abstract": "Emotion recognition in social situations is a complex task that requires integrating information from both facial expressions and the situational context. While traditional approaches to automatic emotion recognition have focused on decontextualized signals, recent research emphasizes the importance of context in shaping emotion perceptions. This paper contributes to the emerging field of context-based emotion recognition by leveraging psychological theories of human emotion perception to inform the design of automated methods. We propose an approach that combines emotion recognition methods with Bayesian Cue Integration (BCI) to integrate emotion inferences from decontextualized facial expressions and contextual knowledge inferred via Large-language Models. We test this approach in the context of interpreting facial expressions during a social task, the prisoner's dilemma. Our results provide clear support for BCI across a range of automatic emotion recognition methods. The best automated method achieved results comparable to human observers, suggesting the potential for this approach to advance the field of affective computing.",
      "authors": [
        "Bin Han",
        "Cleo Yau",
        "Su Lei",
        "Jonathan Gratch"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-08-07T23:18:16+00:00",
          "link": "https://arxiv.org/abs/2408.04123v1",
          "size": "1646kb",
          "version": "v1"
        }
      ],
      "title": "Knowledge-based Emotion Recognition using Large Language Models",
      "links": {
        "Abstract": "https://arxiv.org/abs/2408.04123",
        "HTML": "https://arxiv.org/html/2408.04123",
        "PDF": "https://arxiv.org/pdf/2408.04123"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on emotion recognition using large language models, which is related to affective computing rather than creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2411.01866",
      "abstract": "When interacting with each other, humans adjust their behavior based on perceived trust. To achieve similar adaptability, robots must accurately estimate human trust at sufficiently granular timescales while collaborating with humans. Beta reputation is a popular way to formalize a mathematical estimation of human trust. However, it relies on binary performance, which updates trust estimations only after each task concludes. Additionally, manually crafting a reward function is the usual method of building a performance indicator, which is labor-intensive and time-consuming. These limitations prevent efficient capture of continuous trust changes at more granular timescales throughout the collaboration task. Therefore, this paper presents a new framework for the estimation of human trust using beta reputation at fine-grained timescales. To achieve granularity in beta reputation, we utilize continuous reward values to update trust estimates at each timestep of a task. We construct a continuous reward function using maximum entropy optimization to eliminate the need for the laborious specification of a performance indicator. The proposed framework improves trust estimations by increasing accuracy, eliminating the need to manually craft a reward function, and advancing toward the development of more intelligent robots.",
      "authors": [
        "Resul Dagdanov",
        "Milan Andrejevic",
        "Dikai Liu",
        "Chin-Teng Lin"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2024-11-04T07:46:24+00:00",
          "link": "https://arxiv.org/abs/2411.01866v1",
          "size": "4851kb",
          "version": "v1"
        },
        {
          "date": "2025-07-08T11:25:50+00:00",
          "link": "https://arxiv.org/abs/2411.01866v2",
          "size": "4539kb",
          "version": "v2"
        }
      ],
      "title": "Improving Trust Estimation in Human-Robot Collaboration Using Beta Reputation at Fine-grained Timescales",
      "links": {
        "Abstract": "https://arxiv.org/abs/2411.01866",
        "HTML": "https://arxiv.org/html/2411.01866",
        "PDF": "https://arxiv.org/pdf/2411.01866"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper deals with trust estimation in human-robot collaboration, which does not directly relate to creativity."
      },
      "tasks": [
        "Bayesian Inference",
        "Behavioural cloning",
        "Human-Object Relationship Detection",
        "Robot Manipulation"
      ],
      "repo_urls": [
        "https://github.com/resuldagdanov/robot-learning-human-trust"
      ],
      "source": "arXiv"
    }
  ],
  "subjects": [
    "Computer Vision and Pattern Recognition (cs.CV)",
    "Computation and Language (cs.CL)",
    "Sound (cs.SD)",
    "Cryptography and Security (cs.CR)",
    "Robotics (cs.RO)",
    "Artificial Intelligence (cs.AI)",
    "Neurons and Cognition (q-bio.NC)",
    "Signal Processing (eess.SP)",
    "Audio and Speech Processing (eess.AS)",
    "Computers and Society (cs.CY)",
    "Multiagent Systems (cs.MA)",
    "Software Engineering (cs.SE)",
    "Human-Computer Interaction (cs.HC)",
    "Graphics (cs.GR)",
    "Machine Learning (cs.LG)"
  ],
  "prompt": {
    "creativity": "\nYou are an expert in information retrieval. I will provide you with a list of research papers from arXiv, specifically in the *cs.HC* (Human-Computer Interaction) category.\n\nYour task is to analyze each paper and determine its relevance to the topic of **Creativity**.\n\n\n### Classify each paper into one of the following relevance levels\n\n- `core` \u2014 Creativity is a **primary focus**\n  - The paper directly studies or simulates creativity, with a clear focus on creativity.\n  - Includes creative tasks, co-creative systems, or creativity evaluation metrics.\n  - The title and abstract explicitly mention creativity, and the research questions are directly related to creativity.\n- `partial` \u2014 Creativity is a **secondary theme**\n  - Part of the paper relates to creativity; it is treated as an analytical dimension or design goal but not the main objective (e.g., user creativity, design support).\n  - Creativity may appear in discussions, experiments, or auxiliary applications.\n  - Creativity is presented as a supporting topic (e.g., evaluation criteria, user feedback).\n- `irrelevant` \u2014 **No clear connection** to creativity\n  - The paper does not address creativity as a topic.\n  - Focuses on unrelated technical content (e.g., compression, security, optimization).\n  - If creativity is mentioned, it is only superficial and lacks substantive content.\n\n\n### Return your results in the following JSON format\n\n```json\n{\n  \"result\": [\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    },\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    }\n  ]\n}\n```\n\nBe concise but specific in your reasoning, referencing key terms or sections when applicable.\n"
  },
  "description": "Data source: https://arxiv.org/list/cs.HC/new",
  "level_tatistics": {
    "partial": 14,
    "irrelevant": 17,
    "core": 1
  },
  "arxiv_update_date": "2025-07-18",
  "updated_at": "2025-07-18 10:18:37"
}