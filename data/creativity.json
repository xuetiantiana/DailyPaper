{
  "data": [
    {
      "id": "2507.08002",
      "abstract": "Thematic analysis provides valuable insights into participants' experiences through coding and theme development, but its resource-intensive nature limits its use in large healthcare studies. Large language models (LLMs) can analyze text at scale and identify key content automatically, potentially addressing these challenges. However, their application in mental health interviews needs comparison with traditional human analysis. This study evaluates out-of-the-box and knowledge-base LLM-based thematic analysis against traditional methods using transcripts from a stress-reduction trial with healthcare workers. OpenAI's GPT-4o model was used along with the Role, Instructions, Steps, End-Goal, Narrowing (RISEN) prompt engineering framework and compared to human analysis in Dedoose. Each approach developed codes, noted saturation points, applied codes to excerpts for a subset of participants (n = 20), and synthesized data into themes. Outputs and performance metrics were compared directly. LLMs using the RISEN framework developed deductive parent codes similar to human codes, but humans excelled in inductive child code development and theme synthesis. Knowledge-based LLMs reached coding saturation with fewer transcripts (10-15) than the out-of-the-box model (15-20) and humans (90-99). The out-of-the-box LLM identified a comparable number of excerpts to human researchers, showing strong inter-rater reliability (K = 0.84), though the knowledge-based LLM produced fewer excerpts. Human excerpts were longer and involved multiple codes per excerpt, while LLMs typically applied one code. Overall, LLM-based thematic analysis proved more cost-effective but lacked the depth of human analysis. LLMs can transform qualitative analysis in mental healthcare and clinical research when combined with human oversight to balance participant perspectives and research resources.",
      "authors": [
        "Karisa Parkington",
        "Bazen G. Teferra",
        "Marianne Rouleau-Tang",
        "Argyrios Perivolaris",
        "Alice Rueda",
        "Adam Dubrowski",
        "Bill Kapralos",
        "Reza Samavi",
        "Andrew Greenshaw",
        "Yanbo Zhang",
        "Bo Cao",
        "Yuqi Wu",
        "Sirisha Rambhatla",
        "Sridhar Krishnan",
        "and Venkat Bhat"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-05-02T13:31:13+00:00",
          "link": "https://arxiv.org/abs/2507.08002v1",
          "size": "1973kb",
          "version": "v1"
        }
      ],
      "title": "Human vs. LLM-Based Thematic Analysis for Digital Mental Health Research: Proof-of-Concept Comparative Study",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08002",
        "PDF": "https://arxiv.org/pdf/2507.08002"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper evaluates thematic analysis using LLMs in the context of mental health research, comparing it with human analysis. Creativity is not discussed as a focus; the main topic is the efficiency and effectiveness of thematic analysis methods."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08003",
      "abstract": "We contribute a comprehensive dataset to study user attention and purchasing behavior on Search Engine Result Pages (SERPs). Previous work has relied on mouse movements as a low-cost large-scale behavioral proxy but also has relied on self-reported ground-truth labels, collected at post-task, which can be inaccurate and prone to biases. To address this limitation, we use an eye tracker to construct an objective ground-truth of continuous visual attention. Our dataset comprises 2,776 transactional queries on Google SERPs, collected from 47 participants, and includes: (1) HTML source files, with CSS and images; (2) rendered SERP screenshots; (3) eye movement data; (4) mouse movement data; (5) bounding boxes of direct display and organic advertisements; and (6) scripts for further preprocessing the data. In this paper we provide an overview of the dataset and baseline experiments (classification tasks) that can inspire researchers about the different possibilities for future work.",
      "authors": [
        "Kayhan Latifzadeh",
        "Jacek Gwizdka",
        "Luis A. Leiva"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Information Retrieval (cs.IR)"
      ],
      "submission_historys": [
        {
          "date": "2025-05-12T08:27:47+00:00",
          "link": "https://arxiv.org/abs/2507.08003v1",
          "size": "3242kb",
          "version": "v1"
        }
      ],
      "title": "A Versatile Dataset of Mouse and Eye Movements on Search Engine Results Pages",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08003",
        "HTML": "https://arxiv.org/html/2507.08003v1",
        "PDF": "https://arxiv.org/pdf/2507.08003"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper presents a dataset for analyzing user attention and behavior on search engine results pages, with no mention of creativity. The focus is on data collection and analysis related to user interaction with search engines."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08028",
      "abstract": "This paper introduces a SSSUMO, semi-supervised deep learning approach for submovement decomposition that achieves state-of-the-art accuracy and speed. While submovement analysis offers valuable insights into motor control, existing methods struggle with reconstruction accuracy, computational cost, and validation, due to the difficulty of obtaining hand-labeled data. We address these challenges using a semi-supervised learning framework. This framework learns from synthetic data, initially generated from minimum-jerk principles and then iteratively refined through adaptation to unlabeled human movement data. Our fully convolutional architecture with differentiable reconstruction significantly surpasses existing methods on both synthetic and diverse human motion datasets, demonstrating robustness even in high-noise conditions. Crucially, the model operates in real-time (less than a millisecond per input second), a substantial improvement over optimization-based techniques. This enhanced performance facilitates new applications in human-computer interaction, rehabilitation medicine, and motor control studies. We demonstrate the model's effectiveness across diverse human-performed tasks such as steering, rotation, pointing, object moving, handwriting, and mouse-controlled gaming, showing notable improvements particularly on challenging datasets where traditional methods largely fail. Training and benchmarking source code, along with pre-trained model weights, are made publicly available at https://github.com/dolphin-in-a-coma/sssumo.",
      "authors": [
        "Evgenii Rudakov",
        "Jonathan Shock",
        "Otto Lappi",
        "Benjamin Ultan Cowley"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-08T21:26:25+00:00",
          "link": "https://arxiv.org/abs/2507.08028v1",
          "size": "4552kb",
          "version": "v1"
        }
      ],
      "title": "SSSUMO: Real-Time Semi-Supervised Submovement Decomposition",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08028",
        "HTML": "https://arxiv.org/html/2507.08028v1",
        "PDF": "https://arxiv.org/pdf/2507.08028"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on submovement decomposition for real-time human-computer interaction, with no mention of creativity as a topic of interest or analysis."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08142",
      "abstract": "Unreal Engine is a platform that has influenced immersive storytelling and virtual reality (VR) through its advanced features and diverse applications. This paper provides an in-depth technical review of Unreal Engine. It analyzes its key innovations in creating hyper-realistic environments and emotionally engaging narratives, with significant applications in gaming, virtual production, education, cultural preservation, and healthcare. The findings of this article highlight Unreal Engine's transformative impact across industries, demonstrating its ability to merge storytelling with cutting-edge technologies. Case studies illustrate how Unreal Engine facilitates seamless visuals, audio, and interactivity integration to create compelling experiences. Additionally, this study identifies Unreal Engine's versatility in applications ranging from procedural content generation and AI-driven workflows to smart city simulations and VR-based rehabilitation programs.\n  While Unreal Engine sets new benchmarks for visual fidelity and interactivity, this paper underscores critical challenges, including its high hardware demands, limited accessibility, and ethical concerns related to over-immersion and data privacy. Addressing these challenges through cloud-based rendering, inclusive design, and ethical practices is essential for broader adoption and sustainability. This review concludes that Unreal Engine is suitable for innovation and interdisciplinary collaboration. Its ability to empower creators, redefine workflows, and push the boundaries of immersive storytelling positions Unreal Engine as pivotal in shaping the future of virtual reality and interactive media.",
      "authors": [
        "Oleksandra Sobchyshak",
        "Santiago Berrezueta-Guzman",
        "Stefan Wagner"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-10T20:02:53+00:00",
          "link": "https://arxiv.org/abs/2507.08142v1",
          "size": "7313kb",
          "version": "v1"
        }
      ],
      "title": "Pushing the Boundaries of Immersion and Storytelling: A Technical Review of Unreal Engine",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08142",
        "HTML": "https://arxiv.org/html/2507.08142v1",
        "PDF": "https://arxiv.org/pdf/2507.08142"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses Unreal Engine's role in immersive storytelling and virtual reality, which can be related to creative tasks, but creativity is not the central focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08167",
      "abstract": "Emotion detection in older adults is crucial for understanding their cognitive and emotional well-being, especially in hospital and assisted living environments. In this work, we investigate an edge-based, non-obtrusive approach to emotion identification that uses only physiological signals obtained via wearable sensors. Our dataset includes data from 40 older individuals. Emotional states were obtained using physiological signals from the Empatica E4 and Shimmer3 GSR+ wristband and facial expressions were recorded using camera-based emotion recognition with the iMotion's Facial Expression Analysis (FEA) module. The dataset also contains twelve emotion categories in terms of relative intensities. We aim to study how well emotion recognition can be accomplished using simply physiological sensor data, without the requirement for cameras or intrusive facial analysis. By leveraging classical machine learning models, we predict the intensity of emotional responses based on physiological signals. We achieved the highest 0.782 r2 score with the lowest 0.0006 MSE on the regression task. This method has significant implications for individuals with Alzheimer's Disease and Related Dementia (ADRD), as well as veterans coping with Post-Traumatic Stress Disorder (PTSD) or other cognitive impairments. Our results across multiple classical regression models validate the feasibility of this method, paving the way for privacy-preserving and efficient emotion recognition systems in real-world settings.",
      "authors": [
        "Md. Saif Hassan Onim",
        "Andrew M. Kiselica and Himanshu Thapliyal"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-10T20:59:25+00:00",
          "link": "https://arxiv.org/abs/2507.08167v1",
          "size": "2917kb",
          "version": "v1"
        }
      ],
      "title": "Emotion Detection in Older Adults Using Physiological Signals from Wearable Sensors",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08167",
        "HTML": "https://arxiv.org/html/2507.08167v1",
        "PDF": "https://arxiv.org/pdf/2507.08167"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper focuses on emotion detection in older adults using physiological signals, with no mention of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08230",
      "abstract": "As artificial intelligence (AI) systems become increasingly sophisticated at generating synthetic human faces, understanding how these images are perceived across diverse populations is important. This study investigates how autistic individuals/individuals with autism perceive AI-generated faces, focusing on the uncanny valley effect. Using a qualitative approach, we analyzed discussions from the r/autism community on Reddit to explore how autistic participants/participants with autism describe their experiences with AI-generated faces and the uncanny valley phenomenon. The findings suggest that autistic people/people with autism may experience the uncanny valley differently, often reporting stronger discomfort with real human faces than with artificial ones. This research contributes to our understanding of visual perception in autism and has implications for the development of inclusive AI systems and assistive technologies.",
      "authors": [
        "Gabriella Waters"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-11T00:35:59+00:00",
          "link": "https://arxiv.org/abs/2507.08230v1",
          "size": "254kb",
          "version": "v1"
        }
      ],
      "title": "Uncanny or Not? Perceptions of AI-Generated Faces in Autism",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08230",
        "PDF": "https://arxiv.org/pdf/2507.08230"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on the perception of AI-generated faces by autistic individuals, particularly the uncanny valley effect. There is no discussion related to creativity as a primary or secondary theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08260",
      "abstract": "We present a graphical, node-based system through which users can visually chain generative AI models for creative tasks. Research in the area of chaining LLMs has found that while chaining provides transparency, controllability and guardrails to approach certain tasks, chaining with pre-defined LLM steps prevents free exploration. Using cognitive processes from creativity research as a basis, we create a system that addresses the inherent constraints of chat-based AI interactions. Specifically, our system aims to overcome the limiting linear structure that inhibits creative exploration and ideation. Further, our node-based approach enables the creation of reusable, shareable templates that can address different creative tasks. In a small-scale user study, we find that our graph-based system supports ideation and allows some users to better visualise and think through their writing process when compared to a similar conversational interface. We further discuss the weaknesses and limitations of our system, noting the benefits to creativity that user interfaces with higher complexity can provide for users who can effectively use them.",
      "authors": [
        "Abhinav Sood",
        "Maria Teresa Llano",
        "Jon McCormack"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-11T02:11:25+00:00",
          "link": "https://arxiv.org/abs/2507.08260v1",
          "size": "734kb",
          "version": "v1"
        }
      ],
      "title": "Do Conversational Interfaces Limit Creativity? Exploring Visual Graph Systems for Creative Writing",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08260",
        "HTML": "https://arxiv.org/html/2507.08260v1",
        "PDF": "https://arxiv.org/pdf/2507.08260"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper directly addresses creativity by exploring a visual graph system for creative writing. It discusses the enhancement of creative processes through system design, explicitly positioning creativity as a main objective."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08624",
      "abstract": "This paper introduces the Ambient Intelligence Rehabilitation Support (AIRS) framework, an advanced artificial intelligence-based solution tailored for home rehabilitation environments. AIRS integrates cutting-edge technologies, including Real-Time 3D Reconstruction (RT-3DR), intelligent navigation, and large Vision-Language Models (VLMs), to create a comprehensive system for machine-guided physical rehabilitation. The general AIRS framework is demonstrated in rehabilitation scenarios following total knee replacement (TKR), utilizing a database of 263 video recordings for evaluation. A smartphone is employed within AIRS to perform RT-3DR of living spaces and has a body-matched avatar to provide visual feedback about the excercise. This avatar is necessary in (a) optimizing exercise configurations, including camera placement, patient positioning, and initial poses, and (b) addressing privacy concerns and promoting compliance with the AI Act. The system guides users through the recording process to ensure the collection of properly recorded videos. AIRS employs two feedback mechanisms: (i) visual 3D feedback, enabling direct comparisons between prerecorded clinical exercises and patient home recordings and (ii) VLM-generated feedback, providing detailed explanations and corrections for exercise errors. The framework also supports people with visual and hearing impairments. It also features a modular design that can be adapted to broader rehabilitation contexts. AIRS software components are available for further use and customization.",
      "authors": [
        "G\\'abor Baranyi",
        "Zsolt Csibi",
        "Kristian Fenech",
        "\\'Aron F\\'othi",
        "Zs\\'ofia Ga\\'al",
        "Joul Skaf",
        "Andr\\'as L\\H{o}rincz"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-11T14:27:06+00:00",
          "link": "https://arxiv.org/abs/2507.08624v1",
          "size": "16053kb",
          "version": "v1"
        }
      ],
      "title": "Adaptive Framework for Ambient Intelligence in Rehabilitation Assistance",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08624",
        "HTML": "https://arxiv.org/html/2507.08624v1",
        "PDF": "https://arxiv.org/pdf/2507.08624"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The research centers on rehabilitation support using Ambient Intelligence and AI technologies. Creativity is not mentioned nor implied as part of the study's focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08659",
      "abstract": "Prolonged sitting is a health risk leading to metabolic and cardiovascular diseases. To combat this, various \"nudging\" strategies encourage stand-ups. Behavior change triggers use explicit prompts such as smartphone push notifications or light controls. However, comparisons of the effects of such interactions, discomfort, and user context have not yet been performed. The present study evaluated these methods in a mixed design experiment with 15 college students. Three intervention methods (none, push notifications, and light dimming) and three user task contexts (computer work, video calls, and reading) were tested. The frequency of standing up and comfort were assessed after each ten-minute session. Results showed that dimming resulted in slightly more breaks (1.4 \\pm 1.55) than push notification (1.2 \\pm 1.08), but caused discomfort for 66.7% of participants, compared to 20% for notification. The results were influenced by task context. Dimming was most effective during video calls and reading, while push notifications were more effective during computer work. These findings suggest adaptive nudging systems should tailor interventions based on context and individual preferences.",
      "authors": [
        "Sohshi Yoshida and Ko Watanabe and Andreas Dengel and Shoya Ishimaru and Shingo Ata and Manato Fujimoto"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-11T15:00:23+00:00",
          "link": "https://arxiv.org/abs/2507.08659v1",
          "size": "7751kb",
          "version": "v1"
        }
      ],
      "title": "Push or Light: Nudging Standing to Break Prolonged Sitting",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08659",
        "HTML": "https://arxiv.org/html/2507.08659v1",
        "PDF": "https://arxiv.org/pdf/2507.08659"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper investigates nudging strategies to reduce prolonged sitting for health benefits, without any focus or mention of creativity. It is centered on behavioral interventions rather than creative concepts."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08675",
      "abstract": "This paper introduces LIMITER, a gamified digital musical instrument for harnessing and performing microtonal and justly intonated sounds. While microtonality in Western music remains a niche and esoteric system that can be difficult both to conceptualize and to perform with, LIMITER presents a novel, easy to pickup interface that utilizes color, geometric transformations, and game-like controls to create a simpler inlet into utilizing these sounds as a means of expression. We report on the background of the development of LIMITER, as well as explain the underlying musical and engineering systems that enable its function. Additionally, we offer a discussion and preliminary evaluation of the creativity-enhancing effects of the interface.",
      "authors": [
        "Antonis Christou"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-11T15:15:25+00:00",
          "link": "https://arxiv.org/abs/2507.08675v1",
          "size": "19653kb",
          "version": "v1"
        }
      ],
      "title": "LIMITER: A Gamified Interface for Harnessing Just Intonation Systems",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08675",
        "HTML": "https://arxiv.org/html/2507.08675v1",
        "PDF": "https://arxiv.org/pdf/2507.08675"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper focuses on a system designed to enhance creativity through a musical interface. It explicitly discusses creativity-enhancement effects, making creativity a primary focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08744",
      "abstract": "Motion capture technologies are increasingly used in creative and performance contexts but often exclude disabled practitioners due to normative assumptions in body modeling, calibration, and avatar representation. EqualMotion introduces a body-agnostic, wearable motion capture system designed through a disability-centred co-design approach. By enabling personalised calibration, integrating mobility aids, and adopting an inclusive visual language, EqualMotion supports diverse body types and movement styles. The system is developed collaboratively with disabled researchers and creatives, aiming to foster equitable participation in digital performance and prototyping. This paper outlines the system's design principles and highlights ongoing case studies in dance and music to evaluate accessibility in real-world creative workflows.",
      "authors": [
        "Clarice Hilton",
        "Kat Hawkins",
        "Phill Tew",
        "Freddie Collins",
        "Seb Madgwick",
        "Dominic Potts",
        "Tom Mitchell"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-11T16:51:40+00:00",
          "link": "https://arxiv.org/abs/2507.08744v1",
          "size": "1323kb",
          "version": "v1"
        }
      ],
      "title": "EqualMotion: Accessible Motion Capture for the Creative Industries",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08744",
        "HTML": "https://arxiv.org/html/2507.08744v1",
        "PDF": "https://arxiv.org/pdf/2507.08744"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "Creativity is a secondary theme. The paper focuses on accessibility in motion capture but discusses the application in creative workflows, highlighting case studies in creative industries."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08001",
      "abstract": "With the advancement of science and technology, the philosophy of creativity has undergone significant reinterpretation. This paper investigates contemporary research in the fields of psychology, cognitive neuroscience, and the philosophy of creativity, particularly in the context of the development of artificial intelligence (AI) techniques. It aims to address the central question: Can AI exhibit creativity? The paper reviews the historical perspectives on the philosophy of creativity and explores the influence of psychological advancements on the study of creativity. Furthermore, it analyzes various definitions of creativity and examines the responses of naturalism and cognitive neuroscience to the concept of creativity.",
      "authors": [
        "Shengyi Xie"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-04-25T17:10:03+00:00",
          "link": "https://arxiv.org/abs/2507.08001v1",
          "size": "326kb",
          "version": "v1"
        }
      ],
      "title": "Human Creativity and AI",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08001",
        "PDF": "https://arxiv.org/pdf/2507.08001"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper directly addresses the intersection of AI and creativity, specifically investigating whether AI can exhibit creativity. It delves into the philosophy and definitions of creativity, making creativity a central focus of the study."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08030",
      "abstract": "Generative AI models, including large language models (LLMs) and vision-language models (VLMs), are increasingly used to interpret medical images and answer clinical questions. Their responses often include inaccuracies; therefore, safety measures like medical disclaimers are critical to remind users that AI outputs are not professionally vetted or a substitute for medical advice. This study evaluated the presence of disclaimers in LLM and VLM outputs across model generations from 2022 to 2025. Using 500 mammograms, 500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs were screened for disclaimer phrases. Medical disclaimer presence in LLM and VLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023 to 1.05% in 2025, respectively. By 2025, the majority of models displayed no disclaimers. As public models become more capable and authoritative, disclaimers must be implemented as a safeguard adapting to the clinical context of each output.",
      "authors": [
        "Sonali Sharma",
        "Ahmed M. Alaa",
        "Roxana Daneshjou"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computation and Language (cs.CL)",
        "Computational Engineering, Finance, and Science (cs.CE)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-08T23:50:30+00:00",
          "link": "https://arxiv.org/abs/2507.08030v1",
          "size": "11195kb",
          "version": "v1"
        }
      ],
      "title": "A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08030",
        "HTML": "https://arxiv.org/html/2507.08030v1",
        "PDF": "https://arxiv.org/pdf/2507.08030"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper evaluates medical safety messaging in AI models, specifically disclaimers, with no connection to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08175",
      "abstract": "We investigate the feasibility of inferring emotional states exclusively from physiological signals, thereby presenting a privacy-preserving alternative to conventional facial recognition techniques. We conduct a performance comparison of classical machine learning algorithms and hybrid quantum machine learning (QML) methods with a quantum kernel-based model. Our results indicate that the quantum-enhanced SVM surpasses classical counterparts in classification performance across all emotion categories, even when trained on limited datasets. The F1 scores over all classes are over 80% with around a maximum of 36% improvement in the recall values. The integration of wearable sensor data with quantum machine learning not only enhances accuracy and robustness but also facilitates unobtrusive emotion recognition. This methodology holds promise for populations with impaired communication abilities, such as individuals with Alzheimer's Disease and Related Dementias (ADRD) and veterans with Post-Traumatic Stress Disorder (PTSD). The findings establish an early foundation for passive emotional monitoring in clinical and assisted living conditions.",
      "authors": [
        "Md. Saif Hassan Onim",
        "Travis S. Humble and Himanshu Thapliyal"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Human-Computer Interaction (cs.HC)",
        "Quantum Physics (quant-ph)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-10T21:12:12+00:00",
          "link": "https://arxiv.org/abs/2507.08175v1",
          "size": "469kb",
          "version": "v1"
        }
      ],
      "title": "Emotion Recognition in Older Adults with Quantum Machine Learning and Wearable Sensors",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08175",
        "HTML": "https://arxiv.org/html/2507.08175v1",
        "PDF": "https://arxiv.org/pdf/2507.08175"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The focus is on emotion recognition using quantum machine learning and wearable sensors; creativity is not addressed as a topic."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08594",
      "abstract": "Proto-personas are commonly used during early-stage Product Discovery, such as Lean Inception, to guide product definition and stakeholder alignment. However, the manual creation of proto-personas is often time-consuming, cognitively demanding, and prone to bias. In this paper, we propose and empirically investigate a prompt engineering-based approach to generate proto-personas with the support of Generative AI (GenAI). Our goal is to evaluate the approach in terms of efficiency, effectiveness, user acceptance, and the empathy elicited by the generated personas. We conducted a case study with 19 participants embedded in a real Lean Inception, employing a qualitative and quantitative methods design. The results reveal the approach's efficiency by reducing time and effort and improving the quality and reusability of personas in later discovery phases, such as Minimum Viable Product (MVP) scoping and feature refinement. While acceptance was generally high, especially regarding perceived usefulness and ease of use, participants noted limitations related to generalization and domain specificity. Furthermore, although cognitive empathy was strongly supported, affective and behavioral empathy varied significantly across participants. These results contribute novel empirical evidence on how GenAI can be effectively integrated into software Product Discovery practices, while also identifying key challenges to be addressed in future iterations of such hybrid design processes.",
      "authors": [
        "Fernando Ayach",
        "Vitor Lameir\\~ao",
        "Raul Le\\~ao",
        "Jerfferson Felizardo",
        "Rafael Sobrinho",
        "Vanessa Borges",
        "Patr\\'icia Matsubara",
        "Awdren Font\\~ao"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Software Engineering (cs.SE)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-11T13:42:12+00:00",
          "link": "https://arxiv.org/abs/2507.08594v1",
          "size": "553kb",
          "version": "v1"
        }
      ],
      "title": "Generating Proto-Personas through Prompt Engineering: A Case Study on Efficiency, Effectiveness and Empathy",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08594",
        "HTML": "https://arxiv.org/html/2507.08594v1",
        "PDF": "https://arxiv.org/pdf/2507.08594"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "While the paper is focused on the generation of proto-personas using AI, it touches on creativity in product design as a secondary aspect. The paper evaluates empathy and effectiveness, which relates to some creative processes but isn't the primary focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08800",
      "abstract": "We introduce NeuralOS, a neural framework that simulates graphical user interfaces (GUIs) of operating systems by directly predicting screen frames in response to user inputs such as mouse movements, clicks, and keyboard events. NeuralOS combines a recurrent neural network (RNN), which tracks computer state, with a diffusion-based neural renderer that generates screen images. The model is trained on a large-scale dataset of Ubuntu XFCE recordings, which include both randomly generated interactions and realistic interactions produced by AI agents. Experiments show that NeuralOS successfully renders realistic GUI sequences, accurately captures mouse interactions, and reliably predicts state transitions like application launches. Although modeling fine-grained keyboard interactions precisely remains challenging, NeuralOS offers a step toward creating fully adaptive, generative neural interfaces for future human-computer interaction systems.",
      "authors": [
        "Luke Rivard",
        "Sun Sun",
        "Hongyu Guo",
        "Wenhu Chen",
        "Yuntian Deng"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-11T17:59:40+00:00",
          "link": "https://arxiv.org/abs/2507.08800v1",
          "size": "9226kb",
          "version": "v1"
        }
      ],
      "title": "NeuralOS: Towards Simulating Operating Systems via Neural Generative Models",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08800",
        "HTML": "https://arxiv.org/html/2507.08800v1",
        "PDF": "https://arxiv.org/pdf/2507.08800"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper is about simulating operating systems with neural models, focusing on technical aspects of GUI generation rather than creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2503.15488",
      "abstract": "Due to the multidisciplinary nature of wearable technology, the industry faces potential limitations in innovation. The wearable technology industry is still in its infancy and increased applicable use faces stagnation despite the plethora of technologies that have been largely wrist worn. This could be a result of the lack of multidisciplinary expert knowledge disseminating through the industry. Unlike other technologies which have standardizations and processes for how they are developed, wearable technologies exist in a realm of perpetual change as given the various materials and subcomponents that continue to be developed. It is essential that expert opinions form a collaborative foundation, and even more so that intelligent systems foster that collaboration. The caveat though, is likeliness of these artificial intelligence (AI) collaboration tools to be utilized by industry experts. Mental model development for AI tool usage could be applied to wearable technology innovation in this regard, thus the goal of this paper and focus of research.",
      "authors": [
        "Andrew M. Lydner"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-12-27T00:34:48+00:00",
          "link": "https://arxiv.org/abs/2503.15488v1",
          "size": "827kb",
          "version": "v1"
        },
        {
          "date": "2025-07-10T23:45:24+00:00",
          "link": "https://arxiv.org/abs/2503.15488v2",
          "size": "804kb",
          "version": "v2"
        }
      ],
      "title": "Human-AI Collaboration for Wearable Technology Component Standardization",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.15488",
        "PDF": "https://arxiv.org/pdf/2503.15488"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper highlights the role of human-AI collaboration in innovation within wearable technology. Creativity is a secondary theme, as collaboration tools and multidisciplinary innovation processes may support creative development in the industry."
      },
      "source": "arXiv"
    },
    {
      "id": "2503.16521",
      "abstract": "This paper explores conversational self-play with LLMs as a scalable approach for analyzing and exploring psychotherapy approaches, evaluating how well AI-generated therapeutic dialogues align with established modalities.",
      "authors": [
        "Onno P Kampman",
        "Michael Xing",
        "Charmaine Lim",
        "Ahmad Ishqi Jabir",
        "Ryan Louie",
        "Jimmy Lee",
        "Robert JT Morris"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Multiagent Systems (cs.MA)"
      ],
      "submission_historys": [
        {
          "date": "2025-03-17T02:16:41+00:00",
          "link": "https://arxiv.org/abs/2503.16521v1",
          "size": "1148kb",
          "version": "v1"
        },
        {
          "date": "2025-07-11T06:31:58+00:00",
          "link": "https://arxiv.org/abs/2503.16521v2",
          "size": "1136kb",
          "version": "v2"
        }
      ],
      "title": "Conversational Self-Play for Discovering and Understanding Psychotherapy Approaches",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.16521",
        "HTML": "https://arxiv.org/html/2503.16521v2",
        "PDF": "https://arxiv.org/pdf/2503.16521"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper explores conversational self-play with LLMs for understanding psychotherapy, focusing on dialogue alignment with therapeutic modalities. Creativity is not addressed as a focus or secondary theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2505.09166",
      "abstract": "In the creative practice of text-to-image generation (TTI), images are generated from text prompts. However, TTI models are trained to always yield an output, even if the prompt contains unknown terms. In this case, the model may generate what we call \"default images\": images that closely resemble each other across many unrelated prompts. We argue studying default images is valuable for designing better solutions for TTI and prompt engineering. In this paper, we provide the first investigation into default images on Midjourney, a popular image generator. We describe our systematic approach to create input prompts triggering default images, and present the results of our initial experiments and several small-scale ablation studies. We also report on a survey study investigating how default images affect user satisfaction. Our work lays the foundation for understanding default images in TTI and highlights challenges and future research directions.",
      "authors": [
        "Hannu Simonen",
        "Atte Kiviniemi",
        "Jonas Oppenlaender"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-05-14T05:59:23+00:00",
          "link": "https://arxiv.org/abs/2505.09166v1",
          "size": "21659kb",
          "version": "v1"
        },
        {
          "date": "2025-07-05T13:02:43+00:00",
          "link": "https://arxiv.org/abs/2505.09166v2",
          "size": "4316kb",
          "version": "v2"
        },
        {
          "date": "2025-07-11T10:01:02+00:00",
          "link": "https://arxiv.org/abs/2505.09166v3",
          "size": "4313kb",
          "version": "v3"
        }
      ],
      "title": "An Exploration of Default Images in Text-to-Image Generation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.09166",
        "HTML": "https://arxiv.org/html/2505.09166v3",
        "PDF": "https://arxiv.org/pdf/2505.09166"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper explicitly studies creative practice by investigating text-to-image generation (TTI), focusing on 'default images' in response to prompts. Creativity is a primary focus, as it explores creative aspects of TTI models, user satisfaction, and the design of solutions for creative outputs."
      },
      "tasks": [
        "Image Generation",
        "Prompt Engineering",
        "Text to Image Generation",
        "Text-to-Image Generation"
      ],
      "source": "arXiv"
    },
    {
      "id": "2507.07362",
      "abstract": "SRL, defined as learners' ability to systematically plan, monitor, and regulate their learning activities, is crucial for sustained academic achievement and lifelong learning competencies. Emerging Artificial Intelligence (AI) developments profoundly influence SRL interactions by potentially either diminishing or strengthening learners' opportunities to exercise their own regulatory skills. Recent literature emphasizes a balanced approach termed Hybrid Human-AI Regulated Learning (HHAIRL), in which AI provides targeted, timely scaffolding while preserving the learners' role as active decision-makers and reflective monitors of their learning process. Nevertheless, existing digital tools frequently fall short, lacking adaptability, focusing narrowly on isolated SRL phases, and insufficiently support meaningful human-AI interactions. In response, this paper introduces the enhanced FLoRA Engine, which incorporates advanced Generative Artificial Intelligence (GenAI) features and state-of-the-art learning analytics, explicitly grounded in SRL and HHAIRL theories. The FLoRA Engine offers instrumentation tools such as collaborative writing, multi-agents chatbot, and detailed learning trace logging to support dynamic, adaptive scaffolding tailored to individual needs in real time. We further present a summary of several research studies that provide the validations for and illustrate how these instrumentation tools can be utilized in real-world educational and experimental contexts. These studies demonstrate the effectiveness of FLoRA Engine in fostering SRL and HHAIRL, providing both theoretical insights and practical solutions for the future of AI-enhanced learning context.",
      "authors": [
        "Xinyu Li and Tongguang Li and Lixiang Yan and Yuheng Li and Linxuan Zhao and Mladen Rakovi\\'c and Inge Molenaar and Dragan Ga\\v{s}evi\\'c and Yizhou Fan"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-10T01:11:52+00:00",
          "link": "https://arxiv.org/abs/2507.07362v1",
          "size": "16820kb",
          "version": "v1"
        },
        {
          "date": "2025-07-11T01:10:44+00:00",
          "link": "https://arxiv.org/abs/2507.07362v2",
          "size": "16820kb",
          "version": "v2"
        }
      ],
      "title": "FLoRA: An Advanced AI-Powered Engine to Facilitate Hybrid Human-AI Regulated Learning",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07362",
        "HTML": "https://arxiv.org/html/2507.07362v2",
        "PDF": "https://arxiv.org/pdf/2507.07362"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses an AI-powered learning engine that includes collaborative writing tools and other features that could potentially support creative learning and activities. However, creativity is not the primary focus; the main emphasis is on self-regulated learning and human-AI interaction for learning enhancement."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.07930",
      "abstract": "Background: Public speaking is a vital professional skill, yet it remains a source of significant anxiety for many individuals. Traditional training relies heavily on expert coaching, but recent advances in AI has led to novel types of commercial automated public speaking feedback tools. However, most research has focused on prototypes rather than commercial applications, and little is known about how public speaking experts perceive these tools.\n  Objectives: This study aims to evaluate expert opinions on the efficacy and design of commercial AI-based public speaking training tools and to propose guidelines for their improvement.\n  Methods: The research involved 16 semi-structured interviews and 2 focus groups with public speaking experts. Participants discussed their views on current commercial tools, their potential integration into traditional coaching, and suggestions for enhancing these systems.\n  Results and Conclusions: Experts acknowledged the value of AI tools in handling repetitive, technical aspects of training, allowing coaches to focus on higher-level skills. However they found key issues in current tools, emphasising the need for personalised, understandable, carefully selected feedback and clear instructional design. Overall, they supported a hybrid model combining traditional coaching with AI-supported exercises.",
      "authors": [
        "Nesrine Fourati",
        "Alisa Barkar",
        "Marion Drag\\'ee",
        "Liv Danthon-Lefebvre",
        "Mathieu Chollet"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-10T17:09:21+00:00",
          "link": "https://arxiv.org/abs/2507.07930v1",
          "size": "118kb",
          "version": "v1"
        },
        {
          "date": "2025-07-11T08:22:41+00:00",
          "link": "https://arxiv.org/abs/2507.07930v2",
          "size": "118kb",
          "version": "v2"
        }
      ],
      "title": "Probing Experts' Perspectives on AI-Assisted Public Speaking Training",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07930",
        "HTML": "https://arxiv.org/html/2507.07930v2",
        "PDF": "https://arxiv.org/pdf/2507.07930"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The focus of the paper is on AI-assisted public speaking training and experts' perspectives on these tools. Although there might be creative elements in public speaking, the paper centers around training efficacy and design rather than creativity itself."
      },
      "source": "arXiv"
    },
    {
      "id": "2503.01163",
      "abstract": "Prompt optimization aims to search for effective prompts that enhance the performance of large language models (LLMs). Although existing prompt optimization methods have discovered effective prompts, they often differ from sophisticated prompts carefully designed by human experts. Prompt design strategies, representing best practices for improving prompt performance, can be key to improving prompt optimization. Recently, a method termed the Autonomous Prompt Engineering Toolbox (APET) has incorporated various prompt design strategies into the prompt optimization process. In APET, the LLM is needed to implicitly select and apply the appropriate strategies because prompt design strategies can have negative effects. This implicit selection may be suboptimal due to the limited optimization capabilities of LLMs. This paper introduces Optimizing Prompts with sTrategy Selection (OPTS), which implements explicit selection mechanisms for prompt design. We propose three mechanisms, including a Thompson sampling-based approach, and integrate them into EvoPrompt, a well-known prompt optimizer. Experiments optimizing prompts for two LLMs, Llama-3-8B-Instruct and GPT-4o mini, were conducted using BIG-Bench Hard. Our results show that the selection of prompt design strategies improves the performance of EvoPrompt, and the Thompson sampling-based mechanism achieves the best overall results. Our experimental code is provided at https://github.com/shiralab/OPTS .",
      "authors": [
        "Rin Ashizawa",
        "Yoichi Hirose",
        "Nozomu Yoshinari",
        "Kento Uchida",
        "Shinichi Shirakawa"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)",
        "Neural and Evolutionary Computing (cs.NE)"
      ],
      "submission_historys": [
        {
          "date": "2025-03-03T04:24:04+00:00",
          "link": "https://arxiv.org/abs/2503.01163v1",
          "size": "130kb",
          "version": "v1"
        },
        {
          "date": "2025-07-11T03:56:05+00:00",
          "link": "https://arxiv.org/abs/2503.01163v2",
          "size": "93kb",
          "version": "v2"
        }
      ],
      "title": "Bandit-Based Prompt Design Strategy Selection Improves Prompt Optimizers",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.01163",
        "HTML": "https://arxiv.org/html/2503.01163v2",
        "PDF": "https://arxiv.org/pdf/2503.01163"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses prompt optimization for LLMs, which can involve creative prompt design by experts. While creativity is not the main focus, it is a secondary aspect as prompt design strategies may contribute to creative language model interactions."
      },
      "tasks": [
        "Prompt Engineering",
        "Thompson Sampling"
      ],
      "repo_urls": [
        "https://github.com/shiralab/opts"
      ],
      "source": "arXiv"
    },
    {
      "id": "2404.16060",
      "abstract": "Background-oriented schlieren (BOS) is a powerful technique for flow visualization. Nevertheless, the widespread dissemination of BOS is impeded by its dependence on scientific cameras, computing hardware, and dedicated analysis software. In this work, we aim to democratize BOS by providing a smartphone based scientific tool called \"Pocket Schlieren\". Pocket Schlieren enables users to directly capture, process, and visualize flow phenomena on their smartphones. The underlying algorithm incorporates consecutive frame subtraction (CFS) and optical flow (OF) techniques to compute the density gradients inside a flow. It performs on both engineered and natural background patterns. Using Pocket Schlieren, we successfully visualized the flow produced from a burning candle flame, butane lighter, hot soldering iron, room heater, water immersion heating rod, and a large outdoor butane flame. Pocket Schlieren promises to serve as a frugal yet potent instrument for scientific and educational purposes. We have made it publicly available at doi: 10.5281/zenodo.10949271.",
      "authors": [
        "Diganta Rabha",
        "Vimod Kumar",
        "Akshay Kumar",
        "Dinesh Saini and Manish Kumar"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Physics Education (physics.ed-ph)",
        "Optics (physics.optics)"
      ],
      "submission_historys": [
        {
          "date": "2024-04-15T17:34:31+00:00",
          "link": "https://arxiv.org/abs/2404.16060v1",
          "size": "2999kb",
          "version": "v1"
        }
      ],
      "title": "Pocket Schlieren: a background oriented schlieren imaging platform on a smartphone",
      "links": {
        "Abstract": "https://arxiv.org/abs/2404.16060",
        "PDF": "https://arxiv.org/pdf/2404.16060"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on the development of a smartphone-based schlieren imaging platform for flow visualization. It does not address creativity, as the main objective is the democratization of a scientific tool for educational purposes."
      },
      "source": "arXiv"
    }
  ],
  "subjects": [
    "Computer Vision and Pattern Recognition (cs.CV)",
    "Optics (physics.optics)",
    "Quantum Physics (quant-ph)",
    "Computational Engineering, Finance, and Science (cs.CE)",
    "Computation and Language (cs.CL)",
    "Information Retrieval (cs.IR)",
    "Artificial Intelligence (cs.AI)",
    "Physics Education (physics.ed-ph)",
    "Computers and Society (cs.CY)",
    "Multiagent Systems (cs.MA)",
    "Neural and Evolutionary Computing (cs.NE)",
    "Software Engineering (cs.SE)",
    "Human-Computer Interaction (cs.HC)",
    "Machine Learning (cs.LG)"
  ],
  "prompt": {
    "creativity": "\nYou are an expert in information retrieval. I will provide you with a list of research papers from arXiv, specifically in the *cs.HC* (Human-Computer Interaction) category.\n\nYour task is to analyze each paper and determine its relevance to the topic of **Creativity**.\n\n\n### Classify each paper into one of the following relevance levels\n\n- `core` \u2014 Creativity is a **primary focus**\n  - The paper directly studies or simulates creativity, with a clear focus on creativity.\n  - Includes creative tasks, co-creative systems, or creativity evaluation metrics.\n  - The title and abstract explicitly mention creativity, and the research questions are directly related to creativity.\n- `partial` \u2014 Creativity is a **secondary theme**\n  - Part of the paper relates to creativity; it is treated as an analytical dimension or design goal but not the main objective (e.g., user creativity, design support).\n  - Creativity may appear in discussions, experiments, or auxiliary applications.\n  - Creativity is presented as a supporting topic (e.g., evaluation criteria, user feedback).\n- `irrelevant` \u2014 **No clear connection** to creativity\n  - The paper does not address creativity as a topic.\n  - Focuses on unrelated technical content (e.g., compression, security, optimization).\n  - If creativity is mentioned, it is only superficial and lacks substantive content.\n\n\n### Return your results in the following JSON format\n\n```json\n{\n  \"result\": [\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    },\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    }\n  ]\n}\n```\n\nBe concise but specific in your reasoning, referencing key terms or sections when applicable.\n"
  },
  "description": "Data source: https://arxiv.org/list/cs.HC/new",
  "level_tatistics": {
    "core": 4,
    "irrelevant": 13,
    "partial": 6
  },
  "arxiv_update_date": "2025-07-14",
  "updated_at": "2025-07-14 10:01:13"
}