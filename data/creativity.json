{
  "data": [
    {
      "id": "2507.02865",
      "abstract": "This study explores how Low-Rank Adaptation (LoRA) fine-tuning, guided by human aesthetic evaluations, can enhance the outputs of generative AI models in tangible product design, using lamp design as a case study. By integrating human feedback into the AI model, we aim to improve both the desirability and aesthetic appeal of the generated designs. Comprehensive experiments were conducted, starting with prompt optimization techniques and focusing on LoRA fine-tuning of the Stable Diffusion model. Additionally, methods to convert AI-generated designs into tangible products through 3D realization using 3D printing technologies were investigated. The results indicate that LoRA fine-tuning effectively aligns AI-generated designs with human aesthetic preferences, leading to significant improvements in desirability and aesthetic appeal scores. These findings highlight the potential of human-AI collaboration in tangible product design and provide valuable insights into integrating human feedback into AI design processes.",
      "authors": [
        "Dinuo Liao",
        "James Derek Lomas and Cehao Yu"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-02-23T19:49:36+00:00",
          "link": "https://arxiv.org/abs/2507.02865v1",
          "size": "5698kb",
          "version": "v1"
        }
      ],
      "title": "Enhancing the Aesthetic Appeal of AI-Generated Physical Product Designs through LoRA Fine-Tuning with Human Feedback",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02865",
        "HTML": "https://arxiv.org/html/2507.02865v1",
        "PDF": "https://arxiv.org/pdf/2507.02865"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper directly explores enhancing AI-generated product designs through human feedback to improve aesthetic appeal. It involves creative tasks in product design and emphasizes human-AI collaboration, making creativity a primary focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02866",
      "abstract": "This paper examines how distinct cultures of AI interdisciplinarity emerge through interface design, revealing the formation of new disciplinary cultures at these intersections. Through the Interface-Mediated Cognitive Security (IMCS) framework, I demonstrate how the collision of cybersecurity engineering, cognitive psychology, critical technology studies, and human-computer interaction generates research cultures that transcend traditional disciplinary boundaries. AI interfaces function as transformative boundary objects that necessitate methodological fusion rather than mere collaboration, simultaneously embodying technical architectures, psychological design patterns, and social interaction models. Through systematic visual analysis of generative AI platforms and case studies across public sector, medical, and educational domains, I identify four vulnerability vectors, Reflection Simulation, Authority Modulation, Cognitive Load Exploitation, and Market-Security Tension, that structure interface-mediated cognitive security. This research challenges three significant gaps in interdisciplinary theory: the assumption that disciplines maintain distinct methodological boundaries during collaboration, the belief that technical and social knowledge practices can be cleanly separated, and the presumption that disciplinary integration occurs through formal rather than cultural mechanisms. The empirical evidence demonstrates how interfaces function as sites of epistemological collision, creating methodological pressure zones where traditional disciplinary approaches prove insufficient for analysing the complex socio-technical phenomena at the interface.",
      "authors": [
        "Ben Kereopa-Yorke"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "date": "2025-04-14T22:25:53+00:00",
          "link": "https://arxiv.org/abs/2507.02866v1",
          "size": "558kb",
          "version": "v1"
        }
      ],
      "title": "Engineering Trust, Creating Vulnerability: A Socio-Technical Analysis of AI Interface Design",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02866",
        "PDF": "https://arxiv.org/pdf/2507.02866"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper examines socio-technical aspects of AI interface design and interdisciplinary cultures. While it involves design patterns, the primary focus is on trust, vulnerability, and cultural integration rather than creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02868",
      "abstract": "Although extended reality(XR)-using technologies have started to be discussed in the industrial setting, it is becoming important to understand how to implement them ethically and privacy-preservingly. In our paper, we summarise our experience of developing XR implementations for the off-highway machinery domain by pointing to the main challenges we identified during the work. We believe that our findings can be a starting point for further discussion and future research regarding privacy and ethical challenges in industrial applications of XR.",
      "authors": [
        "Anastasia Sergeeva",
        "Claudia Negri-Ribalta",
        "Gabriele Lenzini"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-04-21T07:42:13+00:00",
          "link": "https://arxiv.org/abs/2507.02868v1",
          "size": "86kb",
          "version": "v1"
        }
      ],
      "title": "Identifying Ethical Challenges in XR Implementations in the Industrial Domain: A Case of Off-Highway Machinery",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02868",
        "HTML": "https://arxiv.org/html/2507.02868v1",
        "PDF": "https://arxiv.org/pdf/2507.02868"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses ethical challenges in XR applications in industrial settings. It does not address creativity; the focus is on ethical and privacy-preserving implementations in technology use."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02869",
      "abstract": "This paper introduces Zara, an AI-driven recruitment support system developed by micro1, as a practical case study illustrating how large language models (LLMs) can enhance the candidate experience through personalized, scalable interview support. Traditionally, recruiters have struggled to deliver individualized candidate feedback due to logistical and legal constraints, resulting in widespread candidate dissatisfaction. Leveraging OpenAI's GPT-4o, Zara addresses these limitations by dynamically generating personalized practice interviews, conducting conversational AI-driven assessments, autonomously delivering structured and actionable feedback, and efficiently answering candidate inquiries using a Retrieval-Augmented Generation (RAG) system. To promote transparency, we have open-sourced the approach Zara uses to generate candidate feedback.",
      "authors": [
        "Nima Yazdani",
        "Aruj Mahajan",
        "Ali Ansari"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-04-21T16:48:47+00:00",
          "link": "https://arxiv.org/abs/2507.02869v1",
          "size": "2654kb",
          "version": "v1"
        }
      ],
      "title": "Zara: An LLM-based Candidate Interview Feedback System",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02869",
        "HTML": "https://arxiv.org/html/2507.02869v1",
        "PDF": "https://arxiv.org/pdf/2507.02869"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on an AI-driven recruitment support system and does not mention creativity or related applications in its approach to candidate feedback."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02905",
      "abstract": "Parallel coordinate plots (PCPs) are a prevalent method to interpret the relationship between the control parameters and metrics. PCPs deliver such an interpretation by color gradation based on a single metric. However, it is challenging to provide such a gradation when multiple metrics are present. Although a naive approach involves calculating a single metric by linearly weighting each metric, such weighting is unclear for users. To address this problem, we first propose a principled formulation for calculating the optimal weight based on a specific preferred metric combination. Although users can simply select their preference from a two-dimensional (2D) plane for bi-metric problems, multi-metric problems require intuitive visualization to allow them to select their preference. We achieved this using various radar charts to visualize the metric trade-offs on the 2D plane reduced by UMAP. In the analysis using pedestrian flow guidance planning, our method identified unique patterns of control parameter importance for each user preference, highlighting the effectiveness of our method.",
      "authors": [
        "Chisa Mori",
        "Shuhei Watanabe",
        "Masaki Onishi",
        "Takayuki Itoh"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-24T06:48:09+00:00",
          "link": "https://arxiv.org/abs/2507.02905v1",
          "size": "2199kb",
          "version": "v1"
        }
      ],
      "title": "Preference-Optimal Multi-Metric Weighting for Parallel Coordinate Plots",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02905",
        "HTML": "https://arxiv.org/html/2507.02905v1",
        "PDF": "https://arxiv.org/pdf/2507.02905"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The study deals with optimization in visualizing parallel coordinate plots, with no apparent discussion on creativity as a topic or objective."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02914",
      "abstract": "The loss of knowledge when skilled operators leave poses a critical issue for companies. This know-how is diverse and unstructured. We propose a novel method that combines knowledge graph embeddings and multi-modal interfaces to collect and retrieve expertise, making it actionable. Our approach supports decision-making on the shop floor. Additionally, we leverage LLMs to improve query understanding and provide adapted answers. As application case studies, we developed a proof-of-concept for quality control in high precision manufacturing.",
      "authors": [
        "Steve Dev\\`enes (1)",
        "Marine Capallera (2)",
        "Robin Cherix (2)",
        "Elena Mugellini (2)",
        "Omar Abou Khaled (2) and Francesco Carrino (1) ((1) Institute of Systems Engineering",
        "HEI-VS",
        "HES-SO University of Applied Sciences and Arts Western Switzerland",
        "(2) HumanTech Institute",
        "HEIA HES-SO University of Applied Sciences and Arts Western Switzerland)"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-25T07:03:52+00:00",
          "link": "https://arxiv.org/abs/2507.02914v1",
          "size": "1281kb",
          "version": "v1"
        }
      ],
      "title": "OAK -- Onboarding with Actionable Knowledge",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02914",
        "HTML": "https://arxiv.org/html/2507.02914v1",
        "PDF": "https://arxiv.org/pdf/2507.02914"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on knowledge management and decision-making in industrial settings with no explicit connection to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02920",
      "abstract": "Healthcare professionals need effective ways to use, understand, and validate AI-driven clinical decision support systems. Existing systems face two key limitations: complex visualizations and a lack of grounding in scientific evidence. We present an integrated decision support system that combines interactive visualizations with a conversational agent to explain diabetes risk assessments. We propose a hybrid prompt handling approach combining fine-tuned language models for analytical queries with general Large Language Models (LLMs) for broader medical questions, a methodology for grounding AI explanations in scientific evidence, and a feature range analysis technique to support deeper understanding of feature contributions. We conducted a mixed-methods study with 30 healthcare professionals and found that the conversational interactions helped healthcare professionals build a clear understanding of model assessments, while the integration of scientific evidence calibrated trust in the system's decisions. Most participants reported that the system supported both patient risk evaluation and recommendation.",
      "authors": [
        "Reza Samimi",
        "Aditya Bhattacharya",
        "Lucija Gosak",
        "Gregor Stiglic",
        "Katrien Verbert"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-25T14:56:20+00:00",
          "link": "https://arxiv.org/abs/2507.02920v1",
          "size": "1858kb",
          "version": "v1"
        }
      ],
      "title": "Visual-Conversational Interface for Evidence-Based Explanation of Diabetes Risk Prediction",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02920",
        "HTML": "https://arxiv.org/html/2507.02920v1",
        "PDF": "https://arxiv.org/pdf/2507.02920"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The focus is on explaining diabetes risk assessments and grounding AI explanations in scientific evidence, unrelated to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03032",
      "abstract": "Noncompliance with medication regimens poses an immense challenge in the management of chronic diseases, often resulting in exacerbated health complications and recurrent hospital admissions. Addressing this gap, our team designed an innovative mobile game aimed at bolstering medication adherence and information retention within the general population. Employing Amazon Mechanical Turk, participants were enlisted and allocated into two cohorts: one engaged with our mobile game and the other perused an informational pamphlet about medication. Both cohorts underwent a pre-intervention quiz, followed by their respective interventions, and concluded with a post-intervention quiz. Primary outcome measures included the difference in quiz scores and the game play duration. The investigation encompassed 243 participants with homogenous baseline attributes. Participants interacting with the mobile game depicted a significant enhancement in their post-intervention scores compared to the pre-intervention scores. We observed a notable correlation of 0.346 (p<0.001) with a robust medium effect size of 0.641 (0.503 - 0.779). Although the duration of game play and post-intervention scores didn't exhibit a direct correlation, a tendency towards superior post-intervention scores was evident among participants who dedicated more time to the game. The interactive mobile game we developed exhibits potential as an engaging instrument for empowering patients and caregivers. Providing critical medication information and the potential side effects in a manner that increases retention would thereby mitigate medication noncompliance. Future research endeavors should focus on optimizing and broadening the application of such mobile interfaces to fortify public health initiatives.",
      "authors": [
        "Don Roosan",
        "Tiffany Khao",
        "Huong Phan and Yan Li"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Other Quantitative Biology (q-bio.OT)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T01:17:20+00:00",
          "link": "https://arxiv.org/abs/2507.03032v1",
          "size": "421kb",
          "version": "v1"
        }
      ],
      "title": "Enhanced knowledge retention through MedScrab: an interactive mobile game",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03032",
        "PDF": "https://arxiv.org/pdf/2507.03032"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses an interactive game for better information retention, which could relate to creative engagement in learning, though not a core focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03147",
      "abstract": "Along with the explosion of large language models, improvements in speech synthesis, advancements in hardware, and the evolution of computer graphics, the current bottleneck in creating digital humans lies in generating character movements that correspond naturally to text or speech inputs.\n  In this work, we present DeepGesture, a diffusion-based gesture synthesis framework for generating expressive co-speech gestures conditioned on multimodal signals-text, speech, emotion, and seed motion. Built upon the DiffuseStyleGesture model, DeepGesture introduces novel architectural enhancements that improve semantic alignment and emotional expressiveness in generated gestures. Specifically, we integrate fast text transcriptions as semantic conditioning and implement emotion-guided classifier-free diffusion to support controllable gesture generation across affective states. A lightweight Transformer backbone combines full self-attention and cross-local attention for effective feature fusion of heterogeneous modalities. To visualize results, we implement a full rendering pipeline in Unity based on BVH output from the model. Evaluation on the ZeroEGGS dataset shows that DeepGesture produces gestures with improved human-likeness and contextual appropriateness, outperforming baselines on Mean Opinion Score and Frechet Gesture Distance metrics. Our system supports interpolation between emotional states and demonstrates generalization to out-of-distribution speech, including synthetic voices-marking a step forward toward fully multimodal, emotionally aware digital humans.",
      "authors": [
        "Thanh Hoang-Minh"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)",
        "Sound (cs.SD)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T20:04:04+00:00",
          "link": "https://arxiv.org/abs/2507.03147v1",
          "size": "17749kb",
          "version": "v1"
        }
      ],
      "title": "DeepGesture: A conversational gesture synthesis system based on emotions and semantics",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03147",
        "HTML": "https://arxiv.org/html/2507.03147v1",
        "PDF": "https://arxiv.org/pdf/2507.03147"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses gesture synthesis which can involve creative aspects. However, creativity is not a primary focus, but rather an implicit element in generating expressive gestures."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03170",
      "abstract": "ASCRIBE-XR, a novel computational platform designed to facilitate the visualization and exploration of 3D volumetric data and mesh data in the context of synchrotron experiments, is described. Using Godot and PC-VR technologies, the platform enables users to dynamically load and manipulate 3D data sets to gain deeper insights into their research. The program's multi-user capabilities, enabled through WebRTC, and MQTT, allow multiple users to share data and visualize together in real-time, promoting a more interactive and engaging research experience. We describe the design and implementation of ASCRIBE-XR, highlighting its key features and capabilities. We will also discuss its utility in the context of synchrotron research, including examples of its application and potential benefits for the scientific community.",
      "authors": [
        "Ronald J. Pandolfi",
        "Jeffrey J. Donatelli",
        "Julian Todd",
        "Daniela Ushizima"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Graphics (cs.GR)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T20:52:40+00:00",
          "link": "https://arxiv.org/abs/2507.03170v1",
          "size": "32994kb",
          "version": "v1"
        }
      ],
      "title": "ASCRIBE-XR: Virtual Reality for Visualization of Scientific Imagery",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03170",
        "HTML": "https://arxiv.org/html/2507.03170v1",
        "PDF": "https://arxiv.org/pdf/2507.03170"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "While the paper discusses virtual reality for scientific visualization, there is no mention of creativity as a focus area."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03243",
      "abstract": "Electric vehicles (EVs) charging infrastructure is directly related to the overall EV user experience and thus impacts the widespread adoption of EVs. Understanding key factors that affect EV users' charging experience is essential for building a robust and user-friendly EV charging infrastructure. This study leverages about $17,000$ charging station (CS) reviews on Google Maps to explore EV user preferences for charging stations, employing ChatGPT 4.0 for aspect-based sentiment analysis. We identify twelve key aspects influencing user satisfaction, ranging from accessibility and reliability to amenities and pricing. Two distinct preference models are developed: a micro-level model focused on individual user satisfaction and a macro-level model capturing collective sentiment towards specific charging stations. Both models utilize the LightGBM algorithm for user preference prediction, achieving strong performance compared to other machine learning approaches. To further elucidate the impact of each aspect on user ratings, we employ SHAP (SHapley Additive exPlanations), a game-theoretic approach for interpreting machine learning models. Our findings highlight the significant impact of positive sentiment towards \"amenities and location\", coupled with negative sentiment regarding \"reliability and maintenance\", on overall user satisfaction. These insights offer actionable guidance to charging station operators, policymakers, and EV manufacturers, empowering them to enhance user experience and foster wider EV adoption.",
      "authors": [
        "Zifei Wang",
        "Emmanuel Abolarin",
        "Kai Wu",
        "Venkatarao Rebba",
        "Jian Hu",
        "Zhen Hu",
        "Shan Bao and Feng Zhou"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-04T01:33:06+00:00",
          "link": "https://arxiv.org/abs/2507.03243v1",
          "size": "8072kb",
          "version": "v1"
        }
      ],
      "title": "Beyond Charging Anxiety: An Explainable Approach to Understanding User Preferences of EV Charging Stations Using Review Data",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03243",
        "HTML": "https://arxiv.org/html/2507.03243v1",
        "PDF": "https://arxiv.org/pdf/2507.03243"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on understanding user preferences for EV charging and is not related to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03286",
      "abstract": "We present Gaze and Glow, an interactive installation that reveals the often-invisible efforts of social media editing. Through narrative personas, experimental videos, and sensor-based interactions, the installation explores how audience attention shapes users' editing practices and emotional experiences. Deployed in a two-month public exhibition, Gaze and Glow engaged viewers and elicited responses. Reflexive thematic analysis of audience feedback highlights how making editing visible prompts new reflections on authenticity, agency, and performativity. We discuss implications for designing interactive systems that support selective memory, user-controlled visibility, and critical engagement with everyday digital self-presentation.",
      "authors": [
        "Yang Hong",
        "Jie-Yi Feng",
        "Yi-Chun Yao",
        "I-Hsuan Cho",
        "Yu-Ting Lin",
        "Ying-Yu Chen"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Multimedia (cs.MM)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-04T04:33:52+00:00",
          "link": "https://arxiv.org/abs/2507.03286v1",
          "size": "12128kb",
          "version": "v1"
        }
      ],
      "title": "Gaze and Glow: Exploring Editing Processes on Social Media through Interactive Exhibition",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03286",
        "HTML": "https://arxiv.org/html/2507.03286v1",
        "PDF": "https://arxiv.org/pdf/2507.03286"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper touches on creative aspects by exploring editing processes and self-presentation through an interactive exhibition. It prompts reflections on authenticity, which can relate to creativity, but creativity is not the primary focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03307",
      "abstract": "Human creative ideation involves both exploration of diverse ideas (divergence) and selective synthesis of explored ideas into coherent combinations (convergence). While processes of divergence and convergence are often interleaved and nested, existing AI-powered creativity support tools (CSTs) lack support for sophisticated orchestration of divergence and convergence. We present Reverger, an AI-powered CST that helps users ideate variations of conceptual directions for modifying a story by scaffolding flexible iteration between divergence and convergence. For divergence, our tool enables recursive exploration of alternative high-level directions for modifying a specific part of the original story. For convergence, it allows users to collect explored high-level directions and synthesize them into concrete variations. Users can then iterate between divergence and convergence until they find a satisfactory outcome. A within-subject study revealed that Reverger permitted participants to explore more unexpected and diverse high-level directions than a comparable baseline. Reverger users also felt that they had more fine-grained control and discovered more effort-worthy outcomes.",
      "authors": [
        "Taewook Kim",
        "Matthew Kay",
        "Yuqian Sun",
        "Melissa Roemmele",
        "Max Kreminski",
        "John Joon Young Chung"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-04T05:25:19+00:00",
          "link": "https://arxiv.org/abs/2507.03307v1",
          "size": "3242kb",
          "version": "v1"
        }
      ],
      "title": "Scaffolding Recursive Divergence and Convergence in Story Ideation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03307",
        "HTML": "https://arxiv.org/html/2507.03307v1",
        "PDF": "https://arxiv.org/pdf/2507.03307"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "This paper primarily focuses on creativity by addressing creative ideation through divergent and convergent thinking. It directly deals with supporting creativity in story ideation, making creativity a central theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03391",
      "abstract": "Video game designers often view confusion as undesirable, yet it is inevitable, as new players must adapt to new interfaces and mechanics in an increasingly varied and innovative game market, which is more popular than ever. Research suggests that confusion can contribute to a positive experience, potentially motivating players to learn. The state of confusion in video games should be further investigated to gain more insight into the learning experience of play and how it affects the player experience. In this article, we design a study to collect learning-related affects for users playing a game prototype that intentionally confuses the player. We assess the gathered affects against a complex learning model, affirming that, in specific instances, the player experience aligns with the learning experiences. Moreover, we identify correlations between these affects and the Player Experience Inventory constructs, particularly concerning flow experiences.",
      "authors": [
        "Thomas Vase Schultz Volden",
        "Oleg Jarma Montoya",
        "Paolo Burelli",
        "Marco Scirea"
      ],
      "license": "http://creativecommons.org/publicdomain/zero/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-04T08:50:08+00:00",
          "link": "https://arxiv.org/abs/2507.03391v1",
          "size": "257kb",
          "version": "v1"
        }
      ],
      "title": "On the dynamics of affective states during play and the role of confusion",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03391",
        "HTML": "https://arxiv.org/html/2507.03391v1",
        "PDF": "https://arxiv.org/pdf/2507.03391"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "While the paper is primarily about affective states and learning in video games, it does discuss creative learning experiences related to player confusion. However, creativity is a secondary aspect."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03520",
      "abstract": "Sleep is important for everyday functioning, overall well-being, and quality of life. Recent advances in wearable sensing technology have enabled continuous, noninvasive, and cost-effective monitoring of sleep patterns in real-world natural living settings. Wrist-worn devices, in particular, are capable of tracking sleep patterns using accelerometers and heart rate sensors. To support sleep research in naturalistic environments using wearable sensors, we introduce the TILES-2018 Sleep Benchmark dataset, which we make publicly available to the research community. This dataset was collected over a 10-week period from 139 hospital employees and includes over 6,000 unique sleep recordings, alongside self-reported survey data from each participant, which includes sleep quality, stress, and anxiety among other measurements. We present in-depth analyses of sleep patterns by combining the TILES-2018 Sleep Benchmark dataset with a previously released dataset (TILES-2018), which follows a similar study protocol. Our analyses include sleep duration, sleep stages, and sleep diaries. Moreover, we report machine learning benchmarks using this dataset as a testbed for tasks including sleep stage classification, prediction of self-reported sleep quality, and classifying demographics. Overall, this dataset provides a valuable resource for advancing foundational studies in sleep behavior modeling.",
      "authors": [
        "Tiantian Feng and Brandon M Booth and Karel Mundnich and Emily Zhou and Benjamin Girault and Kristina Lerman and Shrikanth Narayanan"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-04T12:16:52+00:00",
          "link": "https://arxiv.org/abs/2507.03520v1",
          "size": "2305kb",
          "version": "v1"
        }
      ],
      "title": "TILES-2018 Sleep Benchmark Dataset: A Longitudinal Wearable Sleep Data Set of Hospital Workers for Modeling and Understanding Sleep Behaviors",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03520",
        "HTML": "https://arxiv.org/html/2507.03520v1",
        "PDF": "https://arxiv.org/pdf/2507.03520"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper is focused on sleep behavior modeling and related data analysis. It does not address creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03670",
      "abstract": "Writing longer prompts for an AI assistant to generate a short story increases psychological ownership, a user's feeling that the writing belongs to them. To encourage users to write longer prompts, we evaluated two interaction techniques that modify the prompt entry interface of chat-based generative AI assistants: pressing and holding the prompt submission button, and continuously moving a slider up and down when submitting a short prompt. A within-subjects experiment investigated the effects of such techniques on prompt length and psychological ownership, and results showed that these techniques increased prompt length and led to higher psychological ownership than baseline techniques. A second experiment further augmented these techniques by showing AI-generated suggestions for how the prompts could be expanded. This further increased prompt length, but did not lead to improvements in psychological ownership. Our results show that simple interface modifications like these can elicit more writing from users and improve psychological ownership.",
      "authors": [
        "Nikhita Joshi",
        "Daniel Vogel"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-04T15:44:24+00:00",
          "link": "https://arxiv.org/abs/2507.03670v1",
          "size": "548kb",
          "version": "v1"
        }
      ],
      "title": "Interaction Techniques that Encourage Longer Prompts Can Improve Psychological Ownership when Writing with AI",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03670",
        "HTML": "https://arxiv.org/html/2507.03670v1",
        "PDF": "https://arxiv.org/pdf/2507.03670"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses how writing longer prompts for AI-generated short stories can enhance psychological ownership, involving creative task elements, but creativity itself isn't the main focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03797",
      "abstract": "This paper investigates the viability of Wave Field Synthesis (WFS) for enhancing auditory immersion in VR-based cognitive research. While Virtual Reality (VR) offers significant advantages for studying human perception and behavior, auditory cues are often underutilized. WFS, an advanced audio rendering technique, can create highly realistic and spatially accurate soundscapes, potentially increasing ecological validity. This study evaluates WFS by implementing a sample experiment where participants localize static and moving sound sources in both a WFS-rendered environment and a conventional stereo headphone setup. The research explores the impact of virtual environments, sound types, and durations on localization accuracy and search behavior. Findings indicate that while stereo setups can achieve higher accuracy, WFS provides a more natural and intuitive auditory experience, particularly for directional cues. The study also highlights limitations of current WFS systems, such as the lack of height localization, occlusion simulation, and user-dependent optimization, which affect performance, especially for centrally located sound sources. Despite these challenges, WFS shows promise for specialized auditory perception research, particularly for complex soundscapes where directional information is paramount.",
      "authors": [
        "Benjamin Kahl"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Multimedia (cs.MM)",
        "Sound (cs.SD)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-04T20:30:51+00:00",
          "link": "https://arxiv.org/abs/2507.03797v1",
          "size": "38346kb",
          "version": "v1"
        }
      ],
      "title": "Assessing the Viability of Wave Field Synthesis in VR-Based Cognitive Research",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03797",
        "HTML": "https://arxiv.org/html/2507.03797v1",
        "PDF": "https://arxiv.org/pdf/2507.03797"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper addresses auditory immersion in VR environments using Wave Field Synthesis, which is unrelated to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03892",
      "abstract": "Since its viral emergence in early 2024, Comment Robert-a Weibo-launched social chatbot-has gained widespread attention on the Chinese Internet for its unsolicited and unpredictable comments on user posts. Unlike conventional chatbots that respond only to user prompts, Robert autonomously intervenes in public discourse, representing a novel form of AI-driven social media engagement. This study examines how such autonomous, algorithmic communication reshapes human-AI interaction in everyday online contexts. Using computational linguistics techniques, including topic classification and sentiment analysis, we analyze over 3,900 user-submitted interactions from the \"Robert Victims Alliance\", a grassroots community documenting their exchanges with the chatbot. Topic modeling reveals six key themes: interpersonal relationships, self-identity, academic and career concerns, subcultures, sensitive topics, and social events. Complementing this, mixed-methods emotional analysis uncovers a complex affective spectrum: Robert's casual remarks can evoke warmth and humor but may also conceal covert hostility beneath neutral or polite language. These ambivalent interactions reveal an emerging emotional divide between humans and socially proactive AI, suggesting that while Robert simulates social presence, it often falls short of users' emotional needs. Our study contributes to human-AI interaction research by offering new insights into the affective dynamics and socio-technical implications of unsolicited AI bots' participation in digital public spheres.",
      "authors": [
        "Nuo Chen",
        "Pu Yan",
        "Jia Li",
        "and Qixuan Zhao"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-05T04:13:24+00:00",
          "link": "https://arxiv.org/abs/2507.03892v1",
          "size": "3708kb",
          "version": "v1"
        }
      ],
      "title": "Is AI mingling or bullying me? Exploring User Interactions with a Chatbot in China",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03892",
        "PDF": "https://arxiv.org/pdf/2507.03892"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper analyzes user interactions with a social chatbot in terms of emotional dynamics and socio-technical implications, but does not focus on creativity as a topic."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03902",
      "abstract": "Video conferencing has become a central part of our daily lives, thanks to the COVID-19 pandemic. Unfortunately, so have its many limitations, resulting in poor support for communicative and social behavior and ultimately, Zoom fatigue. New technologies will be required to address these limitations, including many drawn from mixed reality (XR). In this paper, our goals are to equip and encourage future researchers to develop and test such technologies. Toward this end, we first survey research on the shortcomings of video conferencing systems, as defined before and after the pandemic. We then consider the methods that research uses to evaluate support for communicative behavior, and argue that those same methods should be employed in identifying, improving, and validating promising video conferencing technologies. Next, we survey emerging XR solutions to video conferencing's limitations, most off which do not employ head-mounted displays.",
      "authors": [
        "Dani Paul Hove and Benjamin Watson"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Multimedia (cs.MM)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-05T04:53:50+00:00",
          "link": "https://arxiv.org/abs/2507.03902v1",
          "size": "705kb",
          "version": "v1"
        }
      ],
      "title": "The shortcomings of video conferencing technology, methods for revealing them, and emerging XR solutions",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03902",
        "PDF": "https://arxiv.org/pdf/2507.03902"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses the limitations of video conferencing technology and emerging XR solutions. It does not address creativity directly or as a secondary theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03942",
      "abstract": "Makeup plays a vital role in self-expression, identity, and confidence - yet remains an underexplored domain for assistive technology, especially for people with vision impairments. While existing tools support isolated tasks such as color identification or product labeling, they rarely address the procedural complexity of makeup routines: coordinating step sequences, managing product placement, and assessing the final look with accessible feedback. To understand the real-world process, we conducted a contextual inquiry with 15 visually impaired makeup users, capturing real-time makeup application behaviors and their step-by-step information needs and assessment approaches. Our findings reveal embodied, tactile-first strategies; persistent challenges in blending, symmetry, and assessment; and a desire for honest, real-time, goal-aligned feedback. We also interviewed five professional makeup artists, who reviewed participant makeup videos and provided expert responses to participant-raised questions and assessment practices. We contribute a taxonomy of feedback needs in non-visual makeup, and outline design implications for future assistive systems - emphasizing hands-free, conversational interaction and context-aware, procedural support for expressive and independent beauty practices.",
      "authors": [
        "Franklin Mingzhe Li",
        "Akihiko Oharazawa",
        "Chloe Qingyu Zhu",
        "Misty Fan",
        "Daisuke Sato",
        "Chieko Asakawa",
        "Patrick Carrington"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-05T08:00:38+00:00",
          "link": "https://arxiv.org/abs/2507.03942v1",
          "size": "3415kb",
          "version": "v1"
        }
      ],
      "title": "More than One Step at a Time: Designing Procedural Feedback for Non-visual Makeup Routines",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03942",
        "HTML": "https://arxiv.org/html/2507.03942v1",
        "PDF": "https://arxiv.org/pdf/2507.03942"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper explores assistive technology for non-visual makeup routines, which involves expressive tasks such as self-expression and identity. While not directly about creativity, it includes design implications for expressive beauty practices."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04005",
      "abstract": "The execution of effective and imperceptible personality assessments is receiving increasing attention in psychology and human-computer interaction fields. This study explores an interactive approach for personality assessment, focusing on the multiplicity of personality representation. We propose a framework of gamified personality assessment through multi-personality representations (Multi-PR GPA). The framework leverages Large Language Models to empower virtual agents with diverse personalities. These agents elicit multifaceted human personality representations through engaging in interactive games. Drawing upon the multi-type textual data generated throughout the interaction, it achieves two ways of personality assessments (i.e., Direct Assessment and Que-based Assessment) and provides interpretable insights. Grounded in the classic Big Five theory, we implemented a prototype system and conducted a user study to assess the efficacy of Multi-PR GPA. The results underscore the effectiveness of our approach in personality assessment and demonstrate that it achieves superior performance when considering the multiplicity of personality representation.",
      "authors": [
        "Baiqiao Zhang",
        "Xiangxian Li",
        "Chao Zhou",
        "Xinyu Gai",
        "Zhifeng Liao",
        "Juan Liu",
        "Xue Yang",
        "Niqi Liu",
        "Xiaojuan Ma",
        "Yong-jin Liu",
        "Yulong Bian"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-05T11:17:20+00:00",
          "link": "https://arxiv.org/abs/2507.04005v1",
          "size": "7953kb",
          "version": "v1"
        }
      ],
      "title": "Exploring a Gamified Personality Assessment Method through Interaction with Multi-Personality LLM Agents",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04005",
        "PDF": "https://arxiv.org/pdf/2507.04005"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses a gamified personality assessment method using multi-personality LLM agents, focusing on personality and human-computer interaction rather than creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04043",
      "abstract": "As large language models (LLMs) become more common in educational tools and programming environments, questions arise about how these systems should interact with users. This study investigates how different interaction styles with ChatGPT-4o (passive, proactive, and collaborative) affect user performance on simple programming tasks. I conducted a within-subjects experiment where fifteen high school students participated, completing three problems under three distinct versions of the model. Each version was designed to represent a specific style of AI support: responding only when asked, offering suggestions automatically, or engaging the user in back-and-forth dialogue.Quantitative analysis revealed that the collaborative interaction style significantly improved task completion time compared to the passive and proactive conditions. Participants also reported higher satisfaction and perceived helpfulness when working with the collaborative version. These findings suggest that the way an LLM communicates, how it guides, prompts, and responds, can meaningfully impact learning and performance. This research highlights the importance of designing LLMs that go beyond functional correctness to support more interactive, adaptive, and user-centered experiences, especially for novice programmers.",
      "authors": [
        "Kai Deng"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-05T13:52:31+00:00",
          "link": "https://arxiv.org/abs/2507.04043v1",
          "size": "306kb",
          "version": "v1"
        }
      ],
      "title": "Evaluating the Effectiveness of Large Language Models in Solving Simple Programming Tasks: A User-Centered Study",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04043",
        "HTML": "https://arxiv.org/html/2507.04043v1",
        "PDF": "https://arxiv.org/pdf/2507.04043"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper explores user-centered interactions with LLMs, which indirectly supports creativity in learning and programming but doesn't primarily focus on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04095",
      "abstract": "Modern social robots can be considered the descendants of steam engines from the First Industrial Revolution (IR 1.0) and industrial robotic arms from the Third Industrial Revolution (IR 3.0). As some time has passed since the introduction of these robots during the Fourth Industrial Revolution (IR 4.0), challenges and issues in their interaction with humans have emerged, leading researchers to conclude that, like any other AI-based technology, these robots must also be human-centered to meet the needs of their users. This chapter aims to introduce humans and their needs in interactions with robots, ranging from short-term, one-on-one interactions (micro-level) to long-term, macro-level needs at the societal scale. Building upon the principles of human-centered AI, this chapter presents, for the first time, a new framework of human needs called the Dual Pyramid. This framework encompasses a comprehensive list of human needs in robot interactions, from the most fundamental, robot effectiveness to macro level requirements, such as the collaboration with robots in achieving the United Nations 17 Sustainable Development Goals.",
      "authors": [
        "Alireza Mortezapour",
        "Giuliana Vitiello"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-05T16:45:03+00:00",
          "link": "https://arxiv.org/abs/2507.04095v1",
          "size": "1109kb",
          "version": "v1"
        }
      ],
      "title": "Human-centered AI with focus on Human-robot interaction (Book chapter)",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04095",
        "PDF": "https://arxiv.org/pdf/2507.04095"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses human-robot interaction focusing on meeting human needs and sustainability goals, with no specific mention of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04160",
      "abstract": "This paper introduces HyperSumm-RL, a hypertext-aware summarization and interaction analysis framework designed to investigate human perceptions of social robot leadership through long-form dialogue. The system utilizes a structured Natural Language Processing (NLP) workflow that combines transformer-based long dialogue summarization, leadership style modeling, and user response analysis, enabling scalable evaluation of social robots in complex human-robot interaction (HRI) settings. Unlike prior work that focuses on static or task-oriented HRI, HyperSumm-RL captures and hypertextually organizes dynamic conversational exchanges into navigable, semantically rich representations which allows researchers to trace interaction threads, identify influence cues, and analyze leadership framing over time. The contributions of this study are threefold: (1) we present a novel infrastructure for summarizing and linking long, multi-turn dialogues using leadership-style taxonomies; (2) we propose an interactive hypertext model that supports relational navigation across conversational themes, participant responses, and robot behavior modes; and (3) we demonstrate the utility of this system in interpreting participant trust, engagement, and expectation shifts during social robot leadership scenarios. The findings reveal how hypertextual workflows can augment HRI research by enabling transparent, interpretable, and semantically grounded analysis of emergent social dynamics.",
      "authors": [
        "Subasish Das"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-05T20:53:11+00:00",
          "link": "https://arxiv.org/abs/2507.04160v1",
          "size": "123kb",
          "version": "v1"
        }
      ],
      "title": "HyperSumm-RL: A Dialogue Summarization Framework for Modeling Leadership Perception in Social Robots",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04160",
        "HTML": "https://arxiv.org/html/2507.04160v1",
        "PDF": "https://arxiv.org/pdf/2507.04160"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on leadership styles and perceptions in HRI, with no specific link to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04162",
      "abstract": "Breathing is a spontaneous but controllable body function that can be used for hands-free interaction. Our work introduces \"iBreath\", a novel system to detect breathing gestures similar to clicks using bio-impedance. We evaluated iBreath's accuracy and user experience using two lab studies (n=34). Our results show high detection accuracy (F1-scores > 95.2%). Furthermore, the users found the gestures easy to use and comfortable. Thus, we developed eight practical guidelines for the future development of breathing gestures. For example, designers can train users on new gestures within just 50 seconds (five trials), and achieve robust performance with both user-dependent and user-independent models trained on data from 21 participants, each yielding accuracies above 90%. Users preferred single clicks and disliked triple clicks. The median gesture duration is 3.5-5.3 seconds. Our work provides solid ground for researchers to experiment with creating breathing gestures and interactions.",
      "authors": [
        "Mengxi Liu",
        "Daniel Gei{\\ss}ler",
        "Deepika Gurung",
        "Hymalai Bello",
        "Bo Zhou",
        "Sizhen Bian",
        "Paul Lukowicz",
        "Passant Elagroudy"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-05T21:07:53+00:00",
          "link": "https://arxiv.org/abs/2507.04162v1",
          "size": "3333kb",
          "version": "v1"
        }
      ],
      "title": "iBreath: Usage Of Breathing Gestures as Means of Interactions",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04162",
        "HTML": "https://arxiv.org/html/2507.04162v1",
        "PDF": "https://arxiv.org/pdf/2507.04162"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper introduces a new interaction method using breathing gestures, which does not address creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04236",
      "abstract": "Annotations are central to effective data communication, yet most visualization tools treat them as secondary constructs -- manually defined, difficult to reuse, and loosely coupled to the underlying visualization grammar. We propose a declarative extension to Wilkinson's Grammar of Graphics that reifies annotations as first-class design elements, enabling structured specification of annotation targets, types, and positioning strategies. To demonstrate the utility of our approach, we develop a prototype extension called Vega-Lite Annotation. Through comparison with eight existing tools, we show that our approach enhances expressiveness, reduces authoring effort, and enables portable, semantically integrated annotation workflows.",
      "authors": [
        "Md Dilshadur Rahman",
        "Md Rahat-uz- Zaman",
        "Andrew McNutt",
        "Paul Rosen"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Graphics (cs.GR)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-06T03:57:41+00:00",
          "link": "https://arxiv.org/abs/2507.04236v1",
          "size": "1894kb",
          "version": "v1"
        }
      ],
      "title": "AnnoGram: An Annotative Grammar of Graphics Extension",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04236",
        "HTML": "https://arxiv.org/html/2507.04236v1",
        "PDF": "https://arxiv.org/pdf/2507.04236"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "While the paper primarily discusses data annotation within graphics, it touches on creativity as a secondary theme by enhancing the expressiveness and reusability of annotations in visualization tools."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04238",
      "abstract": "The rise of wearable smart devices raises unprecedented opportunities for self-improvement through ubiquitous behavior tracking and guidance. However, the design of effective wearable behavior intervention systems remains relatively unexplored. To address this gap, we conducted controlled studies focusing on the reduction of unwanted words (e.g., filler words, swear words) in daily communication through auditory feedback using wearable technology. We started with a design space exploration, considering various factors such as the type, duration, and timing of the auditory feedback. Then, we conducted pilot studies to reduce the space of design choices and prototyped a system called WSCoach (Wearable Speech Coach), which informs users when they utter unwanted words in near-real-time. To evaluate WSCoach, we compared it with a state-of-the-art mobile application supporting post-hoc conversation analysis. Both approaches were effective in reducing the occurrence of unwanted words, but WSCoach appears to be more effective in the long run. Finally, we discuss guidelines for the design of wearable audio-based behavior monitoring and intervention systems and highlight the potential of wearable technology for facilitating behavior correction and improvement. For supplementary material, please see the META Appendix and our OSF project at https://osf.io/6vhwn/?view_only=489498d3ac2d4703a17475fc6ca65dfa.",
      "authors": [
        "Zhang Youpeng",
        "Nuwan Janaka",
        "Ashwin Ram",
        "Yin Peilin",
        "Tian Yang",
        "Shengdong Zhao",
        "Pierre Dragicevic"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Sound (cs.SD)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-06T04:00:59+00:00",
          "link": "https://arxiv.org/abs/2507.04238v1",
          "size": "1456kb",
          "version": "v1"
        }
      ],
      "title": "WSCoach: Wearable Real-time Auditory Feedback for Reducing Unwanted Words in Daily Communication",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04238",
        "HTML": "https://arxiv.org/html/2507.04238v1",
        "PDF": "https://arxiv.org/pdf/2507.04238"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper is centered on behavior modification through wearable technology for speech improvement, with no mention or analysis of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04241",
      "abstract": "Visually impaired individuals often require a guide runner to safely participate in outdoor running. However, maintaining synchronized pacing with verbal cues or tethers can be mentally taxing and physically restrictive. Existing solutions primarily focus on navigation or obstacle avoidance but overlook the importance of real-time interpersonal rhythm coordination during running. We introduce RunPacer, a smartwatch-based vibrotactile feedback system that delivers synchronized rhythmic pulses to both runners. In contrast to conventional guide-running systems that rely heavily on continuous verbal communication or mechanical tethering, RunPacer emphasizes interpersonal cadence alignment as its core interaction model. By pre-setting a target step frequency or dynamically adapting to the guide's natural pace, the system ensures that both runners receive identical haptic cues, enabling them to maintain coordinated motion intuitively and efficiently. This poster presents the system architecture, positions it within prior research on haptic entrainment, and outlines the vision for future field deployment, including potential multimodal feedback extensions. RunPacer contributes a lightweight, socially cooperative, and non-visual assistive framework that reimagines co-running as a shared, embodied, and accessible experience.",
      "authors": [
        "Yichen Yu",
        "Huan-Song Xu"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-06T04:23:11+00:00",
          "link": "https://arxiv.org/abs/2507.04241v1",
          "size": "530kb",
          "version": "v1"
        }
      ],
      "title": "RunPacer: A Smartwatch-Based Vibrotactile Feedback System for Symmetric Co-Running by Visually Impaired Individuals and Guides",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04241",
        "HTML": "https://arxiv.org/html/2507.04241v1",
        "PDF": "https://arxiv.org/pdf/2507.04241"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper discusses the development of a vibrotactile feedback system for co-running. Its core focus is assisting visually impaired individuals with navigation, not creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04278",
      "abstract": "Descriptive Multimodal Emotion Recognition (DMER) is a newly proposed task that aims to describe a person's emotional state using free-form natural language. Unlike traditional discriminative methods that rely on predefined emotion taxonomies, DMER provides greater flexibility in emotional expression, enabling fine-grained and interpretable emotion representations. However, this free-form prediction paradigm introduces significant challenges in evaluation. Existing methods either depend on ground-truth descriptions that require substantial manual effort or simplify the task by shifting the focus from evaluating descriptions to evaluating emotion labels. However, the former suffers from the labor-intensive collection of comprehensive descriptions, while the latter overlooks critical aspects such as emotional temporal dynamics, intensity, and uncertainty. To address these limitations, we propose DMER-Ranker, a novel evaluation strategy that reformulates the traditional ``prediction-ground truth'' comparison into the ``prediction-prediction'' comparison, eliminating the need for ground-truth descriptions. We then employ the Bradley-Terry algorithm to convert pairwise comparison results into model-level rankings. Additionally, we explore the possibility of automatic preference prediction and introduce DMER-Preference, the first preference dataset specifically designed for human emotions. Our work advances the field of DMER and lays the foundation for more intelligent human-computer interaction systems.",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Haoyu Chen",
        "Zebang Cheng",
        "Fan Zhang",
        "Ziyu Jia",
        "Ziyang Ma",
        "Fei Ma",
        "Xiaojiang Peng",
        "Jianhua Tao"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-06T07:37:59+00:00",
          "link": "https://arxiv.org/abs/2507.04278v1",
          "size": "10398kb",
          "version": "v1"
        }
      ],
      "title": "DMER-Ranker: Learning to Rank Emotion Descriptions in the Absence of Ground Truth",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04278",
        "HTML": "https://arxiv.org/html/2507.04278v1",
        "PDF": "https://arxiv.org/pdf/2507.04278"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper deals with emotion description, which involves aspects of interpretability and expression, indirectly related to creativity by facilitating expressiveness in emotional representation."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04398",
      "abstract": "Academic writing increasingly involves multimodal tasks requiring students to integrate visual information and textual arguments. While generative AI (GenAI) tools, like ChatGPT, offer new pathways for supporting academic writing, little is known about how students' GenAI literacy influences their independent multimodal writing skills or how chatbot interaction strategies (passive reactive vs. proactive scaffolding) impact learning. This study examined 79 higher education students' multimodal academic writing performance using a comparative research design. Students completed writing tasks integrating visual data under two chatbot-assisted conditions (passive vs. proactive) and subsequently without AI assistance. Their writing performance was rigorously evaluated across five dimensions, including insightfulness, visual data integration, organisation, linguistic quality, and critical thinking. Ordinal logistic regression and correlation analyses revealed that higher levels of GenAI literacy significantly predicted stronger independent multimodal writing performance immediately after AI assistance removal, particularly for students using passive chatbots requiring active prompting. These results highlight the critical role of GenAI literacy and specific chatbot interaction strategies in shaping students' capacities for independent multimodal academic writing. Our findings emphasise the need for purposeful integration of GenAI literacy training into curricula and balancing external scaffolding support with autonomous learning opportunities. This research offers valuable recommendations for educators leveraging AI-enhanced pedagogies to optimise student writing outcomes and technological engagement strategies.",
      "authors": [
        "Yueqiao Jin",
        "Kaixun Yang",
        "Roberto Martinez-Maldonado",
        "Dragan Ga\\v{s}evi\\'c",
        "Lixiang Yan"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-06T14:01:06+00:00",
          "link": "https://arxiv.org/abs/2507.04398v1",
          "size": "13199kb",
          "version": "v1"
        }
      ],
      "title": "Do Students Write Better Post-AI Support? Effects of Generative AI Literacy and Chatbot Interaction Strategies on Multimodal Academic Writing",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04398",
        "HTML": "https://arxiv.org/html/2507.04398v1",
        "PDF": "https://arxiv.org/pdf/2507.04398"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "This paper explores generative AI applications in academic writing, which may involve aspects of creative expression and innovation in writing strategies as secondary themes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04454",
      "abstract": "Collaborative Problem-Solving (CPS) markers capture key aspects of effective teamwork, such as staying on task, avoiding interruptions, and generating constructive ideas. An AI system that reliably detects these markers could help teachers identify when a group is struggling or demonstrating productive collaboration. Such a system requires an automated pipeline composed of multiple components. In this work, we evaluate how CPS detection is impacted by automating two critical components: transcription and speech segmentation. On the public Weights Task Dataset (WTD), we find CPS detection performance with automated transcription and segmentation methods is comparable to human-segmented and manually transcribed data; however, we find the automated segmentation methods reduces the number of utterances by 26.5%, impacting the the granularity of the data. We discuss the implications for developing AI-driven tools that support collaborative learning in classrooms.",
      "authors": [
        "Videep Venkatesha",
        "Mariah Bradford",
        "Nathaniel Blanchard"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-06T16:25:18+00:00",
          "link": "https://arxiv.org/abs/2507.04454v1",
          "size": "209kb",
          "version": "v1"
        }
      ],
      "title": "Dude, where's my utterance? Evaluating the effects of automatic segmentation and transcription on CPS detection",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04454",
        "HTML": "https://arxiv.org/html/2507.04454v1",
        "PDF": "https://arxiv.org/pdf/2507.04454"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "Collaborative Problem Solving (CPS) includes creative idea generation as a component, but the main focus is on CPS detection rather than creativity explicitly."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04469",
      "abstract": "This systematic literature review examines the role of large language models (LLMs) in UI/UX design, synthesizing findings from 38 peer-reviewed studies published between 2022 and 2025. We identify key LLMs in use, including GPT-4, Gemini, and PaLM, and map their integration across the design lifecycle, from ideation to evaluation. Common practices include prompt engineering, human-in-the-loop workflows, and multimodal input. While LLMs are reshaping design processes, challenges such as hallucination, prompt instability, and limited explainability persist. Our findings highlight LLMs as emerging collaborators in design, and we propose directions for the ethical, inclusive, and effective integration of these technologies.",
      "authors": [
        "Ammar Ahmed",
        "Ali Shariq Imran"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-06T17:18:05+00:00",
          "link": "https://arxiv.org/abs/2507.04469v1",
          "size": "2224kb",
          "version": "v1"
        }
      ],
      "title": "The role of large language models in UI/UX design: A systematic literature review",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04469",
        "HTML": "https://arxiv.org/html/2507.04469v1",
        "PDF": "https://arxiv.org/pdf/2507.04469"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses the integration of large language models in the UI/UX design process, specifically mentioning their role in ideation, which relates to creativity. However, creativity is not the primary focus; it is considered a secondary theme in the application of LLMs to design."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04491",
      "abstract": "Large language models (LLMs) are rapidly being integrated into psychological research as research tools, evaluation targets, human simulators, and cognitive models. However, recent evidence reveals severe measurement unreliability: Personality assessments collapse under factor analysis, moral preferences reverse with punctuation changes, and theory-of-mind accuracy varies widely with trivial rephrasing. These \"measurement phantoms\"--statistical artifacts masquerading as psychological phenomena--threaten the validity of a growing body of research. Guided by the dual-validity framework that integrates psychometrics with causal inference, we present a six-stage workflow that scales validity requirements to research ambition--using LLMs to code text requires basic reliability and accuracy, while claims about psychological properties demand comprehensive construct validation. Researchers must (1) explicitly define their research goal and corresponding validity requirements, (2) develop and validate computational instruments through psychometric testing, (3) design experiments that control for computational confounds, (4) execute protocols with transparency, (5) analyze data using methods appropriate for non-independent observations, and (6) report findings within demonstrated boundaries and use results to refine theory. We illustrate the workflow through an example of model evaluation--\"LLM selfhood\"--showing how systematic validation can distinguish genuine computational phenomena from measurement artifacts. By establishing validated computational instruments and transparent practices, this workflow provides a path toward building a robust empirical foundation for AI psychology research.",
      "authors": [
        "Zhicheng Lin"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-06T18:06:12+00:00",
          "link": "https://arxiv.org/abs/2507.04491v1",
          "size": "442kb",
          "version": "v1"
        }
      ],
      "title": "A validity-guided workflow for robust large language model research in psychology",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04491",
        "PDF": "https://arxiv.org/pdf/2507.04491"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on the validity and reliability of large language models in psychological research. There is no specific mention or direct investigation of creativity. It is centered on psychometric testing and research methodology rather than creative processes or outcomes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02877",
      "abstract": "Circular genome visualizations are essential for exploring structural variants and gene regulation. However, existing tools often require complex scripting and manual configuration, making the process time-consuming, error-prone, and difficult to learn. To address these challenges, we introduce AuraGenome, an LLM-powered framework for rapid, reusable, and scalable generation of multi-layered circular genome visualizations. AuraGenome combines a semantic-driven multi-agent workflow with an interactive visual analytics system. The workflow employs seven specialized LLM-driven agents, each assigned distinct roles such as intent recognition, layout planning, and code generation, to transform raw genomic data into tailored visualizations. The system supports multiple coordinated views tailored for genomic data, offering ring, radial, and chord-based layouts to represent multi-layered circular genome visualizations. In addition to enabling interactions and configuration reuse, the system supports real-time refinement and high-quality report export. We validate its effectiveness through two case studies and a comprehensive user study. AuraGenome is available at: https://github.com/Darius18/AuraGenome.",
      "authors": [
        "Chi Zhang",
        "Yu Dong",
        "Yang Wang",
        "Yuetong Han",
        "Guihua Shan and Bixia Tang"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Genomics (q-bio.GN)",
        "Artificial Intelligence (cs.AI)",
        "Graphics (cs.GR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-18T03:29:30+00:00",
          "link": "https://arxiv.org/abs/2507.02877v1",
          "size": "13133kb",
          "version": "v1"
        }
      ],
      "title": "AuraGenome: An LLM-Powered Framework for On-the-Fly Reusable and Scalable Circular Genome Visualizations",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02877",
        "HTML": "https://arxiv.org/html/2507.02877v1",
        "PDF": "https://arxiv.org/pdf/2507.02877"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses a framework for genome visualizations using LLMs, with no indication of creativity in its objectives or applications."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02900",
      "abstract": "Talking Head Generation (THG) has emerged as a transformative technology in computer vision, enabling the synthesis of realistic human faces synchronized with image, audio, text, or video inputs. This paper provides a comprehensive review of methodologies and frameworks for talking head generation, categorizing approaches into 2D--based, 3D--based, Neural Radiance Fields (NeRF)--based, diffusion--based, parameter-driven techniques and many other techniques. It evaluates algorithms, datasets, and evaluation metrics while highlighting advancements in perceptual realism and technical efficiency critical for applications such as digital avatars, video dubbing, ultra-low bitrate video conferencing, and online education. The study identifies challenges such as reliance on pre--trained models, extreme pose handling, multilingual synthesis, and temporal consistency. Future directions include modular architectures, multilingual datasets, hybrid models blending pre--trained and task-specific layers, and innovative loss functions. By synthesizing existing research and exploring emerging trends, this paper aims to provide actionable insights for researchers and practitioners in the field of talking head generation. For the complete survey, code, and curated resource list, visit our GitHub repository: https://github.com/VineetKumarRakesh/thg.",
      "authors": [
        "Vineet Kumar Rakesh",
        "Soumya Mazumdar",
        "Research Pratim Maity",
        "Sarbajit Pal",
        "Amitabha Das",
        "Tapas Samanta"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Graphics (cs.GR)",
        "Human-Computer Interaction (cs.HC)",
        "Multimedia (cs.MM)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-23T06:49:42+00:00",
          "link": "https://arxiv.org/abs/2507.02900v1",
          "size": "557kb",
          "version": "v1"
        }
      ],
      "title": "Advancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functions",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02900",
        "HTML": "https://arxiv.org/html/2507.02900v1",
        "PDF": "https://arxiv.org/pdf/2507.02900"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The survey addresses talking head generation technology, focusing on methodologies and technical aspects without any mention of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02913",
      "abstract": "Many e-learning platforms assert their ability or potential to improve students' self-regulated learning (SRL), however the cyclical and undirected nature of SRL theoretical models represent significant challenges for representation within contemporary machine learning frameworks. We apply SRL-informed features to trace data in order to advance modelling of students' SRL activities, to improve predictability and explainability regarding the causal effects of learning in an eLearning environment. We demonstrate that these features improve predictive accuracy and validate the value of further research into cyclic modelling techniques for SRL.",
      "authors": [
        "Andrew Schwabe",
        "\\\"Ozg\\\"ur Akg\\\"un",
        "Ella Haig"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-25T04:47:53+00:00",
          "link": "https://arxiv.org/abs/2507.02913v1",
          "size": "771kb",
          "version": "v1"
        }
      ],
      "title": "Toward Cyclic A.I. Modelling of Self-Regulated Learning: A Case Study with E-Learning Trace Data",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02913",
        "HTML": "https://arxiv.org/html/2507.02913v1",
        "PDF": "https://arxiv.org/pdf/2507.02913"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper investigates e-learning and self-regulated learning modeling without any direct reference to creativity or related enhancement."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02922",
      "abstract": "Machine learning enables the extraction of useful information from large, diverse datasets. However, despite many successful applications, machine learning continues to suffer from performance and transparency issues. These challenges can be partially attributed to the limited use of domain knowledge by machine learning models. This research proposes using the domain knowledge represented in conceptual models to improve the preparation of the data used to train machine learning models. We develop and demonstrate a method, called the Conceptual Modeling for Machine Learning (CMML), which is comprised of guidelines for data preparation in machine learning and based on conceptual modeling constructs and principles. To assess the impact of CMML on machine learning outcomes, we first applied it to two real-world problems to evaluate its impact on model performance. We then solicited an assessment by data scientists on the applicability of the method. These results demonstrate the value of CMML for improving machine learning outcomes.",
      "authors": [
        "V.C. Storey",
        "J. Parsons",
        "A. Castellanos",
        "M. Tremblay",
        "R. Lukyanenko",
        "W. Maass",
        "A. Castillo"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-25T15:34:55+00:00",
          "link": "https://arxiv.org/abs/2507.02922v1",
          "size": "1143kb",
          "version": "v1"
        }
      ],
      "title": "Domain Knowledge in Artificial Intelligence: Using Conceptual Modeling to Increase Machine Learning Accuracy and Explainability",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02922",
        "PDF": "https://arxiv.org/pdf/2507.02922"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses using domain knowledge to improve machine learning performance and explainability, not creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02950",
      "abstract": "This study provides the first comprehensive evaluation of large language model (LLM) performance across three counseling roles in Japanese-language therapeutic contexts. We simultaneously assessed counselor artificial intelligence (AI) systems (GPT-4-turbo with zeroshot prompting or Structured Multi-step Dialogue Prompts (SMDP), Claude-3-Opus-SMDP), client AI simulations, and evaluation AI systems (o3, Claude-3.7-Sonnet, Gemini-2.5-pro). Human experts (n = 15) with extensive counseling experience evaluated AI-generated dialogues using the Motivational Interviewing Treatment Integrity (MITI) Coding Manual 4.2.1.\n  Notably, SMDP implementation significantly enhanced counselor AI performance across all MITI global ratings compared with zeroshot prompting, with no significant differences between GPT-SMDP and Opus-SMDP. Evaluation AIs showed comparable performance to human raters for Cultivating Change Talk but systematically overestimated Softening Sustain Talk and the overall quality metrics. Model-specific biases emerged: Gemini emphasized power-sharing, o3 focused on technical proficiency, and Sonnet prioritized emotional expression. Client AI simulations exhibited a limited emotional range and unnaturally high compliance, indicating the need for enhanced realism.\n  These findings establish benchmarks for AI-assisted counseling in non-English contexts and identify critical areas for improvement through advanced prompt engineering, retrieval-augmented generation, and targeted fine-tuning, with important implications for developing culturally sensitive AI mental health tools.",
      "authors": [
        "Keita Kiuchi",
        "Yoshikazu Fujimoto",
        "Hideyuki Goto",
        "Tomonori Hosokawa",
        "Makoto Nishimura",
        "Yosuke Sato and Izumi Sezai"
      ],
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-28T21:50:29+00:00",
          "link": "https://arxiv.org/abs/2507.02950v1",
          "size": "48kb",
          "version": "v1"
        }
      ],
      "title": "Evaluating AI Counseling in Japanese: Counselor, Client, and Evaluator Roles Assessed by Motivational Interviewing Criteria",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02950",
        "HTML": "https://arxiv.org/html/2507.02950v1",
        "PDF": "https://arxiv.org/pdf/2507.02950"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "While it covers AI counseling and motivational interviewing, creativity is not mentioned or implied as part of the study."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03049",
      "abstract": "In the field of Human-Robot Interaction (HRI), a fundamental challenge is to facilitate human understanding of robots. The emerging domain of eXplainable HRI (XHRI) investigates methods to generate explanations and evaluate their impact on human-robot interactions. Previous works have highlighted the need to personalise the level of detail of these explanations to enhance usability and comprehension. Our paper presents a framework designed to update and retrieve user knowledge-memory models, allowing for adapting the explanations' level of detail while referencing previously acquired concepts. Three architectures based on our proposed framework that use Large Language Models (LLMs) are evaluated in two distinct scenarios: a hospital patrolling robot and a kitchen assistant robot. Experimental results demonstrate that a two-stage architecture, which first generates an explanation and then personalises it, is the framework architecture that effectively reduces the level of detail only when there is related user knowledge.",
      "authors": [
        "Ferran Gebell\\'i",
        "Ana\\'is Garrell",
        "Jan-Gerrit Habekost",
        "S\\'everin Lemaignan",
        "Stefan Wermter",
        "Raquel Ros"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T10:40:39+00:00",
          "link": "https://arxiv.org/abs/2507.03049v1",
          "size": "2300kb",
          "version": "v1"
        }
      ],
      "title": "Personalised Explanations in Long-term Human-Robot Interactions",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03049",
        "HTML": "https://arxiv.org/html/2507.03049v1",
        "PDF": "https://arxiv.org/pdf/2507.03049"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on personalizing explanations in human-robot interactions, with no direct focus on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03156",
      "abstract": "Large language model assistants (LLM-assistants) present new opportunities to transform software development. Developers are increasingly adopting these tools across tasks, including coding, testing, debugging, documentation, and design. Yet, despite growing interest, there is no synthesis of how LLM-assistants affect software developer productivity. In this paper, we present a systematic literature review of 37 peer-reviewed studies published between January 2014 and December 2024 that examine this impact. Our analysis reveals that LLM-assistants offer both considerable benefits and critical risks. Commonly reported gains include minimized code search, accelerated development, and the automation of trivial and repetitive tasks. However, studies also highlight concerns around cognitive offloading, reduced team collaboration, and inconsistent effects on code quality. While the majority of studies (92%) adopt a multi-dimensional perspective by examining at least two SPACE dimensions, reflecting increased awareness of the complexity of developer productivity, only 14% extend beyond three dimensions, indicating substantial room for more integrated evaluations. Satisfaction, Performance, and Efficiency are the most frequently investigated dimensions, whereas Communication and Activity remain underexplored. Most studies are exploratory (64%) and methodologically diverse, but lack longitudinal and team-based evaluations. This review surfaces key research gaps and provides recommendations for future research and practice. All artifacts associated with this study are publicly available at https://zenodo.org/records/15788502.",
      "authors": [
        "Amr Mohamed",
        "Maram Assi",
        "Mariam Guizani"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Software Engineering (cs.SE)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T20:25:49+00:00",
          "link": "https://arxiv.org/abs/2507.03156v1",
          "size": "1246kb",
          "version": "v1"
        }
      ],
      "title": "The Impact of LLM-Assistants on Software Developer Productivity: A Systematic Literature Review",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03156",
        "HTML": "https://arxiv.org/html/2507.03156v1",
        "PDF": "https://arxiv.org/pdf/2507.03156"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper reviews the impact of LLM-assistants on software developer productivity without a focus on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03330",
      "abstract": "Cooking plays a vital role in everyday independence and well-being, yet remains challenging for people with vision impairments due to limited support for tracking progress and receiving contextual feedback. Object status - the condition or transformation of ingredients and tools - offers a promising but underexplored foundation for context-aware cooking support. In this paper, we present OSCAR (Object Status Context Awareness for Recipes), a technical pipeline that explores the use of object status recognition to enable recipe progress tracking in non-visual cooking. OSCAR integrates recipe parsing, object status extraction, visual alignment with cooking steps, and time-causal modeling to support real-time step tracking. We evaluate OSCAR on 173 instructional videos and a real-world dataset of 12 non-visual cooking sessions recorded by BLV individuals in their homes. Our results show that object status consistently improves step prediction accuracy across vision-language models, and reveal key factors that impact performance in real-world conditions, such as implicit tasks, camera placement, and lighting. We contribute the pipeline of context-aware recipe progress tracking, an annotated real-world non-visual cooking dataset, and design insights to guide future context-aware assistive cooking systems.",
      "authors": [
        "Franklin Mingzhe Li",
        "Kaitlyn Ng",
        "Bin Zhu",
        "Patrick Carrington"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-04T06:30:50+00:00",
          "link": "https://arxiv.org/abs/2507.03330v1",
          "size": "25374kb",
          "version": "v1"
        }
      ],
      "title": "Exploring Object Status Recognition for Recipe Progress Tracking in Non-Visual Cooking",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03330",
        "HTML": "https://arxiv.org/html/2507.03330v1",
        "PDF": "https://arxiv.org/pdf/2507.03330"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper is centered around non-visual cooking assistance using technology, with no relevant discussion on creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03730",
      "abstract": "The research focus of GUI agents is shifting from text-dependent to pure-vision-based approaches, which, though promising, prioritize comprehensive pre-training data collection while neglecting contextual modeling challenges. We probe the characteristics of element and history contextual modeling in GUI agent and summarize: 1) the high-density and loose-relation of element context highlight the existence of many unrelated elements and their negative influence; 2) the high redundancy of history context reveals the inefficient history modeling in current GUI agents. In this work, we propose a context-aware simplification framework for building an efficient and effective GUI Agent, termed SimpAgent. To mitigate potential interference from numerous unrelated elements, we introduce a masking-based element pruning method that circumvents the intractable relation modeling through an efficient masking mechanism. To reduce the redundancy in historical information, we devise a consistency-guided history compression module, which enhances implicit LLM-based compression through innovative explicit guidance, achieving an optimal balance between performance and efficiency. With the above components, SimpAgent reduces 27% FLOPs and achieves superior GUI navigation performances. Comprehensive navigation experiments across diverse web and mobile environments demonstrate the effectiveness and potential of our agent.",
      "authors": [
        "Gongwei Chen",
        "Xurui Zhou",
        "Rui Shao",
        "Yibo Lyu",
        "Kaiwen Zhou",
        "Shuai Wang",
        "Wentao Li",
        "Yinchuan Li",
        "Zhongang Qi",
        "Liqiang Nie"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-04T17:37:15+00:00",
          "link": "https://arxiv.org/abs/2507.03730v1",
          "size": "11911kb",
          "version": "v1"
        }
      ],
      "title": "Less is More: Empowering GUI Agent with Context-Aware Simplification",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03730",
        "HTML": "https://arxiv.org/html/2507.03730v1",
        "PDF": "https://arxiv.org/pdf/2507.03730"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper focuses on GUI agents and their efficiency improvements, with no mention or connection to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03866",
      "abstract": "We present a data-domain sampling regime for quantifying CNNs' graphic perception behaviors. This regime lets us evaluate CNNs' ratio estimation ability in bar charts from three perspectives: sensitivity to training-test distribution discrepancies, stability to limited samples, and relative expertise to human observers. After analyzing 16 million trials from 800 CNNs models and 6,825 trials from 113 human participants, we arrived at a simple and actionable conclusion: CNNs can outperform humans and their biases simply depend on the training-test distance. We show evidence of this simple, elegant behavior of the machines when they interpret visualization images. osf.io/gfqc3 provides registration, the code for our sampling regime, and experimental results.",
      "authors": [
        "Shuning Jiang",
        "Wei-Lun Chao",
        "Daniel Haehn",
        "Hanspeter Pfister",
        "Jian Chen"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-05T02:30:28+00:00",
          "link": "https://arxiv.org/abs/2507.03866v1",
          "size": "4212kb",
          "version": "v1"
        }
      ],
      "title": "A Rigorous Behavior Assessment of CNNs Using a Data-Domain Sampling Regime",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03866",
        "HTML": "https://arxiv.org/html/2507.03866v1",
        "PDF": "https://arxiv.org/pdf/2507.03866"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper explores CNNs' visual perception and behavior analysis related to bar charts, without involving creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03871",
      "abstract": "The use of reinforcement learning (RL) methods to support health behavior change via personalized and just-in-time adaptive interventions is of significant interest to health and behavioral science researchers focused on problems such as smoking cessation support and physical activity promotion. However, RL methods are often applied to these domains using a small collection of context variables to mitigate the significant data scarcity issues that arise from practical limitations on the design of adaptive intervention trials. In this paper, we explore an approach to significantly expanding the state space of an adaptive intervention without impacting data efficiency. The proposed approach enables intervention participants to provide natural language descriptions of aspects of their current state. It then leverages inference with pre-trained large language models (LLMs) to better align the policy of a base RL method with these state descriptions. To evaluate our method, we develop a novel physical activity intervention simulation environment that generates text-based state descriptions conditioned on latent state variables using an auxiliary LLM. We show that this approach has the potential to significantly improve the performance of online policy learning methods.",
      "authors": [
        "Karine Karine and Benjamin M. Marlin"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-05T02:52:51+00:00",
          "link": "https://arxiv.org/abs/2507.03871v1",
          "size": "442kb",
          "version": "v1"
        }
      ],
      "title": "Enhancing Adaptive Behavioral Interventions with LLM Inference from Participant-Described States",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03871",
        "HTML": "https://arxiv.org/html/2507.03871v1",
        "PDF": "https://arxiv.org/pdf/2507.03871"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The research focuses on adaptive behavioral interventions and RL methods for health behavior change, not creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04009",
      "abstract": "Large language models (LLMs) have shown impressive performance on general-purpose tasks, yet adapting them to specific domains remains challenging due to the scarcity of high-quality domain data. Existing data synthesis tools often struggle to extract reliable fine-tuning data from heterogeneous documents effectively. To address this limitation, we propose Easy Dataset, a unified framework for synthesizing fine-tuning data from unstructured documents via an intuitive graphical user interface (GUI). Specifically, Easy Dataset allows users to easily configure text extraction models and chunking strategies to transform raw documents into coherent text chunks. It then leverages a persona-driven prompting approach to generate diverse question-answer pairs using public-available LLMs. Throughout the pipeline, a human-in-the-loop visual interface facilitates the review and refinement of intermediate outputs to ensure data quality. Experiments on a financial question-answering task show that fine-tuning LLMs on the synthesized dataset significantly improves domain-specific performance while preserving general knowledge. The source code and installable package are available at https://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub stars.",
      "authors": [
        "Ziyang Miao",
        "Qiyu Sun",
        "Jingyuan Wang",
        "Yuchen Gong",
        "Yaowei Zheng",
        "Shiqi Li",
        "Richong Zhang"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-05T11:38:59+00:00",
          "link": "https://arxiv.org/abs/2507.04009v1",
          "size": "461kb",
          "version": "v1"
        }
      ],
      "title": "Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM Fine-Tuning Data from Unstructured Documents",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04009",
        "HTML": "https://arxiv.org/html/2507.04009v1",
        "PDF": "https://arxiv.org/pdf/2507.04009"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper presents a framework for synthesizing LLM fine-tuning data from unstructured documents. It focuses on domain adaptation and data synthesis, without a clear connection to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04182",
      "abstract": "Although the amount of available spoken content is steadily increasing, extracting information and knowledge from speech recordings remains challenging. Beyond enhancing traditional information retrieval methods such as speech search and keyword spotting, novel approaches for navigating and searching spoken content need to be explored and developed. In this paper, we propose a novel navigational method for speech archives that leverages recent advances in language and multimodal generative models. We demonstrate our approach with a Web application that organizes data into a structured format using interactive mind maps and image generation tools. The system is implemented using the TED-LIUM~3 dataset, which comprises over 2,000 speech transcripts and audio files of TED Talks. Initial user tests using a System Usability Scale (SUS) questionnaire indicate the application's potential to simplify the exploration of large speech collections.",
      "authors": [
        "Sirina H{\\aa}land and Trond Karlsen Str{\\o}m and Petra Galu\\v{s}\\v{c}\\'akov\\'a"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Information Retrieval (cs.IR)",
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)",
        "Sound (cs.SD)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-05T22:38:10+00:00",
          "link": "https://arxiv.org/abs/2507.04182v1",
          "size": "3695kb",
          "version": "v1"
        }
      ],
      "title": "Navigating Speech Recording Collections with AI-Generated Illustrations",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04182",
        "HTML": "https://arxiv.org/html/2507.04182v1",
        "PDF": "https://arxiv.org/pdf/2507.04182"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper explores novel navigation methods for speech archives, potentially fostering creativity in information retrieval, but it's not the primary focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04189",
      "abstract": "Understanding character relationships is essential for interpreting complex narratives and conducting socially grounded AI research. However, manual annotation is time-consuming and low in coverage, while large language models (LLMs) often produce hallucinated or logically inconsistent outputs. We present SymbolicThought, a human-in-the-loop framework that combines LLM-based extraction with symbolic reasoning. The system constructs editable character relationship graphs, refines them using seven types of logical constraints, and enables real-time validation and conflict resolution through an interactive interface. To support logical supervision and explainable social analysis, we release a dataset of 160 interpersonal relationships with corresponding logical structures. Experiments show that SymbolicThought improves annotation accuracy and consistency while significantly reducing time cost, offering a practical tool for narrative understanding, explainable AI, and LLM evaluation.",
      "authors": [
        "Runcong Zhao",
        "Qinglin Zhu",
        "Hainiu Xu",
        "Bin Liang",
        "Yulan He",
        "Lin Gui"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-05T23:46:35+00:00",
          "link": "https://arxiv.org/abs/2507.04189v1",
          "size": "1612kb",
          "version": "v1"
        }
      ],
      "title": "SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04189",
        "PDF": "https://arxiv.org/pdf/2507.04189"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on symbolic reasoning and language model consistency in character relationship understanding, with no direct mention of or focus on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04295",
      "abstract": "Effective feedback is essential for student learning but is time-intensive for teachers. We present LearnLens, a modular, LLM-based system that generates personalised, curriculum-aligned feedback in science education. LearnLens comprises three components: (1) an error-aware assessment module that captures nuanced reasoning errors; (2) a curriculum-grounded generation module that uses a structured, topic-linked memory chain rather than traditional similarity-based retrieval, improving relevance and reducing noise; and (3) an educator-in-the-loop interface for customisation and oversight. LearnLens addresses key challenges in existing systems, offering scalable, high-quality feedback that empowers both teachers and students.",
      "authors": [
        "Runcong Zhao",
        "Artem Borov",
        "Jiazheng Li",
        "Yulan He"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-06T08:39:26+00:00",
          "link": "https://arxiv.org/abs/2507.04295v1",
          "size": "1802kb",
          "version": "v1"
        }
      ],
      "title": "LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04295",
        "PDF": "https://arxiv.org/pdf/2507.04295"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "While the paper focuses on personalized feedback in education, creativity might be a secondary concern as feedback could enhance creative solutions and thinking in educational contexts, although not a primary focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04340",
      "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as a key enabling technology for aligning AI behavior with human preferences. The traditional way to collect data in RLHF is via pairwise comparisons: human raters are asked to indicate which one of two samples they prefer. We present an interactive visualization that better exploits the human visual ability to compare and explore whole groups of samples. The interface is comprised of two linked views: 1) an exploration view showing a contextual overview of all sampled behaviors organized in a hierarchical clustering structure; and 2) a comparison view displaying two selected groups of behaviors for user queries. Users can efficiently explore large sets of behaviors by iterating between these two views. Additionally, we devised an active learning approach suggesting groups for comparison. As shown by our evaluation in six simulated robotics tasks, our approach increases the final policy returns by 69.34%. It leads to lower error rates and better policies. We open-source the code that can be easily integrated into the RLHF training loop, supporting research on human-AI alignment.",
      "authors": [
        "Jan Kompatscher",
        "Danqing Shi",
        "Giovanna Varni",
        "Tino Weinkauf",
        "Antti Oulasvirta"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-06T10:52:14+00:00",
          "link": "https://arxiv.org/abs/2507.04340v1",
          "size": "18914kb",
          "version": "v1"
        }
      ],
      "title": "Interactive Groupwise Comparison for Reinforcement Learning from Human Feedback",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04340",
        "HTML": "https://arxiv.org/html/2507.04340v1",
        "PDF": "https://arxiv.org/pdf/2507.04340"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper primarily discusses reinforcement learning and human feedback alignment without direct relevance to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04352",
      "abstract": "As AI hype continues to grow, organizations face pressure to broadcast or downplay purported AI initiatives - even when contrary to truth. This paper introduces AI-washing as overstating (deceptive boasting) or understating (deceptive denial) a company's real AI usage. A 2x2 experiment (N = 401) examines how these false claims affect consumer attitudes and purchase intentions. Results reveal a pronounced asymmetry: deceptive denial evokes more negative moral judgments than honest negation, while deceptive boasting has no effects. We show that perceived betrayal mediates these outcomes. By clarifying how AI-washing erodes trust, the study highlights clear ethical implications for policymakers, marketers, and researchers striving for transparency.",
      "authors": [
        "Greg Nyilasy and Harsha Gangadharbatla"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-06T11:28:45+00:00",
          "link": "https://arxiv.org/abs/2507.04352v1",
          "size": "1062kb",
          "version": "v1"
        }
      ],
      "title": "AI-washing: The Asymmetric Effects of Its Two Types on Consumer Moral Judgments",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04352",
        "PDF": "https://arxiv.org/pdf/2507.04352"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The study on AI-washing and consumer perception does not explore creativity in any substantive way."
      },
      "source": "arXiv"
    },
    {
      "id": "2402.08855",
      "abstract": "Large language models (LLMs) have become ubiquitous in providing different forms of writing assistance to different writers. However, LLM-powered writing systems often fall short in capturing the nuanced personalization and control needed to effectively support users -- particularly for those who lack experience with prompt engineering. To address these challenges, we introduce GhostWriter, an AI-enhanced design probe that enables users to exercise enhanced agency and personalization during writing. GhostWriter leverages LLMs to implicitly learn the user's intended writing style for seamless personalization, while exposing explicit teaching moments for style refinement and reflection. We study 18 participants who use GhostWriter on two distinct writing tasks, observing that it helps users craft personalized text generations and empowers them by providing multiple ways to control the system's writing style. Based on this study, we present insights on how specific design choices can promote greater user agency in AI-assisted writing and discuss people's evolving relationships with such systems. We conclude by offering design recommendations for future work.",
      "authors": [
        "Catherine Yeh",
        "Gonzalo Ramos",
        "Rachel Ng",
        "Andy Huntington",
        "Richard Banks"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2024-02-13T23:48:59+00:00",
          "link": "https://arxiv.org/abs/2402.08855v1",
          "size": "5786kb",
          "version": "v1"
        },
        {
          "date": "2025-03-23T19:08:00+00:00",
          "link": "https://arxiv.org/abs/2402.08855v2",
          "size": "6836kb",
          "version": "v2"
        },
        {
          "date": "2025-07-04T04:19:09+00:00",
          "link": "https://arxiv.org/abs/2402.08855v3",
          "size": "5619kb",
          "version": "v3"
        }
      ],
      "title": "GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency",
      "links": {
        "Abstract": "https://arxiv.org/abs/2402.08855",
        "PDF": "https://arxiv.org/pdf/2402.08855"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "GhostWriter is directly related to creativity, focusing on enhancing collaborative writing experiences via personalization and user agency, which are crucial for creative writing tasks."
      },
      "tasks": [
        "Prompt Engineering"
      ],
      "source": "arXiv"
    },
    {
      "id": "2405.14341",
      "abstract": "Many toolkit developers seek to streamline the visualization programming process through structured support such as prescribed templates and example galleries. However, few projects examine how users organize their own visualization programs and how their coding choices may deviate from the intents of toolkit developers, impacting visualization prototyping and design. Further, is it possible to infer users' reasoning indirectly through their code, even when users copy code from other sources? We explore this question through a qualitative analysis of 715 D3 programs on Observable. We identify three levels of program organization based on how users decompose their code into smaller blocks: Program-, Chart-, and Component-Level code decomposition, with a strong preference for Component-Level reasoning. In a series of interviews, we corroborate that these levels reflect how Observable users reason about visualization programs. We compare common user-made components with those theorized in the Grammar of Graphics to assess overlap in user and toolkit developer reasoning. We find that, while the Grammar of Graphics covers basic visualizations well, it falls short in describing complex visualization types, especially those with animation, interaction, and parameterization components. Our findings highlight how user practices differ from formal grammars and reinforce ongoing efforts to rethink visualization toolkit support, including augmenting learning tools and AI assistants to better reflect real-world coding strategies.",
      "authors": [
        "Melissa Lin",
        "Heer Patel",
        "Medina Lamkin",
        "Hannah Bako",
        "Leilani Battle"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-05-23T09:11:47+00:00",
          "link": "https://arxiv.org/abs/2405.14341v1",
          "size": "346kb",
          "version": "v1"
        },
        {
          "date": "2024-12-27T11:05:23+00:00",
          "link": "https://arxiv.org/abs/2405.14341v2",
          "size": "640kb",
          "version": "v2"
        },
        {
          "date": "2024-12-30T14:21:01+00:00",
          "link": "https://arxiv.org/abs/2405.14341v3",
          "size": "640kb",
          "version": "v3"
        },
        {
          "date": "2025-05-25T02:40:33+00:00",
          "link": "https://arxiv.org/abs/2405.14341v4",
          "size": "195kb",
          "version": "v4"
        },
        {
          "date": "2025-07-04T20:03:09+00:00",
          "link": "https://arxiv.org/abs/2405.14341v5",
          "size": "196kb",
          "version": "v5"
        }
      ],
      "title": "How do Observable Users Decompose D3 Code? A Qualitative Study",
      "links": {
        "Abstract": "https://arxiv.org/abs/2405.14341",
        "HTML": "https://arxiv.org/html/2405.14341v5",
        "PDF": "https://arxiv.org/pdf/2405.14341"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The study examines how users organize visualization code, indirectly engaging with creative processes in terms of coding choices and prototyping but does not treat creativity as its main focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2409.18162",
      "abstract": "The emergence of large language models (LLMs), augmented reality (AR), and user interface/user experience (UI/UX) design in therapies for children, especially with disorders like autism spectrum disorder (ASD), is studied in detail in this review study. 150 publications were collected by a thorough literature search throughout PubMed, ACM, IEEE Xplore, Elsevier, and Google Scholar; 60 of them were chosen based on their methodological rigor and relevance to the focus area. Three of the primary areas are studied and covered in this review: how AR can improve social and learning results, how LLMs can support communication, and how UI/UX design affects how effective these technologies can be. Results show that while LLMs can provide individualized learning and communication support, AR has shown promise in enhancing social skills, motivation, and attention. For children with ASD, accessible and engaging interventions rely heavily on effective UI/UX design, but there is still a significant lack of robotics-based education and therapeutic programs specifically tailored for autistic children. To optimize the benefits of these technologies in ASD therapies and immersive education, the study emphasizes the need for additional research to address difficulties related to customization, accessibility, and integration.",
      "authors": [
        "Biplov Paneru",
        "Bishwash Paneru"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Social and Information Networks (cs.SI)"
      ],
      "submission_historys": [
        {
          "date": "2024-09-26T17:19:25+00:00",
          "link": "https://arxiv.org/abs/2409.18162v1",
          "size": "843kb",
          "version": "v1"
        },
        {
          "date": "2025-07-06T11:42:56+00:00",
          "link": "https://arxiv.org/abs/2409.18162v2",
          "size": "1074kb",
          "version": "v2"
        }
      ],
      "title": "The Nexus of AR/VR, AI, UI/UX, and Robotics Technologies in Enhancing Learning and Social Interaction for Children with Autism Spectrum Disorders: A Systematic Review",
      "links": {
        "Abstract": "https://arxiv.org/abs/2409.18162",
        "PDF": "https://arxiv.org/pdf/2409.18162"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This systematic review focuses on AR/VR and AI for children with autism, emphasizing learning and social interaction improvements rather than creativity."
      },
      "tasks": [],
      "source": "arXiv"
    },
    {
      "id": "2409.19281",
      "abstract": "Mixed Reality (MR) platforms enable users to interact with three-dimensional holographic instructions during the assembly and fabrication of highly custom and parametric architectural constructions without the necessity of two-dimensional drawings. Previous MR fabrication projects have primarily relied on digital menus and custom buttons as the interface for user interaction with the MR environment. Despite this approach being widely adopted, it is limited in its ability to allow for direct human interaction with physical objects to modify fabrication instructions within the MR environment. This research integrates user interactions with physical objects through real-time gesture recognition as input to modify, update or generate new digital information enabling reciprocal stimuli between the physical and the virtual environment. Consequently, the digital environment is generative of the user's provided interaction with physical objects to allow seamless feedback in the fabrication process. This research investigates gesture recognition for feedback-based MR workflows for robotic fabrication, human assembly, and quality control in the construction of the UnLog Tower.",
      "authors": [
        "Alexander Htet Kyaw",
        "Lawson Spencer",
        "Sasa Zivkovic",
        "Leslie Lok"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Emerging Technologies (cs.ET)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "2024-09-28T08:17:51+00:00",
          "link": "https://arxiv.org/abs/2409.19281v1",
          "size": "1773kb",
          "version": "v1"
        },
        {
          "date": "2025-07-03T20:16:54+00:00",
          "link": "https://arxiv.org/abs/2409.19281v2",
          "size": "1773kb",
          "version": "v2"
        }
      ],
      "title": "Gesture Recognition for Feedback Based Mixed Reality and Robotic Fabrication: A Case Study of the UnLog Tower",
      "links": {
        "Abstract": "https://arxiv.org/abs/2409.19281",
        "PDF": "https://arxiv.org/pdf/2409.19281"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The study on gesture recognition and mixed reality in robotics could relate to creativity in terms of innovative human-computer interaction and design, though it is not the primary focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2502.11273",
      "abstract": "Rideshare workers experience unpredictable working conditions due to gig work platforms' reliance on opaque AI and algorithmic systems. In response to these challenges, we found that labor organizers want data to help them advocate for legislation to increase the transparency and accountability of these platforms. To address this need, we collaborated with a Colorado-based rideshare union to develop FairFare, a tool that crowdsources and analyzes workers' data to estimate the take rate -- the percentage of the rider price retained by the rideshare platform. We deployed FairFare with our partner organization that collaborated with us in collecting data on 76,000+ trips from 45 drivers over 18 months. During evaluation interviews, organizers reported that FairFare helped influence the bill language and passage of Colorado Senate Bill 24-75, calling for greater transparency and data disclosure of platform operations, and create a national narrative. Finally, we reflect on complexities of translating quantitative data into policy outcomes, nature of community based audits, and design implications for future transparency tools.",
      "authors": [
        "Dana Calacci",
        "Varun Nagaraj Rao",
        "Samantha Dalal",
        "Catherine Di",
        "Kok-Wei Pua",
        "Andrew Schwartz",
        "Danny Spitzberg",
        "Andr\\'es Monroy-Hern\\'andez"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "2025-02-16T21:30:26+00:00",
          "link": "https://arxiv.org/abs/2502.11273v1",
          "size": "3047kb",
          "version": "v1"
        },
        {
          "date": "2025-07-06T15:02:19+00:00",
          "link": "https://arxiv.org/abs/2502.11273v2",
          "size": "2563kb",
          "version": "v2"
        }
      ],
      "title": "FairFare: A Tool for Crowdsourcing Rideshare Data to Empower Labor Organizers",
      "links": {
        "Abstract": "https://arxiv.org/abs/2502.11273",
        "HTML": "https://arxiv.org/html/2502.11273v2",
        "PDF": "https://arxiv.org/pdf/2502.11273"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses a tool for data transparency in ridesharing platforms focused on labor rights and policy impact. Creativity is not addressed in the content."
      },
      "tasks": [],
      "source": "arXiv"
    },
    {
      "id": "2503.16492",
      "abstract": "Effective Human-Robot Interaction (HRI) is crucial for enhancing accessibility and usability in real-world robotics applications. However, existing solutions often rely on gestures or language commands, making interaction inefficient and ambiguous, particularly for users with physical impairments. In this paper, we introduce FAM-HRI, an efficient multi-modal framework for human-robot interaction that integrates language and gaze inputs via foundation models. By leveraging lightweight Meta ARIA glasses, our system captures real-time multi-modal signals and utilizes large language models (LLMs) to fuse user intention with scene context, enabling intuitive and precise robot manipulation. Our method accurately determines gaze fixation time interval, reducing noise caused by the gaze dynamic nature. Experimental evaluations demonstrate that FAM-HRI achieves a high success rate in task execution while maintaining a low interaction time, providing a practical solution for individuals with limited physical mobility or motor impairments.",
      "authors": [
        "Yuzhi Lai",
        "Shenghai Yuan",
        "Boya Zhang",
        "Benjamin Kiefer",
        "Peizheng Li",
        "Tianchen Deng and Andreas Zell"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "2025-03-11T02:30:15+00:00",
          "link": "https://arxiv.org/abs/2503.16492v1",
          "size": "8498kb",
          "version": "v1"
        },
        {
          "date": "2025-07-04T08:41:00+00:00",
          "link": "https://arxiv.org/abs/2503.16492v2",
          "size": "8112kb",
          "version": "v2"
        }
      ],
      "title": "FAM-HRI: Foundation-Model Assisted Multi-Modal Human-Robot Interaction Combining Gaze and Speech",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.16492",
        "HTML": "https://arxiv.org/html/2503.16492v2",
        "PDF": "https://arxiv.org/pdf/2503.16492"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on improving human-robot interaction using gaze and speech, targeting accessibility for users with impairments. Creativity is not a focus or mentioned as a relevant dimension."
      },
      "source": "arXiv"
    },
    {
      "id": "2505.09166",
      "abstract": "In the creative practice of text-to-image generation (TTI), images are generated from text prompts. However, TTI models are trained to always yield an output, even if the prompt contains unknown terms. In this case, the model may generate what we call \"default images\": images that closely resemble each other across many unrelated prompts. We argue studying default images is valuable for designing better solutions for TTI and prompt engineering. In this paper, we provide the first investigation into default images on Midjourney, a popular image generator. We describe our systematic approach to create input prompts triggering default images, and present the results of our initial experiments and several small-scale ablation studies. We also report on a survey study investigating how default images affect user satisfaction. Our work lays the foundation for understanding default images in TTI and highlights challenges and future research directions.",
      "authors": [
        "Hannu Simonen",
        "Atte Kiviniemi",
        "Jonas Oppenlaender"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-05-14T05:59:23+00:00",
          "link": "https://arxiv.org/abs/2505.09166v1",
          "size": "21659kb",
          "version": "v1"
        },
        {
          "date": "2025-07-05T13:02:43+00:00",
          "link": "https://arxiv.org/abs/2505.09166v2",
          "size": "4316kb",
          "version": "v2"
        }
      ],
      "title": "An Exploration of Default Images in Text-to-Image Generation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.09166",
        "HTML": "https://arxiv.org/html/2505.09166v2",
        "PDF": "https://arxiv.org/pdf/2505.09166"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper studies text-to-image generation, specifically default images, an inherent creative process. It addresses the understanding and improvement of creative practices in TTI, focusing directly on creative output through technology."
      },
      "tasks": [
        "Image Generation",
        "Prompt Engineering",
        "Text to Image Generation",
        "Text-to-Image Generation"
      ],
      "source": "arXiv"
    },
    {
      "id": "2506.18317",
      "abstract": "Indoor localization opens the path to potentially transformative applications. Although many indoor localization methods have been proposed over the years, they remain too impractical for widespread deployment in the real world. In this paper, we introduce PeepLoc, a deployable and scalable Wi-Fi-based solution for indoor localization that relies only on pre-existing devices and infrastructure. Specifically, PeepLoc works on any mobile device with an unmodified Wi-Fi transceiver and in any indoor environment with a sufficient number of Wi-Fi access points (APs) and pedestrian traffic. At the core of PeepLoc is (a) a mechanism which allows any Wi-Fi device to obtain non-cooperative time-of-flight (ToF) to any Wi-Fi AP and (b) a novel bootstrapping mechanism that relies on pedestrian dead reckoning (PDR) and crowdsourcing to opportunistically initialize pre-existing APs as anchor points within an environment. We implement PeepLoc using commodity hardware and evaluate it extensively across 4 campus buildings. We show PeepLoc leads to a mean and median positional error of 3.41 m and 3.06 m respectively, which is superior to existing deployed indoor localization systems and is competitive with commodity GPS in outdoor environments.",
      "authors": [
        "Emerson Sie",
        "Enguang Fan",
        "Federico Cifuentes-Urtubey",
        "Deepak Vasisht"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Networking and Internet Architecture (cs.NI)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-23T06:04:45+00:00",
          "link": "https://arxiv.org/abs/2506.18317v1",
          "size": "6913kb",
          "version": "v1"
        },
        {
          "date": "2025-07-05T07:44:19+00:00",
          "link": "https://arxiv.org/abs/2506.18317v2",
          "size": "6845kb",
          "version": "v2"
        }
      ],
      "title": "Crowdsourcing Ubiquitous Indoor Localization with Non-Cooperative Wi-Fi Ranging",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18317",
        "HTML": "https://arxiv.org/html/2506.18317v2",
        "PDF": "https://arxiv.org/pdf/2506.18317"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper centers on Wi-Fi-based indoor localization techniques, focusing on the technical implementation and accuracy improvements without relevance to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01436",
      "abstract": "Despite the ubiquity of visualization examples published on the web, retargeting existing custom chart implementations to new datasets remains difficult, time-intensive, and tedious. The adaptation process assumes author familiarity with both the implementation of the example as well as how the new dataset might need to be transformed to fit into the example code. With recent advances in Large Language Models (LLMs), automatic adaptation of code can be achieved from high-level user prompts, reducing the barrier for visualization retargeting. To better understand how LLMs can assist retargeting and its potential limitations, we characterize and evaluate the performance of LLM assistance across multiple datasets and charts of varying complexity, categorizing failures according to type and severity. In our evaluation, we compare two approaches: (1) directly instructing the LLM model to fully generate and adapt code by treating code as text inputs and (2) a more constrained program synthesis pipeline where the LLM guides the code construction process by providing structural information (e.g., visual encodings) based on properties of the example code and data. We find that both approaches struggle when new data has not been appropriately transformed, and discuss important design recommendations for future retargeting systems.",
      "authors": [
        "Luke S. Snyder",
        "Chenglong Wang",
        "Steven M. Drucker"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T07:43:43+00:00",
          "link": "https://arxiv.org/abs/2507.01436v1",
          "size": "1603kb",
          "version": "v1"
        },
        {
          "date": "2025-07-06T18:15:55+00:00",
          "link": "https://arxiv.org/abs/2507.01436v2",
          "size": "1603kb",
          "version": "v2"
        }
      ],
      "title": "Challenges & Opportunities with LLM-Assisted Visualization Retargeting",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01436",
        "HTML": "https://arxiv.org/html/2507.01436v2",
        "PDF": "https://arxiv.org/pdf/2507.01436"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper addresses visualization retargeting with LLMs, which involves adapting visualization examples to new datasets. Creativity is a secondary theme as LLM assistance could potentially enable creative exploration in visualization design, but it's not the main focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2309.10448",
      "abstract": "When working with generative artificial intelligence (AI), users may see productivity gains, but the AI-generated content may not match their preferences exactly. To study this effect, we introduce a Bayesian framework in which heterogeneous users choose how much information to share with the AI, facing a trade-off between output fidelity and communication cost. We show that the interplay between these individual-level decisions and AI training may lead to societal challenges. Outputs may become more homogenized, especially when the AI is trained on AI-generated content, potentially triggering a homogenization death spiral. And any AI bias may propagate to become societal bias. A solution to the homogenization and bias issues is to reduce human-AI interaction frictions and enable users to flexibly share information, leading to personalized outputs without sacrificing productivity.",
      "authors": [
        "Francisco Castro",
        "Jian Gao",
        "S\\'ebastien Martin"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "General Economics (econ.GN)",
        "Economics (q-fin.EC)"
      ],
      "submission_historys": [
        {
          "date": "2023-09-19T09:09:59+00:00",
          "link": "https://arxiv.org/abs/2309.10448v1",
          "size": "1584kb",
          "version": "v1"
        },
        {
          "date": "2023-10-13T00:52:52+00:00",
          "link": "https://arxiv.org/abs/2309.10448v2",
          "size": "1584kb",
          "version": "v2"
        },
        {
          "date": "2025-06-22T15:45:31+00:00",
          "link": "https://arxiv.org/abs/2309.10448v3",
          "size": "2313kb",
          "version": "v3"
        },
        {
          "date": "2025-07-06T14:31:28+00:00",
          "link": "https://arxiv.org/abs/2309.10448v4",
          "size": "2313kb",
          "version": "v4"
        }
      ],
      "title": "Human-AI Interactions and Societal Pitfalls",
      "links": {
        "Abstract": "https://arxiv.org/abs/2309.10448",
        "PDF": "https://arxiv.org/pdf/2309.10448"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "While the main focus is on human-AI interactions and societal effects, the paper indirectly relates to creativity through AI bias and personalization impacts on generated content, hinting at creative diversity in AI outputs."
      },
      "tasks": [],
      "source": "arXiv"
    },
    {
      "id": "2311.16429",
      "abstract": "The increasing adoption and commercialization of generalized Large Language Models (LLMs) have profoundly impacted various aspects of our daily lives. Initially embraced by the computer science community, the versatility of LLMs has found its way into diverse domains. In particular, the software engineering realm has witnessed the most transformative changes. With LLMs increasingly serving as AI Pair Programming Assistants spurred the development of specialized models aimed at aiding software engineers. Although this new paradigm offers numerous advantages, it also presents critical challenges and open problems. To identify the potential and prevailing obstacles, we systematically reviewed contemporary scholarly publications, emphasizing the perspectives of software developers and usability concerns. Preliminary findings underscore pressing concerns about data privacy, bias, and misinformation. Additionally, we identified several usability challenges, including prompt engineering, increased cognitive demands, and mistrust. Finally, we introduce 12 open problems that we have identified through our survey, covering these various domains.",
      "authors": [
        "Sajed Jalil"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Software Engineering (cs.SE)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2023-11-28T02:18:54+00:00",
          "link": "https://arxiv.org/abs/2311.16429v1",
          "size": "109kb",
          "version": "v1"
        },
        {
          "date": "2025-07-05T20:19:58+00:00",
          "link": "https://arxiv.org/abs/2311.16429v2",
          "size": "107kb",
          "version": "v2"
        }
      ],
      "title": "The Transformative Influence of LLMs on Software Development & Developer Productivity",
      "links": {
        "Abstract": "https://arxiv.org/abs/2311.16429",
        "HTML": "https://arxiv.org/html/2311.16429v2",
        "PDF": "https://arxiv.org/pdf/2311.16429"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses the use of LLMs in software development, emphasizing productivity, usability challenges, and bias without focusing on creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2411.12089",
      "abstract": "In the real world, objects reveal internal textures when sliced or cut, yet this behavior is not well-studied in 3D generation tasks today. For example, slicing a virtual 3D watermelon should reveal flesh and seeds. Given that no available dataset captures an object's full internal structure and collecting data from all slices is impractical, generative methods become the obvious approach. However, current 3D generation and inpainting methods often focus on visible appearance and overlook internal textures. To bridge this gap, we introduce FruitNinja, the first method to generate internal textures for 3D objects undergoing geometric and topological changes. Our approach produces objects via 3D Gaussian Splatting (3DGS) with both surface and interior textures synthesized, enabling real-time slicing and rendering without additional optimization. FruitNinja leverages a pre-trained diffusion model to progressively inpaint cross-sectional views and applies voxel-grid-based smoothing to achieve cohesive textures throughout the object. Our OpaqueAtom GS strategy overcomes 3DGS limitations by employing densely distributed opaque Gaussians, avoiding biases toward larger particles that destabilize training and sharp color transitions for fine-grained textures. Experimental results show that FruitNinja substantially outperforms existing approaches, showcasing unmatched visual quality in real-time rendered internal views across arbitrary geometry manipulations.",
      "authors": [
        "Fangyu Wu",
        "Yuhao Chen"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Graphics (cs.GR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-11-18T22:00:19+00:00",
          "link": "https://arxiv.org/abs/2411.12089v1",
          "size": "8313kb",
          "version": "v1"
        },
        {
          "date": "2024-11-21T02:40:40+00:00",
          "link": "https://arxiv.org/abs/2411.12089v2",
          "size": "8294kb",
          "version": "v2"
        },
        {
          "date": "2025-07-03T22:14:28+00:00",
          "link": "https://arxiv.org/abs/2411.12089v3",
          "size": "8293kb",
          "version": "v3"
        }
      ],
      "title": "FruitNinja: 3D Object Interior Texture Generation with Gaussian Splatting",
      "links": {
        "Abstract": "https://arxiv.org/abs/2411.12089",
        "HTML": "https://arxiv.org/html/2411.12089v3",
        "PDF": "https://arxiv.org/pdf/2411.12089"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper presents a new method for generating internal textures of 3D objects, which involves creative aspects of generative methods and virtual artistic techniques, with creativity being central to its innovation."
      },
      "conference_url_abs": "http://openaccess.thecvf.com//content/CVPR2025/html/Wu_FruitNinja_3D_Object_Interior_Texture_Generation_with_Gaussian_Splatting_CVPR_2025_paper.html",
      "tasks": [
        "3D Generation",
        "3DGS",
        "Texture Synthesis"
      ],
      "source": "arXiv"
    },
    {
      "id": "2501.16780",
      "abstract": "The global aging population faces considerable challenges, particularly in communication, due to the prevalence of hearing and speech impairments. To address these, we introduce the AVE speech, a comprehensive multi-modal dataset for speech recognition tasks. The dataset includes a 100-sentence Mandarin corpus with audio signals, lip-region video recordings, and six-channel electromyography (EMG) data, collected from 100 participants. Each subject read the entire corpus ten times, with each sentence averaging approximately two seconds in duration, resulting in over 55 hours of multi-modal speech data per modality. Experiments demonstrate that combining these modalities significantly improves recognition performance, particularly in cross-subject and high-noise environments. To our knowledge, this is the first publicly available sentence-level dataset integrating these three modalities for large-scale Mandarin speech recognition. We expect this dataset to drive advancements in both acoustic and non-acoustic speech recognition research, enhancing cross-modal learning and human-machine interaction.",
      "authors": [
        "Dongliang Zhou",
        "Yakun Zhang",
        "Jinghan Wu",
        "Xingyu Zhang",
        "Liang Xie",
        "Erwei Yin"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Sound (cs.SD)",
        "Human-Computer Interaction (cs.HC)",
        "Multimedia (cs.MM)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "date": "2025-01-28T08:05:22+00:00",
          "link": "https://arxiv.org/abs/2501.16780v1",
          "size": "3661kb",
          "version": "v1"
        },
        {
          "date": "2025-07-05T14:55:44+00:00",
          "link": "https://arxiv.org/abs/2501.16780v2",
          "size": "3538kb",
          "version": "v2"
        }
      ],
      "title": "AVE Speech: A Comprehensive Multi-Modal Dataset for Speech Recognition Integrating Audio, Visual, and Electromyographic Signals",
      "links": {
        "Abstract": "https://arxiv.org/abs/2501.16780",
        "HTML": "https://arxiv.org/html/2501.16780v2",
        "PDF": "https://arxiv.org/pdf/2501.16780"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on speech recognition technologies and datasets. There is no mention or exploration of creativity as a theme or focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2502.08566",
      "abstract": "Recent advancements in Augmented Reality (AR) have demonstrated applications in architecture, design, and fabrication. Compared to conventional 2D construction drawings, AR can be used to superimpose contextual instructions, display 3D spatial information and enable on-site engagement. Despite the potential of AR, the widespread adoption of the technology in the industry is limited by its precision. Precision is important for projects requiring strict construction tolerances, design fidelity, and fabrication feedback. For example, the manufacturing of glulam beams requires tolerances of less than 2mm. The goal of this project is to explore the industrial application of using multiple fiducial markers for high-precision AR fabrication. While the method has been validated in lab settings with a precision of 0.97, this paper focuses on fabricating glulam beams in a factory setting with an industry manufacturer, Unalam Factory.",
      "authors": [
        "Alexander Htet Kyaw",
        "Arvin Xu",
        "Sasa Zivkovic",
        "Gwyllim Jahn",
        "Cameron Newnham",
        "Nick Van Den Berg"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Emerging Technologies (cs.ET)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-02-12T16:56:07+00:00",
          "link": "https://arxiv.org/abs/2502.08566v1",
          "size": "5453kb",
          "version": "v1"
        },
        {
          "date": "2025-07-03T20:00:11+00:00",
          "link": "https://arxiv.org/abs/2502.08566v2",
          "size": "617kb",
          "version": "v2"
        }
      ],
      "title": "AR Glulam: Accurate Augmented Reality Using Multiple Fiducial Markers for Glulam Fabrication",
      "links": {
        "Abstract": "https://arxiv.org/abs/2502.08566",
        "PDF": "https://arxiv.org/pdf/2502.08566"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper deals with the use of augmented reality for precision in glulam fabrication. Creativity is not a theme or discussed in the paper."
      },
      "tasks": [],
      "source": "arXiv"
    },
    {
      "id": "2502.17776",
      "abstract": "Tip-of-the-tongue (TOT) search occurs when a user struggles to recall a specific identifier, such as a document title. While common, existing search systems often fail to effectively support TOT scenarios. Research on TOT retrieval is further constrained by the challenge of collecting queries, as current approaches rely heavily on community question-answering (CQA) websites, leading to labor-intensive evaluation and domain bias. To overcome these limitations, we introduce two methods for eliciting TOT queries - leveraging large language models (LLMs) and human participants - to facilitate simulated evaluations of TOT retrieval systems. Our LLM-based TOT user simulator generates synthetic TOT queries at scale, achieving high correlations with how CQA-based TOT queries rank TOT retrieval systems when tested in the Movie domain. Additionally, these synthetic queries exhibit high linguistic similarity to CQA-derived queries. For human-elicited queries, we developed an interface that uses visual stimuli to place participants in a TOT state, enabling the collection of natural queries. In the Movie domain, system rank correlation and linguistic similarity analyses confirm that human-elicited queries are both effective and closely resemble CQA-based queries. These approaches reduce reliance on CQA-based data collection while expanding coverage to underrepresented domains, such as Landmark and Person. LLM-elicited queries for the Movie, Landmark, and Person domains have been released as test queries in the TREC 2024 TOT track, with human-elicited queries scheduled for inclusion in the TREC 2025 TOT track. Additionally, we provide source code for synthetic query generation and the human query collection interface, along with curated visual stimuli used for eliciting TOT queries.",
      "authors": [
        "Yifan He",
        "To Eun Kim",
        "Fernando Diaz",
        "Jaime Arguello",
        "Bhaskar Mitra"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Information Retrieval (cs.IR)",
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-02-25T02:11:42+00:00",
          "link": "https://arxiv.org/abs/2502.17776v1",
          "size": "494kb",
          "version": "v1"
        },
        {
          "date": "2025-07-04T20:45:42+00:00",
          "link": "https://arxiv.org/abs/2502.17776v2",
          "size": "537kb",
          "version": "v2"
        }
      ],
      "title": "Tip of the Tongue Query Elicitation for Simulated Evaluation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2502.17776",
        "HTML": "https://arxiv.org/html/2502.17776v2",
        "PDF": "https://arxiv.org/pdf/2502.17776"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper is centered on improving search for tip-of-the-tongue scenarios. There is no indication that creativity is a component of its research focus or outcomes."
      },
      "tasks": [
        "Community Question Answering",
        "Question Answering",
        "Retrieval"
      ],
      "repo_urls": [
        "https://github.com/kimdanny/llm-tot-query-elicitation",
        "https://github.com/kimdanny/human-tot-query-elicitation-mturk"
      ],
      "source": "arXiv"
    },
    {
      "id": "2503.13843",
      "abstract": "The current state of modern web interfaces, especially in regards to accessibility focused usage is extremely lacking. Traditional methods for web interaction, such as scripting languages and screen readers, often lack the flexibility to handle dynamic content or the intelligence to interpret high-level user goals. To address these limitations, we introduce WebNav, a novel agent for multi-modal web navigation. WebNav leverages a dual Large Language Model (LLM) architecture to translate natural language commands into precise, executable actions on a graphical user interface. The system combines vision-based context from screenshots with a dynamic DOM-labeling browser extension to robustly identify interactive elements. A high-level 'Controller' LLM strategizes the next step toward a user's goal, while a second 'Assistant' LLM generates the exact parameters for execution. This separation of concerns allows for sophisticated task decomposition and action formulation. Our work presents the complete architecture and implementation of WebNav, demonstrating a promising approach to creating more intelligent web automation agents.",
      "authors": [
        "Trisanth Srinivasan and Santosh Patapati"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-03-18T02:33:27+00:00",
          "link": "https://arxiv.org/abs/2503.13843v1",
          "size": "122kb",
          "version": "v1"
        },
        {
          "date": "2025-07-06T08:41:00+00:00",
          "link": "https://arxiv.org/abs/2503.13843v2",
          "size": "78kb",
          "version": "v2"
        }
      ],
      "title": "WebNav: An Intelligent Agent for Voice-Controlled Web Navigation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.13843",
        "HTML": "https://arxiv.org/html/2503.13843v2",
        "PDF": "https://arxiv.org/pdf/2503.13843"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The focus is on developing a voice-controlled web navigation agent. The paper does not explore or mention creativity in any form."
      },
      "conference": "webnav-an-intelligent-agent-for-voice",
      "conference_url_abs": "https://arxiv.org/abs/2503.13843",
      "tasks": [],
      "source": "arXiv"
    },
    {
      "id": "2504.15133",
      "abstract": "In this paper, we introduce EasyEdit2, a framework designed to enable plug-and-play adjustability for controlling Large Language Model (LLM) behaviors. EasyEdit2 supports a wide range of test-time interventions, including safety, sentiment, personality, reasoning patterns, factuality, and language features. Unlike its predecessor, EasyEdit2 features a new architecture specifically designed for seamless model steering. It comprises key modules such as the steering vector generator and the steering vector applier, which enable automatic generation and application of steering vectors to influence the model's behavior without modifying its parameters. One of the main advantages of EasyEdit2 is its ease of use-users do not need extensive technical knowledge. With just a single example, they can effectively guide and adjust the model's responses, making precise control both accessible and efficient. Empirically, we report model steering performance across different LLMs, demonstrating the effectiveness of these techniques. We have released the source code on GitHub at https://github.com/zjunlp/EasyEdit along with a demonstration notebook. In addition, we provide a demo video at https://www.youtube.com/watch?v=AkfoiPfp5rQ for a quick introduction.",
      "authors": [
        "Ziwen Xu",
        "Shuxun Wang",
        "Kewei Xu",
        "Haoming Xu",
        "Mengru Wang",
        "Xinle Deng",
        "Yunzhi Yao",
        "Guozhou Zheng",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-04-21T14:33:55+00:00",
          "link": "https://arxiv.org/abs/2504.15133v1",
          "size": "3426kb",
          "version": "v1"
        },
        {
          "date": "2025-07-06T04:59:13+00:00",
          "link": "https://arxiv.org/abs/2504.15133v2",
          "size": "3986kb",
          "version": "v2"
        }
      ],
      "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models",
      "links": {
        "Abstract": "https://arxiv.org/abs/2504.15133",
        "HTML": "https://arxiv.org/html/2504.15133v2",
        "PDF": "https://arxiv.org/pdf/2504.15133"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "EasyEdit2 is about steering LLMs, focusing on controllability and user-guided adjustments without requiring technical expertise. It doesn't explore creativity in language models or model outputs."
      },
      "tasks": [
        "Language Modeling",
        "Language Modelling",
        "Large Language Model"
      ],
      "repo_urls": [
        "https://github.com/zjunlp/easyedit"
      ],
      "source": "arXiv"
    },
    {
      "id": "2504.19131",
      "abstract": "3D generative AI enables rapid and accessible creation of 3D models from text or image inputs. However, translating these outputs into physical objects remains a challenge due to the constraints in the physical world. Recent studies have focused on improving the capabilities of 3D generative AI to produce fabricable outputs, with 3D printing as the main fabrication method. However, this workshop paper calls for a broader perspective by considering how fabrication methods align with the capabilities of 3D generative AI. As a case study, we present a novel system using discrete robotic assembly and 3D generative AI to make physical objects. Through this work, we identified five key aspects to consider in a physical making process based on the capabilities of 3D generative AI. 1) Fabrication Constraints: Current text-to-3D models can generate a wide range of 3D designs, requiring fabrication methods that can adapt to the variability of generative AI outputs. 2) Time: While generative AI can generate 3D models in seconds, fabricating physical objects can take hours or even days. Faster production could enable a closer iterative design loop between humans and AI in the making process. 3) Sustainability: Although text-to-3D models can generate thousands of models in the digital world, extending this capability to the real world would be resource-intensive, unsustainable and irresponsible. 4) Functionality: Unlike digital outputs from 3D generative AI models, the fabrication method plays a crucial role in the usability of physical objects. 5) Accessibility: While generative AI simplifies 3D model creation, the need for fabrication equipment can limit participation, making AI-assisted creation less inclusive. These five key aspects provide a framework for assessing how well a physical making process aligns with the capabilities of 3D generative AI and values in the world.",
      "authors": [
        "Alexander Htet Kyaw",
        "Se Hwan Jeon",
        "Miana Smith",
        "Neil Gershenfeld"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-04-27T07:07:12+00:00",
          "link": "https://arxiv.org/abs/2504.19131v1",
          "size": "5267kb",
          "version": "v1"
        },
        {
          "date": "2025-07-03T19:12:00+00:00",
          "link": "https://arxiv.org/abs/2504.19131v2",
          "size": "2134kb",
          "version": "v2"
        }
      ],
      "title": "Making Physical Objects with Generative AI and Robotic Assembly: Considering Fabrication Constraints, Sustainability, Time, Functionality, and Accessibility",
      "links": {
        "Abstract": "https://arxiv.org/abs/2504.19131",
        "HTML": "https://arxiv.org/html/2504.19131v2",
        "PDF": "https://arxiv.org/pdf/2504.19131"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses the creation of physical objects using generative AI, involving creative aspects like iterative design. While it isn't focused on creativity itself, it touches on aspects related to creative processes in 3D model generation and fabrication."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.02715",
      "abstract": "We present a demo of UltrasonicSpheres, a novel system for location-specific audio delivery using wearable earphones that decode ultrasonic signals into audible sound. Unlike conventional beamforming setups, UltrasonicSpheres relies on single ultrasonic speakers to broadcast localized audio with multiple channels, each encoded on a distinct ultrasonic carrier frequency. Users wearing our acoustically transparent earphones can demodulate their selected stream, such as exhibit narrations in a chosen language, while remaining fully aware of ambient environmental sounds. The experience preserves spatial audio perception, giving the impression that the sound originates directly from the physical location of the source. This enables personalized, localized audio without requiring pairing, tracking, or additional infrastructure. Importantly, visitors not equipped with the earphones are unaffected, as the ultrasonic signals are inaudible to the human ear. Our demo invites participants to explore multiple co-located audio zones and experience how UltrasonicSpheres supports unobtrusive delivery of personalized sound in public spaces.",
      "authors": [
        "Michael K\\\"uttner",
        "Valeria Zitz",
        "Kathrin Gerling",
        "Michael Beigl",
        "Tobias R\\\"oddiger"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Sound (cs.SD)",
        "Human-Computer Interaction (cs.HC)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-03T10:13:24+00:00",
          "link": "https://arxiv.org/abs/2506.02715v1",
          "size": "4388kb",
          "version": "v1"
        },
        {
          "date": "2025-07-05T08:37:11+00:00",
          "link": "https://arxiv.org/abs/2506.02715v2",
          "size": "4388kb",
          "version": "v2"
        }
      ],
      "title": "UltrasonicSpheres: Localized, Multi-Channel Sound Spheres Using Off-the-Shelf Speakers and Earables",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.02715",
        "HTML": "https://arxiv.org/html/2506.02715v2",
        "PDF": "https://arxiv.org/pdf/2506.02715"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses a system for delivering localized audio via ultrasonic signals, focusing on technology and user experience in audio delivery without any direct focus on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.06162",
      "abstract": "Scientific recommender systems, such as Google Scholar and Web of Science, are essential tools for discovery. Search algorithms that power work through stigmergy, a collective intelligence mechanism that surfaces useful paths through repeated engagement. While generally effective, this \"rich-get-richer\" dynamic results in a small number of high-profile papers that dominate visibility. This essay argues argue that these algorithm over-reliance on popularity fosters intellectual homogeneity and exacerbates structural inequities, stifling innovative and diverse perspectives critical for scientific progress. We propose an overhaul of search platforms to incorporate user-specific calibration, allowing researchers to manually adjust the weights of factors like popularity, recency, and relevance. We also advise platform developers on how text embeddings and LLMs could be implemented in ways that increase user autonomy. While our suggestions are particularly pertinent to aligning recommender systems with scientific values, these ideas are broadly applicable to information access systems in general. Designing platforms that increase user autonomy is an important step toward more robust and dynamic information",
      "authors": [
        "Zackary Okun Dunivin",
        "Paul E. Smaldino"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Information Retrieval (cs.IR)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-06T15:27:23+00:00",
          "link": "https://arxiv.org/abs/2506.06162v1",
          "size": "10kb",
          "version": "v1"
        },
        {
          "date": "2025-07-04T03:51:55+00:00",
          "link": "https://arxiv.org/abs/2506.06162v2",
          "size": "11kb",
          "version": "v2"
        }
      ],
      "title": "Recommender systems, stigmergy, and the tyranny of popularity",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.06162",
        "HTML": "https://arxiv.org/html/2506.06162v2",
        "PDF": "https://arxiv.org/pdf/2506.06162"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper critiques recommender systems and suggests design changes to foster more diverse perspectives. Creativity is indirectly related as it advocates for diversity and innovation in academic communications, presenting creativity as a benefit of improved systems."
      },
      "tasks": [
        "Recommendation Systems",
        "Word Embeddings"
      ],
      "source": "arXiv"
    },
    {
      "id": "2506.10974",
      "abstract": "Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficient and robust step toward fully automated data science.",
      "authors": [
        "Yixin Ou",
        "Yujie Luo",
        "Jingsheng Zheng",
        "Lanning Wei",
        "Shuofei Qiao",
        "Jintian Zhang",
        "Da Zheng",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)",
        "Multiagent Systems (cs.MA)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-12T17:59:32+00:00",
          "link": "https://arxiv.org/abs/2506.10974v1",
          "size": "1011kb",
          "version": "v1"
        },
        {
          "date": "2025-07-06T16:05:42+00:00",
          "link": "https://arxiv.org/abs/2506.10974v2",
          "size": "1007kb",
          "version": "v2"
        }
      ],
      "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.10974",
        "PDF": "https://arxiv.org/pdf/2506.10974"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The focus is on automating data science tasks using LLM agents, emphasizing efficiency and automation rather than creativity. There is no evident discussion or analysis of creativity-related aspects."
      },
      "tasks": [
        "Code Generation",
        "Large Language Model"
      ],
      "repo_urls": [
        "https://github.com/innovatingai/automind"
      ],
      "source": "arXiv"
    },
    {
      "id": "2507.00050",
      "abstract": "Human Activity Recognition (HAR), which uses data from Inertial Measurement Unit (IMU) sensors, has many practical applications in healthcare and assisted living environments. However, its use in real-world scenarios has been limited by the lack of comprehensive IMU-based HAR datasets that cover a wide range of activities and the lack of transparency in existing HAR models. Zero-shot HAR (ZS-HAR) overcomes the data limitations, but current models struggle to explain their decisions, making them less transparent. This paper introduces a novel IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity Recognition Network (SEZ-HARN). It can recognize activities not encountered during training and provide skeleton videos to explain its decision-making process. We evaluate the effectiveness of the proposed SEZ-HARN on four benchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its performance against three state-of-the-art black-box ZS-HAR models. The experiment results demonstrate that SEZ-HARN produces realistic and understandable explanations while achieving competitive Zero-shot recognition accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\\% of the best-performing black-box model on PAMAP2 while maintaining comparable performance on the other three datasets.",
      "authors": [
        "Devin Y. De Silva",
        "Sandareka Wickramanayake",
        "Dulani Meedeniya",
        "Sanka Rasnayaka"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-25T02:10:34+00:00",
          "link": "https://arxiv.org/abs/2507.00050v1",
          "size": "4008kb",
          "version": "v1"
        },
        {
          "date": "2025-07-05T21:04:06+00:00",
          "link": "https://arxiv.org/abs/2507.00050v2",
          "size": "3412kb",
          "version": "v2"
        }
      ],
      "title": "SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.00050",
        "HTML": "https://arxiv.org/html/2507.00050v2",
        "PDF": "https://arxiv.org/pdf/2507.00050"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper addresses Human Activity Recognition with a focus on zero-shot learning and explainability. Creativity surfaces as a secondary aspect through the model's novel design and interpretation capability, but it is not the core topic."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01431",
      "abstract": "Grading handwritten, open-ended responses remains a major bottleneck in large university STEM courses. We introduce Pensieve (https://www.pensieve.co), an AI-assisted grading platform that leverages large language models (LLMs) to transcribe and evaluate student work, providing instructors with rubric-aligned scores, transcriptions, and confidence ratings. Unlike prior tools that focus narrowly on specific tasks like transcription or rubric generation, Pensieve supports the entire grading pipeline-from scanned student submissions to final feedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and has graded more than 300,000 student responses. We present system details and empirical results across four core STEM disciplines: Computer Science, Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces grading time by an average of 65%, while maintaining a 95.4% agreement rate with instructor-assigned grades for high-confidence predictions.",
      "authors": [
        "Yoonseok Yang",
        "Minjune Kim",
        "Marlon Rondinelli",
        "Keren Shao"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T07:33:19+00:00",
          "link": "https://arxiv.org/abs/2507.01431v1",
          "size": "19486kb",
          "version": "v1"
        },
        {
          "date": "2025-07-07T05:10:47+00:00",
          "link": "https://arxiv.org/abs/2507.01431v2",
          "size": "9733kb",
          "version": "v2"
        }
      ],
      "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01431",
        "HTML": "https://arxiv.org/html/2507.01431v2",
        "PDF": "https://arxiv.org/pdf/2507.01431"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on an AI-assisted grading platform for STEM courses, which helps reduce grading time and improve consistency. There is no clear connection to creativity; the study emphasizes efficiency and accuracy in grading educational content."
      },
      "source": "arXiv"
    },
    {
      "id": "2406.04794",
      "abstract": "Studies on software tutoring systems for complex learning have shown that confusion has a beneficial relationship with the learning experience and student engagement (Arguel et al., 2017). Causing confusion can prevent boredom while signs of confusion can serve as a signal of genuine learning and as a predecessor for frustration. There is little to no research on the role of confusion in early childhood education and playful learning, as these studies primarily focus on high school and university students during complex learning tasks. Despite that, the field acknowledges that confusion may be caused by inconsistency between information and a student's internal model referred to as cognitive disequilibrium known from the theory of cognitive development, which was originally theorized based on observational studies on young children (D'Mello and Graesser, 2012). Therefore, there is reason to expect that the virtues of confusion also apply to young children engaging in learning activities, such as playful learning. To investigate the role of confusion in playful learning, we conducted an observational study, in which the behavior and expressed emotions of young children were collected by familiar pedagogues, using a web app, while they engaged with playful learning games designed for kindergartens. The expressed emotions were analyzed using a likelihood metric to determine the likely transitions between emotions (D'Mello and Graesser, 2012). The preliminary results showed that during short play sessions, children express confusion, frustration, and boredom. Furthermore, the observed emotional transitions were matched with previously established models of affect dynamics during complex learning. We argue that games with a learning objective can benefit by purposely confusing the player and how the player's confusion may be managed to improve the learning experience.",
      "authors": [
        "Thomas Vase Schultz Volden and Paolo Burelli"
      ],
      "license": "http://creativecommons.org/publicdomain/zero/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-06-07T09:44:56+00:00",
          "link": "https://arxiv.org/abs/2406.04794v1",
          "size": "396kb",
          "version": "v1"
        }
      ],
      "title": "Children's expressed emotions during playful learning games",
      "links": {
        "Abstract": "https://arxiv.org/abs/2406.04794",
        "PDF": "https://arxiv.org/pdf/2406.04794"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses playful learning games for children and touches on emotional engagement, potentially linking to creativity. Creativity may be a secondary theme through the playful, game-based learning approach."
      },
      "source": "arXiv"
    },
    {
      "id": "2505.07142",
      "abstract": "The paper investigates the integration of Large Language Models (LLMs) into Conversational Agents (CAs) to encourage a shift in consumption patterns from a demand-driven to a supply-based paradigm. Specifically, the research examines the role of anthropomorphic design in delivering environmentally conscious messages by comparing two CA designs: a personified agent representing an appliance and a traditional, non-personified assistant. A lab study (N=26) assessed the impact of these designs on interaction, perceived self-efficacy, and engagement. Results indicate that LLM-based CAs significantly enhance users' self-reported eco-friendly behaviors, with participants expressing greater confidence in managing energy consumption. While the anthropomorphic design did not notably affect self-efficacy, those interacting with the personified agent reported a stronger sense of connection with the system. These findings suggest that although anthropomorphic CAs may improve user engagement, both designs hold promise for fostering sustainable behaviors in home energy management.",
      "authors": [
        "Mathyas Giudici",
        "Samuele Scherini",
        "Pascal Chaussumier",
        "Stefano Ginocchio",
        "and Franca Garzotto"
      ],
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-05-11T22:58:26+00:00",
          "link": "https://arxiv.org/abs/2505.07142v1",
          "size": "11319kb",
          "version": "v1"
        },
        {
          "date": "2025-05-13T13:15:18+00:00",
          "link": "https://arxiv.org/abs/2505.07142v2",
          "size": "11318kb",
          "version": "v2"
        }
      ],
      "title": "Exploring Anthropomorphism in Conversational Agents for Environmental Sustainability",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.07142",
        "HTML": "https://arxiv.org/html/2505.07142",
        "PDF": "https://arxiv.org/pdf/2505.07142"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This research examines anthropomorphism in conversational agents for environmental sustainability. It doesn't relate to creativity, focusing on user engagement and eco-friendly behavior rather than creative interaction."
      },
      "source": "arXiv"
    },
    {
      "id": "2106.12839",
      "abstract": "Graph neural networks (GNNs) are a class of powerful machine learning tools that model node relations for making predictions of nodes or links. GNN developers rely on quantitative metrics of the predictions to evaluate a GNN, but similar to many other neural networks, it is difficult for them to understand if the GNN truly learns characteristics of a graph as expected. We propose an approach to corresponding an input graph to its node embedding (aka latent space), a common component of GNNs that is later used for prediction. We abstract the data and tasks, and develop an interactive multi-view interface called CorGIE to instantiate the abstraction. As the key function in CorGIE, we propose the K-hop graph layout to show topological neighbors in hops and their clustering structure. To evaluate the functionality and usability of CorGIE, we present how to use CorGIE in two usage scenarios, and conduct a case study with five GNN experts.",
      "authors": [
        "Zipeng Liu",
        "Yang Wang",
        "J\\\"urgen Bernard",
        "Tamara Munzner"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2021-06-24T08:59:53+00:00",
          "link": "https://arxiv.org/abs/2106.12839v1",
          "size": "5357kb",
          "version": "v1"
        },
        {
          "date": "2021-11-08T09:51:37+00:00",
          "link": "https://arxiv.org/abs/2106.12839v2",
          "size": "7438kb",
          "version": "v2"
        }
      ],
      "title": "Visualizing Graph Neural Networks with CorGIE: Corresponding a Graph to Its Embedding",
      "links": {
        "Abstract": "https://arxiv.org/abs/2106.12839",
        "PDF": "https://arxiv.org/pdf/2106.12839"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on visualizing Graph Neural Networks' embeddings using CorGIE but does not address creativity or incorporate creative processes or systems."
      },
      "tasks": [],
      "repo_urls": [
        "https://github.com/zipengliu/CorGIE"
      ],
      "source": "arXiv"
    },
    {
      "id": "2411.01866",
      "abstract": "When interacting with each other, humans adjust their behavior based on perceived trust. However, to achieve similar adaptability, robots must accurately estimate human trust at sufficiently granular timescales during the human-robot collaboration task. A beta reputation is a popular way to formalize a mathematical estimation of human trust. However, it relies on binary performance, which updates trust estimations only after each task concludes. Additionally, manually crafting a reward function is the usual method of building a performance indicator, which is labor-intensive and time-consuming. These limitations prevent efficiently capturing continuous changes in trust at more granular timescales throughout the collaboration task. Therefore, this paper presents a new framework for the estimation of human trust using a beta reputation at fine-grained timescales. To achieve granularity in beta reputation, we utilize continuous reward values to update trust estimations at each timestep of a task. We construct a continuous reward function using maximum entropy optimization to eliminate the need for the laborious specification of a performance indicator. The proposed framework improves trust estimations by increasing accuracy, eliminating the need for manually crafting a reward function, and advancing toward developing more intelligent robots. The source code is publicly available. https://github.com/resuldagdanov/robot-learning-human-trust",
      "authors": [
        "Resul Dagdanov",
        "Milan Andrejevic",
        "Dikai Liu",
        "Chin-Teng Lin"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2024-11-04T07:46:24+00:00",
          "link": "https://arxiv.org/abs/2411.01866v1",
          "size": "4851kb",
          "version": "v1"
        }
      ],
      "title": "Improving Trust Estimation in Human-Robot Collaboration Using Beta Reputation at Fine-grained Timescales",
      "links": {
        "Abstract": "https://arxiv.org/abs/2411.01866",
        "HTML": "https://arxiv.org/html/2411.01866",
        "PDF": "https://arxiv.org/pdf/2411.01866"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper addresses trust estimation in human-robot collaboration without mentioning creativity, focusing instead on trust and performance metrics."
      },
      "tasks": [
        "Bayesian Inference",
        "Behavioural cloning",
        "Human-Object Relationship Detection",
        "Robot Manipulation"
      ],
      "repo_urls": [
        "https://github.com/resuldagdanov/robot-learning-human-trust"
      ],
      "source": "arXiv"
    }
  ],
  "subjects": [
    "Computer Vision and Pattern Recognition (cs.CV)",
    "Networking and Internet Architecture (cs.NI)",
    "Multimedia (cs.MM)",
    "Computation and Language (cs.CL)",
    "Other Quantitative Biology (q-bio.OT)",
    "Sound (cs.SD)",
    "Cryptography and Security (cs.CR)",
    "Robotics (cs.RO)",
    "Information Retrieval (cs.IR)",
    "Artificial Intelligence (cs.AI)",
    "Emerging Technologies (cs.ET)",
    "Audio and Speech Processing (eess.AS)",
    "Social and Information Networks (cs.SI)",
    "Computers and Society (cs.CY)",
    "Multiagent Systems (cs.MA)",
    "Software Engineering (cs.SE)",
    "Human-Computer Interaction (cs.HC)",
    "Graphics (cs.GR)",
    "Machine Learning (cs.LG)",
    "General Economics (econ.GN)",
    "Genomics (q-bio.GN)",
    "Economics (q-fin.EC)"
  ],
  "prompt": {
    "creativity": "\nYou are an expert in information retrieval. I will provide you with a list of research papers from arXiv, specifically in the *cs.HC* (Human-Computer Interaction) category.\n\nYour task is to analyze each paper and determine its relevance to the topic of **Creativity**.\n\n\n### Classify each paper into one of the following relevance levels\n\n- `core` \u2014 Creativity is a **primary focus**\n  - The paper directly studies or simulates creativity, with a clear focus on creativity.\n  - Includes creative tasks, co-creative systems, or creativity evaluation metrics.\n  - The title and abstract explicitly mention creativity, and the research questions are directly related to creativity.\n- `partial` \u2014 Creativity is a **secondary theme**\n  - Part of the paper relates to creativity; it is treated as an analytical dimension or design goal but not the main objective (e.g., user creativity, design support).\n  - Creativity may appear in discussions, experiments, or auxiliary applications.\n  - Creativity is presented as a supporting topic (e.g., evaluation criteria, user feedback).\n- `irrelevant` \u2014 **No clear connection** to creativity\n  - The paper does not address creativity as a topic.\n  - Focuses on unrelated technical content (e.g., compression, security, optimization).\n  - If creativity is mentioned, it is only superficial and lacks substantive content.\n\n\n### Return your results in the following JSON format\n\n```json\n{\n  \"result\": [\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    },\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    }\n  ]\n}\n```\n\nBe concise but specific in your reasoning, referencing key terms or sections when applicable.\n"
  },
  "description": "Data source: https://arxiv.org/list/cs.HC/new",
  "level_tatistics": {
    "core": 5,
    "irrelevant": 50,
    "partial": 22
  },
  "arxiv_update_date": "2025-07-08",
  "updated_at": "2025-07-08 11:14:52"
}