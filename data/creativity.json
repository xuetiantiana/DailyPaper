{
  "data": [
    {
      "id": "2507.08804",
      "abstract": "AI-augmented systems are traditionally designed to streamline human decision-making by minimizing cognitive load, clarifying arguments, and optimizing efficiency. However, in a world where algorithmic certainty risks becoming an Orwellian tool of epistemic control, true intellectual growth demands not passive acceptance but active struggle. Drawing on the dystopian visions of George Orwell and Philip K. Dick - where reality is unstable, perception malleable, and truth contested - this paper introduces Cognitive Dissonance AI (CD-AI): a novel framework that deliberately sustains uncertainty rather than resolving it. CD-AI does not offer closure, but compels users to navigate contradictions, challenge biases, and wrestle with competing truths. By delaying resolution and promoting dialectical engagement, CD-AI enhances reflective reasoning, epistemic humility, critical thinking, and adaptability in complex decision-making. This paper examines the theoretical foundations of the approach, presents an implementation model, explores its application in domains such as ethics, law, politics, and science, and addresses key ethical concerns - including decision paralysis, erosion of user autonomy, cognitive manipulation, and bias in AI reasoning. In reimagining AI as an engine of doubt rather than a deliverer of certainty, CD-AI challenges dominant paradigms of AI-augmented reasoning and offers a new vision - one in which AI sharpens the mind not by resolving conflict, but by sustaining it. Rather than reinforcing Huxleyan complacency or pacifying the user into intellectual conformity, CD-AI echoes Nietzsche's vision of the Uebermensch - urging users to transcend passive cognition through active epistemic struggle.",
      "authors": [
        "Delia Deliu"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "2025-04-23T03:18:05+00:00",
          "link": "https://arxiv.org/abs/2507.08804v1",
          "size": "363kb",
          "version": "v1"
        }
      ],
      "title": "Cognitive Dissonance Artificial Intelligence (CD-AI): The Mind at War with Itself. Harnessing Discomfort to Sharpen Critical Thinking",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08804",
        "PDF": "https://arxiv.org/pdf/2507.08804"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses Cognitive Dissonance AI, which encourages intellectual struggle and critical thinking. While not explicitly focused on creativity, fostering adaptive reasoning may relate as a secondary theme to creative thinking."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08805",
      "abstract": "This paper introduces iREACT, a novel VR simulation addressing key limitations in traditional cardiac arrest (CA) training. Conventional methods struggle to replicate the dynamic nature of real CA events, hindering Crew Resource Management (CRM) skill development. iREACT provides a non-linear, collaborative environment where teams respond to changing patient states, mirroring real CA complexities. By capturing multi-modal data (user actions, cognitive load, visual gaze) and offering real-time and post-session feedback, iREACT enhances CRM assessment beyond traditional methods. A formative evaluation with medical experts underscores its usability and educational value, with potential applications in other high-stakes training scenarios to improve teamwork, communication, and decision-making.",
      "authors": [
        "Mike Kentros",
        "Manos Kamarianakis",
        "Michael Cole",
        "Vitaliy Popov",
        "Antonis Protopsaltis",
        "George Papagiannakis"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)",
        "Graphics (cs.GR)"
      ],
      "submission_historys": [
        {
          "date": "2025-04-23T12:48:47+00:00",
          "link": "https://arxiv.org/abs/2507.08805v1",
          "size": "6384kb",
          "version": "v1"
        }
      ],
      "title": "Non-linear, Team-based VR Training for Cardiac Arrest Care with enhanced CRM Toolkit",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08805",
        "HTML": "https://arxiv.org/html/2507.08805v1",
        "PDF": "https://arxiv.org/pdf/2507.08805"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on VR training for cardiac arrest care, emphasizing teamwork, communication, and decision-making skills rather than creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08914",
      "abstract": "Adolescents increasingly rely on online technologies to explore their identities, form social connections, and access information and entertainment. However, their growing digital engagement exposes them to significant online risks, particularly in underrepresented contexts like West Africa. This study investigates the online experiences of 409 secondary school adolescents in Nigeria's Federal Capital Territory (FCT), focusing on their access to technology, exposure to risks, coping strategies, key stakeholders influencing their online interactions, and recommendations for improving online safety. Using self-administered surveys, we found that while most adolescents reported moderate access to online technology and connectivity, those who encountered risks frequently reported exposure to inappropriate content and online scams. Blocking and reporting tools were the most commonly used strategies, though some adolescents responded with inaction due to limited resources or awareness. Parents emerged as the primary support network, though monitoring practices and communication varied widely. Guided by Protection Motivation Theory (PMT), our analysis interprets adolescents' online safety behaviors as shaped by both their threat perceptions and their confidence in available coping strategies. A thematic analysis of their recommendations highlights the need for greater awareness and education, parental mediation, enhanced safety tools, stricter age restrictions, improved content moderation, government accountability, and resilience-building initiatives. Our findings underscore the importance of culturally and contextually relevant interventions to empower adolescents in navigating the digital world, with implications for parents, educators, designers, and policymakers.",
      "authors": [
        "Munachimso B. Oguine and Ozioma C. Oguine and Karla Badillo-Urquiola and Oluwasogo Adekunle Okunade"
      ],
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-11T15:39:01+00:00",
          "link": "https://arxiv.org/abs/2507.08914v1",
          "size": "586kb",
          "version": "v1"
        }
      ],
      "title": "'Teens Need to Be Educated on the Danger': Digital Access, Online Risks, and Safety Practices Among Nigerian Adolescents",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08914",
        "PDF": "https://arxiv.org/pdf/2507.08914"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The study investigates online risks and safety practices among Nigerian adolescents, with no emphasis or connection to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08973",
      "abstract": "As we move towards a future of autonomous vehicles, questions regarding their method of communication have arisen. One of the common questions concerns the placement of the signaling used to communicate with pedestrians and road users, but little work has been published fully dedicated to exploring this. This paper uses a simulation made in the Unity game engine to record the visibility of fifteen different vehicles, specifically regarding the visibility of frontal elements by a pedestrian on the sidewalk. Variables include the vehicle position, number of vehicles on the road, and minimum and maximum distance of the recorded points. It was concluded that the areas of the vehicle most often seen by pedestrians on the sidewalk attempting to cross the road were the frontal frontal fenders and the headlights, with the frontal wheels, frontal doors, bumper, and side mirrors are less visible alternatives. These findings are valuable in the future design of signaling for autonomous vehicles, in order to ensure pedestrians are able to see them on approaching vehicles. The software used provides a platform for similar works in the future to be conducted.",
      "authors": [
        "Jose Gonzalez-Belmonte and Jaerock Kwon"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-11T19:04:23+00:00",
          "link": "https://arxiv.org/abs/2507.08973v1",
          "size": "9716kb",
          "version": "v1"
        }
      ],
      "title": "Analytical Study on the Visibility of Potential Positions for External Human-Machine Interfaces",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08973",
        "HTML": "https://arxiv.org/html/2507.08973v1",
        "PDF": "https://arxiv.org/pdf/2507.08973"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper deals with visibility issues in signaling autonomous vehicles, focusing on pedestrian and vehicle interactions without addressing creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.09100",
      "abstract": "In decision-making conversations, experts must navigate complex choices and make on-the-spot decisions while engaged in conversation. Although extensive historical data often exists, the real-time nature of these scenarios makes it infeasible for decision-makers to review and leverage relevant information. This raises an interesting question: What if experts could utilize relevant past data in real-time decision-making through insights derived from past data? To explore this, we implemented a conversational user interface, taking doctor-patient interactions as an example use case. Our system continuously listens to the conversation, identifies patient problems and doctor-suggested solutions, and retrieves related data from an embedded dataset, generating concise insights using a pipeline built around a retrieval-based Large Language Model (LLM) agent. We evaluated the prototype by embedding Health Canada datasets into a vector database and conducting simulated studies using sample doctor-patient dialogues, showing effectiveness but also challenges, setting directions for the next steps of our work.",
      "authors": [
        "Mohammad Abolnejadian",
        "Shakiba Amirshahi",
        "Matthew Brehmer",
        "Anamaria Crisan"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-12T00:59:41+00:00",
          "link": "https://arxiv.org/abs/2507.09100v1",
          "size": "4227kb",
          "version": "v1"
        }
      ],
      "title": "AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.09100",
        "HTML": "https://arxiv.org/html/2507.09100v1",
        "PDF": "https://arxiv.org/pdf/2507.09100"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper focuses on augmenting expert decision-making with historical data, which can involve creative thinking, especially in problem-solving and decision-making processes, but creativity is not the primary focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.09190",
      "abstract": "Protecting personal computers (PCs) from unauthorized access typically relies on password authentication, which is know to suffer from cognitive burden and weak credentials. As many users nowadays carry mobile devices with advanced security features throughout their day, there is an opportunity to leverage these devices to improve authentication to PCs. In this paper we utilize a token-based passwordless approach where users authenticate to their PC by confirming the authentication request on their smartphones or smartwatches. Upon a request to login to the PC, or to evaluate privileges, the PC issues an authentication request that users receive on their mobile devices, where users can confirm or deny the request. We evaluate button tap and biometric fingerprint verification as confirmation variants, and compare their authentication duration, success rate, and usability to traditional password-based authentication in a user study with 30 participants and a total of 1,200 authentication attempts. Smartwatch-based authentication outperformed password-based authentication and smartphone-based variants in authentication duration, while showing comparable success rates. Participants rated smartwatch-based authentication highest in usability, followed by password-based authentication and smartphone-based authentication.",
      "authors": [
        "Andreas Pramendorfer and Rainhard Dieter Findling"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-12T08:17:59+00:00",
          "link": "https://arxiv.org/abs/2507.09190v1",
          "size": "67kb",
          "version": "v1"
        }
      ],
      "title": "User-to-PC Authentication Through Confirmation on Mobile Devices: On Usability and Performance",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.09190",
        "HTML": "https://arxiv.org/html/2507.09190v1",
        "PDF": "https://arxiv.org/pdf/2507.09190"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper discusses authentication methods and their usability and performance. It lacks any focus or mention of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.09262",
      "abstract": "Accurate assessment of mental workload (MW) is crucial for understanding cognitive processes during visualization tasks. While EEG-based measures are emerging as promising alternatives to conventional assessment techniques, such as selfreport measures, studies examining consistency across these different methodologies are limited. In a preliminary study, we observed indications of potential discrepancies between EEGbased and self-reported MW measures. Motivated by these preliminary observations, our study further explores the discrepancies between EEG-based and self-reported MW assessment methods through an experiment involving visualization tasks. In the experiment, we employ two benchmark tasks: the Visualization Literacy Assessment Test (VLAT) and a Spatial Visualization (SV) task. EEG signals are recorded from participants using a 32-channel system at a sampling rate of 128 Hz during the visualization tasks. For each participant, MW is estimated using an EEG-based model built on a Graph Attention Network (GAT) architecture, and these estimates are compared with conventional MW measures to examine potential discrepancies. Our findings reveal notable discrepancies between task difficulty and EEG-based MW estimates, as well as between EEG-based and self-reported MW measures across varying task difficulty levels. Additionally, the observed patterns suggest the presence of unconscious cognitive effort that may not be captured by selfreport alone.",
      "authors": [
        "Soobin Yim",
        "Sangbong Yoo",
        "Chanyoung Yoon",
        "Chanyoung Jung",
        "Chansoo Kim",
        "Yun Jang",
        "and Ghulam Jilani Quadri"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-12T11:49:03+00:00",
          "link": "https://arxiv.org/abs/2507.09262v1",
          "size": "2199kb",
          "version": "v1"
        }
      ],
      "title": "Discrepancies in Mental Workload Estimation: Self-Reported versus EEG-Based Measures in Data Visualization Evaluation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.09262",
        "HTML": "https://arxiv.org/html/2507.09262v1",
        "PDF": "https://arxiv.org/pdf/2507.09262"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper is about mental workload estimation and discrepancies between EEG-based and self-reported measures during data visualization tasks. Creativity is not addressed in this context."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.09489",
      "abstract": "The design of urban road networks significantly influences traffic conditions, underscoring the importance of informed traffic planning. Traffic planning experts rely on specialized platforms to simulate traffic systems, assessing the efficacy of the road network across various states of modifications. Nevertheless, a prevailing issue persists: many existing traffic planning platforms exhibit inefficiencies in flexibly interacting with the road network's structure and attributes and intuitively comparing multiple states during the iterative planning process. This paper introduces TraSculptor, an interactive planning decision-making system. To develop TraSculptor, we identify and address two challenges: interactive modification of road networks and intuitive comparison of multiple network states. For the first challenge, we establish flexible interactions to enable experts to easily and directly modify the road network on the map. For the second challenge, we design a comparison view with a history tree of multiple states and a road-state matrix to facilitate intuitive comparison of road network states. To evaluate TraSculptor, we provided a usage scenario where the Braess's paradox was showcased, invited experts to perform a case study on the Sioux Falls network, and collected expert feedback through interviews.",
      "authors": [
        "Zikun Deng",
        "Yuanbang Liu",
        "Mingrui Zhu",
        "Da Xiang",
        "Haiyue Yu",
        "Zicheng Su",
        "Qinglong Lu",
        "Tobias Schreck",
        "Yi Cai"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-13T04:23:41+00:00",
          "link": "https://arxiv.org/abs/2507.09489v1",
          "size": "6710kb",
          "version": "v1"
        }
      ],
      "title": "TraSculptor: Visual Analytics for Enhanced Decision-Making in Road Traffic Planning",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.09489",
        "HTML": "https://arxiv.org/html/2507.09489v1",
        "PDF": "https://arxiv.org/pdf/2507.09489"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper addresses visual analytics in road traffic planning. The focus is on decision-making and comparisons of traffic states, with no clear mention or exploration of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.09549",
      "abstract": "Prototyping is widely regarded in Human-Computer Interaction as an iterative process through which ideas are tested and refined, often via visual mockups, screen flows, and coded simulations. This position paper critiques the visual-centric norms embedded in prototyping culture by drawing from the lived experiences of blind scholars and insights from cultural disability studies. It discusses how dominant methods of prototyping rely on an unexamined fidelity to sight, privileging what can be rendered visibly coherent while marginalizing other modes of knowing and making. By repositioning prototyping as a situated, embodied, and relational practice, this paper challenges HCI to rethink what kinds of design participation are legitimized and which are excluded when prototyping is reduced to screen-based simulations.",
      "authors": [
        "Hrittika Bhowmick",
        "Shilpaa Anand"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-13T09:25:19+00:00",
          "link": "https://arxiv.org/abs/2507.09549v1",
          "size": "54kb",
          "version": "v1"
        }
      ],
      "title": "The Spectacle of Fidelity: Blind Resistance and the Wizardry of Prototyping",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.09549",
        "HTML": "https://arxiv.org/html/2507.09549v1",
        "PDF": "https://arxiv.org/pdf/2507.09549"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper critiques current prototyping practices and suggests alternative approaches. While creativity is not the primary focus, it indirectly addresses creativity through exploring different design and prototyping practices."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.09664",
      "abstract": "Programming-by-prompting with generative AI offers a new paradigm for end-user programming, shifting the focus from syntactic fluency to semantic intent. This shift holds particular promise for non-programmers such as educators, who can describe instructional goals in natural language to generate interactive learning content. Yet in bypassing direct code authoring, many of programming's core affordances - such as traceability, stepwise refinement, and behavioral testing - are lost. We propose the Chain-of-Abstractions (CoA) framework as a way to recover these affordances while preserving the expressive flexibility of natural language. CoA decomposes the synthesis process into a sequence of cognitively meaningful, task-aligned representations that function as checkpoints for specification, inspection, and refinement. We instantiate this approach in SimStep, an authoring environment for teachers that scaffolds simulation creation through four intermediate abstractions: Concept Graph, Scenario Graph, Learning Goal Graph, and UI Interaction Graph. To address ambiguities and misalignments, SimStep includes an inverse correction process that surfaces in-filled model assumptions and enables targeted revision without requiring users to manipulate code. Evaluations with educators show that CoA enables greater authoring control and interpretability in programming-by-prompting workflows.",
      "authors": [
        "Zoe Kaputa",
        "Anika Rajaram",
        "Vryan Almanon Feliciano",
        "Zhuoyue Lyu",
        "Maneesh Agrawala",
        "Hari Subramonyam"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-13T14:54:17+00:00",
          "link": "https://arxiv.org/abs/2507.09664v1",
          "size": "9812kb",
          "version": "v1"
        }
      ],
      "title": "SimStep: Chain-of-Abstractions for Incremental Specification and Debugging of AI-Generated Interactive Simulations",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.09664",
        "HTML": "https://arxiv.org/html/2507.09664v1",
        "PDF": "https://arxiv.org/pdf/2507.09664"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "Creativity is involved in the context of developing interactive learning content through programming-by-prompting, focusing on educators' ability to specify and refine creative simulations."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.09917",
      "abstract": "Spatial time series visualization offers scientific research pathways and analytical decision-making tools across various spatiotemporal domains. Despite many advanced methodologies, the seamless integration of temporal and spatial information remains a challenge. The space-time cube (STC) stands out as a promising approach for the synergistic presentation of spatial and temporal information, with successful applications across various spatiotemporal datasets. However, the STC is plagued by well-known issues such as visual occlusion and depth ambiguity, which are further exacerbated when dealing with large-scale spatial time series data. In this study, we introduce a novel technical framework termed VolumeSTCube, designed for continuous spatiotemporal phenomena. It first leverages the concept of the STC to transform discretely distributed spatial time series data into continuously volumetric data. Subsequently, volume rendering and surface rendering techniques are employed to visualize the transformed volumetric data. Volume rendering is utilized to mitigate visual occlusion, while surface rendering provides pattern details by enhanced lighting information. Lastly, we design interactions to facilitate the exploration and analysis from temporal, spatial, and spatiotemporal perspectives. VolumeSTCube is evaluated through a computational experiment, a real-world case study with one expert, and a controlled user study with twelve non-experts, compared against a baseline from prior work, showing its superiority and effectiveness in largescale spatial time series analysis.",
      "authors": [
        "Zikun Deng",
        "Jiabao Huang",
        "Chenxi Ruan",
        "Jialing Li",
        "Shaowu Gao",
        "Yi Cai"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T04:49:54+00:00",
          "link": "https://arxiv.org/abs/2507.09917v1",
          "size": "16588kb",
          "version": "v1"
        }
      ],
      "title": "Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.09917",
        "HTML": "https://arxiv.org/html/2507.09917v1",
        "PDF": "https://arxiv.org/pdf/2507.09917"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses spatial time series visualization and data rendering, with no clear connection to creativity in terms of research questions or goals."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.09959",
      "abstract": "360{\\deg} videos enable users to freely choose their viewing paths, but blind and low vision (BLV) users are often excluded from this interactive experience. To bridge this gap, we present Branch Explorer, a system that transforms 360{\\deg} videos into branching narratives -- stories that dynamically unfold based on viewer choices -- to support interactive viewing for BLV audiences. Our formative study identified three key considerations for accessible branching narratives: providing diverse branch options, ensuring coherent story progression, and enabling immersive navigation among branches. To address these needs, Branch Explorer employs a multi-modal machine learning pipeline to generate diverse narrative paths, allowing users to flexibly make choices at detected branching points and seamlessly engage with each storyline through immersive audio guidance. Evaluation with 12 BLV viewers showed that Branch Explorer significantly enhanced user agency and engagement in 360{\\deg} video viewing. Users also developed personalized strategies for exploring 360{\\deg} content. We further highlight implications for supporting accessible exploration of videos and virtual environments.",
      "authors": [
        "Shuchang Xu",
        "Xiaofu Jin",
        "Wenshuo Zhang",
        "Huamin Qu",
        "Yukang Yan"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T06:16:41+00:00",
          "link": "https://arxiv.org/abs/2507.09959v1",
          "size": "4025kb",
          "version": "v1"
        }
      ],
      "title": "Branch Explorer: Leveraging Branching Narratives to Support Interactive 360{\\deg} Video Viewing for Blind and Low Vision Users",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.09959",
        "HTML": "https://arxiv.org/html/2507.09959v1",
        "PDF": "https://arxiv.org/pdf/2507.09959"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper addresses interactive experiences for BLV users through branching narratives, which involves creative narrative generation but isn't primarily focused on creativity itself."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.10024",
      "abstract": "Design studies aim to create visualization solutions for real-world problems of different application domains. Recently, the emergence of large language models (LLMs) has introduced new opportunities to enhance the design study process, providing capabilities such as creative problem-solving, data handling, and insightful analysis. However, despite their growing popularity, there remains a lack of systematic understanding of how LLMs can effectively assist researchers in visualization-specific design studies. In this paper, we conducted a multi-stage qualitative study to fill this gap, involving 30 design study researchers from diverse backgrounds and expertise levels. Through in-depth interviews and carefully-designed questionnaires, we investigated strategies for utilizing LLMs, the challenges encountered, and the practices used to overcome them. We further compiled and summarized the roles that LLMs can play across different stages of the design study process. Our findings highlight practical implications to inform visualization practitioners, and provide a framework for leveraging LLMs to enhance the design study process in visualization research.",
      "authors": [
        "Shaolun Ruan",
        "Rui Sheng",
        "Xiaolin Wen",
        "Jiachen Wang",
        "Tianyi Zhang",
        "Yong Wang",
        "Tim Dwyer",
        "Jiannan Li"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T08:06:12+00:00",
          "link": "https://arxiv.org/abs/2507.10024v1",
          "size": "19939kb",
          "version": "v1"
        }
      ],
      "title": "Qualitative Study for LLM-assisted Design Study Process: Strategies, Challenges, and Roles",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.10024",
        "HTML": "https://arxiv.org/html/2507.10024v1",
        "PDF": "https://arxiv.org/pdf/2507.10024"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper explores the role of LLMs in enhancing design studies, including creative problem-solving as part of visualization processes, but creativity isn't the main focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.10043",
      "abstract": "Immersive analytics is gaining attention across multiple domains due to its capability to facilitate intuitive data analysis in expansive environments through user interaction with data. However, creating immersive analytics systems for specific tasks is challenging due to the need for programming expertise and significant development effort. Despite the introduction of various immersive visualization authoring toolkits, domain experts still face hurdles in adopting immersive analytics into their workflow, particularly when faced with dynamically changing tasks and data in real time. To lower such technical barriers, we introduce XROps, a web-based authoring system that allows users to create immersive analytics applications through interactive visual programming, without the need for low-level scripting or coding. XROps enables dynamic immersive analytics authoring by allowing users to modify each step of the data visualization process with immediate feedback, enabling them to build visualizations on-the-fly and adapt to changing environments. It also supports the integration and visualization of real-time sensor data from XR devices, a key feature of immersive analytics, facilitating the creation of various analysis scenarios. We evaluated the usability of XROps through a user study and demonstrate its efficacy and usefulness in several example scenarios. We have released a web platform (https://vience.io/xrops) to demonstrate various examples to supplement our findings.",
      "authors": [
        "Suemin Jeon",
        "JunYoung Choi",
        "Haejin Jeong",
        "Won-Ki Jeong"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T08:19:32+00:00",
          "link": "https://arxiv.org/abs/2507.10043v1",
          "size": "13623kb",
          "version": "v1"
        }
      ],
      "title": "XROps: A Visual Workflow Management System for Dynamic Immersive Analytics",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.10043",
        "HTML": "https://arxiv.org/html/2507.10043v1",
        "PDF": "https://arxiv.org/pdf/2507.10043"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "While the primary focus is on immersive analytics, the paper mentions facilitating intuitive interactions and lowering barriers to creation, which indirectly supports creative workflows."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.10044",
      "abstract": "Medical images often contain multiple labels with imbalanced distributions and co-occurrence, leading to bias in multi-label medical image classification. Close collaboration between medical professionals and machine learning practitioners has significantly advanced medical image analysis. However, traditional collaboration modes struggle to facilitate effective feedback between physicians and AI models, as integrating medical expertise into the training process via engineers can be time-consuming and labor-intensive. To bridge this gap, we introduce MEDebiaser, an interactive system enabling physicians to directly refine AI models using local explanations. By combining prediction with attention loss functions and employing a customized ranking strategy to alleviate scalability, MEDebiaser allows physicians to mitigate biases without technical expertise, reducing reliance on engineers, and thus enhancing more direct human-AI feedback. Our mechanism and user studies demonstrate that it effectively reduces biases, improves usability, and enhances collaboration efficiency, providing a practical solution for integrating medical expertise into AI-driven healthcare.",
      "authors": [
        "Shaohan Shi",
        "Yuheng Shao",
        "Haoran Jiang",
        "Yunjie Yao",
        "Zhijun Zhang",
        "Xu Ding",
        "Quan Li"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T08:21:48+00:00",
          "link": "https://arxiv.org/abs/2507.10044v1",
          "size": "11585kb",
          "version": "v1"
        }
      ],
      "title": "MEDebiaser: A Human-AI Feedback System for Mitigating Bias in Multi-label Medical Image Classification",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.10044",
        "HTML": "https://arxiv.org/html/2507.10044v1",
        "PDF": "https://arxiv.org/pdf/2507.10044"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The focus is on bias mitigation in medical image classification, with no clear connection to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.10099",
      "abstract": "ReDemon UI synthesizes React applications from user demonstrations, enabling designers and non-expert programmers to create UIs that integrate with standard UI prototyping workflows. Users provide a static mockup sketch with event handler holes and demonstrate desired runtime behaviors by interacting with the rendered mockup and editing the sketch. ReDemon UI identifies reactive data and synthesizes a React program with correct state update logic. We utilize enumerative synthesis for simple UIs and LLMs for more complex UIs.",
      "authors": [
        "Jay Lee",
        "Gyuhyeok Oh",
        "Joongwon Ahn",
        "Xiaokang Qiu"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Programming Languages (cs.PL)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T09:34:33+00:00",
          "link": "https://arxiv.org/abs/2507.10099v1",
          "size": "1088kb",
          "version": "v1"
        }
      ],
      "title": "ReDemon UI: Reactive Synthesis by Demonstration for Web UI",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.10099",
        "HTML": "https://arxiv.org/html/2507.10099v1",
        "PDF": "https://arxiv.org/pdf/2507.10099"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The tool enables non-expert users to create web UIs from demonstrations. This supports user creativity as part of a design goal, though it is not the primary focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.10102",
      "abstract": "INTRODUCTION: Older adults with early-stage dementia often retain procedural memory, enabling continued use of familiar technologies. Additionally, symbolic anchors such as photos or personalized content may serve as memory cues to reinforce digital engagement. This study explores how these mechanisms support technology use in dementia care within the South Korean context.\n  METHODS: We conducted in-depth interviews with 11 professional caregivers of community-dwelling older adults with cognitive decline. Grounded theory methods guided the analysis, using iterative coding and constant comparison to identify emergent themes.\n  RESULTS: Caregivers reported that familiar digital routines (e.g., taking photos) persisted through procedural memory. Symbolic anchors such as family photos or recognizable icons enhanced interaction and emotional engagement. However, unfamiliar or anthropomorphic technologies often triggered fear or symbolic resistance.\n  DISCUSSION: Findings highlight the dual role of procedural memory and symbolic anchors in sustaining digital engagement. Designing culturally responsive and cognitively accessible technologies may enhance autonomy and well-being in dementia care.\n  Keywords: procedural memory, symbolic anchors, dementia care, digital engagement, older adults, cultural adaptation, caregiving technologies",
      "authors": [
        "Jeongone Seo",
        "Kyung-zoon Hong",
        "Sol Baik"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T09:35:26+00:00",
          "link": "https://arxiv.org/abs/2507.10102v1",
          "size": "205kb",
          "version": "v1"
        }
      ],
      "title": "When Familiarity Remains: Procedural Memory, Symbolic Anchors, and Digital Engagement in Dementia Care",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.10102",
        "PDF": "https://arxiv.org/pdf/2507.10102"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses digital engagement and dementia care, with no mention or focus on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.10240",
      "abstract": "Our society increasingly depends on intelligent systems to solve complex problems, ranging from recommender systems suggesting the next movie to watch to AI models assisting in medical diagnoses for hospitalized patients. With the iterative improvement of diagnostic accuracy and efficiency, AI holds significant potential to mitigate medical misdiagnoses by preventing numerous deaths and reducing an economic burden of approximately 450 EUR billion annually. However, a key obstacle to AI adoption lies in the lack of transparency: many automated systems function as \"black boxes,\" providing predictions without revealing the underlying processes. This opacity can hinder experts' ability to trust and rely on AI systems. Visual analytics (VA) provides a compelling solution by combining AI models with interactive visualizations. These specialized charts and graphs empower users to incorporate their domain expertise to refine and improve the models, bridging the gap between AI and human understanding. In this work, we define, categorize, and explore how VA solutions can foster trust across the stages of a typical AI pipeline. We propose a design space for innovative visualizations and present an overview of our previously developed VA dashboards, which support critical tasks within the various pipeline stages, including data processing, feature engineering, hyperparameter tuning, understanding, debugging, refining, and comparing models.",
      "authors": [
        "Angelos Chatzimparmpas"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T13:03:17+00:00",
          "link": "https://arxiv.org/abs/2507.10240v1",
          "size": "16847kb",
          "version": "v1"
        }
      ],
      "title": "Visual Analytics for Explainable and Trustworthy Artificial Intelligence",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.10240",
        "HTML": "https://arxiv.org/html/2507.10240v1",
        "PDF": "https://arxiv.org/pdf/2507.10240"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "While the paper discusses visual analytics solutions for promoting explainability and trust in AI, it does not address creativity or creative tasks."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.10427",
      "abstract": "Socially Assistive Robotics (SAR) has shown promise in supporting emotion regulation for neurodivergent children. Recently, there has been increasing interest in leveraging advanced technologies to assist parents in co-regulating emotions with their children. However, limited research has explored the integration of large language models (LLMs) with SAR to facilitate emotion co-regulation between parents and children with neurodevelopmental disorders. To address this gap, we developed an LLM-powered social robot by deploying a speech communication module on the MiRo-E robotic platform. This supervised autonomous system integrates LLM prompts and robotic behaviors to deliver tailored interventions for both parents and neurodivergent children. Pilot tests were conducted with two parent-child dyads, followed by a qualitative analysis. The findings reveal MiRo-E's positive impacts on interaction dynamics and its potential to facilitate emotion regulation, along with identified design and technical challenges. Based on these insights, we provide design implications to advance the future development of LLM-powered SAR for mental health applications.",
      "authors": [
        "Jing Li and Felix Schijve and Sheng Li and Yuye Yang and Jun Hu and Emilia Barakova"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T16:16:12+00:00",
          "link": "https://arxiv.org/abs/2507.10427v1",
          "size": "2054kb",
          "version": "v1"
        }
      ],
      "title": "Towards Emotion Co-regulation with LLM-powered Socially Assistive Robots: Integrating LLM Prompts and Robotic Behaviors to Support Parent-Neurodivergent Child Dyads",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.10427",
        "HTML": "https://arxiv.org/html/2507.10427v1",
        "PDF": "https://arxiv.org/pdf/2507.10427"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper deals with emotion co-regulation using socially assistive robots and LLMs, focusing on emotional support rather than creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.10469",
      "abstract": "Advancements in artificial intelligence (AI) have significantly enhanced the realism and interactivity of non-player characters (NPCs) in virtual reality (VR), creating more engaging and believable user experiences. This paper evaluates AI-driven NPCs within a VR interrogation simulator, focusing on their perceived realism, usability, and system performance. The simulator features two AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage participants in a scenario to determine the suspect's guilt or innocence. A user study with 18 participants assessed the system using the System Usability Scale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent Believability Questionnaire, alongside latency measurements for speech-to-text (STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency. Results showed an average cycle latency of 7 seconds, influenced by the increasing conversational context. Believability scored 6.67 out of 10, with high ratings in behavior, social relationships, and intelligence but moderate scores in emotion and personality. The system achieved a SUS score of 79.44, indicating good usability. These findings demonstrate the potential of large language models to improve NPC realism and interaction in VR while highlighting challenges in reducing system latency and enhancing emotional depth. This research contributes to the development of more sophisticated AI-driven NPCs, revealing the need for performance optimization to achieve increasingly immersive virtual experiences.",
      "authors": [
        "Mikko Korkiakoski",
        "Saeid Sheikhi",
        "Jesper Nyman",
        "Jussi Saariniemi",
        "Kalle Tapio",
        "Panos Kostakos"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Multimedia (cs.MM)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T16:50:29+00:00",
          "link": "https://arxiv.org/abs/2507.10469v1",
          "size": "13517kb",
          "version": "v1"
        }
      ],
      "title": "An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived Realism and Performance in Virtual Reality Environments",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.10469",
        "HTML": "https://arxiv.org/html/2507.10469v1",
        "PDF": "https://arxiv.org/pdf/2507.10469"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This research evaluates AI-powered NPCs' realism and performance in VR environments with no emphasis or mention of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.10479",
      "abstract": "People with vision impairments (VIPs) often rely on their remaining vision when interacting with user interfaces. Simulating visual impairments is an effective tool for designers, fostering awareness of the challenges faced by VIPs. While previous research has introduced various vision impairment simulators, none have yet been developed with the direct involvement of VIPs or thoroughly evaluated from their perspective. To address this gap, we developed VIP-Sim. This symptom-based vision simulator was created through a participatory design process tailored explicitly for this purpose, involving N=7 VIPs. 21 symptoms, like field loss or light sensitivity, can be overlaid on desktop design tools. Most participants felt VIP-Sim could replicate their symptoms. VIP-Sim was received positively, but concerns about exclusion in design and comprehensiveness of the simulation remain, mainly whether it represents the experiences of other VIPs.",
      "authors": [
        "Max R\\\"adler",
        "Mark Colley",
        "and Enrico Rukzio"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T16:57:29+00:00",
          "link": "https://arxiv.org/abs/2507.10479v1",
          "size": "20659kb",
          "version": "v1"
        }
      ],
      "title": "VIP-Sim: A User-Centered Approach to Vision Impairment Simulation for Accessible Design",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.10479",
        "HTML": "https://arxiv.org/html/2507.10479v1",
        "PDF": "https://arxiv.org/pdf/2507.10479"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on simulating vision impairments for accessible design, not on creativity. It does not mention or explore creativity-related topics."
      },
      "source": "arXiv"
    },
    {
      "id": "2408.10887",
      "abstract": "This paper presents a mini-review of the current state of research in mobile manipulators with variable levels of autonomy, emphasizing their associated challenges and application environments. The need for mobile manipulators in different environments is evident due to the unique challenges and risks each presents. Many systems deployed in these environments are not fully autonomous, requiring human-robot teaming to ensure safe and reliable operations under uncertainties. Through this analysis, we identify gaps and challenges in the literature on Variable Autonomy, including cognitive workload and communication delays, and propose future directions, including whole-body Variable Autonomy for mobile manipulators, virtual reality frameworks, and large language models to reduce operators' complexity and cognitive load in some challenging and uncertain scenarios.",
      "authors": [
        "Cesar Alan Contreras",
        "Alireza Rastegarpanah",
        "Rustam Stolkin",
        "Manolis Chiou"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-08-20T14:18:35+00:00",
          "link": "https://arxiv.org/abs/2408.10887v1",
          "size": "69kb",
          "version": "v1"
        }
      ],
      "title": "A Mini-Review on Mobile Manipulators with Variable Autonomy",
      "links": {
        "Abstract": "https://arxiv.org/abs/2408.10887",
        "HTML": "https://arxiv.org/html/2408.10887v1",
        "PDF": "https://arxiv.org/pdf/2408.10887"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This review of mobile manipulators with variable autonomy centers on challenges and solutions in human-robot interaction, not related to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.08978",
      "abstract": "Increasingly, students begin learning aspects of security and privacy during their primary and secondary education (grades K-12 in the United States). Individual U.S. states and some national organizations publish teaching standards -- guidance that outlines expectations for what students should learn -- which often form the basis for course curricula. However, research has not yet examined what is covered by these standards and whether the topics align with what the broader security and privacy community thinks students should know. To shed light on these questions, we started by collecting computer science teaching standards from all U.S. states and eight national organizations. After manually examining a total of 11,954 standards, we labeled 3,778 of them as being related to security and privacy, further classifying these into 103 topics. Topics ranged from technical subjects like encryption, network security, and embedded systems to social subjects such as laws, ethics, and appropriate online behavior. Subsequently, we interviewed 11 security and privacy professionals to examine how the teaching standards align with their expectations. We found that, while the specific topics they mentioned mostly overlapped with those of existing standards, professionals placed a greater emphasis on threat modeling and security mindset.",
      "authors": [
        "Katherine Limes",
        "Nathan Malkin",
        "Kelsey R. Fulton"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-11T19:20:08+00:00",
          "link": "https://arxiv.org/abs/2507.08978v1",
          "size": "908kb",
          "version": "v1"
        }
      ],
      "title": "Characterizing Security and Privacy Teaching Standards for Schools in the United States",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.08978",
        "HTML": "https://arxiv.org/html/2507.08978v1",
        "PDF": "https://arxiv.org/pdf/2507.08978"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper examines security and privacy teaching standards, which are unrelated to creativity as it focuses on educational content standards."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.09089",
      "abstract": "Despite widespread adoption, the impact of AI tools on software development in the wild remains understudied. We conduct a randomized controlled trial (RCT) to understand how AI tools at the February-June 2025 frontier affect the productivity of experienced open-source developers. 16 developers with moderate AI experience complete 246 tasks in mature projects on which they have an average of 5 years of prior experience. Each task is randomly assigned to allow or disallow usage of early 2025 AI tools. When AI tools are allowed, developers primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet. Before starting tasks, developers forecast that allowing AI will reduce completion time by 24%. After completing the study, developers estimate that allowing AI reduced completion time by 20%. Surprisingly, we find that allowing AI actually increases completion time by 19%--AI tooling slowed developers down. This slowdown also contradicts predictions from experts in economics (39% shorter) and ML (38% shorter). To understand this result, we collect and evaluate evidence for 20 properties of our setting that a priori could contribute to the observed slowdown effect--for example, the size and quality standards of projects, or prior developer experience with AI tooling. Although the influence of experimental artifacts cannot be entirely ruled out, the robustness of the slowdown effect across our analyses suggests it is unlikely to primarily be a function of our experimental design.",
      "authors": [
        "Joel Becker",
        "Nate Rush",
        "Elizabeth Barnes",
        "David Rein"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Software Engineering (cs.SE)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-12T00:16:33+00:00",
          "link": "https://arxiv.org/abs/2507.09089v1",
          "size": "15205kb",
          "version": "v1"
        }
      ],
      "title": "Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.09089",
        "PDF": "https://arxiv.org/pdf/2507.09089"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The focus is on the productivity impact of AI tools on software developers, without any mention of creativity or related aspects."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.09111",
      "abstract": "Human-Object Interaction (HOI) detection is crucial for robot-human assistance, enabling context-aware support. However, models trained on clean datasets degrade in real-world conditions due to unforeseen corruptions, leading to inaccurate prediction. To address this, we introduce the first robustness benchmark for HOI detection, evaluating model resilience under diverse challenges. Despite advances, current models struggle with environmental variability, occlusion, and noise. Our benchmark, RoHOI, includes 20 corruption types based on HICO-DET and V-COCO datasets and a new robustness-focused metric. We systematically analyze existing models in the related field, revealing significant performance drops under corruptions. To improve robustness, we propose a Semantic-Aware Masking-based Progressive Learning (SAMPL) strategy to guide the model to be optimized based on holistic and partial cues, dynamically adjusting the model's optimization to enhance robust feature learning. Extensive experiments show our approach outperforms state-of-the-art methods, setting a new standard for robust HOI detection. Benchmarks, datasets, and code will be made publicly available at https://github.com/Kratos-Wen/RoHOI.",
      "authors": [
        "Di Wen",
        "Kunyu Peng",
        "Kailun Yang",
        "Yufan Chen",
        "Ruiping Liu",
        "Junwei Zheng",
        "Alina Roitberg",
        "Rainer Stiefelhagen"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)",
        "Robotics (cs.RO)",
        "Image and Video Processing (eess.IV)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-12T01:58:04+00:00",
          "link": "https://arxiv.org/abs/2507.09111v1",
          "size": "12813kb",
          "version": "v1"
        }
      ],
      "title": "RoHOI: Robustness Benchmark for Human-Object Interaction Detection",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.09111",
        "HTML": "https://arxiv.org/html/2507.09111v1",
        "PDF": "https://arxiv.org/pdf/2507.09111"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper addresses robustness in Human-Object Interaction detection and does not mention creativity or related aspects in its study or outcomes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.09376",
      "abstract": "Accurate sound propagation simulation is essential for delivering immersive experiences in virtual applications, yet industry methods for acoustic modeling often do not account for the full breadth of acoustic wave phenomena. This paper proposes a novel two-dimensional (2D) finite-difference time-domain (FDTD) framework that simulates sound propagation as a wave-based model in Unreal Engine, with an emphasis on capturing lower frequency wave phenomena, embedding occlusion, diffraction, reflection and interference in generated impulse responses. The process begins by discretizing the scene geometry into a 2D grid via a top-down projection from which obstacle masks and boundary conditions are derived. A Python-based FDTD solver injects a sine sweep at a source position, and virtual quadraphonic microphone arrays record pressure field responses at pre-defined listener positions. De-convolution of the pressure responses yields multi-channel impulse responses that retain spatial directionality which are then integrated into Unreal Engine's audio pipeline for dynamic playback. Benchmark tests confirm agreement with analytical expectations, and the paper outlines hybrid extensions aimed at commercial viability.",
      "authors": [
        "Bilkent Samsurya"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Sound (cs.SD)",
        "Human-Computer Interaction (cs.HC)",
        "Multimedia (cs.MM)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-12T18:46:26+00:00",
          "link": "https://arxiv.org/abs/2507.09376v1",
          "size": "2181kb",
          "version": "v1"
        }
      ],
      "title": "Acoustic Wave Modeling Using 2D FDTD: Applications in Unreal Engine For Dynamic Sound Rendering",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.09376",
        "HTML": "https://arxiv.org/html/2507.09376v1",
        "PDF": "https://arxiv.org/pdf/2507.09376"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The focus is on acoustic wave modeling for sound rendering in virtual environments. The paper does not specifically address creativity or related concepts."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.09482",
      "abstract": "Human emotions are complex, with sarcasm being a subtle and distinctive form. Despite progress in sarcasm research, sarcasm generation remains underexplored, primarily due to the overreliance on textual modalities and the neglect of visual cues, as well as the mismatch between image content and sarcastic intent in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm generation dataset with 4,970 samples, each containing an image, a sarcastic text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation framework that integrates Proximal Policy Optimization (PPO) and contrastive learning. PPO utilizes reward scores from DIP to steer the generation of sarcastic texts, while contrastive learning encourages the model to favor outputs with higher reward scores. These strategies improve overall generation quality and produce texts with more pronounced sarcastic intent. We evaluate ViSP across five metric sets and find it surpasses all baselines, including large language models, underscoring their limitations in sarcasm generation. Furthermore, we analyze the distributions of Sarcasm Scores and Factual Incongruity for both M2SaG and the texts generated by ViSP. The generated texts exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity (0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic content than the original dataset. % The dataset and code will be publicly available. Our dataset and code will be released at \\textit{https://github.com/wclapply/ViSP}.",
      "authors": [
        "Changli Wang",
        "Rui Wu",
        "Fang Yin"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-13T04:03:05+00:00",
          "link": "https://arxiv.org/abs/2507.09482v1",
          "size": "1156kb",
          "version": "v1"
        }
      ],
      "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.09482",
        "HTML": "https://arxiv.org/html/2507.09482v1",
        "PDF": "https://arxiv.org/pdf/2507.09482"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on sarcasm generation using machine learning techniques. While creativity might be an implicit element of generating sarcastic texts, the paper does not explicitly discuss or study creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.09495",
      "abstract": "Multi-agent reinforcement learning faces fundamental challenges that conventional approaches have failed to overcome: exponentially growing joint action spaces, non-stationary environments where simultaneous learning creates moving targets, and partial observability that constrains coordination. Current methods remain reactive, employing stimulus-response mechanisms that fail when facing novel scenarios. We argue for a transformative paradigm shift from reactive to proactive multi-agent intelligence through generative AI-based reinforcement learning. This position advocates reconceptualizing agents not as isolated policy optimizers, but as sophisticated generative models capable of synthesizing complex multi-agent dynamics and making anticipatory decisions based on predictive understanding of future interactions. Rather than responding to immediate observations, generative-RL agents can model environment evolution, predict other agents' behaviors, generate coordinated action sequences, and engage in strategic reasoning accounting for long-term dynamics. This approach leverages pattern recognition and generation capabilities of generative AI to enable proactive decision-making, seamless coordination through enhanced communication, and dynamic adaptation to evolving scenarios. We envision this paradigm shift will unlock unprecedented possibilities for distributed intelligence, moving beyond individual optimization toward emergent collective behaviors representing genuine collaborative intelligence. The implications extend across autonomous systems, robotics, and human-AI collaboration, promising solutions to coordination challenges intractable under traditional reactive frameworks.",
      "authors": [
        "Hang Wang",
        "Junshan Zhang"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Emerging Technologies (cs.ET)",
        "Human-Computer Interaction (cs.HC)",
        "Robotics (cs.RO)",
        "Systems and Control (cs.SY)",
        "Systems and Control (eess.SY)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-13T05:02:43+00:00",
          "link": "https://arxiv.org/abs/2507.09495v1",
          "size": "88kb",
          "version": "v1"
        }
      ],
      "title": "GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.09495",
        "HTML": "https://arxiv.org/html/2507.09495v1",
        "PDF": "https://arxiv.org/pdf/2507.09495"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "This paper discusses multi-agent systems leveraging generative AI for intelligent decision-making. While creativity is not the primary focus, the use of generative AI suggests an exploration of creative processes in agent-based systems."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.09637",
      "abstract": "Code review is a well-established and valued practice in the software engineering community contributing to both code quality and interpersonal benefits. However, there are challenges in both tools and processes that give rise to misalignments and frustrations. Recent research seeks to address this by automating code review entirely, but we believe that this risks losing the majority of the interpersonal benefits such as knowledge transfer and shared ownership.\n  We believe that by better understanding the cognitive processes involved in code review, it would be possible to improve tool support, with out without AI, and make code review both more efficient, more enjoyable, while increasing or maintaining all of its benefits. In this paper, we conduct an ethnographic think-aloud study involving 10 participants and 34 code reviews. We build a cognitive model of code review bottom up through thematic, statistical, temporal, and sequential analysis of the transcribed material. Through the data, the similarities between the cognitive process in code review and decision-making processes, especially recognition-primed decision-making, become apparent.\n  The result is the Code Review as Decision-Making (CRDM) model that shows how the developers move through two phases during the code review; first an orientation phase to establish context and rationale and then an analytical phase to understand, assess, and plan the rest of the review. Throughout the process several decisions must be taken, on writing comments, finding more information, voting, running the code locally, verifying continuous integration results, etc.\n  Analysis software and process-coded data publicly available at: https://doi.org/10.5281/zenodo.15758266",
      "authors": [
        "Lo Gullstrand Heander",
        "and Emma S\\\"oderberg",
        "and Christofer Rydenf\\\"alt"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Software Engineering (cs.SE)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-13T14:04:16+00:00",
          "link": "https://arxiv.org/abs/2507.09637v1",
          "size": "1885kb",
          "version": "v1"
        }
      ],
      "title": "Code Review as Decision-Making -- Building a Cognitive Model from the Questions Asked During Code Review",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.09637",
        "HTML": "https://arxiv.org/html/2507.09637v1",
        "PDF": "https://arxiv.org/pdf/2507.09637"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The study examines the cognitive processes in code review, focusing on decision-making. The content does not explicitly discuss creativity or related aspects."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.09788",
      "abstract": "Recent advances in Large Language Models (LLM) have led to a new class of autonomous agents, renewing and expanding interest in the area. LLM-powered Multiagent Systems (MAS) have thus emerged, both for assistive and simulation purposes, yet tools for realistic human behavior simulation -- with its distinctive challenges and opportunities -- remain underdeveloped. Existing MAS libraries and tools lack fine-grained persona specifications, population sampling facilities, experimentation support, and integrated validation, among other key capabilities, limiting their utility for behavioral studies, social simulation, and related applications. To address these deficiencies, in this work we introduce TinyTroupe, a simulation toolkit enabling detailed persona definitions (e.g., nationality, age, occupation, personality, beliefs, behaviors) and programmatic control via numerous LLM-driven mechanisms. This allows for the concise formulation of behavioral problems of practical interest, either at the individual or group level, and provides effective means for their solution. TinyTroupe's components are presented using representative working examples, such as brainstorming and market research sessions, thereby simultaneously clarifying their purpose and demonstrating their usefulness. Quantitative and qualitative evaluations of selected aspects are also provided, highlighting possibilities, limitations, and trade-offs. The approach, though realized as a specific Python implementation, is meant as a novel conceptual contribution, which can be partially or fully incorporated in other contexts. The library is available as open source at https://github.com/microsoft/tinytroupe.",
      "authors": [
        "Paulo Salem",
        "Robert Sim",
        "Christopher Olsen",
        "Prerit Saxena",
        "Rafael Barcelos",
        "Yi Ding"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Multiagent Systems (cs.MA)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-13T21:00:27+00:00",
          "link": "https://arxiv.org/abs/2507.09788v1",
          "size": "1685kb",
          "version": "v1"
        }
      ],
      "title": "TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.09788",
        "HTML": "https://arxiv.org/html/2507.09788v1",
        "PDF": "https://arxiv.org/pdf/2507.09788"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "While the focus is on multiagent persona simulation, the toolkit is used for tasks like brainstorming, suggesting a secondary theme related to creativity in formulating behavioral problems and solutions."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.10131",
      "abstract": "Accurate inference of human intent enables human-robot collaboration without constraining human control or causing conflicts between humans and robots. We present GUIDER (Global User Intent Dual-phase Estimation for Robots), a probabilistic framework that enables a robot to estimate the intent of human operators. GUIDER maintains two coupled belief layers, one tracking navigation goals and the other manipulation goals. In the Navigation phase, a Synergy Map blends controller velocity with an occupancy grid to rank interaction areas. Upon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud. The Manipulation phase combines U2Net saliency, FastSAM instance saliency, and three geometric grasp-feasibility tests, with an end-effector kinematics-aware update rule that evolves object probabilities in real-time. GUIDER can recognize areas and objects of intent without predefined goals. We evaluated GUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and compared it with two baselines, one for navigation and one for manipulation. Across the 25 trials, GUIDER achieved a median stability of 93-100% during navigation, compared with 60-100% for the BOIR baseline, with an improvement of 39.5% in a redirection scenario (T5). During manipulation, stability reached 94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a redirection task (T3). In geometry-constrained trials (manipulation), GUIDER recognized the object intent three times earlier than Trajectron (median remaining time to confident prediction 23.6 s vs 7.8 s). These results validate our dual-phase framework and show improvements in intent inference in both phases of mobile manipulation tasks.",
      "authors": [
        "Cesar Alan Contreras",
        "Manolis Chiou",
        "Alireza Rastegarpanah",
        "Michal Szulik",
        "Rustam Stolkin"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T10:21:27+00:00",
          "link": "https://arxiv.org/abs/2507.10131v1",
          "size": "10214kb",
          "version": "v1"
        }
      ],
      "title": "Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.10131",
        "HTML": "https://arxiv.org/html/2507.10131v1",
        "PDF": "https://arxiv.org/pdf/2507.10131"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper is centered on intent prediction for robotics and human-robot collaboration, without a focus on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.10135",
      "abstract": "Carousels have become the de-facto interface in online services. However, there is a lack of research in carousels, particularly examining how recommender systems may be designed differently than the traditional single-list interfaces. One of the key elements for understanding how to design a system for a particular interface is understanding how users browse. For carousels, users may browse in a number of different ways due to the added complexity of multiple topic defined-lists and swiping to see more items.\n  Eye tracking is the key to understanding user behavior by providing valuable, direct information on how users see and navigate. In this work, we provide the first extensive analysis of the eye tracking behavior in carousel recommenders under the free-browsing setting. To understand how users browse, we examine the following research questions : 1) where do users start browsing, 2) how do users transition from item to item within the same carousel and across carousels, and 3) how does genre preference impact transitions?\n  This work addresses a gap in the field and provides the first extensive empirical results of eye tracked browsing behavior in carousels for improving recommenders. Taking into account the insights learned from the above questions, our final contribution is to provide suggestions to help carousel recommender system designers optimize their systems for user browsing behavior. The most important suggestion being to reorder the ranked item positions to account for browsing after swiping.These contributions aim not only to help improve current systems, but also to encourage and allow the design of new user models, systems, and metrics that are better suited to the complexity of carousel interfaces.",
      "authors": [
        "Santiago de Leon-Martinez",
        "Robert Moro",
        "Branislav Kveton",
        "Maria Bielikova"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Information Retrieval (cs.IR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T10:26:27+00:00",
          "link": "https://arxiv.org/abs/2507.10135v1",
          "size": "12104kb",
          "version": "v1"
        }
      ],
      "title": "Riding the Carousel: The First Extensive Eye Tracking Analysis of Browsing Behavior in Carousel Recommenders",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.10135",
        "HTML": "https://arxiv.org/html/2507.10135v1",
        "PDF": "https://arxiv.org/pdf/2507.10135"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on improving carousel recommenders using eye-tracking data and user browsing behavior, with no mention or focus on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.10208",
      "abstract": "Research into explainable artificial intelligence (XAI) for data analysis tasks suffer from a large number of contradictions and lack of concrete design recommendations stemming from gaps in understanding the tasks that require AI assistance. In this paper, we drew on multiple fields such as visual analytics, cognition, and dashboard design to propose a method for categorising and comparing XAI studies under three dimensions: what, why, and who. We identified the main problems as: inadequate descriptions of tasks, context-free studies, and insufficient testing with target users. We propose that studies should specifically report on their users' domain, AI, and data analysis expertise to illustrate the generalisability of their findings. We also propose study guidelines for designing and reporting XAI tasks to improve the XAI community's ability to parse the rapidly growing field. We hope that our contribution can help researchers and designers better identify which studies are most relevant to their work, what gaps exist in the research, and how to handle contradictory results regarding XAI design.",
      "authors": [
        "Hamzah Ziadeh",
        "Hendrik Knoche"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T12:26:45+00:00",
          "link": "https://arxiv.org/abs/2507.10208v1",
          "size": "97kb",
          "version": "v1"
        }
      ],
      "title": "Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.10208",
        "HTML": "https://arxiv.org/html/2507.10208v1",
        "PDF": "https://arxiv.org/pdf/2507.10208"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The study is focused on categorizing explainable AI research; there is no clear connection to creativity in its scope or objectives."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.10500",
      "abstract": "While autonomous driving technologies continue to advance, current Advanced Driver Assistance Systems (ADAS) remain limited in their ability to interpret scene context or engage with drivers through natural language. These systems typically rely on predefined logic and lack support for dialogue-based interaction, making them inflexible in dynamic environments or when adapting to driver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a modular framework that integrates Generative AI components including large language models, vision-to-text interpretation, and structured function calling to enable real-time, interpretable, and adaptive driver assistance. SC-ADAS supports multi-turn dialogue grounded in visual and sensor context, allowing natural language recommendations and driver-confirmed ADAS control. Implemented in the CARLA simulator with cloud-based Generative AI, the system executes confirmed user intents as structured ADAS commands without requiring model fine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and revisited multi-turn interactions, highlighting trade-offs such as increased latency from vision-based context retrieval and token growth from accumulated dialogue history. These results demonstrate the feasibility of combining conversational reasoning, scene perception, and modular ADAS control to support the next generation of intelligent driver assistance.",
      "authors": [
        "Kyungtae Han",
        "Yitao Chen",
        "Rohit Gupta",
        "Onur Altintas"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T17:24:07+00:00",
          "link": "https://arxiv.org/abs/2507.10500v1",
          "size": "1983kb",
          "version": "v1"
        }
      ],
      "title": "Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.10500",
        "HTML": "https://arxiv.org/html/2507.10500v1",
        "PDF": "https://arxiv.org/pdf/2507.10500"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on a conversational driver assistance system using generative AI. While it involves dialogue and interaction, it does not address creativity as a concept or research focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.10510",
      "abstract": "AI Video Chat emerges as a new paradigm for Real-time Communication (RTC), where one peer is not a human, but a Multimodal Large Language Model (MLLM). This makes interaction between humans and AI more intuitive, as if chatting face-to-face with a real person. However, this poses significant challenges to latency, because the MLLM inference takes up most of the response time, leaving very little time for video streaming. Due to network uncertainty and instability, transmission latency becomes a critical bottleneck preventing AI from being like a real person. To address this, we propose Artic, an AI-oriented Real-time Communication framework, exploring the network requirement shift from \"humans watching video\" to \"AI understanding video\". To reduce bitrate dramatically while maintaining MLLM accuracy, we propose Context-Aware Video Streaming that recognizes the importance of each video region for chat and allocates bitrate almost exclusively to chat-important regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive Frame Rate that leverages previous frames to substitute for lost/delayed frames while avoiding bitrate waste. To evaluate the impact of video streaming quality on MLLM accuracy, we build the first benchmark, named Degraded Video Understanding Benchmark (DeViBench). Finally, we discuss some open questions and ongoing solutions for AI Video Chat.",
      "authors": [
        "Jiangkai Wu",
        "Zhiyuan Ren",
        "Liming Liu",
        "Xinggong Zhang"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Networking and Internet Architecture (cs.NI)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Multimedia (cs.MM)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T17:34:49+00:00",
          "link": "https://arxiv.org/abs/2507.10510v1",
          "size": "5360kb",
          "version": "v1"
        }
      ],
      "title": "Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.10510",
        "HTML": "https://arxiv.org/html/2507.10510v1",
        "PDF": "https://arxiv.org/pdf/2507.10510"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses real-time communication with AI focusing on video streaming challenges and solutions. It does not relate to creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2402.09750",
      "abstract": "Large language models (LLMs) are increasingly integrated into creative coding, yet how users reflect, and how different co-creation conditions influence reflective behavior, remains underexplored. This study investigates situated, moment-to-moment reflection in creative coding under two prompting strategies: the entire task invocation (T1) and decomposed subtask invocation (T2), to examine their effects on reflective behavior. Our mixed-method results reveal three distinct reflection types and show that T2 encourages more frequent, strategic, and generative reflection, fostering diagnostic reasoning and goal redefinition. These findings offer insights into how LLM-based tools foster deeper creative engagement through structured, behaviorally grounded reflection support.",
      "authors": [
        "Anqi Wang",
        "Zhizhuo Yin",
        "Yulu Hu",
        "Yuanyuan Mao",
        "Lei Han",
        "Xin Tong",
        "Keqin Jiao",
        "Pan Hui"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2024-02-15T07:00:06+00:00",
          "link": "https://arxiv.org/abs/2402.09750v1",
          "size": "1816kb",
          "version": "v1"
        },
        {
          "date": "2025-07-12T00:54:14+00:00",
          "link": "https://arxiv.org/abs/2402.09750v2",
          "size": "1130kb",
          "version": "v2"
        }
      ],
      "title": "Pinning \"Reflection\" on the Agenda: Investigating Reflection in Human-LLM Co-Creation for Creative Coding",
      "links": {
        "Abstract": "https://arxiv.org/abs/2402.09750",
        "HTML": "https://arxiv.org/html/2402.09750v2",
        "PDF": "https://arxiv.org/pdf/2402.09750"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The study explores creative coding and reflection in co-creation with large language models (LLMs). It focuses on creative engagement and structured reflection, directly involving creativity as a primary aspect."
      },
      "tasks": [],
      "source": "arXiv"
    },
    {
      "id": "2403.08969",
      "abstract": "In recent years, numerous researchers have begun investigating how virtual reality (VR) tracking and interaction data can be used for a variety of machine learning purposes, including user identification, predicting cybersickness, and estimating learning gains. One constraint for this research area is the dearth of open datasets. In this paper, we present a new open dataset captured with our VR-based Full-scale Assembly Simulation Testbed (FAST). This dataset consists of data collected from 108 participants (50 females, 56 males, 2 non-binary) learning how to assemble two distinct full-scale structures in VR. In addition to explaining how the dataset was collected and describing the data included, we discuss how the dataset may be used by future researchers.",
      "authors": [
        "Alec G. Moore",
        "Tiffany D. Do",
        "Nayan N. Chawla",
        "Antonia Jimenez Iriarte",
        "and Ryan P. McMahan"
      ],
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2024-03-13T21:30:01+00:00",
          "link": "https://arxiv.org/abs/2403.08969v1",
          "size": "10080kb",
          "version": "v1"
        },
        {
          "date": "2025-07-13T13:16:20+00:00",
          "link": "https://arxiv.org/abs/2403.08969v2",
          "size": "10080kb",
          "version": "v2"
        }
      ],
      "title": "The Full-scale Assembly Simulation Testbed (FAST) Dataset",
      "links": {
        "Abstract": "https://arxiv.org/abs/2403.08969",
        "HTML": "https://arxiv.org/html/2403.08969v2",
        "PDF": "https://arxiv.org/pdf/2403.08969"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper presents a VR dataset for assembly simulation with potential applications in machine learning. Creativity is not mentioned as a focus or theme."
      },
      "tasks": [
        "User Identification"
      ],
      "source": "arXiv"
    },
    {
      "id": "2406.16173",
      "abstract": "Collecting mobile datasets remains challenging for academic researchers due to limited data access and technical barriers. Commercial organizations often possess exclusive access to mobile data, leading to a \"data monopoly\" that restricts the independence of academic research. Existing open-source mobile data collection frameworks primarily focus on mobile sensing data rather than screen content, which is crucial for various research studies. We present Crepe, a no-code Android app that enables researchers to collect information displayed on screen through simple demonstrations of target data. Crepe utilizes a novel Graph Query technique which augments the structures of mobile UI screens to support flexible identification, location, and collection of specific data pieces. The tool emphasizes participants' privacy and agency by providing full transparency over collected data and allowing easy opt-out. We designed and built Crepe for research purposes only and in scenarios where researchers obtain explicit consent from participants. Code for Crepe will be open-sourced to support future academic research data collection.",
      "authors": [
        "Yuwen Lu",
        "Meng Chen",
        "Qi Zhao",
        "Victor Cox",
        "Yang Yang",
        "Meng Jiang",
        "Jay Brockman",
        "Tamara Kay",
        "Toby Jia-Jun Li"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-06-23T17:53:10+00:00",
          "link": "https://arxiv.org/abs/2406.16173v1",
          "size": "2264kb",
          "version": "v1"
        },
        {
          "date": "2025-07-12T02:59:50+00:00",
          "link": "https://arxiv.org/abs/2406.16173v2",
          "size": "4980kb",
          "version": "v2"
        }
      ],
      "title": "Crepe: A Mobile Screen Data Collector Using Graph Query",
      "links": {
        "Abstract": "https://arxiv.org/abs/2406.16173",
        "HTML": "https://arxiv.org/html/2406.16173v2",
        "PDF": "https://arxiv.org/pdf/2406.16173"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses a no-code tool for collecting mobile screen data using graph queries. Creativity is not addressed in the tool's design or purpose."
      },
      "source": "arXiv"
    },
    {
      "id": "2503.16465",
      "abstract": "Autonomous graphical user interface (GUI) agents powered by multimodal large language models have shown great promise. However, a critical yet underexplored issue persists: over-execution, where the agent executes tasks in a fully autonomous way, without adequate assessment of its action confidence to compromise an adaptive human-agent collaboration. This poses substantial risks in complex scenarios, such as those involving ambiguous user instructions, unexpected interruptions, and environmental hijacks. To address the issue, we introduce OS-Kairos, an adaptive GUI agent capable of predicting confidence levels at each interaction step and efficiently deciding whether to act autonomously or seek human intervention. OS-Kairos is developed through two key mechanisms: (i) collaborative probing that annotates confidence scores at each interaction step; (ii) confidence-driven interaction that leverages these confidence scores to elicit the ability of adaptive interaction. Experimental results show that OS-Kairos substantially outperforms existing models on our curated dataset featuring complex scenarios, as well as on established benchmarks such as AITZ and Meta-GUI, with 24.59\\%$\\sim$87.29\\% improvements in task success rate. OS-Kairos facilitates an adaptive human-agent collaboration, prioritizing effectiveness, generality, scalability, and efficiency for real-world GUI interaction. The dataset and codes are available at https://github.com/Wuzheng02/OS-Kairos.",
      "authors": [
        "Pengzhou Cheng",
        "Zheng Wu",
        "Zongru Wu",
        "Aston Zhang",
        "Zhuosheng Zhang",
        "Gongshen Liu"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-02-26T12:31:16+00:00",
          "link": "https://arxiv.org/abs/2503.16465v1",
          "size": "20402kb",
          "version": "v1"
        },
        {
          "date": "2025-06-14T02:07:11+00:00",
          "link": "https://arxiv.org/abs/2503.16465v2",
          "size": "19254kb",
          "version": "v2"
        },
        {
          "date": "2025-07-14T15:56:44+00:00",
          "link": "https://arxiv.org/abs/2503.16465v3",
          "size": "19254kb",
          "version": "v3"
        }
      ],
      "title": "OS-Kairos: Adaptive Interaction for MLLM-Powered GUI Agents",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.16465",
        "HTML": "https://arxiv.org/html/2503.16465v3",
        "PDF": "https://arxiv.org/pdf/2503.16465"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The research presents a GUI agent that enhances human-agent collaboration by predicting confidence levels. Creativity is not mentioned and does not appear to be a component of the study."
      },
      "tasks": [],
      "repo_urls": [
        "https://github.com/wuzheng02/os-kairos"
      ],
      "source": "arXiv"
    },
    {
      "id": "2506.22941",
      "abstract": "Access to accurate and actionable harm reduction information can directly impact the health outcomes of People Who Use Drugs (PWUD), yet existing online channels often fail to meet their diverse and dynamic needs due to limitations in adaptability, accessibility, and the pervasive impact of stigma. Large Language Models (LLMs) present a novel opportunity to enhance information provision, but their application in such a high-stakes domain is under-explored and presents socio-technical challenges. This paper investigates how LLMs can be responsibly designed to support the information needs of PWUD. Through a qualitative workshop involving diverse stakeholder groups (academics, harm reduction practitioners, and an online community moderator), we explored LLM capabilities, identified potential use cases, and delineated core design considerations. Our findings reveal that while LLMs can address some existing information barriers (e.g., by offering responsive, multilingual, and potentially less stigmatising interactions), their effectiveness is contingent upon overcoming challenges related to ethical alignment with harm reduction principles, nuanced contextual understanding, effective communication, and clearly defined operational boundaries. We articulate design pathways emphasising collaborative co-design with experts and PWUD to develop LLM systems that are helpful, safe, and responsibly governed. This work contributes empirically grounded insights and actionable design considerations for the responsible development of LLMs as supportive tools within the harm reduction ecosystem.",
      "authors": [
        "Kaixuan Wang",
        "Jason T. Jacques",
        "Chenxin Diao",
        "and Carl-Cyril J Dreue"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-28T16:15:47+00:00",
          "link": "https://arxiv.org/abs/2506.22941v1",
          "size": "4041kb",
          "version": "v1"
        },
        {
          "date": "2025-07-02T13:02:17+00:00",
          "link": "https://arxiv.org/abs/2506.22941v2",
          "size": "4078kb",
          "version": "v2"
        },
        {
          "date": "2025-07-13T00:53:06+00:00",
          "link": "https://arxiv.org/abs/2506.22941v3",
          "size": "4059kb",
          "version": "v3"
        }
      ],
      "title": "Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22941",
        "PDF": "https://arxiv.org/pdf/2506.22941"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "Creativity could be linked as a design goal in developing harm reduction tools, but it's not explicitly discussed as a central theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.03147",
      "abstract": "Along with the explosion of large language models, improvements in speech synthesis, advancements in hardware, and the evolution of computer graphics, the current bottleneck in creating digital humans lies in generating character movements that correspond naturally to text or speech inputs.\n  In this work, we present DeepGesture, a diffusion-based gesture synthesis framework for generating expressive co-speech gestures conditioned on multimodal signals - text, speech, emotion, and seed motion. Built upon the DiffuseStyleGesture model, DeepGesture introduces novel architectural enhancements that improve semantic alignment and emotional expressiveness in generated gestures. Specifically, we integrate fast text transcriptions as semantic conditioning and implement emotion-guided classifier-free diffusion to support controllable gesture generation across affective states. To visualize results, we implement a full rendering pipeline in Unity based on BVH output from the model. Evaluation on the ZeroEGGS dataset shows that DeepGesture produces gestures with improved human-likeness and contextual appropriateness. Our system supports interpolation between emotional states and demonstrates generalization to out-of-distribution speech, including synthetic voices - marking a step forward toward fully multimodal, emotionally aware digital humans.\n  Project page: https://deepgesture.github.io",
      "authors": [
        "Thanh Hoang-Minh"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)",
        "Sound (cs.SD)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T20:04:04+00:00",
          "link": "https://arxiv.org/abs/2507.03147v1",
          "size": "17749kb",
          "version": "v1"
        },
        {
          "date": "2025-07-14T05:34:27+00:00",
          "link": "https://arxiv.org/abs/2507.03147v2",
          "size": "18854kb",
          "version": "v2"
        }
      ],
      "title": "DeepGesture: A conversational gesture synthesis system based on emotions and semantics",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.03147",
        "HTML": "https://arxiv.org/html/2507.03147v2",
        "PDF": "https://arxiv.org/pdf/2507.03147"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "Focuses on gesture synthesis with creative aspects in digital human expressions but does not directly study creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04278",
      "abstract": "With the recent success of large language models, Explainable Multimodal Emotion Recognition (EMER), also known as Descriptive MER (DMER), has attracted growing attention from researchers. Unlike traditional discriminative methods that rely on predefined emotion taxonomies, EMER aims to describe a person's emotional state using free-form natural language, thereby enabling fine-grained and interpretable emotion representations. However, this free-form prediction paradigm introduces significant challenges in evaluation. Existing approaches either depend on ground-truth descriptions, which require extensive manual annotations and often fail to capture the full complexity of human emotions, or simplify the evaluation task by shifting focus from assessing descriptions to evaluating emotion labels. However, this simplification overlooks critical aspects such as emotional temporal dynamics, intensity, and uncertainty. To address these limitations, we propose EMER-Ranker, a novel evaluation strategy that reformulates the traditional ``prediction-ground truth'' comparison into the ``prediction-prediction'' comparison, eliminating the need for ground-truth descriptions. We then apply the Bradley-Terry algorithm to convert pairwise comparison outcomes into model-level rankings. Additionally, we explore the potential for automatic preference prediction and introduce EMER-Preference, the first preference dataset specifically designed for human emotions. Our work advances the field of EMER and lays the foundation for more intelligent human-computer interaction systems.",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Haoyu Chen",
        "Zebang Cheng",
        "Fan Zhang",
        "Ziyu Jia",
        "Ziyang Ma",
        "Fei Ma",
        "Xiaojiang Peng",
        "Jianhua Tao"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-06T07:37:59+00:00",
          "link": "https://arxiv.org/abs/2507.04278v1",
          "size": "10398kb",
          "version": "v1"
        },
        {
          "date": "2025-07-10T03:28:00+00:00",
          "link": "https://arxiv.org/abs/2507.04278v2",
          "size": "10398kb",
          "version": "v2"
        },
        {
          "date": "2025-07-14T09:39:44+00:00",
          "link": "https://arxiv.org/abs/2507.04278v3",
          "size": "10393kb",
          "version": "v3"
        }
      ],
      "title": "EMER-Ranker: Learning to Rank Emotion Descriptions in the Absence of Ground Truth",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04278",
        "HTML": "https://arxiv.org/html/2507.04278v3",
        "PDF": "https://arxiv.org/pdf/2507.04278"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "EMER focuses on emotion recognition rather than creativity, with an emphasis on emotion descriptions and evaluation."
      },
      "source": "arXiv"
    },
    {
      "id": "2408.05700",
      "abstract": "A growing share of human interactions now occurs online, where the expression and perception of emotions are often amplified and distorted. Yet, the interplay between different emotions and the extent to which they are driven by external stimuli or social feedback remains poorly understood. We calibrate a multivariate Hawkes self-exciting point process to model the temporal expression of six basic emotions in YouTube Live chats. This framework captures both temporal and cross-emotional dependencies while allowing us to disentangle the influence of video content (exogenous) from peer interactions (endogenous). We find that emotional expressions are up to four times more strongly driven by peer interaction than by video content. Positivity is more contagious, spreading three times more readily, whereas negativity is more memorable, lingering nearly twice as long. Moreover, we observe asymmetric cross-excitation, with negative emotions frequently triggering positive ones, a pattern consistent with trolling dynamics, but not the reverse. These findings highlight the central role of social interaction in shaping emotional dynamics online and the risks of emotional manipulation as human-chatbot interactions become increasingly realistic.",
      "authors": [
        "Yishan Luo",
        "Didier Sornette",
        "Sandro Claudio Lera"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Social and Information Networks (cs.SI)",
        "Human-Computer Interaction (cs.HC)",
        "Applications (stat.AP)"
      ],
      "submission_historys": [
        {
          "date": "2024-08-11T05:42:45+00:00",
          "link": "https://arxiv.org/abs/2408.05700v1",
          "size": "1411kb",
          "version": "v1"
        },
        {
          "date": "2025-03-30T18:17:15+00:00",
          "link": "https://arxiv.org/abs/2408.05700v2",
          "size": "2135kb",
          "version": "v2"
        },
        {
          "date": "2025-07-12T14:30:59+00:00",
          "link": "https://arxiv.org/abs/2408.05700v3",
          "size": "2474kb",
          "version": "v3"
        }
      ],
      "title": "Quantification of Interdependent Emotion Dynamics in Online Interactions",
      "links": {
        "Abstract": "https://arxiv.org/abs/2408.05700",
        "HTML": "https://arxiv.org/html/2408.05700v3",
        "PDF": "https://arxiv.org/pdf/2408.05700"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The focus is on modeling and understanding emotion dynamics in online interactions without any relation to creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2409.13748",
      "abstract": "We present TheraGen, an advanced AI-powered mental health chatbot utilizing the LLaMA 2 7B model. This approach builds upon recent advancements in language models and transformer architectures. TheraGen provides all-day personalized, compassionate mental health care by leveraging a large dataset of 1 million conversational entries, combining anonymized therapy transcripts, online mental health discussions, and psychological literature, including APA resources. Our implementation employs transfer learning, fine-tuning, and advanced training techniques to optimize performance. TheraGen offers a user-friendly interface for seamless interaction, providing empathetic responses and evidence-based coping strategies. Evaluation results demonstrate high user satisfaction rates, with 94% of users reporting improved mental well-being. The system achieved a BLEU score of 0.67 and a ROUGE score of 0.62, indicating strong response accuracy. With an average response time of 1395 milliseconds, TheraGen ensures real-time, efficient support. While not a replacement for professional therapy, TheraGen serves as a valuable complementary tool, significantly improving user well-being and addressing the accessibility gap in mental health treatments. This paper details TheraGen's architecture, training methodology, ethical considerations, and future directions, contributing to the growing field of AI-assisted mental healthcare and offering a scalable solution to the pressing need for mental health support.",
      "authors": [
        "Kartikey Doshi",
        "Jimit Shah",
        "Narendra Shekokar"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-09-12T17:15:44+00:00",
          "link": "https://arxiv.org/abs/2409.13748v1",
          "size": "1809kb",
          "version": "v1"
        },
        {
          "date": "2025-07-11T19:43:13+00:00",
          "link": "https://arxiv.org/abs/2409.13748v2",
          "size": "0kb",
          "version": "v2"
        }
      ],
      "title": "TheraGen: Therapy for Every Generation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2409.13748",
        "PDF": "https://arxiv.org/pdf/2409.13748"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on an AI-powered mental health chatbot and its performance in providing mental health support. There is no explicit mention of creativity as a focus or theme."
      },
      "tasks": [
        "Chatbot",
        "Transfer Learning"
      ],
      "source": "arXiv"
    },
    {
      "id": "2501.08736",
      "abstract": "We present Holoview, an augmented reality (AR) system designed to support immersive and interactive learning of human anatomy. Holoview enables users to dynamically explore volumetric anatomical data through intuitive hand gestures in a 3D AR environment, allowing inspection of individual organs and cross-sectional views via clipping and bioscope features. The system adopts a lightweight client-server architecture optimized for real-time performance on the HoloLens through hybrid and foveated rendering. Our user study demonstrated Holoview's educational effectiveness, with participants showing a 135 percent improvement in task-specific knowledge and reporting increased confidence in understanding anatomical structures. The system was perceived as engaging and intuitive, particularly for organ selection and cross-sectional exploration, with low cognitive load and increasing ease of use over time. These findings highlight Holoview's potential to enhance anatomy learning through immersive, user-centered AR experiences.",
      "authors": [
        "Anshul Goswami and Ojaswa Sharma"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Graphics (cs.GR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-01-15T11:29:26+00:00",
          "link": "https://arxiv.org/abs/2501.08736v1",
          "size": "23617kb",
          "version": "v1"
        },
        {
          "date": "2025-01-16T02:24:48+00:00",
          "link": "https://arxiv.org/abs/2501.08736v2",
          "size": "23618kb",
          "version": "v2"
        },
        {
          "date": "2025-04-05T11:26:44+00:00",
          "link": "https://arxiv.org/abs/2501.08736v3",
          "size": "23518kb",
          "version": "v3"
        },
        {
          "date": "2025-07-14T07:35:39+00:00",
          "link": "https://arxiv.org/abs/2501.08736v4",
          "size": "35679kb",
          "version": "v4"
        }
      ],
      "title": "Holoview: An Immersive Mixed-Reality Visualization System for Anatomical Education",
      "links": {
        "Abstract": "https://arxiv.org/abs/2501.08736",
        "HTML": "https://arxiv.org/html/2501.08736v4",
        "PDF": "https://arxiv.org/pdf/2501.08736"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper is about an augmented reality system for anatomical education, emphasizing immersive learning and interaction. Creativity is not discussed as part of this system's objectives or features."
      },
      "source": "arXiv"
    },
    {
      "id": "2503.09639",
      "abstract": "Can we simulate a sandbox society with generative agents to model human behavior, thereby reducing the over-reliance on real human trials for assessing public policies? In this work, we investigate the feasibility of simulating health-related decision-making, using vaccine hesitancy, defined as the delay in acceptance or refusal of vaccines despite the availability of vaccination services (MacDonald, 2015), as a case study. To this end, we introduce the VacSim framework with 100 generative agents powered by Large Language Models (LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1) instantiate a population of agents with demographics based on census data; 2) connect the agents via a social network and model vaccine attitudes as a function of social dynamics and disease-related information; 3) design and evaluate various public health interventions aimed at mitigating vaccine hesitancy. To align with real-world results, we also introduce simulation warmup and attitude modulation to adjust agents' attitudes. We propose a series of evaluations to assess the reliability of various LLM simulations. Experiments indicate that models like Llama and Qwen can simulate aspects of human behavior but also highlight real-world alignment challenges, such as inconsistent responses with demographic profiles. This early exploration of LLM-driven simulations is not meant to serve as definitive policy guidance; instead, it serves as a call for action to examine social simulation for policy development.",
      "authors": [
        "Abe Bohan Hou",
        "Hongru Du",
        "Yichen Wang",
        "Jingyu Zhang",
        "Zixiao Wang",
        "Paul Pu Liang",
        "Daniel Khashabi",
        "Lauren Gardner",
        "Tianxing He"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Multiagent Systems (cs.MA)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Computers and Society (cs.CY)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-03-12T02:54:15+00:00",
          "link": "https://arxiv.org/abs/2503.09639v1",
          "size": "1918kb",
          "version": "v1"
        },
        {
          "date": "2025-03-16T06:03:01+00:00",
          "link": "https://arxiv.org/abs/2503.09639v2",
          "size": "1918kb",
          "version": "v2"
        },
        {
          "date": "2025-04-02T15:30:46+00:00",
          "link": "https://arxiv.org/abs/2503.09639v3",
          "size": "1469kb",
          "version": "v3"
        },
        {
          "date": "2025-07-13T07:36:02+00:00",
          "link": "https://arxiv.org/abs/2503.09639v4",
          "size": "1392kb",
          "version": "v4"
        }
      ],
      "title": "Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.09639",
        "HTML": "https://arxiv.org/html/2503.09639v4",
        "PDF": "https://arxiv.org/pdf/2503.09639"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper studies simulation of human behavior for public health policy modeling, specifically focusing on vaccine hesitancy. Creativity is not addressed in its methodology or content."
      },
      "tasks": [],
      "source": "arXiv"
    },
    {
      "id": "2504.17921",
      "abstract": "In this paper, we investigate how concept-based models (CMs) respond to out-of-distribution (OOD) inputs. CMs are interpretable neural architectures that first predict a set of high-level concepts (e.g., stripes, black) and then predict a task label from those concepts. In particular, we study the impact of concept interventions (i.e., operations where a human expert corrects a CM's mispredicted concepts at test time) on CMs' task predictions when inputs are OOD. Our analysis reveals a weakness in current state-of-the-art CMs, which we term leakage poisoning, that prevents them from properly improving their accuracy when intervened on for OOD inputs. To address this, we introduce MixCEM, a new CM that learns to dynamically exploit leaked information missing from its concepts only when this information is in-distribution. Our results across tasks with and without complete sets of concept annotations demonstrate that MixCEMs outperform strong baselines by significantly improving their accuracy for both in-distribution and OOD samples in the presence and absence of concept interventions.",
      "authors": [
        "Mateo Espinosa Zarlenga",
        "Gabriele Dominici",
        "Pietro Barbiero",
        "Zohreh Shams",
        "Mateja Jamnik"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Cryptography and Security (cs.CR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-04-24T20:24:31+00:00",
          "link": "https://arxiv.org/abs/2504.17921v1",
          "size": "34469kb",
          "version": "v1"
        },
        {
          "date": "2025-07-12T17:34:54+00:00",
          "link": "https://arxiv.org/abs/2504.17921v2",
          "size": "34606kb",
          "version": "v2"
        }
      ],
      "title": "Avoiding Leakage Poisoning: Concept Interventions Under Distribution Shifts",
      "links": {
        "Abstract": "https://arxiv.org/abs/2504.17921",
        "HTML": "https://arxiv.org/html/2504.17921v2",
        "PDF": "https://arxiv.org/pdf/2504.17921"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper examines concept-based models and their adaptation to distribution shifts, focusing on accuracy improvements. Creativity is not mentioned or implied in the research goals."
      },
      "tasks": [],
      "source": "arXiv"
    },
    {
      "id": "2505.08245",
      "abstract": "The advancement of large language models (LLMs) has outpaced traditional evaluation methodologies. This progress presents novel challenges, such as measuring human-like psychological constructs, moving beyond static and task-specific benchmarks, and establishing human-centered evaluation. These challenges intersect with psychometrics, the science of quantifying the intangible aspects of human psychology, such as personality, values, and intelligence. This review paper introduces and synthesizes the emerging interdisciplinary field of LLM Psychometrics, which leverages psychometric instruments, theories, and principles to evaluate, understand, and enhance LLMs. The reviewed literature systematically shapes benchmarking principles, broadens evaluation scopes, refines methodologies, validates results, and advances LLM capabilities. Diverse perspectives are integrated to provide a structured framework for researchers across disciplines, enabling a more comprehensive understanding of this nascent field. Ultimately, the review provides actionable insights for developing future evaluation paradigms that align with human-level AI and promote the advancement of human-centered AI systems for societal benefit. A curated repository of LLM psychometric resources is available at https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.",
      "authors": [
        "Haoran Ye",
        "Jing Jin",
        "Yuhang Xie",
        "Xin Zhang",
        "Guojie Song"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-05-13T05:47:51+00:00",
          "link": "https://arxiv.org/abs/2505.08245v1",
          "size": "5732kb",
          "version": "v1"
        },
        {
          "date": "2025-07-13T08:25:01+00:00",
          "link": "https://arxiv.org/abs/2505.08245v2",
          "size": "5809kb",
          "version": "v2"
        }
      ],
      "title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.08245",
        "HTML": "https://arxiv.org/html/2505.08245v2",
        "PDF": "https://arxiv.org/pdf/2505.08245"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on LLM psychometrics and human-centered AI evaluation, without specific exploration of creativity."
      },
      "tasks": [
        "Benchmarking",
        "Language Modeling",
        "Language Modelling",
        "Large Language Model"
      ],
      "repo_urls": [
        "https://github.com/valuebyte-ai/awesome-llm-psychometrics"
      ],
      "source": "arXiv"
    },
    {
      "id": "2507.04189",
      "abstract": "Understanding character relationships is essential for interpreting complex narratives and conducting socially grounded AI research. However, manual annotation is time-consuming and low in coverage, while large language models (LLMs) often produce hallucinated or logically inconsistent outputs. We present SymbolicThought, a human-in-the-loop framework that combines LLM-based extraction with symbolic reasoning. The system constructs editable character relationship graphs, refines them using seven types of logical constraints, and enables real-time validation and conflict resolution through an interactive interface. To support logical supervision and explainable social analysis, we release a dataset of 160 interpersonal relationships with corresponding logical structures. Experiments show that SymbolicThought improves annotation accuracy and consistency while significantly reducing time cost, offering a practical tool for narrative understanding, explainable AI, and LLM evaluation.",
      "authors": [
        "Runcong Zhao",
        "Qinglin Zhu",
        "Hainiu Xu",
        "Bin Liang",
        "Lin Gui",
        "Yulan He"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-05T23:46:35+00:00",
          "link": "https://arxiv.org/abs/2507.04189v1",
          "size": "1612kb",
          "version": "v1"
        },
        {
          "date": "2025-07-13T22:06:13+00:00",
          "link": "https://arxiv.org/abs/2507.04189v2",
          "size": "1612kb",
          "version": "v2"
        }
      ],
      "title": "SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04189",
        "PDF": "https://arxiv.org/pdf/2507.04189"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper addresses narrative understanding and symbolic reasoning without direct mention or exploration of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04295",
      "abstract": "Effective feedback is essential for student learning but is time-intensive for teachers. We present LearnLens, a modular, LLM-based system that generates personalised, curriculum-aligned feedback in science education. LearnLens comprises three components: (1) an error-aware assessment module that captures nuanced reasoning errors; (2) a curriculum-grounded generation module that uses a structured, topic-linked memory chain rather than traditional similarity-based retrieval, improving relevance and reducing noise; and (3) an educator-in-the-loop interface for customisation and oversight. LearnLens addresses key challenges in existing systems, offering scalable, high-quality feedback that empowers both teachers and students.",
      "authors": [
        "Runcong Zhao",
        "Artem Bobrov",
        "Jiazheng Li",
        "Yulan He"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-06T08:39:26+00:00",
          "link": "https://arxiv.org/abs/2507.04295v1",
          "size": "1802kb",
          "version": "v1"
        },
        {
          "date": "2025-07-11T18:21:09+00:00",
          "link": "https://arxiv.org/abs/2507.04295v2",
          "size": "1803kb",
          "version": "v2"
        }
      ],
      "title": "LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04295",
        "PDF": "https://arxiv.org/pdf/2507.04295"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on providing personalized feedback in education using a structured memory chain, with no mention of creativity as a focus or theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.07216",
      "abstract": "Reliable data is a cornerstone of modern organizational systems. A notable data integrity challenge stems from label bias, which refers to systematic errors in a label, a covariate that is central to a quantitative analysis, such that its quality differs across social groups. This type of bias has been conceptually and empirically explored and is widely recognized as a pressing issue across critical domains. However, effective methodologies for addressing it remain scarce. In this work, we propose Decoupled Confident Learning (DeCoLe), a principled machine learning based framework specifically designed to detect mislabeled instances in datasets affected by label bias, enabling bias aware mislabelling detection and facilitating data quality improvement. We theoretically justify the effectiveness of DeCoLe and evaluate its performance in the impactful context of hate speech detection, a domain where label bias is a well documented challenge. Empirical results demonstrate that DeCoLe excels at bias aware mislabeling detection, consistently outperforming alternative approaches for label error detection. Our work identifies and addresses the challenge of bias aware mislabeling detection and offers guidance on how DeCoLe can be integrated into organizational data management practices as a powerful tool to enhance data reliability.",
      "authors": [
        "Yunyi Li",
        "Maria De-Arteaga and Maytal Saar-Tsechansky"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Databases (cs.DB)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-09T18:44:36+00:00",
          "link": "https://arxiv.org/abs/2507.07216v1",
          "size": "4503kb",
          "version": "v1"
        },
        {
          "date": "2025-07-11T16:34:30+00:00",
          "link": "https://arxiv.org/abs/2507.07216v2",
          "size": "3394kb",
          "version": "v2"
        }
      ],
      "title": "Bias-Aware Mislabeling Detection via Decoupled Confident Learning",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07216",
        "HTML": "https://arxiv.org/html/2507.07216v2",
        "PDF": "https://arxiv.org/pdf/2507.07216"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper addresses bias-aware mislabeling detection in datasets, particularly in the context of hate speech detection, without any connection to the topic of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.07610",
      "abstract": "Humans can directly imagine and manipulate visual images in their minds, a capability known as spatial visualization. While multi-modal Large Language Models (MLLMs) support imagination-based reasoning, spatial visualization remains insufficiently evaluated, typically embedded within broader mathematical and logical assessments. Existing evaluations often rely on IQ tests or math competitions that may overlap with training data, compromising assessment reliability. To this end, we introduce SpatialViz-Bench, a comprehensive multi-modal benchmark for spatial visualization with 12 tasks across 4 sub-abilities, comprising 1,180 automatically generated problems. Our evaluation of 33 state-of-the-art MLLMs not only reveals wide performance variations and demonstrates the benchmark's strong discriminative power, but also uncovers counter-intuitive findings: models exhibit unexpected behaviors by showing difficulty perception that misaligns with human intuition, displaying dramatic 2D-to-3D performance cliffs, and defaulting to formula derivation despite spatial tasks requiring visualization alone. SpatialVizBench empirically demonstrates that state-of-the-art MLLMs continue to exhibit deficiencies in spatial visualization tasks, thereby addressing a significant lacuna in the field. The benchmark is publicly available.",
      "authors": [
        "Siting Wang",
        "Luoyang Sun",
        "Cheng Deng",
        "Kun Shao",
        "Minnan Pei",
        "Zheng Tian",
        "Haifeng Zhang",
        "Jun Wang"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-10T10:27:20+00:00",
          "link": "https://arxiv.org/abs/2507.07610v1",
          "size": "3775kb",
          "version": "v1"
        },
        {
          "date": "2025-07-14T07:38:44+00:00",
          "link": "https://arxiv.org/abs/2507.07610v2",
          "size": "4137kb",
          "version": "v2"
        }
      ],
      "title": "SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07610",
        "PDF": "https://arxiv.org/pdf/2507.07610"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper introduces a benchmark for evaluating spatial visualization tasks in MLLMs, with no discussion around creativity or creative processes."
      },
      "datasets": [
        {
          "dataset_name": "PLM-Team/Spatial-Visualization-Benchmark",
          "downloads": "168",
          "likes": "0",
          "link": "https://huggingface.co/datasets/PLM-Team/Spatial-Visualization-Benchmark"
        }
      ],
      "source": "arXiv"
    },
    {
      "id": "2408.10887",
      "abstract": "This paper presents a mini-review of the current state of research in mobile manipulators with variable levels of autonomy, emphasizing their associated challenges and application environments. The need for mobile manipulators in different environments is evident due to the unique challenges and risks each presents. Many systems deployed in these environments are not fully autonomous, requiring human-robot teaming to ensure safe and reliable operations under uncertainties. Through this analysis, we identify gaps and challenges in the literature on Variable Autonomy, including cognitive workload and communication delays, and propose future directions, including whole-body Variable Autonomy for mobile manipulators, virtual reality frameworks, and large language models to reduce operators' complexity and cognitive load in some challenging and uncertain scenarios.",
      "authors": [
        "Cesar Alan Contreras",
        "Alireza Rastegarpanah",
        "Rustam Stolkin",
        "Manolis Chiou"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-08-20T14:18:35+00:00",
          "link": "https://arxiv.org/abs/2408.10887v1",
          "size": "69kb",
          "version": "v1"
        }
      ],
      "title": "A Mini-Review on Mobile Manipulators with Variable Autonomy",
      "links": {
        "Abstract": "https://arxiv.org/abs/2408.10887",
        "HTML": "https://arxiv.org/html/2408.10887",
        "PDF": "https://arxiv.org/pdf/2408.10887"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This review of mobile manipulators with variable autonomy centers on challenges and solutions in human-robot interaction, not related to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2406.10268",
      "abstract": "In mathematical proof education, there remains a need for interventions that help students learn to write mathematical proofs. Research has shown that timely feedback can be very helpful to students learning new skills. While for many years natural language processing models have struggled to perform well on tasks related to mathematical texts, recent developments in natural language processing have created the opportunity to complete the task of giving students instant feedback on their mathematical proofs. In this paper, we present a set of training methods and models capable of autograding freeform mathematical proofs by leveraging existing large language models and other machine learning techniques. The models are trained using proof data collected from four different proof by induction problems. We use four different robust large language models to compare their performances, and all achieve satisfactory performances to various degrees. Additionally, we recruit human graders to grade the same proofs as the training data, and find that the best grading model is also more accurate than most human graders. With the development of these grading models, we create and deploy an autograder for proof by induction problems and perform a user study with students. Results from the study shows that students are able to make significant improvements to their proofs using the feedback from the autograder, but students still do not trust the AI autograders as much as they trust human graders. Future work can improve on the autograder feedback and figure out ways to help students trust AI autograders.",
      "authors": [
        "Chenyan Zhao",
        "Mariana Silva",
        "and Seth Poulsen"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-06-11T15:30:26+00:00",
          "link": "https://arxiv.org/abs/2406.10268v1",
          "size": "885kb",
          "version": "v1"
        },
        {
          "date": "2025-02-19T06:18:20+00:00",
          "link": "https://arxiv.org/abs/2406.10268v2",
          "size": "894kb",
          "version": "v2"
        }
      ],
      "title": "Autograding Mathematical Induction Proofs with Natural Language Processing",
      "links": {
        "Abstract": "https://arxiv.org/abs/2406.10268",
        "HTML": "https://arxiv.org/html/2406.10268",
        "PDF": "https://arxiv.org/pdf/2406.10268"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The primary focus is on autograding mathematical proofs using natural language processing; there is no mention or exploration of creativity."
      },
      "tasks": [
        "Mathematical Induction",
        "Mathematical Proofs"
      ],
      "source": "arXiv"
    },
    {
      "id": "2507.07911",
      "abstract": "Immersive virtual reality (VR) is a promising tool for stress reduction and relaxation, traditionally relying on visual and auditory stimuli. This study examines the role of olfactory stimuli in enhancing these effects, using a randomized within-subject design. Thirty participants aged 18-60 experienced VR scenarios simulating a calming seaside environment, with sessions lasting 45 minutes, in two conditions: with and without a \"Beach\" essential oil scent (Yankee Candle) administered via diffuser. Stress and relaxation were assessed through self-reported surveys and physiological measures, specifically ECG-based heart rate variability (HRV). Results showed no significant difference in self-reported relaxation scores (p=0.371) between conditions, but HRV analysis revealed a significant stress reduction (p=0.002) with olfactory input, with HF increasing 108% from the Math Stress Test to the scented relaxation condition, compared to 44% without scent. Additionally, 71.4% of participants expressed willingness to use olfactory-enhanced VR for relaxation, suggesting practical appeal. These findings indicate that olfactory stimuli may enhance relaxation subconsciously, underscoring the importance of multisensory integration in VR. Future work could explore personalized scents and long-term effects to optimize VR- based interventions for emotional and physical well-being.",
      "authors": [
        "Yasmin Elsaddik Valdivieso",
        "Mohd Faisal",
        "Karim Alghoul",
        "Monireh (Monica) Vahdati",
        "Kamran Gholizadeh Hamlabadi",
        "Fedwa Laamarti",
        "Hussein Al Osman",
        "Abdulmotaleb El Saddik"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Multimedia (cs.MM)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-10T16:45:10+00:00",
          "link": "https://arxiv.org/abs/2507.07911v1",
          "size": "602kb",
          "version": "v1"
        }
      ],
      "title": "The Potential of Olfactory Stimuli in Stress Reduction through Virtual Reality",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07911",
        "PDF": "https://arxiv.org/pdf/2507.07911"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The research investigates the use of olfactory stimuli for stress reduction in VR environments. Creativity is not mentioned or implied as a focus or theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2312.12399",
      "abstract": "Visuospatial Neglect (VSN) affects spatial awareness, leading to functional and motor challenges. This study explores virtual reality (VR) as a potential complementary tool for VSN rehabilitation, offering a novel environment that intends to support therapy outcomes. Specifically, we aim to explore the initial experiences of patients and physiotherapists engaging with the protocol. VSN occurs in approximately 30% of stroke survivors, often presenting as inattention to one side of space. While conventional therapies rely on repetitive motor tasks, VR has emerged as a promising alternative for targeted and patient-centered rehabilitation. However, evidence on the integration of audio-visual cues in VR for VSN is limited. A preliminary VR task integrating audio-visual cues was co-designed with two physiotherapists. The task was then tested with two VSN patients over 12 sessions. Preliminary findings suggest potential benefits in patient experience, with one patient reporting increased confidence in mobility. However, outcomes varied, and the results are exploratory.",
      "authors": [
        "Andrew Danso",
        "Patti Nijhuis",
        "Alessandro Ansani",
        "Martin Hartmann",
        "Gulnara Minkkinen",
        "Geoff Luck",
        "Joshua S. Bamford",
        "Sarah Faber",
        "Kat Agres",
        "Solange Glasser",
        "Teppo S\\\"ark\\\"am\\\"o",
        "Rebekah Rousi",
        "Marc R. Thompson"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2023-12-19T18:35:01+00:00",
          "link": "https://arxiv.org/abs/2312.12399v1",
          "size": "1009kb",
          "version": "v1"
        },
        {
          "date": "2024-12-02T16:55:03+00:00",
          "link": "https://arxiv.org/abs/2312.12399v2",
          "size": "3841kb",
          "version": "v2"
        }
      ],
      "title": "Development and User Experiences of a Novel Virtual Reality Task for Post-stroke Visuospatial Neglect: An Exploratory Pilot Study",
      "links": {
        "Abstract": "https://arxiv.org/abs/2312.12399",
        "PDF": "https://arxiv.org/pdf/2312.12399"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This study examines VR for VSN rehabilitation, emphasizing therapy outcomes and patient experiences. Creativity is not a focus or theme within the study."
      },
      "source": "arXiv"
    },
    {
      "id": "2106.15599",
      "abstract": "The population of elderly people has been increasing at a rapid rate over the last few decades and their population is expected to further increase in the upcoming future. Their increasing population is associated with their increasing needs due to problems like physical disabilities, cognitive issues, weakened memory and disorganized behavior, that elderly people face with increasing age. To reduce their financial burden on the world economy and to enhance their quality of life, it is essential to develop technology-based solutions that are adaptive, assistive and intelligent in nature. Intelligent Affect Aware Systems that can not only analyze but also predict the behavior of elderly people in the context of their day to day interactions with technology in an IoT-based environment, holds immense potential for serving as a long-term solution for improving the user experience of elderly in smart homes. This work therefore proposes the framework for an Intelligent Affect Aware environment for elderly people that can not only analyze the affective components of their interactions but also predict their likely user experience even before they start engaging in any activity in the given smart home environment. This forecasting of user experience would provide scope for enhancing the same, thereby increasing the assistive and adaptive nature of such intelligent systems. To uphold the efficacy of this proposed framework for improving the quality of life of elderly people in smart homes, it has been tested on three datasets and the results are presented and discussed.",
      "authors": [
        "Nirmalya Thakur and Chia Y. Han"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Emerging Technologies (cs.ET)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2021-06-29T17:34:16+00:00",
          "link": "https://arxiv.org/abs/2106.15599v1",
          "size": "1252kb",
          "version": "v1"
        }
      ],
      "title": "Framework for an Intelligent Affect Aware Smart Home Environment for Elderly People",
      "links": {
        "Abstract": "https://arxiv.org/abs/2106.15599",
        "PDF": "https://arxiv.org/pdf/2106.15599"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on affect-aware systems for elderly people in smart homes, aiming to improve user experience through intelligent prediction. Creativity is not addressed."
      },
      "tasks": [],
      "source": "arXiv"
    },
    {
      "id": "2107.07344",
      "abstract": "The increasing population of elderly people is associated with the need to meet their increasing requirements and to provide solutions that can improve their quality of life in a smart home. In addition to fear and anxiety towards interfacing with systems; cognitive disabilities, weakened memory, disorganized behavior and even physical limitations are some of the problems that elderly people tend to face with increasing age. The essence of providing technology-based solutions to address these needs of elderly people and to create smart and assisted living spaces for the elderly; lies in developing systems that can adapt by addressing their diversity and can augment their performances in the context of their day to day goals. Therefore, this work proposes a framework for development of a Personalized Intelligent Assistant to help elderly people perform Activities of Daily Living (ADLs) in a smart and connected Internet of Things (IoT) based environment. This Personalized Intelligent Assistant can analyze different tasks performed by the user and recommend activities by considering their daily routine, current affective state and the underlining user experience. To uphold the efficacy of this proposed framework, it has been tested on a couple of datasets for modelling an average user and a specific user respectively. The results presented show that the model achieves a performance accuracy of 73.12% when modelling a specific user, which is considerably higher than its performance while modelling an average user, this upholds the relevance for development and implementation of this proposed framework.",
      "authors": [
        "Nirmalya Thakur and Chia Y. Han"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2021-06-29T17:36:07+00:00",
          "link": "https://arxiv.org/abs/2107.07344v1",
          "size": "1799kb",
          "version": "v1"
        }
      ],
      "title": "Framework for A Personalized Intelligent Assistant to Elderly People for Activities of Daily Living",
      "links": {
        "Abstract": "https://arxiv.org/abs/2107.07344",
        "PDF": "https://arxiv.org/pdf/2107.07344"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses personalized intelligent assistants for daily living activities in smart homes for the elderly. The focus is on user experience and assistance, not creativity."
      },
      "tasks": [],
      "source": "arXiv"
    },
    {
      "id": "2408.10905",
      "abstract": "This study explores whether labeling AI as \"trustworthy\" or \"reliable\" influences user perceptions and acceptance of automotive AI technologies. Using a one-way between-subjects design, the research involved 478 online participants who were presented with guidelines for either trustworthy or reliable AI. Participants then evaluated three vignette scenarios and completed a modified version of the Technology Acceptance Model, which included variables such as perceived ease of use, human-like trust, and overall attitude. Although labeling AI as \"trustworthy\" did not significantly influence judgments on specific scenarios, it increased perceived ease of use and human-like trust, particularly benevolence. This suggests a positive impact on usability and an anthropomorphic effect on user perceptions. The study provides insights into how specific labels can influence attitudes toward AI technology.",
      "authors": [
        "John Dorsch and Ophelia Deroy"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Emerging Technologies (cs.ET)"
      ],
      "submission_historys": [
        {
          "date": "2024-08-20T14:48:24+00:00",
          "link": "https://arxiv.org/abs/2408.10905v1",
          "size": "3343kb",
          "version": "v1"
        }
      ],
      "title": "The impact of labeling automotive AI as \"trustworthy\" or \"reliable\" on user evaluation and technology acceptance",
      "links": {
        "Abstract": "https://arxiv.org/abs/2408.10905",
        "HTML": "https://arxiv.org/html/2408.10905",
        "PDF": "https://arxiv.org/pdf/2408.10905"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The study examines the effects of labeling AI on user perceptions and acceptance, without any connection to creativity or creative tasks."
      },
      "tasks": [],
      "source": "arXiv"
    }
  ],
  "subjects": [
    "Computer Vision and Pattern Recognition (cs.CV)",
    "Networking and Internet Architecture (cs.NI)",
    "Multimedia (cs.MM)",
    "Computation and Language (cs.CL)",
    "Databases (cs.DB)",
    "Image and Video Processing (eess.IV)",
    "Sound (cs.SD)",
    "Systems and Control (eess.SY)",
    "Cryptography and Security (cs.CR)",
    "Robotics (cs.RO)",
    "Information Retrieval (cs.IR)",
    "Artificial Intelligence (cs.AI)",
    "Applications (stat.AP)",
    "Systems and Control (cs.SY)",
    "Programming Languages (cs.PL)",
    "Emerging Technologies (cs.ET)",
    "Audio and Speech Processing (eess.AS)",
    "Social and Information Networks (cs.SI)",
    "Computers and Society (cs.CY)",
    "Multiagent Systems (cs.MA)",
    "Software Engineering (cs.SE)",
    "Human-Computer Interaction (cs.HC)",
    "Graphics (cs.GR)",
    "Machine Learning (cs.LG)"
  ],
  "prompt": {
    "creativity": "\nYou are an expert in information retrieval. I will provide you with a list of research papers from arXiv, specifically in the *cs.HC* (Human-Computer Interaction) category.\n\nYour task is to analyze each paper and determine its relevance to the topic of **Creativity**.\n\n\n### Classify each paper into one of the following relevance levels\n\n- `core` \u2014 Creativity is a **primary focus**\n  - The paper directly studies or simulates creativity, with a clear focus on creativity.\n  - Includes creative tasks, co-creative systems, or creativity evaluation metrics.\n  - The title and abstract explicitly mention creativity, and the research questions are directly related to creativity.\n- `partial` \u2014 Creativity is a **secondary theme**\n  - Part of the paper relates to creativity; it is treated as an analytical dimension or design goal but not the main objective (e.g., user creativity, design support).\n  - Creativity may appear in discussions, experiments, or auxiliary applications.\n  - Creativity is presented as a supporting topic (e.g., evaluation criteria, user feedback).\n- `irrelevant` \u2014 **No clear connection** to creativity\n  - The paper does not address creativity as a topic.\n  - Focuses on unrelated technical content (e.g., compression, security, optimization).\n  - If creativity is mentioned, it is only superficial and lacks substantive content.\n\n\n### Return your results in the following JSON format\n\n```json\n{\n  \"result\": [\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    },\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    }\n  ]\n}\n```\n\nBe concise but specific in your reasoning, referencing key terms or sections when applicable.\n"
  },
  "description": "Data source: https://arxiv.org/list/cs.HC/new",
  "level_tatistics": {
    "irrelevant": 46,
    "partial": 12,
    "core": 1
  },
  "arxiv_update_date": "2025-07-15",
  "updated_at": "2025-07-15 14:02:12"
}