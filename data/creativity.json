{
  "data": [
    {
      "id": "2507.07238",
      "abstract": "The work involved in gathering, wrangling, cleaning, and otherwise preparing data for analysis is often the most time consuming and tedious aspect of data work. Although many studies describe data preparation within the context of data science workflows, there has been little research on data preparation in data journalism. We address this gap with a hybrid form of thematic analysis that combines deductive codes derived from existing accounts of data science workflows and inductive codes arising from an interview study with 36 professional data journalists. We extend a previous model of data science work to incorporate detailed activities of data preparation. We synthesize 60 dirty data issues from 16 taxonomies on dirty data and our interview data, and we provide a novel taxonomy to characterize these dirty data issues as discrepancies between mental models. We also identify four challenges faced by journalists: diachronic, regional, fragmented, and disparate data sources.",
      "authors": [
        "Stephen Kasica",
        "Charles Berret",
        "and Tamara Munzner"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-09T19:14:57+00:00",
          "link": "https://arxiv.org/abs/2507.07238v1",
          "size": "1104kb",
          "version": "v1"
        }
      ],
      "title": "Dirty Data in the Newsroom: Comparing Data Preparation in Journalism and Data Science",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07238",
        "HTML": "https://arxiv.org/html/2507.07238v1",
        "PDF": "https://arxiv.org/pdf/2507.07238"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on data preparation in journalism and data science, with no mention or focus on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.07362",
      "abstract": "SRL, defined as learners' ability to systematically plan, monitor, and regulate their learning activities, is crucial for sustained academic achievement and lifelong learning competencies. Emerging Artificial Intelligence (AI) developments profoundly influence SRL interactions by potentially either diminishing or strengthening learners' opportunities to exercise their own regulatory skills. Recent literature emphasizes a balanced approach termed Hybrid Human-AI Regulated Learning (HHAIRL), in which AI provides targeted, timely scaffolding while preserving the learners' role as active decision-makers and reflective monitors of their learning process. Nevertheless, existing digital tools frequently fall short, lacking adaptability, focusing narrowly on isolated SRL phases, and insufficiently support meaningful human-AI interactions. In response, this paper introduces the enhanced \\flora Engine, which incorporates advanced Generative Artificial Intelligence (GenAI) features and state-of-the-art learning analytics, explicitly grounded in SRL and HHAIRL theories. The \\flora Engine offers instrumentation tools such as collaborative writing, multi-agents chatbot, and detailed learning trace logging to support dynamic, adaptive scaffolding tailored to individual needs in real time. We further present a summary of several research studies that provide the validations for and illustrate how these instrumentation tools can be utilized in real-world educational and experimental contexts. These studies demonstrate the effectiveness of \\flora Engine in fostering SRL and HHAIRL, providing both theoretical insights and practical solutions for the future of AI-enhanced learning context.",
      "authors": [
        "Xinyu Li and Tongguang Li and Lixiang Yan and Yuheng Li and Linxuan Zhao and Mladen Rakovi\\'c and Inge Molenaar and Dragan Ga\\v{s}evi\\'c and Yizhou Fan"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-10T01:11:52+00:00",
          "link": "https://arxiv.org/abs/2507.07362v1",
          "size": "16820kb",
          "version": "v1"
        }
      ],
      "title": "FLoRA: An Advanced AI-Powered Engine to Facilitate Hybrid Human-AI Regulated Learning",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07362",
        "HTML": "https://arxiv.org/html/2507.07362v1",
        "PDF": "https://arxiv.org/pdf/2507.07362"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses AI-enhanced learning (HHAIRL), including tools like collaborative writing, which can foster creativity as part of self-regulated learning, but creativity is not the primary focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.07550",
      "abstract": "This position paper explores pluriperspectivism as a core element of human creative experience and its relevance to humanrobot cocreativity We propose a layered fivedimensional model to guide the design of cocreative behaviors and the analysis of interaction dynamics This model is based on literature and results from an interview study we conducted with 10 visual artists and 8 arts educators examining how pluriperspectivism supports creative practice The findings of this study provide insight in how robots could enhance human creativity through adaptive contextsensitive behavior demonstrating the potential of pluriperspectivism This paper outlines future directions for integrating pluriperspectivism with visionlanguage models VLMs to support context sensitivity in cocreative robots",
      "authors": [
        "Marianne Bossema",
        "Rob Saunders",
        "Aske Plaat",
        "Somaya Ben Allouch"
      ],
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-10T08:47:41+00:00",
          "link": "https://arxiv.org/abs/2507.07550v1",
          "size": "2358kb",
          "version": "v1"
        }
      ],
      "title": "Pluri-perspectivism in Human-robot Co-creativity with Older Adults",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07550",
        "HTML": "https://arxiv.org/html/2507.07550v1",
        "PDF": "https://arxiv.org/pdf/2507.07550"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper directly addresses human-robot co-creativity and explores enhancing human creativity, making creativity the primary focus of the study."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.07551",
      "abstract": "The accelerating growth of photographic collections has outpaced manual cataloguing, motivating the use of vision language models (VLMs) to automate metadata generation. This study examines whether Al-generated catalogue descriptions can approximate human-written quality and how generative Al might integrate into cataloguing workflows in archival and museum collections. A VLM (InternVL2) generated catalogue descriptions for photographic prints on labelled cardboard mounts with archaeological content, evaluated by archive and archaeology experts and non-experts in a human-centered, experimental framework. Participants classified descriptions as AI-generated or expert-written, rated quality, and reported willingness to use and trust in AI tools. Classification performance was above chance level, with both groups underestimating their ability to detect Al-generated descriptions. OCR errors and hallucinations limited perceived quality, yet descriptions rated higher in accuracy and usefulness were harder to classify, suggesting that human review is necessary to ensure the accuracy and quality of catalogue descriptions generated by the out-of-the-box model, particularly in specialized domains like archaeological cataloguing. Experts showed lower willingness to adopt AI tools, emphasizing concerns on preservation responsibility over technical performance. These findings advocate for a collaborative approach where AI supports draft generation but remains subordinate to human verification, ensuring alignment with curatorial values (e.g., provenance, transparency). The successful integration of this approach depends not only on technical advancements, such as domain-specific fine-tuning, but even more on establishing trust among professionals, which could both be fostered through a transparent and explainable AI pipeline.",
      "authors": [
        "Line Abele",
        "Gerrit Anders",
        "Tolgahan Ayd{\\i}n",
        "J\\\"urgen Buder",
        "Helen Fischer",
        "Dominik Kimmel",
        "Markus Huff"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Digital Libraries (cs.DL)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-10T08:49:15+00:00",
          "link": "https://arxiv.org/abs/2507.07551v1",
          "size": "5631kb",
          "version": "v1"
        }
      ],
      "title": "ArchiveGPT: A human-centered evaluation of using a vision language model for image cataloguing",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07551",
        "PDF": "https://arxiv.org/pdf/2507.07551"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper explores the integration of AI tools in cataloguing workflows, which involves creativity in generating descriptive metadata. Creativity is considered as part of design goals and human-AI collaboration, but it is not the central theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.07560",
      "abstract": "Human and automation capabilities are the foundation of every human-autonomy interaction and interaction pattern. Therefore, machines need to understand the capacity and performance of human doing, and adapt their own behavior, accordingly. In this work, we address the concept of conjugated capabilities, i.e. capabilities that are dependent or interrelated and between which effort can be distributed. These may be used to overcome human limitations, by shifting effort from a deficient to a conjugated capability with performative resources. For example: A limited arm's reach may be compensated by tilting the torso forward. We analyze the interrelation between elementary capabilities within the IMBA standard to uncover potential conjugation, and show evidence in data of post-rehabilitation patients. From the conjugated capabilities, within the example application of stationary manufacturing, we create a network of interrelations. With this graph, a manifold of potential uses is enabled. We showcase the graph's usage in optimizing IMBA test design to accelerate data recordings, and discuss implications of conjugated capabilities on task allocation between the human and an autonomy.",
      "authors": [
        "Nils Mandischer",
        "Larissa F\\\"uller",
        "Torsten Alles",
        "Frank Flemisch",
        "Lars Mikelsons"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Multiagent Systems (cs.MA)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-10T08:59:18+00:00",
          "link": "https://arxiv.org/abs/2507.07560v1",
          "size": "484kb",
          "version": "v1"
        }
      ],
      "title": "Conjugated Capabilities: Interrelations of Elementary Human Capabilities and Their Implication on Human-Machine Task Allocation and Capability Testing Procedures",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07560",
        "HTML": "https://arxiv.org/html/2507.07560v1",
        "PDF": "https://arxiv.org/pdf/2507.07560"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on human-automation capability interrelations and task allocation, with no specific emphasis on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.07930",
      "abstract": "Background: Public speaking is a vital professional skill, yet it remains a source of significant anxiety for many individuals. Traditional training relies heavily on expert coaching, but recent advances in AI has led to novel types of commercial automated public speaking feedback tools. However, most research has focused on prototypes rather than commercial applications, and little is known about how public speaking experts perceive these tools.\n  Objectives: This study aims to evaluate expert opinions on the efficacy and design of commercial AI-based public speaking training tools and to propose guidelines for their improvement.\n  Methods: The research involved 16 semi-structured interviews and 2 focus groups with public speaking experts. Participants discussed their views on current commercial tools, their potential integration into traditional coaching, and suggestions for enhancing these systems.\n  Results and Conclusions: Experts acknowledged the value of AI tools in handling repetitive, technical aspects of training, allowing coaches to focus on higher-level skills. However they found key issues in current tools, emphasising the need for personalised, understandable, carefully selected feedback and clear instructional design. Overall, they supported a hybrid model combining traditional coaching with AI-supported exercises.",
      "authors": [
        "Nesrine Fourati",
        "Alisa Barkar",
        "Marion Drag\\'ee",
        "Liv Danthon-Lefebvre",
        "Mathieu Chollet"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-10T17:09:21+00:00",
          "link": "https://arxiv.org/abs/2507.07930v1",
          "size": "118kb",
          "version": "v1"
        }
      ],
      "title": "Probing Experts' Perspectives on AI-Assisted Public Speaking Training",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07930",
        "HTML": "https://arxiv.org/html/2507.07930v1",
        "PDF": "https://arxiv.org/pdf/2507.07930"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper investigates AI-assisted tools for public speaking training, which relates indirectly to creativity as it involves AI's role in skill development and enhancement. Creativity is not the primary focus, but the presence of AI in skill training can support creative expressions in public speaking."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.07216",
      "abstract": "Reliable data is a cornerstone of modern organizational systems. A notable data integrity challenge stems from label bias, which refers to systematic errors in a label, a covariate that is central to a quantitative analysis, such that its quality differs across social groups. This type of bias has been conceptually and empirically explored and is widely recognized as a pressing issue across critical domains. However, effective methodologies for addressing it remain scarce. In this work, we propose Decoupled Confident Learning (DeCoLe), a principled machine learning based framework specifically designed to detect mislabeled instances in datasets affected by label bias, enabling bias aware mislabelling detection and facilitating data quality improvement. We theoretically justify the effectiveness of DeCoLe and evaluate its performance in the impactful context of hate speech detection, a domain where label bias is a well documented challenge. Empirical results demonstrate that DeCoLe excels at bias aware mislabeling detection, consistently outperforming alternative approaches for label error detection. Our work identifies and addresses the challenge of bias aware mislabeling detection and offers guidance on how DeCoLe can be integrated into organizational data management practices as a powerful tool to enhance data reliability.",
      "authors": [
        "Yunyi Li",
        "Maria De-Arteaga and Maytal Saar-Tsechansky"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Databases (cs.DB)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-09T18:44:36+00:00",
          "link": "https://arxiv.org/abs/2507.07216v1",
          "size": "4503kb",
          "version": "v1"
        }
      ],
      "title": "Bias-Aware Mislabeling Detection via Decoupled Confident Learning",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07216",
        "HTML": "https://arxiv.org/html/2507.07216v1",
        "PDF": "https://arxiv.org/pdf/2507.07216"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper is primarily about data integrity and detecting mislabeled instances. It focuses on improving data reliability rather than addressing creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.07327",
      "abstract": "Previous work has shown that the addition of haptic feedback to the hands can improve awareness of tool-tissue interactions and enhance performance of teleoperated tasks in robot-assisted minimally invasive surgery. However, hand-based haptic feedback occludes direct interaction with the manipulanda of surgeon console in teleoperated surgical robots. We propose relocating haptic feedback to the wrist using a wearable haptic device so that haptic feedback mechanisms do not need to be integrated into the manipulanda. However, it is unknown if such feedback will be effective, given that it is not co-located with the finger movements used for manipulation. To test if relocated haptic feedback improves force application during teleoperated tasks using da Vinci Research Kit (dVRK) surgical robot, participants learned to palpate a phantom tissue to desired forces. A soft pneumatic wrist-worn haptic device with an anchoring system renders tool-tissue interaction forces to the wrist of the user. Participants performed the palpation task with and without wrist-worn haptic feedback and were evaluated for the accuracy of applied forces. Participants demonstrated statistically significant lower force error when wrist-worn haptic feedback was provided. Participants also performed the palpation task with longer movement times when provided wrist-worn haptic feedback, indicating that the haptic feedback may have caused participants to operate at a different point in the speed-accuracy tradeoff curve.",
      "authors": [
        "Brian B. Vuong",
        "Josie Davidson",
        "Sangheui Cheon",
        "Kyujin Cho",
        "Allison M. Okamura"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-09T23:03:30+00:00",
          "link": "https://arxiv.org/abs/2507.07327v1",
          "size": "16022kb",
          "version": "v1"
        }
      ],
      "title": "Effects of Wrist-Worn Haptic Feedback on Force Accuracy and Task Speed during a Teleoperated Robotic Surgery Task",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07327",
        "PDF": "https://arxiv.org/pdf/2507.07327"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The study investigates haptic feedback in teleoperated robotic surgery with no connection to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.07387",
      "abstract": "We introduce Digital Salon, a comprehensive hair authoring system that supports real-time 3D hair generation, simulation, and rendering. Unlike existing methods that focus on isolated parts of 3D hair modeling and involve a heavy computation process or network training, Digital Salon offers a holistic and interactive system that lowers the technical barriers of 3D hair modeling through natural language-based interaction. The system guides users through four key stages: text-guided hair retrieval, real-time hair simulation, interactive hair refinement, and hair-conditioned image generation. This cohesive workflow makes advanced hair design accessible to users of varying skill levels and dramatically streamlines the creative process in digital media with an intuitive, versatile, and efficient solution for hair modeling. User studies show that our system can outperform traditional hair modeling workflows for rapid prototyping. Furthermore, we provide insights into the benefits of our system with future potential of deploying our system in real salon environments. More details can be found on our project page: https://digital-salon.github.io/.",
      "authors": [
        "Chengan He",
        "Jorge Alejandro Amador Herrera",
        "Zhixin Shu",
        "Xin Sun",
        "Yao Feng",
        "S\\\"oren Pirk",
        "Dominik L. Michels",
        "Meng Zhang",
        "Tuanfeng Y. Wang",
        "Julie Dorsey",
        "Holly Rushmeier",
        "Yi Zhou"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Graphics (cs.GR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-10T02:58:51+00:00",
          "link": "https://arxiv.org/abs/2507.07387v1",
          "size": "21483kb",
          "version": "v1"
        }
      ],
      "title": "Digital Salon: An AI and Physics-Driven Tool for 3D Hair Grooming and Simulation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07387",
        "HTML": "https://arxiv.org/html/2507.07387v1",
        "PDF": "https://arxiv.org/pdf/2507.07387"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "Digital Salon focuses on a system for 3D hair grooming that streamlines the creative process, making creativity a primary focus as it enhances user creativity in digital media."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.07610",
      "abstract": "Humans can directly imagine and manipulate visual images in their minds, a capability known as spatial visualization. While multi-modal Large Language Models (MLLMs) support imagination-based reasoning, spatial visualization remains insufficiently evaluated, typically embedded within broader mathematical and logical assessments. Existing evaluations often rely on IQ tests or math competitions that may overlap with training data, compromising assessment reliability. To this end, we introduce SpatialViz-Bench, a comprehensive multi-modal benchmark for spatial visualization with 12 tasks across 4 sub-abilities, comprising 1,180 automatically generated problems. Our evaluation of 33 state-of-the-art MLLMs not only reveals wide performance variations and demonstrates the benchmark's strong discriminative power, but also uncovers counter-intuitive findings: models exhibit unexpected behaviors by showing difficulty perception that misaligns with human intuition, displaying dramatic 2D-to-3D performance cliffs, and defaulting to formula derivation despite spatial tasks requiring visualization alone. SpatialVizBench empirically demonstrates that state-of-the-art MLLMs continue to exhibit deficiencies in spatial visualization tasks, thereby addressing a significant lacuna in the field. The benchmark is publicly available.",
      "authors": [
        "Siting Wang",
        "Luoyang Sun",
        "Cheng Deng",
        "Kun Shao",
        "Minnan Pei",
        "Zheng Tian",
        "Haifeng Zhang",
        "Jun Wang"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-10T10:27:20+00:00",
          "link": "https://arxiv.org/abs/2507.07610v1",
          "size": "3775kb",
          "version": "v1"
        }
      ],
      "title": "SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07610",
        "PDF": "https://arxiv.org/pdf/2507.07610"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper introduces a benchmark for evaluating spatial visualization in models, focusing on reasoning and visualization capabilities rather than creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.07661",
      "abstract": "The applications of fingertip haptic devices have spread to various fields from revolutionizing virtual reality and medical training simulations to facilitating remote robotic operations, proposing great potential for enhancing user experiences, improving training outcomes, and new forms of interaction. In this work, we present FiDTouch, a 3D wearable haptic device that delivers cutaneous stimuli to the finger pad, such as contact, pressure, encounter, skin stretch, and vibrotactile feedback. The application of a tiny inverted Delta robot in the mechanism design allows providing accurate contact and fast changing dynamic stimuli to the finger pad surface. The performance of the developed display was evaluated in a two-stage user study of the perception of static spatial contact stimuli and skin stretch stimuli generated on the finger pad. The proposed display, by providing users with precise touch and force stimuli, can enhance user immersion and efficiency in the fields of human-computer and human-robot interactions.",
      "authors": [
        "Daria Trinitatova and Dzmitry Tsetserukou"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-10T11:36:27+00:00",
          "link": "https://arxiv.org/abs/2507.07661v1",
          "size": "2840kb",
          "version": "v1"
        }
      ],
      "title": "FiDTouch: A 3D Wearable Haptic Display for the Finger Pad",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07661",
        "HTML": "https://arxiv.org/html/2507.07661v1",
        "PDF": "https://arxiv.org/pdf/2507.07661"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper describes a haptic device that enhances user experience, which can support creative tasks in virtual reality and human-computer interaction, but creativity is not directly studied."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.07881",
      "abstract": "The rise of conversational AI (CAI), powered by large language models, is transforming how individuals access and interact with digital information. However, these tools may inadvertently amplify existing digital inequalities. This study investigates whether differences in formal education are associated with CAI avoidance, leveraging behavioral data from an online experiment (N = 1,636). Participants were randomly assigned to a control or an information-seeking task, either a traditional online search or a CAI (Perplexity AI). Task avoidance (operationalized as survey abandonment or providing unrelated responses during task assignment) was significantly higher in the CAI group (51%) compared to the search (30.9%) and control (16.8%) groups, with the highest CAI avoidance among participants with lower education levels (~74.4%). Structural equation modeling based on the theoretical framework UTAUT2 and LASSO regressions reveal that education is strongly associated with CAI avoidance, even after accounting for various cognitive and affective predictors of technology adoption. These findings underscore education's central role in shaping AI adoption and the role of self-selection biases in AI-related research, stressing the need for inclusive design to ensure equitable access to emerging technologies.",
      "authors": [
        "Roberto Ulloa",
        "Juhi Kulshrestha",
        "Celina Kacperski"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-10T16:05:11+00:00",
          "link": "https://arxiv.org/abs/2507.07881v1",
          "size": "1272kb",
          "version": "v1"
        }
      ],
      "title": "Opting Out of Generative AI: a Behavioral Experiment on the Role of Education in Perplexity AI Avoidance",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07881",
        "PDF": "https://arxiv.org/pdf/2507.07881"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The study explores AI tool adoption with a focus on behavior and education, touching on the creation and comprehension of AI content. Creativity may relate to content generation and interaction, but is not the main focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.07911",
      "abstract": "Immersive virtual reality (VR) is a promising tool for stress reduction and relaxation, traditionally relying on visual and auditory stimuli. This study examines the role of olfactory stimuli in enhancing these effects, using a randomized within-subject design. Thirty participants aged 18-60 experienced VR scenarios simulating a calming seaside environment, with sessions lasting 45 minutes, in two conditions: with and without a \"Beach\" essential oil scent (Yankee Candle) administered via diffuser. Stress and relaxation were assessed through self-reported surveys and physiological measures, specifically ECG-based heart rate variability (HRV). Results showed no significant difference in self-reported relaxation scores (p=0.371) between conditions, but HRV analysis revealed a significant stress reduction (p=0.002) with olfactory input, with HF increasing 108% from the Math Stress Test to the scented relaxation condition, compared to 44% without scent. Additionally, 71.4% of participants expressed willingness to use olfactory-enhanced VR for relaxation, suggesting practical appeal. These findings indicate that olfactory stimuli may enhance relaxation subconsciously, underscoring the importance of multisensory integration in VR. Future work could explore personalized scents and long-term effects to optimize VR- based interventions for emotional and physical well-being.",
      "authors": [
        "Yasmin Elsaddik Valdivieso",
        "Mohd Faisal",
        "Karim Alghoul",
        "Monireh (Monica) Vahdati",
        "Kamran Gholizadeh Hamlabadi",
        "Fedwa Laamarti",
        "Hussein Al Osman",
        "Abdulmotaleb El Saddik"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Multimedia (cs.MM)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-10T16:45:10+00:00",
          "link": "https://arxiv.org/abs/2507.07911v1",
          "size": "602kb",
          "version": "v1"
        }
      ],
      "title": "The Potential of Olfactory Stimuli in Stress Reduction through Virtual Reality",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07911",
        "PDF": "https://arxiv.org/pdf/2507.07911"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on stress reduction using olfactory stimuli in VR environments. Creativity is not mentioned or studied, and the primary concern is well-being and relaxation."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.07916",
      "abstract": "Phishing has become a prominent risk in modern cybersecurity, often used to bypass technological defences by exploiting predictable human behaviour. Warning dialogues are a standard mitigation measure, but the lack of explanatory clarity and static content limits their effectiveness. In this paper, we report on our research to assess the capacity of Large Language Models (LLMs) to generate clear, concise, and scalable explanations for phishing warnings. We carried out a large-scale between-subjects user study (N = 750) to compare the influence of warning dialogues supplemented with manually generated explanations against those generated by two LLMs, Claude 3.5 Sonnet and Llama 3.3 70B. We investigated two explanatory styles (feature-based and counterfactual) for their effects on behavioural metrics (click-through rate) and perceptual outcomes (e.g., trust, risk, clarity). The results indicate that well-constructed LLM-generated explanations can equal or surpass manually crafted explanations in reducing susceptibility to phishing; Claude-generated warnings exhibited particularly robust performance. Feature-based explanations were more effective for genuine phishing attempts, whereas counterfactual explanations diminished false-positive rates. Other variables such as workload, gender, and prior familiarity with warning dialogues significantly moderated warning effectiveness. These results indicate that LLMs can be used to automatically build explanations for warning users against phishing, and that such solutions are scalable, adaptive, and consistent with human-centred values.",
      "authors": [
        "Federico Maria Cau",
        "Giuseppe Desolda",
        "Francesco Greco",
        "Lucio Davide Spano and Luca Vigan\\`o"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-10T16:54:05+00:00",
          "link": "https://arxiv.org/abs/2507.07916v1",
          "size": "1069kb",
          "version": "v1"
        }
      ],
      "title": "Can Large Language Models Improve Phishing Defense? A Large-Scale Controlled Experiment on Warning Dialogue Explanations",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.07916",
        "HTML": "https://arxiv.org/html/2507.07916v1",
        "PDF": "https://arxiv.org/pdf/2507.07916"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper discusses the use of LLMs for improving phishing warnings in cybersecurity contexts. It does not address creativity in any form, focusing instead on explanation clarity and security measures."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.04278",
      "abstract": "With the recent success of Large Language Models (LLMs), Descriptive Multimodal Emotion Recognition (DMER) has garnered increasing attention, which aims to describe a person's emotional state using free-form natural language. Unlike traditional discriminative methods that rely on predefined emotion taxonomies, DMER offers greater flexibility in emotional expression, enabling fine-grained and interpretable emotion representations. However, this free-form prediction paradigm exposes significant challenges in evaluation. Existing methods either depend on ground-truth descriptions that require substantial manual annotations or simplify the task by shifting the focus from evaluating descriptions to evaluating emotion labels. However, this simplification overlooks critical aspects such as emotional temporal dynamics, intensity, and uncertainty. To address these limitations, we draw inspiration from Reinforcement Learning from Human Feedback (RLHF) and propose DMER-Ranker, a novel evaluation strategy that reformulates the traditional ``prediction-ground truth'' comparison into the ``prediction-prediction'' comparison, eliminating the need for ground-truth descriptions. We then employ the Bradley-Terry algorithm to convert pairwise comparison results into model-level rankings. Additionally, we explore the possibility of automatic preference prediction and introduce DMER-Preference, the first preference dataset specifically designed for human emotions. Our work advances the field of DMER and lays the foundation for more intelligent human-computer interaction systems.",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Haoyu Chen",
        "Zebang Cheng",
        "Fan Zhang",
        "Ziyu Jia",
        "Ziyang Ma",
        "Fei Ma",
        "Xiaojiang Peng",
        "Jianhua Tao"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-06T07:37:59+00:00",
          "link": "https://arxiv.org/abs/2507.04278v1",
          "size": "10398kb",
          "version": "v1"
        },
        {
          "date": "2025-07-10T03:28:00+00:00",
          "link": "https://arxiv.org/abs/2507.04278v2",
          "size": "10398kb",
          "version": "v2"
        }
      ],
      "title": "DMER-Ranker: Learning to Rank Emotion Descriptions in the Absence of Ground Truth",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.04278",
        "HTML": "https://arxiv.org/html/2507.04278v2",
        "PDF": "https://arxiv.org/pdf/2507.04278"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "DMER-Ranker develops a new evaluation strategy for emotion recognition, which involves some degree of creative engagement in emotion description. Creativity is a secondary theme linked to interpreting emotions creatively."
      },
      "source": "arXiv"
    },
    {
      "id": "2403.13318",
      "abstract": "Successful human-robot teaming will require robots to adapt autonomously to a human teammate's internal state, where a critical element of such adaptation is the ability to estimate the human's workload in unknown situations. Existing workload models use machine learning to model the relationship between physiological signals and workload. These methods often struggle to generalize to unknown tasks, as the relative importance of various physiological signals change significantly between tasks. Many of these changes constitute a meaningful shift in the data's distribution, which violates a core assumption made by the underlying machine learning approach. A survey of machine learning techniques designed to overcome these challenges is presented, where common techniques are evaluated using three criteria: portability, model complexity, and adaptability. These criteria are used to analyze each technique's applicability to estimating workload during unknown tasks in dynamic environments and guide future empirical experimentation.",
      "authors": [
        "Josh Bhagat Smith",
        "and Julie A. Adams"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-03-20T05:46:56+00:00",
          "link": "https://arxiv.org/abs/2403.13318v1",
          "size": "13555kb",
          "version": "v1"
        },
        {
          "date": "2025-07-10T15:36:30+00:00",
          "link": "https://arxiv.org/abs/2403.13318v2",
          "size": "1444kb",
          "version": "v2"
        }
      ],
      "title": "A Survey of Machine Learning for Estimating Workload: Considering Unknown Tasks",
      "links": {
        "Abstract": "https://arxiv.org/abs/2403.13318",
        "HTML": "https://arxiv.org/html/2403.13318v2",
        "PDF": "https://arxiv.org/pdf/2403.13318"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper surveys machine learning to estimate workload. It focuses on human-robot interaction and does not address creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2407.16803",
      "abstract": "In order to unlock the potential of diverse sensors, we investigate a method to transfer knowledge between time-series modalities using a multimodal \\textit{temporal} representation space for Human Activity Recognition (HAR). Specifically, we explore the setting where the modality used in testing has no labeled data during training, which we refer to as Unsupervised Modality Adaptation (UMA). We categorize existing UMA approaches as Student-Teacher or Contrastive Alignment methods. These methods typically compress continuous-time data samples into single latent vectors during alignment, inhibiting their ability to transfer temporal information through real-world temporal distortions. To address this, we introduce Cross-modal Transfer Through Time (C3T), which preserves temporal information during alignment to handle dynamic sensor data better. C3T achieves this by aligning a set of temporal latent vectors across sensing modalities. Our extensive experiments on various camera+IMU datasets demonstrate that C3T outperforms existing methods in UMA by at least 8% in accuracy and shows superior robustness to temporal distortions such as time-shift, misalignment, and dilation. Our findings suggest that C3T has significant potential for developing generalizable models for time-series sensor data, opening new avenues for various multimodal applications.",
      "authors": [
        "Abhi Kamboj",
        "Anh Duy Nguyen",
        "Minh N. Do"
      ],
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)",
        "Signal Processing (eess.SP)"
      ],
      "submission_historys": [
        {
          "date": "2024-07-23T19:06:44+00:00",
          "link": "https://arxiv.org/abs/2407.16803v1",
          "size": "1392kb",
          "version": "v1"
        },
        {
          "date": "2024-11-07T17:10:15+00:00",
          "link": "https://arxiv.org/abs/2407.16803v2",
          "size": "6145kb",
          "version": "v2"
        },
        {
          "date": "2025-06-09T15:03:39+00:00",
          "link": "https://arxiv.org/abs/2407.16803v3",
          "size": "2091kb",
          "version": "v3"
        },
        {
          "date": "2025-07-10T06:16:59+00:00",
          "link": "https://arxiv.org/abs/2407.16803v4",
          "size": "2091kb",
          "version": "v4"
        }
      ],
      "title": "C3T: Cross-modal Transfer Through Time for Sensor-based Human Activity Recognition",
      "links": {
        "Abstract": "https://arxiv.org/abs/2407.16803",
        "HTML": "https://arxiv.org/html/2407.16803v4",
        "PDF": "https://arxiv.org/pdf/2407.16803"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses a method for sensor-based human activity recognition with a focus on temporal data alignment. There is no mention of creativity or creative tasks."
      },
      "tasks": [
        "Action Recognition",
        "Activity Recognition",
        "Human Activity Recognition",
        "Temporal Action Localization",
        "Time Series"
      ],
      "source": "arXiv"
    },
    {
      "id": "2409.18813",
      "abstract": "Eye-tracking technology has gained significant attention in recent years due to its wide range of applications in human-computer interaction, virtual and augmented reality, and wearable health. Traditional RGB camera-based eye-tracking systems often struggle with poor temporal resolution and computational constraints, limiting their effectiveness in capturing rapid eye movements. To address these limitations, we propose EyeTrAES, a novel approach using neuromorphic event cameras for high-fidelity tracking of natural pupillary movement that shows significant kinematic variance. One of EyeTrAES's highlights is the use of a novel adaptive windowing/slicing algorithm that ensures just the right amount of descriptive asynchronous event data accumulation within an event frame, across a wide range of eye movement patterns. EyeTrAES then applies lightweight image processing functions over accumulated event frames from just a single eye to perform pupil segmentation and tracking. We show that these methods boost pupil tracking fidelity by 6+%, achieving IoU~=92%, while incurring at least 3x lower latency than competing pure event-based eye tracking alternatives [38]. We additionally demonstrate that the microscopic pupillary motion captured by EyeTrAES exhibits distinctive variations across individuals and can thus serve as a biometric fingerprint. For robust user authentication, we train a lightweight per-user Random Forest classifier using a novel feature vector of short-term pupillary kinematics, comprising a sliding window of pupil (location, velocity, acceleration) triples. Experimental studies with two different datasets demonstrate that the EyeTrAES-based authentication technique can simultaneously achieve high authentication accuracy (~=0.82) and low processing latency (~=12ms), and significantly outperform multiple state-of-the-art competitive baselines.",
      "authors": [
        "Argha Sen",
        "Nuwan Bandara",
        "Ila Gokarn",
        "Thivya Kandappu",
        "Archan Misra"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-09-27T15:06:05+00:00",
          "link": "https://arxiv.org/abs/2409.18813v1",
          "size": "6333kb",
          "version": "v1"
        },
        {
          "date": "2025-07-07T17:22:09+00:00",
          "link": "https://arxiv.org/abs/2409.18813v2",
          "size": "3779kb",
          "version": "v2"
        },
        {
          "date": "2025-07-10T15:45:38+00:00",
          "link": "https://arxiv.org/abs/2409.18813v3",
          "size": "3779kb",
          "version": "v3"
        }
      ],
      "title": "EyeTrAES: Fine-grained, Low-Latency Eye Tracking via Adaptive Event Slicing",
      "links": {
        "Abstract": "https://arxiv.org/abs/2409.18813",
        "HTML": "https://arxiv.org/html/2409.18813v3",
        "PDF": "https://arxiv.org/pdf/2409.18813"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper presents an eye-tracking technology for biometric authentication and human-computer interaction but lacks any connection to creativity or creative processes."
      },
      "tasks": [
        "Descriptive",
        "Pupil Tracking"
      ],
      "source": "arXiv"
    },
    {
      "id": "2503.23760",
      "abstract": "This research addresses the question, which characteristics a cognitive architecture must have to leverage the benefits of natural language in Co-Constructive Task Learning (CCTL). To provide context, we first discuss Interactive Task Learning (ITL), the mechanisms of the human memory system, and the significance of natural language and multi-modality. Next, we examine the current state of cognitive architectures, analyzing their capabilities to inform a concept of CCTL grounded in multiple sources. We then integrate insights from various research domains to develop a unified framework. Finally, we conclude by identifying the remaining challenges and requirements necessary to achieve CCTL in Human-Robot Interaction (HRI).",
      "authors": [
        "Manuel Scheibl",
        "Birte Richter",
        "Alissa M\\\"uller",
        "Michael Beetz",
        "Britta Wrede"
      ],
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-03-31T06:23:14+00:00",
          "link": "https://arxiv.org/abs/2503.23760v1",
          "size": "1232kb",
          "version": "v1"
        },
        {
          "date": "2025-07-10T10:55:31+00:00",
          "link": "https://arxiv.org/abs/2503.23760v2",
          "size": "1232kb",
          "version": "v2"
        }
      ],
      "title": "Towards a cognitive architecture to enable natural language interaction in co-constructive task learning",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.23760",
        "HTML": "https://arxiv.org/html/2503.23760v2",
        "PDF": "https://arxiv.org/pdf/2503.23760"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper deals with cognitive architectures for natural language interaction in task learning, focusing on human-robot interaction and task learning, without a clear focus on creativity."
      },
      "tasks": [],
      "source": "arXiv"
    },
    {
      "id": "2503.07599",
      "abstract": "Generative AI is transforming education by enabling personalized, on-demand learning experiences. However, AI tutors lack the ability to assess a learner's cognitive state in real time, limiting their adaptability. Meanwhile, electroencephalography (EEG)-based neuroadaptive systems have successfully enhanced engagement by dynamically adjusting learning content. This paper presents NeuroChat, a proof-of-concept neuroadaptive AI tutor that integrates real-time EEG-based engagement tracking with generative AI. NeuroChat continuously monitors a learner's cognitive engagement and dynamically adjusts content complexity, response style, and pacing using a closed-loop system. We evaluate this approach in a pilot study (n=24), comparing NeuroChat to a standard LLM-based chatbot. Results indicate that NeuroChat enhances cognitive and subjective engagement but does not show an immediate effect on learning outcomes. These findings demonstrate the feasibility of real-time cognitive feedback in LLMs, highlighting new directions for adaptive learning, AI tutoring, and human-AI interaction.",
      "authors": [
        "D\\\"unya Baradari",
        "Nataliya Kosmyna",
        "Oscar Petrov",
        "Rebecah Kaplun",
        "Pattie Maes"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Emerging Technologies (cs.ET)"
      ],
      "submission_historys": [
        {
          "date": "2025-03-10T17:57:20+00:00",
          "link": "https://arxiv.org/abs/2503.07599v1",
          "size": "7733kb",
          "version": "v1"
        }
      ],
      "title": "NeuroChat: A Neuroadaptive AI Chatbot for Customizing Learning Experiences",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.07599",
        "PDF": "https://arxiv.org/pdf/2503.07599"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The NeuroChat system aims to adapt learning experiences based on cognitive engagement, which can enhance user creativity in learning contexts. Creativity is a secondary theme related to customizing learning experiences."
      },
      "tasks": [
        "Chatbot",
        "EEG"
      ],
      "source": "arXiv"
    },
    {
      "id": "2501.16609",
      "abstract": "While much work on web agents emphasizes the promise of autonomously performing tasks on behalf of users, in reality, agents often fall short on complex tasks in real-world contexts and modeling user preference. This presents an opportunity for humans to collaborate with the agent and leverage the agent's capabilities effectively. We propose CowPilot, a framework supporting autonomous as well as human-agent collaborative web navigation, and evaluation across task success and task efficiency. CowPilot reduces the number of steps humans need to perform by allowing agents to propose next steps, while users are able to pause, reject, or take alternative actions. During execution, users can interleave their actions with the agent by overriding suggestions or resuming agent control when needed. We conducted case studies on five common websites and found that the human-agent collaborative mode achieves the highest success rate of 95% while requiring humans to perform only 15.2% of the total steps. Even with human interventions during task execution, the agent successfully drives up to half of task success on its own. CowPilot can serve as a useful tool for data collection and agent evaluation across websites, which we believe will enable research in how users and agents can work together. Video demonstrations are available at https://oaishi.github.io/cowpilot.html",
      "authors": [
        "Faria Huq",
        "Zora Zhiruo Wang",
        "Frank F. Xu",
        "Tianyue Ou",
        "Shuyan Zhou",
        "Jeffrey P. Bigham",
        "Graham Neubig"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-01-28T00:56:53+00:00",
          "link": "https://arxiv.org/abs/2501.16609v1",
          "size": "6876kb",
          "version": "v1"
        },
        {
          "date": "2025-02-09T23:03:56+00:00",
          "link": "https://arxiv.org/abs/2501.16609v2",
          "size": "6876kb",
          "version": "v2"
        },
        {
          "date": "2025-04-05T23:49:31+00:00",
          "link": "https://arxiv.org/abs/2501.16609v3",
          "size": "6801kb",
          "version": "v3"
        }
      ],
      "title": "CowPilot: A Framework for Autonomous and Human-Agent Collaborative Web Navigation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2501.16609",
        "HTML": "https://arxiv.org/html/2501.16609",
        "PDF": "https://arxiv.org/pdf/2501.16609"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper focuses on human-agent collaborative web navigation and user interaction, which involves some creative elements like users interleaving their actions with the agent's. However, creativity is not the main focus."
      },
      "tasks": [],
      "source": "arXiv"
    },
    {
      "id": "2301.06555",
      "abstract": "Brain-Computer Interfaces (BCI) have allowed for direct communication from the brain to external applications for the automatic detection of cognitive processes such as error recognition. Error-related potentials (ErrPs) are a particular brain signal elicited when one commits or observes an erroneous event. However, due to the noisy properties of the brain and recording devices, ErrPs vary from instance to instance as they are combined with an assortment of other brain signals, biological noise, and external noise, making the classification of ErrPs a non-trivial problem. Recent works have revealed particular cognitive processes such as awareness, embodiment, and predictability that contribute to ErrP variations. In this paper, we explore the performance of classifier transferability when trained on different ErrP variation datasets generated by varying the levels of awareness and embodiment for a given task. In particular, we look at transference between observational and interactive ErrP categories when elicited by similar and differing tasks. Our empirical results provide an exploratory analysis into the ErrP transferability problem from a data perspective.",
      "authors": [
        "Benjamin Poole and Minwoo Lee"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2023-01-16T18:39:18+00:00",
          "link": "https://arxiv.org/abs/2301.06555v1",
          "size": "911kb",
          "version": "v1"
        }
      ],
      "title": "Error-related Potential Variability: Exploring the Effects on Classification and Transferability",
      "links": {
        "Abstract": "https://arxiv.org/abs/2301.06555",
        "PDF": "https://arxiv.org/pdf/2301.06555"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on brain-computer interfaces and error-related potentials, with no mention of creativity. It addresses classification and transferability of error-related brain signals, which are unrelated to creativity."
      },
      "tasks": [],
      "source": "arXiv"
    },
    {
      "id": "2410.18845",
      "abstract": "As the application of AI continues to expand, students in technology programs are poised to be both producers and users of the technologies. They are also positioned to engage with AI applications within and outside the classroom. While focusing on the curriculum when examining students' AI knowledge is common, extending this connection to students' everyday interactions with AI provides a more complete picture of their learning. In this paper, we explore student's awareness and engagement with AI in the context of school and their daily lives. Over six weeks, 22 undergraduate students participated in a reflective journal study and submitted a weekly journal entry about their interactions with AI. The participants were recruited from a technology and society course that focuses on the implications of technology on people, communities, and processes. In their weekly journal entries, participants reflected on interactions with AI on campus (coursework, advertises campus events, or seminars) and beyond (social media, news, or conversations with friends and family). The journal prompts were designed to help them think through what they had read, watched, or been told and reflect on the development of their own perspectives, knowledge, and literacy on the topic. Overall, students described nine categories of interactions: coursework, news and current events, using software and applications, university events, social media related to their work, personal discussions with friends and family, interacting with content, and gaming. Students reported that completing the diaries allowed them time for reflection and made them more aware of the presence of AI in their daily lives and of its potential benefits and drawbacks. This research contributes to the ongoing work on AI awareness and literacy by bringing in perspectives from beyond a formal educational context.",
      "authors": [
        "Ashish Hingle",
        "Aditya Johri"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "2024-10-24T15:26:34+00:00",
          "link": "https://arxiv.org/abs/2410.18845v1",
          "size": "707kb",
          "version": "v1"
        }
      ],
      "title": "Expanding AI Awareness Through Everyday Interactions with AI: A Reflective Journal Study",
      "links": {
        "Abstract": "https://arxiv.org/abs/2410.18845",
        "HTML": "https://arxiv.org/html/2410.18845",
        "PDF": "https://arxiv.org/pdf/2410.18845"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper explores AI awareness and engagement through reflective journaling. While it does not focus on creativity, it could involve creative insights through reflections and discussions around AI applications, making creativity a secondary theme."
      },
      "tasks": [],
      "source": "arXiv"
    }
  ],
  "subjects": [
    "Computer Vision and Pattern Recognition (cs.CV)",
    "Multimedia (cs.MM)",
    "Computation and Language (cs.CL)",
    "Databases (cs.DB)",
    "Cryptography and Security (cs.CR)",
    "Robotics (cs.RO)",
    "Artificial Intelligence (cs.AI)",
    "Emerging Technologies (cs.ET)",
    "Signal Processing (eess.SP)",
    "Computers and Society (cs.CY)",
    "Multiagent Systems (cs.MA)",
    "Human-Computer Interaction (cs.HC)",
    "Graphics (cs.GR)",
    "Digital Libraries (cs.DL)",
    "Machine Learning (cs.LG)"
  ],
  "prompt": {
    "creativity": "\nYou are an expert in information retrieval. I will provide you with a list of research papers from arXiv, specifically in the *cs.HC* (Human-Computer Interaction) category.\n\nYour task is to analyze each paper and determine its relevance to the topic of **Creativity**.\n\n\n### Classify each paper into one of the following relevance levels\n\n- `core` \u2014 Creativity is a **primary focus**\n  - The paper directly studies or simulates creativity, with a clear focus on creativity.\n  - Includes creative tasks, co-creative systems, or creativity evaluation metrics.\n  - The title and abstract explicitly mention creativity, and the research questions are directly related to creativity.\n- `partial` \u2014 Creativity is a **secondary theme**\n  - Part of the paper relates to creativity; it is treated as an analytical dimension or design goal but not the main objective (e.g., user creativity, design support).\n  - Creativity may appear in discussions, experiments, or auxiliary applications.\n  - Creativity is presented as a supporting topic (e.g., evaluation criteria, user feedback).\n- `irrelevant` \u2014 **No clear connection** to creativity\n  - The paper does not address creativity as a topic.\n  - Focuses on unrelated technical content (e.g., compression, security, optimization).\n  - If creativity is mentioned, it is only superficial and lacks substantive content.\n\n\n### Return your results in the following JSON format\n\n```json\n{\n  \"result\": [\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    },\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    }\n  ]\n}\n```\n\nBe concise but specific in your reasoning, referencing key terms or sections when applicable.\n"
  },
  "description": "Data source: https://arxiv.org/list/cs.HC/new",
  "level_tatistics": {
    "partial": 9,
    "irrelevant": 12,
    "core": 2
  },
  "arxiv_update_date": "2025-07-11",
  "updated_at": "2025-07-11 10:01:47"
}