{
  "data": [
    {
      "id": "2507.02122",
      "abstract": "Effective communication in serious illness and palliative care is essential but often under-taught due to limited access to training resources like standardized patients. We present PAL (Palliative Assisted Learning-bot), a conversational system that simulates emotionally nuanced patient interactions and delivers structured feedback grounded in an existing empathy-based framework. PAL supports text and voice modalities and is designed to scaffold clinical skill-building through repeated, low-cost practice. Through a mixed-methods study with 17 U.S. medical trainees and clinicians, we explore user engagement with PAL, evaluate usability, and examine design tensions around modalities, emotional realism, and feedback delivery. Participants found PAL helpful for reflection and skill refinement, though some noted limitations in emotional authenticity and the adaptability of feedback. We contribute: (1) empirical evidence that large language models can support palliative communication training; (2) design insights for modality-aware, emotionally sensitive simulation tools; and (3) implications for systems that support emotional labor, cooperative learning, and AI-augmented training in high-stakes care settings.",
      "authors": [
        "Neil K. R. Sehgal and Hita Kambhamettu and Allen Chang and Andrew Zhu and Lyle Ungar and Sharath Chandra Guntuku"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T20:09:52+00:00",
          "link": "https://arxiv.org/abs/2507.02122v1",
          "size": "1721kb",
          "version": "v1"
        }
      ],
      "title": "PAL: Designing Conversational Agents as Scalable, Cooperative Patient Simulators for Palliative-Care Training",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02122",
        "HTML": "https://arxiv.org/html/2507.02122v1",
        "PDF": "https://arxiv.org/pdf/2507.02122"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on effective communication for palliative care training using a conversational agent. Creativity is not a primary or secondary theme; the focus is on emotional realism and communication training."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02138",
      "abstract": "This study introduces and evaluates Healthy Choice, an innovative theory-driven and AI-enhanced simulation platform designed to cultivate nutrition literacy through interactive scenario-based learning experiences. We collected feedback from 114 university students with diverse backgrounds who completed simulated product selection scenarios. Quantitative ratings of usefulness and ease of use demonstrated high user satisfaction.",
      "authors": [
        "Shan Li",
        "Guozhu Ding"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T20:50:36+00:00",
          "link": "https://arxiv.org/abs/2507.02138v1",
          "size": "982kb",
          "version": "v1"
        }
      ],
      "title": "A Theory-driven and AI-enhanced Simulation Platform for Cultivating Nutrition Literacy",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02138",
        "PDF": "https://arxiv.org/pdf/2507.02138"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses an AI-enhanced platform for nutrition literacy. There is no mention of creativity as a primary or secondary theme. The focus is on nutrition education, not on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02156",
      "abstract": "The StorySpace project studies the role new interface technologies might play in high school education. With this approach in mind, StorySpace is specifically designed to support and enhance classroom narrative, an already well-established classroom activity. StorySpace strives to achieve this through adherence to three design goals. The first is to trigger student reflection and interpretation. The narrative medium created by StorySpace should represent the topic of classroom discussion and learning in all its complexity. In building their representation, the students will then be confronted with that same complexity. The medium should also itself be exciting and compelling, making classroom narrative interesting and fun.",
      "authors": [
        "Benjamin Watson",
        "Janet Kim",
        "Tim McEneany",
        "Tom Moher",
        "Claudia Hindo",
        "Louis Gomez",
        "Stephen Fransen"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Emerging Technologies (cs.ET)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T21:24:35+00:00",
          "link": "https://arxiv.org/abs/2507.02156v1",
          "size": "501kb",
          "version": "v1"
        }
      ],
      "title": "StorySpace: Technology supporting reflection, expression, and discourse in classroom narrative",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02156",
        "PDF": "https://arxiv.org/pdf/2507.02156"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper explores supporting reflection and expression in classroom narratives, which can relate to enhancing creative storytelling. Creativity is a secondary theme related to the design goal of making narrative creation exciting and compelling."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02180",
      "abstract": "Large language Models have only been widely available since 2022 and yet in less than three years have had a significant impact on approaches to education and educational technology. Here we review the domains in which they have been used, and discuss a variety of use cases, their successes and failures. We then progress to discussing how this is changing the dynamic for learners and educators, consider the main design challenges facing LLMs if they are to become truly helpful and effective as educational systems, and reflect on the learning paradigms they support. We make clear that the new interaction paradigms they bring are significant and argue that this approach will become so ubiquitous it will become the default way in which we interact with technologies, and revolutionise what people expect from computer systems in general. This leads us to present some specific and significant considerations for the design of educational technology in the future that are likely to be needed to ensure acceptance by the changing expectations of learners and users.",
      "authors": [
        "Russell Beale"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T22:23:26+00:00",
          "link": "https://arxiv.org/abs/2507.02180v1",
          "size": "73kb",
          "version": "v1"
        }
      ],
      "title": "The Revolution Has Arrived: What the Current State of Large Language Models in Education Implies for the Future",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02180",
        "HTML": "https://arxiv.org/html/2507.02180v1",
        "PDF": "https://arxiv.org/pdf/2507.02180"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses the impact of large language models on education, including potential changes in learning paradigms. While creativity is not explicitly mentioned, the paper implies potential for innovative interaction and expression, touching upon creative educational outputs."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02186",
      "abstract": "With the broad availability of large language models and their ability to generate vast outputs using varied prompts and configurations, determining the best output for a given task requires an intensive evaluation process, one where machine learning practitioners must decide how to assess the outputs and then carefully carry out the evaluation. This process is both time-consuming and costly. As practitioners work with an increasing number of models, they must now evaluate outputs to determine which model and prompt performs best for a given task. LLMs are increasingly used as evaluators to filter training data, evaluate model performance, assess harms and risks, or assist human evaluators with detailed assessments. We present EvalAssist, a framework that simplifies the LLM-as-a-judge workflow. The system provides an online criteria development environment, where users can interactively build, test, and share custom evaluation criteria in a structured and portable format. We support a set of LLM-based evaluation pipelines that leverage off-the-shelf LLMs and use a prompt-chaining approach we developed and contributed to the UNITXT open-source library. Additionally, our system also includes specially trained evaluators to detect harms and risks in LLM outputs. We have deployed the system internally in our organization with several hundreds of users.",
      "authors": [
        "Zahra Ashktorab",
        "Elizabeth M. Daly",
        "Erik Miehling",
        "Werner Geyer",
        "Martin Santillan Cooper",
        "Tejaswini Pedapati",
        "Michael Desmond",
        "Qian Pan",
        "and Hyo Jin Do"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T22:45:39+00:00",
          "link": "https://arxiv.org/abs/2507.02186v1",
          "size": "1666kb",
          "version": "v1"
        }
      ],
      "title": "EvalAssist: A Human-Centered Tool for LLM-as-a-Judge",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02186",
        "HTML": "https://arxiv.org/html/2507.02186v1",
        "PDF": "https://arxiv.org/pdf/2507.02186"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "EvalAssist is designed for evaluating outputs from large language models, which can include creative tasks as part of the evaluation criteria. Creativity is treated as an analytical dimension in the evaluation process, but it is not the core focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02187",
      "abstract": "There is growing industry interest in creating unobtrusive designs for electrooculography (EOG) sensing of eye gestures on glasses (e.g. JINS MEME and Apple eyewear). We present VergeIO, the first EOG-based glasses that enables depth-aware eye interaction using vergence with an optimized electrode layout and novel smart glass prototype. It can distinguish between four and six depth-based eye gestures with 83-98% accuracy using personalized models in a user study across 11 users and 1,320 gesture instances. It generalizes to unseen users with an accuracy of 80-98% without any calibration. To reduce false detections, we incorporate a motion artifact detection pipeline and a preamble-based activation scheme. The system uses dry sensors without any adhesives or gel, and operates in real time with 3 mW power consumption by the sensing front-end, making it suitable for always-on sensing.",
      "authors": [
        "Xiyuxing Zhang",
        "Duc Vu",
        "Chengyi Shen",
        "Yuntao Wang",
        "Yuanchun Shi",
        "Justin Chan"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T22:47:37+00:00",
          "link": "https://arxiv.org/abs/2507.02187v1",
          "size": "6159kb",
          "version": "v1"
        }
      ],
      "title": "VergeIO: Depth-Aware Eye Interaction on Glasses",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02187",
        "HTML": "https://arxiv.org/html/2507.02187v1",
        "PDF": "https://arxiv.org/pdf/2507.02187"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on depth-aware eye interaction using electrooculography in glasses, which is a technical development unrelated to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02229",
      "abstract": "Collaborative problem solving (CPS) is a complex cognitive, social, and emotional process that is increasingly prevalent in educational and professional settings. This study investigates the emotional states of individuals during CPS using a mixed-methods approach. Teams of four first completed a novel CPS task. Immediately after, each individual was placed in an isolated room where they reviewed the video of their group performing the task and self-reported their internal experiences throughout the task. We performed a linguistic analysis of these internal monologues, providing insights into the range of emotions individuals experience during CPS. Our analysis showed distinct patterns in language use, including characteristic unigrams and bigrams, key words and phrases, emotion labels, and semantic similarity between emotion-related words.",
      "authors": [
        "Sifatul Anindho",
        "Videep Venkatesha",
        "Mariah Bradford",
        "Anne M. Cleary",
        "Nathaniel Blanchard"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T01:41:49+00:00",
          "link": "https://arxiv.org/abs/2507.02229v1",
          "size": "4810kb",
          "version": "v1"
        }
      ],
      "title": "An Exploration of Internal States in Collaborative Problem Solving",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02229",
        "HTML": "https://arxiv.org/html/2507.02229v1",
        "PDF": "https://arxiv.org/pdf/2507.02229"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The study is on emotional states during collaborative problem solving. While such tasks could involve creativity, the focus is on emotional and cognitive processes, not specifically on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02254",
      "abstract": "This paper presents a software architecture for 3D interaction techniques (ITs) and an object oriented, toolkit-independent framework that implements such architecture. ITs are composed of basic filters connected in a dataflow, where virtual input devices and objects in the scene are sources of information. An execution model defines the general flow of information between filters. This framework has been designed to be extensible: new information types, new input devices, new execution models, or new interaction techniques can easily be added. Application specific code and application specific ITs are seamlessly integrated into this architecture.",
      "authors": [
        "Pablo Figueroa",
        "Mark Green",
        "Benjamin Watson"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T03:05:00+00:00",
          "link": "https://arxiv.org/abs/2507.02254v1",
          "size": "59kb",
          "version": "v1"
        }
      ],
      "title": "A framework for 3D interaction techniques",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02254",
        "PDF": "https://arxiv.org/pdf/2507.02254"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper centers on a framework for 3D interaction techniques, with no direct focus on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02283",
      "abstract": "This paper examines a critical yet unexplored dimension of the AI alignment problem: the potential for Large Language Models (LLMs) to inherit and amplify existing misalignments between human espoused theories and theories-in-use. Drawing on action science research, we argue that LLMs trained on human-generated text likely absorb and reproduce Model 1 theories-in-use - a defensive reasoning pattern that both inhibits learning and creates ongoing anti-learning dynamics at the dyad, group, and organisational levels. Through a detailed case study of an LLM acting as an HR consultant, we show how its advice, while superficially professional, systematically reinforces unproductive problem-solving approaches and blocks pathways to deeper organisational learning. This represents a specific instance of the alignment problem where the AI system successfully mirrors human behaviour but inherits our cognitive blind spots. This poses particular risks if LLMs are integrated into organisational decision-making processes, potentially entrenching anti-learning practices while lending authority to them. The paper concludes by exploring the possibility of developing LLMs capable of facilitating Model 2 learning - a more productive theory-in-use - and suggests this effort could advance both AI alignment research and action science practice. This analysis reveals an unexpected symmetry in the alignment challenge: the process of developing AI systems properly aligned with human values could yield tools that help humans themselves better embody those same values.",
      "authors": [
        "Tim Rogers",
        "Ben Teehankee"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T03:44:45+00:00",
          "link": "https://arxiv.org/abs/2507.02283v1",
          "size": "325kb",
          "version": "v1"
        }
      ],
      "title": "Misaligned from Within: Large Language Models Reproduce Our Double-Loop Learning Blindness",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02283",
        "PDF": "https://arxiv.org/pdf/2507.02283"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on AI alignment and cognitive blind spots of language models in organizational settings, with no apparent exploration or study of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02300",
      "abstract": "Human-centered explainability has become a critical foundation for the responsible development of interactive information systems, where users must be able to understand, interpret, and scrutinize AI-driven outputs to make informed decisions. This systematic survey of literature aims to characterize recent progress in user studies on explainability in interactive information systems by reviewing how explainability has been conceptualized, designed, and evaluated in practice. Following PRISMA guidelines, eight academic databases were searched, and 100 relevant articles were identified. A structural encoding approach was then utilized to extract and synthesize insights from these articles. The main contributions include 1) five dimensions that researchers have used to conceptualize explainability; 2) a classification scheme of explanation designs; 3) a categorization of explainability measurements into six user-centered dimensions. The review concludes by reflecting on ongoing challenges and providing recommendations for future exploration of related issues. The findings shed light on the theoretical foundations of human-centered explainability, informing the design of interactive information systems that better align with diverse user needs and promoting the development of systems that are transparent, trustworthy, and accountable.",
      "authors": [
        "Yuhao Zhang",
        "Jiaxin An",
        "Ben Wang",
        "Yan Zhang",
        "Jiqun Liu"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T04:12:45+00:00",
          "link": "https://arxiv.org/abs/2507.02300v1",
          "size": "1973kb",
          "version": "v1"
        }
      ],
      "title": "Human-Centered Explainability in Interactive Information Systems: A Survey",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02300",
        "HTML": "https://arxiv.org/html/2507.02300v1",
        "PDF": "https://arxiv.org/pdf/2507.02300"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The survey centers on human-centered explainability in interactive information systems, with no mention of creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02306",
      "abstract": "Usability evaluation is crucial in human-centered design but can be costly, requiring expert time and user compensation. In this work, we developed a method for synthetic heuristic evaluation using multimodal LLMs' ability to analyze images and provide design feedback. Comparing our synthetic evaluations to those by experienced UX practitioners across two apps, we found our evaluation identified 73% and 77% of usability issues, which exceeded the performance of 5 experienced human evaluators (57% and 63%). Compared to human evaluators, the synthetic evaluation's performance maintained consistent performance across tasks and excelled in detecting layout issues, highlighting potential attentional and perceptual strengths of synthetic evaluation. However, synthetic evaluation struggled with recognizing some UI components and design conventions, as well as identifying across screen violations. Additionally, testing synthetic evaluations over time and accounts revealed stable performance. Overall, our work highlights the performance differences between human and LLM-driven evaluations, informing the design of synthetic heuristic evaluations.",
      "authors": [
        "Ruican Zhong",
        "David W. McDonald",
        "Gary Hsieh"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T04:27:16+00:00",
          "link": "https://arxiv.org/abs/2507.02306v1",
          "size": "3048kb",
          "version": "v1"
        }
      ],
      "title": "Synthetic Heuristic Evaluation: A Comparison between AI- and Human-Powered Usability Evaluation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02306",
        "HTML": "https://arxiv.org/html/2507.02306v1",
        "PDF": "https://arxiv.org/pdf/2507.02306"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The research is on usability evaluation techniques rather than creativity, focusing on synthetic versus human-powered evaluation methods."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02350",
      "abstract": "Traditional video-induced emotion physiological datasets often use whole-trial annotation, assigning a single emotion label to all data collected during an entire trial. This coarse-grained annotation approach misaligns with the dynamic and temporally localized nature of emotional responses as they unfold with video narratives, introducing label noise that limits emotion recognition algorithm evaluation and performance. To solve the label noise problem caused by coarse-grained annotation, we propose a fine-grained annotation method through an immediate recall paradigm. This paradigm integrates an immediate video replay phase after the initial stimulus viewing, allowing participants to precisely mark the onset timestamp, emotion label, and intensity based on their immediate recall. We validate this paradigm through physiological evidence and recognition performance. Physiological validation of multimodal signals within participant-marked windows revealed rhythm-specific EEG patterns and arousal-dependent GSR responses-with SCRs appearing in 91% of high-arousal versus 6% of low-arousal emotion windows. These objective physiological data changes strongly aligned with subjective annotations, confirming annotation precision. For recognition performance, classification experiments showed that models trained on fine-grained annotations achieved 9.7% higher accuracy than traditional whole-trial labeling, despite using less data. This work not only addresses label noise through fine-grained annotation but also demonstrates that annotation precision outweighs data scale in determining emotion recognition performance.",
      "authors": [
        "Hao Tang",
        "Songyun Xie",
        "Xinzhou Xie",
        "Can Liao",
        "Xin Zhang",
        "Bohan Li",
        "Zhongyu Tian",
        "Dalu Zheng"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T06:23:51+00:00",
          "link": "https://arxiv.org/abs/2507.02350v1",
          "size": "10401kb",
          "version": "v1"
        }
      ],
      "title": "From Coarse to Fine-Grained Emotion Annotation: An Immediate Recall Paradigm with Validation through Physiological Evidence and Recognition Performance",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02350",
        "HTML": "https://arxiv.org/html/2507.02350v1",
        "PDF": "https://arxiv.org/pdf/2507.02350"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper addresses emotion annotation techniques in physiological datasets and emotion recognition, without a focus on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02432",
      "abstract": "We investigate the use of musically structured, closed-loop vibration patterns as a passive biofeedback intervention for relaxation and sleep initiation. By encoding rhythmic meter structures into smartwatch vibrations and adapting their frequency to be slightly slower than the user's real-time heart rate, our system aims to reduce arousal through tactile entrainment, offering a non-invasive alternative to auditory or open-loop approaches previously used in sleep and anxiety contexts. In the first study (N=20), we compared five adaptive vibration rhythms for their effects on heart rate and subjective perceptions of relaxation in a resting context. In the second study (N=28), we evaluated the most promising pattern from Study 1 in a prolonged sleep initiation setting. Results showed increased parasympathetic activity and perceived relaxation during short-term stimulation, but no significant effects on sleep-related measures during the sleep onset phase. This work contributes to the understanding of how wearable haptic feedback can support relaxation and sleep, offering design insights and identifying methodological considerations for effectively integrating haptic interaction into self-directed interventions.",
      "authors": [
        "Jueun Lee",
        "Dennis Moschina",
        "Supraja Ramesh",
        "Tobias R\\\"oddiger",
        "Kai Kunze",
        "Michael Beigl"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T08:40:27+00:00",
          "link": "https://arxiv.org/abs/2507.02432v1",
          "size": "952kb",
          "version": "v1"
        }
      ],
      "title": "Closed-Loop Rhythmic Haptic Biofeedback via Smartwatch for Relaxation and Sleep Onset",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02432",
        "HTML": "https://arxiv.org/html/2507.02432v1",
        "PDF": "https://arxiv.org/pdf/2507.02432"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper investigates haptic feedback for relaxation and sleep. It focuses on biofeedback mechanisms, with no mention or consideration of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02453",
      "abstract": "Wearable haptic interventions offer promising support for relaxation through slow, vibrotactile biofeedback. Despite their potential, current applications focus on stress-inducing procedures and fixed vibration patterns, with limited consideration of body location and dynamic biofeedback during restful states. This study investigates the effects of haptic biofeedback adjusted from real-time heart rate during eyes-closed wakeful rest, comparing four wearable body placements: the wrist, hand, forearm, and shoulder. Heart rate, alpha wave activity on the ear, subjective restfulness, and vibration experience were measured across these conditions. Results show that biofeedback reduced heart rate at the wrist, shoulder, and forearm, while alpha power measured at the ear remained unchanged. Subjective restfulness was rated highest at the shoulder and forearm, which were also the most preferred locations. In addition, participants reported greater comfort, relaxation, and further increased sleepiness at the forearm compared to the wrist, which was more easily recognizable. These findings suggest that the forearm and shoulder are ideal for unobtrusive relaxation feedback for wakeful rest, while the wrist may require design improvements for subjective experience.",
      "authors": [
        "Jueun Lee",
        "Martin Flipe",
        "Philipp Lepold",
        "Tobias R\\\"oddiger",
        "Michael Beigl"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T09:11:06+00:00",
          "link": "https://arxiv.org/abs/2507.02453v1",
          "size": "2004kb",
          "version": "v1"
        }
      ],
      "title": "Haptic Biofeedback for Wakeful Rest: Does Stimulation Location Make a Difference?",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02453",
        "HTML": "https://arxiv.org/html/2507.02453v1",
        "PDF": "https://arxiv.org/pdf/2507.02453"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This study evaluates the placement of haptic biofeedback devices for relaxation. It is oriented towards physiological responses rather than creative processes or creativity-related outcomes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02537",
      "abstract": "Conversational agents have made significant progress since ELIZA, expanding their role across various domains, including healthcare, education, and customer service. As these agents become increasingly integrated into daily human interactions, the need for emotional intelligence, particularly empathetic listening, becomes increasingly essential. In this study, we explore how Large Language Models (LLMs) respond when tasked with generating emotionally rich interactions. Starting from a small dataset manually crafted by an expert to reflect empathic behavior, we extended the conversations using two LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the dialogues using both sentiment analysis (via VADER) and expert assessments. While the generated conversations often mirrored the intended emotional structure, human evaluation revealed important differences in the perceived empathy and coherence of the responses. These findings suggest that emotion modeling in dialogues requires not only structural alignment in the expressed emotions but also qualitative depth, highlighting the importance of combining automated and humancentered methods in the development of emotionally competent agents.",
      "authors": [
        "Paulo Ricardo Knob",
        "Leonardo Scholler",
        "Juliano Rigatti",
        "Soraia Raupp Musse"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T11:32:41+00:00",
          "link": "https://arxiv.org/abs/2507.02537v1",
          "size": "1320kb",
          "version": "v1"
        }
      ],
      "title": "Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02537",
        "HTML": "https://arxiv.org/html/2507.02537v1",
        "PDF": "https://arxiv.org/pdf/2507.02537"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper explores the emotional intelligence of chatbots, focusing on empathetic dialogue which indirectly connects to creativity as emotional interaction requires creative generation of responses."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02682",
      "abstract": "CAVE displays offer many advantages over other virtual reality (VR) displays, including a large, unencumbering viewing space. Unfortunately, the typical tracking subsystems used with CAVE displays tether the user and lessen this advantage. We have designed a simple, low-cost feet tracker that is wireless, leaving the user free to move. The tracker can be assembled for less than $200 US, and achieves an accuracy of 10 cm at a 20 Hz sampling rate. We have tested the prototype with two applications: a visualization supporting close visual inspection, and a walkthrough of the campus. Although the tracking was convincing, it was clear that the tracker's limitations make it less than ideal for applications requiring precise visual inspection. However, the freedom of motion allowed by the tracker was a compelling supplement to our campus walkthrough, allowing users to stroll and look around corners.",
      "authors": [
        "Ehud Sharlin",
        "Pablo Figueroa",
        "Mark Green",
        "Benjamin Watson"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Emerging Technologies (cs.ET)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T14:45:36+00:00",
          "link": "https://arxiv.org/abs/2507.02682v1",
          "size": "143kb",
          "version": "v1"
        }
      ],
      "title": "A wireless, inexpensive optical tracker for the CAVE",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02682",
        "PDF": "https://arxiv.org/pdf/2507.02682"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper is about developing a wireless optical tracker for VR systems, which is technical and not related to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02745",
      "abstract": "As chatbots driven by large language models (LLMs) are increasingly deployed in everyday contexts, their ability to recover from errors through effective apologies is critical to maintaining user trust and satisfaction. In a preregistered study with Prolific workers (N=162), we examine user preferences for three types of apologies (rote, explanatory, and empathic) issued in response to three categories of common LLM mistakes (bias, unfounded fabrication, and factual errors). We designed a pairwise experiment in which participants evaluated chatbot responses consisting of an initial error, a subsequent apology, and a resolution. Explanatory apologies were generally preferred, but this varied by context and user. In the bias scenario, empathic apologies were favored for acknowledging emotional impact, while hallucinations, though seen as serious, elicited no clear preference, reflecting user uncertainty. Our findings show the complexity of effective apology in AI systems. We discuss key insights such as personalization and calibration that future systems must navigate to meaningfully repair trust.",
      "authors": [
        "Zahra Ashktorab",
        "Alessandra Buccella",
        "Jason D'Cruz",
        "Zoe Fowler",
        "Andrew Gill",
        "Kei Yan Leung",
        "P.D. Magnus",
        "John Richards",
        "Kush R. Varshney"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T16:05:18+00:00",
          "link": "https://arxiv.org/abs/2507.02745v1",
          "size": "4398kb",
          "version": "v1"
        }
      ],
      "title": "Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory Apologies from LLM Chatbots",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02745",
        "HTML": "https://arxiv.org/html/2507.02745v1",
        "PDF": "https://arxiv.org/pdf/2507.02745"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The study examines user preferences for chatbot apologies, which involves creative language use to manage user trust, thus touching upon creativity as a secondary aspect."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02800",
      "abstract": "Speech neuroprostheses aim to restore communication for people with severe paralysis by decoding speech directly from neural activity. To accelerate algorithmic progress, a recent benchmark released intracranial recordings from a paralyzed participant attempting to speak, along with a baseline decoding algorithm. Prior work on the benchmark showed impressive accuracy gains. However, these gains increased computational costs and were not demonstrated in a real-time decoding setting. Here, we make three contributions that pave the way towards accurate, efficient, and real-time neural speech decoding. First, we incorporate large amounts of time masking during training. On average, over $50\\%$ of each trial is masked. Second, we replace the gated recurrent unit (GRU) architecture used in the baseline algorithm with a compact Transformer. The Transformer architecture uses $77\\%$ fewer parameters, cuts peak GPU memory usage by $36\\%$ relative, and is significantly faster to calibrate relative to the GRU. Third, we design a lightweight variant of an existing test-time adaptation method developed for decoding handwriting from neural activity. Our variant adapts the model using multiple time masked augmentations of a single trial and requires only one gradient step per trial. Together, these contributions reduce word error rate by $19.5\\%$ and effectively mitigate performance degradations across held-out days in a real-time decoding setting while substantially lowering computational costs.",
      "authors": [
        "Ebrahim Feghhi",
        "Shreyas Kaasyap",
        "Nima Hadidi",
        "Jonathan C. Kao"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T17:02:54+00:00",
          "link": "https://arxiv.org/abs/2507.02800v1",
          "size": "1637kb",
          "version": "v1"
        }
      ],
      "title": "Time-Masked Transformers with Lightweight Test-Time Adaptation for Neural Speech Decoding",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02800",
        "HTML": "https://arxiv.org/html/2507.02800v1",
        "PDF": "https://arxiv.org/pdf/2507.02800"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on neural speech decoding and algorithmic improvements for speech neuroprostheses, with no mention of creativity-related topics."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02819",
      "abstract": "Data scientists often formulate predictive modeling tasks involving fuzzy, hard-to-define concepts, such as the \"authenticity\" of student writing or the \"healthcare need\" of a patient. Yet the process by which data scientists translate fuzzy concepts into a concrete, proxy target variable remains poorly understood. We interview fifteen data scientists in education (N=8) and healthcare (N=7) to understand how they construct target variables for predictive modeling tasks. Our findings suggest that data scientists construct target variables through a bricolage process, involving iterative negotiation between high-level measurement objectives and low-level practical constraints. Data scientists attempt to satisfy five major criteria for a target variable through bricolage: validity, simplicity, predictability, portability, and resource requirements. To achieve this, data scientists adaptively use problem (re)formulation strategies, such as swapping out one candidate target variable for another when the first fails to meet certain criteria (e.g., predictability), or composing multiple outcomes into a single target variable to capture a more holistic set of modeling objectives. Based on our findings, we present opportunities for future HCI, CSCW, and ML research to better support the art and science of target variable construction.",
      "authors": [
        "Luke Guerdan",
        "Devansh Saxena",
        "Stevie Chancellor",
        "Zhiwei Steven Wu",
        "Kenneth Holstein"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T17:33:24+00:00",
          "link": "https://arxiv.org/abs/2507.02819v1",
          "size": "11155kb",
          "version": "v1"
        }
      ],
      "title": "Measurement as Bricolage: Examining How Data Scientists Construct Target Variables for Predictive Modeling Tasks",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02819",
        "HTML": "https://arxiv.org/html/2507.02819v1",
        "PDF": "https://arxiv.org/pdf/2507.02819"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses data scientists' creative processes in constructing target variables, which involves creative problem-solving and adaptive methods, making creativity a secondary theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01968",
      "abstract": "Purpose: Financial service companies manage huge volumes of data which requires timely error identification and resolution. The associated tasks to resolve these errors frequently put financial analyst workforces under significant pressure leading to resourcing challenges and increased business risk. To address this challenge, we introduce a formal task allocation model which considers both business orientated goals and analyst well-being.\n  Methodology: We use a Genetic Algorithm (GA) to optimise our formal model to allocate and schedule tasks to analysts. The proposed solution is able to allocate tasks to analysts with appropriate skills and experience, while taking into account staff well-being objectives.\n  Findings: We demonstrate our GA model outperforms baseline heuristics, current working practice, and is applicable to a range of single and multi-objective real-world scenarios. We discuss the potential for metaheuristics (such as GAs) to efficiently find sufficiently good allocations which can provide recommendations for financial service managers in-the-loop.\n  Originality: A key gap in existing allocation and scheduling models, is fully considering worker well-being. This paper presents an allocation model which explicitly optimises for well-being while still improving on current working practice for efficiency.",
      "authors": [
        "Chris Duckworth",
        "Zlatko Zlatev",
        "James Sciberras",
        "Peter Hallett",
        "Enrico Gerding"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "General Finance (q-fin.GN)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-18T11:37:25+00:00",
          "link": "https://arxiv.org/abs/2507.01968v1",
          "size": "433kb",
          "version": "v1"
        }
      ],
      "title": "Optimising task allocation to balance business goals and worker well-being for financial service workforces",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01968",
        "HTML": "https://arxiv.org/html/2507.01968v1",
        "PDF": "https://arxiv.org/pdf/2507.01968"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper is about optimizing task allocation for well-being and business efficiency in financial services, with no mention of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02183",
      "abstract": "Generative AI tools - most notably large language models (LLMs) like ChatGPT and Codex - are rapidly revolutionizing computer science education. These tools can generate, debug, and explain code, thereby transforming the landscape of programming instruction. This paper examines the profound opportunities that AI offers for enhancing computer science education in general, from coding assistance to fostering innovative pedagogical practices and streamlining assessments. At the same time, it highlights challenges including academic integrity concerns, the risk of over-reliance on AI, and difficulties in verifying originality. We discuss what computer science educators should teach in the AI era, how to best integrate these technologies into curricula, and the best practices for assessing student learning in an environment where AI can generate code, prototypes and user feedback. Finally, we propose a set of policy recommendations designed to harness the potential of generative AI while preserving the integrity and rigour of computer science education. Empirical data and emerging studies are used throughout to support our arguments.",
      "authors": [
        "Russell Beale"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T22:28:45+00:00",
          "link": "https://arxiv.org/abs/2507.02183v1",
          "size": "67kb",
          "version": "v1"
        }
      ],
      "title": "Computer Science Education in the Age of Generative AI",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02183",
        "HTML": "https://arxiv.org/html/2507.02183v1",
        "PDF": "https://arxiv.org/pdf/2507.02183"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper covers how generative AI transforms computer science education, including fostering innovative practices. Creativity could be a secondary theme in its potential to enhance and innovate educational methods, particularly in coding and prototype generation."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02207",
      "abstract": "As fusion energy technologies approach demonstration and commercial deployment, understanding public perspectives on future fusion facilities will be critical for achieving social license, especially because fusion energy facilities, unlike large fission reactors, may be sited in closer proximity to people and communities, due to distinct regulatory frameworks. In a departure from the 'decide-announce-defend' approach typically used to site energy infrastructure, we develop a participatory design methodology for collaboratively designing fusion energy facilities with prospective host communities. We present here our findings from a participatory design workshop that brought together 22 community participants and 34 engineering students. Our analysis of the textual and visual data from this workshop shows a range of design values and decision-making criteria with 'integrity' and 'respect' ranking highest among values and 'economic benefits' and 'environmental protection/safety' ranking highest among decision-making criteria. Salient design themes that emerge across facility concepts include connecting the history and legacy of the community to the design of the facility, care for workers, transparency and access to the facility, and health and safety of the host community. Participants reported predominantly positive sentiments, expressing joy and surprise as the workshop progressed from learning about fusion to designing the hypothetical facility. Our findings suggest that carrying out participatory design in the early stages of technology development can invite and make concrete public hopes and concerns, improve understanding of, and curiosity about, an emerging technology, build toward social license, and inform context-specific development of fusion energy facilities.",
      "authors": [
        "Nathan Kawamoto",
        "Daniel Hoover",
        "Jonathan Xie",
        "Jacob Walters",
        "Katie Snyder",
        "Aditi Verma"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Physics and Society (physics.soc-ph)",
        "Human-Computer Interaction (cs.HC)",
        "Physics Education (physics.ed-ph)",
        "Plasma Physics (physics.plasm-ph)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T00:01:39+00:00",
          "link": "https://arxiv.org/abs/2507.02207v1",
          "size": "10249kb",
          "version": "v1"
        }
      ],
      "title": "Public perspectives on the design of fusion energy facilities",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02207",
        "PDF": "https://arxiv.org/pdf/2507.02207"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The participatory design method for fusion energy facilities involves engaging communities creatively, especially in integrating design values. Creativity appears as a supporting theme but is not the primary focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02320",
      "abstract": "Electroencephalography (EEG) is one of the most common signals used to capture the electrical activity of the brain, and the decoding of EEG, to acquire the user intents, has been at the forefront of brain-computer/machine interfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods with machine learning, the advent of deep learning approaches have gradually revolutionized the field by providing an end-to-end long-cascaded architecture, which can learn more discriminative features automatically. Among these, Transformer is renowned for its strong handling capability of sequential data by the attention mechanism, and the application of Transformers in various EEG processing tasks is increasingly prevalent. This article delves into a relevant survey, summarizing the latest application of Transformer models in EEG decoding since it appeared. The evolution of the model architecture is followed to sort and organize the related advances, in which we first elucidate the fundamentals of the Transformer that benefits EEG decoding and its direct application. Then, the common hybrid architectures by integrating basic Transformer with other deep learning techniques (convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial networks, diffusion models, etc.) is overviewed in detail. The research advances of applying the modified intrinsic structures of customized Transformer have also been introduced. Finally, the current challenges and future development prospects in this rapidly evolving field are discussed. This paper aims to help readers gain a clear understanding of the current state of Transformer applications in EEG decoding and to provide valuable insights for future research endeavors.",
      "authors": [
        "Haodong Zhang",
        "Hongqi Li"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T05:12:06+00:00",
          "link": "https://arxiv.org/abs/2507.02320v1",
          "size": "2565kb",
          "version": "v1"
        }
      ],
      "title": "Transformer-based EEG Decoding: A Survey",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02320",
        "HTML": "https://arxiv.org/html/2507.02320v1",
        "PDF": "https://arxiv.org/pdf/2507.02320"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This survey on EEG signal processing using Transformers does not directly relate to creativity, instead, it revolves around EEG decoding and brain-computer interfaces."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02400",
      "abstract": "In the future, mobility will be strongly shaped by the increasing use of digitalization. Not only will individual road users be highly interconnected, but also the road and associated infrastructure. At that point, a Digital Twin becomes particularly appealing because, unlike a basic simulation, it offers a continuous, bilateral connection linking the real and virtual environments. This paper describes the digital reconstruction used to develop the Digital Twin of the Test Area Autonomous Driving-Baden-W\\\"urttemberg (TAF-BW), Germany. The TAF-BW offers a variety of different road sections, from high-traffic urban intersections and tunnels to multilane motorways. The test area is equipped with a comprehensive Vehicle-to-Everything (V2X) communication infrastructure and multiple intelligent intersections equipped with camera sensors to facilitate real-time traffic flow monitoring. The generation of authentic data as input for the Digital Twin was achieved by extracting object lists at the intersections. This process was facilitated by the combined utilization of camera images from the intelligent infrastructure and LiDAR sensors mounted on a test vehicle. Using a unified interface, recordings from real-world detections of traffic participants can be resimulated. Additionally, the simulation framework's design and the reconstruction process is discussed. The resulting framework is made publicly available for download and utilization at: https://digit4taf-bw.fzi.de The demonstration uses two case studies to illustrate the application of the digital twin and its interfaces: the analysis of traffic signal systems to optimize traffic flow and the simulation of security-related scenarios in the communications sector.",
      "authors": [
        "Maximilian Zipfl",
        "Pascal Zwick",
        "Patrick Schulz",
        "Marc Rene Zofka",
        "Albert Schotschneider",
        "Helen Gremmelmaier",
        "Nikolai Polley",
        "Ferdinand M\\\"utsch",
        "Kevin Simon",
        "Fabian Gottselig",
        "Michael Frey",
        "Sergio Marschall",
        "Akim Stark",
        "Maximilian M\\\"uller",
        "Marek Wehmer",
        "Mihai Kocsis",
        "Dominic Waldenmayer",
        "Florian Schnepf",
        "Erik Heinrich",
        "Sabrina Pletz",
        "Matthias K\\\"olle",
        "Karin Langbein-Euchner",
        "Alexander Viehl",
        "Raoul Z\\\"ollner",
        "and J. Marius Z\\\"ollner"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T07:53:26+00:00",
          "link": "https://arxiv.org/abs/2507.02400v1",
          "size": "10097kb",
          "version": "v1"
        }
      ],
      "title": "DigiT4TAF -- Bridging Physical and Digital Worlds for Future Transportation Systems",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02400",
        "HTML": "https://arxiv.org/html/2507.02400v1",
        "PDF": "https://arxiv.org/pdf/2507.02400"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on digital twins for transportation systems, primarily discussing simulation and infrastructure. Creativity is not a topic of this research."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02438",
      "abstract": "Shared control combines human intention with autonomous decision-making, from low-level safety overrides to high-level task guidance, enabling systems that adapt to users while ensuring safety and performance. This enhances task effectiveness and user experience across domains such as assistive robotics, teleoperation, and autonomous driving. However, existing shared control methods, based on e.g. Model Predictive Control, Control Barrier Functions, or learning-based control, struggle with feasibility, scalability, or safety guarantees, particularly since the user input is unpredictable.\n  To address these challenges, we propose an assistive controller framework based on Constrained Optimal Control Problem that incorporates an offline-computed Control Invariant Set, enabling online computation of control actions that ensure feasibility, strict constraint satisfaction, and minimal override of user intent. Moreover, the framework can accommodate structured class of non-convex constraints, which are common in real-world scenarios. We validate the approach through a large-scale user study with 66 participants--one of the most extensive in shared control research--using a computer game environment to assess task load, trust, and perceived control, in addition to performance. The results show consistent improvements across all these aspects without compromising safety and user intent.",
      "authors": [
        "Shivam Chaubey",
        "Francesco Verdoja",
        "Shankar Deka",
        "Ville Kyrki"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Human-Computer Interaction (cs.HC)",
        "Systems and Control (cs.SY)",
        "Systems and Control (eess.SY)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T08:52:05+00:00",
          "link": "https://arxiv.org/abs/2507.02438v1",
          "size": "208kb",
          "version": "v1"
        }
      ],
      "title": "MISC: Minimal Intervention Shared Control with Guaranteed Safety under Non-Convex Constraints",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02438",
        "HTML": "https://arxiv.org/html/2507.02438v1",
        "PDF": "https://arxiv.org/pdf/2507.02438"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The research is centered on shared control in assistive robotics and autonomous systems, aimed at enhancing safety and user experience. There is no direct connection to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02510",
      "abstract": "Cross-subject motor imagery (CS-MI) classification in brain-computer interfaces (BCIs) is a challenging task due to the significant variability in Electroencephalography (EEG) patterns across different individuals. This variability often results in lower classification accuracy compared to subject-specific models, presenting a major barrier to developing calibration-free BCIs suitable for real-world applications. In this paper, we introduce a novel approach that significantly enhances cross-subject MI classification performance through optimized preprocessing and deep learning techniques. Our approach involves direct classification of Short-Time Fourier Transform (STFT)-transformed EEG data, optimized STFT parameters, and a balanced batching strategy during training of a Convolutional Neural Network (CNN). This approach is uniquely validated across four different datasets, including three widely-used benchmark datasets leading to substantial improvements in cross-subject classification, achieving 67.60% on the BCI Competition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on Dataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we systematically investigate the classification performance using MI windows ranging from the full 4-second window to 1-second windows. These results establish a new benchmark for generalizable, calibration-free MI classification in addition to contributing a robust open-access dataset to advance research in this domain.",
      "authors": [
        "Ahmed G. Habashi",
        "Ahmed M. Azab",
        "Seif Eldawlatly",
        "and Gamal M. Aly"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Human-Computer Interaction (cs.HC)",
        "Neural and Evolutionary Computing (cs.NE)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T10:17:39+00:00",
          "link": "https://arxiv.org/abs/2507.02510v1",
          "size": "1144kb",
          "version": "v1"
        }
      ],
      "title": "TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02510",
        "PDF": "https://arxiv.org/pdf/2507.02510"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper addresses classification accuracy in brain-computer interfaces using deep learning techniques. It does not discuss creativity in any form."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02578",
      "abstract": "Adaptive Cyber-Physical Systems (CPS) are systems that integrate both physical and computational capabilities, which can adjust in response to changing parameters. Furthermore, they increasingly incorporate human-machine collaboration, allowing them to benefit from the individual strengths of humans and machines. Human-Machine Teaming (HMT) represents the most advanced paradigm of human-machine collaboration, envisioning seamless teamwork between humans and machines. However, achieving effective and seamless HMT in adaptive CPS is challenging. While adaptive CPS already benefit from feedback loops such as MAPE-K, there is still a gap in integrating humans into these feedback loops due to different operational cadences of humans and machines. Further, HMT requires constant monitoring of human operators, collecting potentially sensitive information about their actions and behavior. Respecting the privacy and human values of the actors of the CPS is crucial for the success of human-machine teams. This research addresses these challenges by: (1) developing novel methods and processes for integrating HMT into adaptive CPS, focusing on human-machine interaction principles and their incorporation into adaptive feedback loops found in CPS, and (2) creating frameworks for integrating, verifying, and validating ethics and human values throughout the system lifecycle, starting from requirements engineering.",
      "authors": [
        "Zoe Pfister"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Software Engineering (cs.SE)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T12:34:52+00:00",
          "link": "https://arxiv.org/abs/2507.02578v1",
          "size": "273kb",
          "version": "v1"
        }
      ],
      "title": "Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02578",
        "HTML": "https://arxiv.org/html/2507.02578v1",
        "PDF": "https://arxiv.org/pdf/2507.02578"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on human-machine collaboration and ethics in adaptive systems without a specific focus on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.02593",
      "abstract": "Access to high-quality labeled data remains a limiting factor in applied supervised learning. While label variation (LV), i.e., differing labels for the same instance, is common, especially in natural language processing, annotation frameworks often still rest on the assumption of a single ground truth. This overlooks human label variation (HLV), the occurrence of plausible differences in annotations, as an informative signal. Similarly, active learning (AL), a popular approach to optimizing the use of limited annotation budgets in training ML models, often relies on at least one of several simplifying assumptions, which rarely hold in practice when acknowledging HLV. In this paper, we examine foundational assumptions about truth and label nature, highlighting the need to decompose observed LV into signal (e.g., HLV) and noise (e.g., annotation error). We survey how the AL and (H)LV communities have addressed -- or neglected -- these distinctions and propose a conceptual framework for incorporating HLV throughout the AL loop, including instance selection, annotator choice, and label representation. We further discuss the integration of large language models (LLM) as annotators. Our work aims to lay a conceptual foundation for HLV-aware active learning, better reflecting the complexities of real-world annotation.",
      "authors": [
        "Cornelia Gruber and Helen Alber and Bernd Bischl and G\\\"oran Kauermann and Barbara Plank and Matthias A{\\ss}enmacher"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)",
        "Machine Learning (stat.ML)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-03T12:59:28+00:00",
          "link": "https://arxiv.org/abs/2507.02593v1",
          "size": "131kb",
          "version": "v1"
        }
      ],
      "title": "Revisiting Active Learning under (Human) Label Variation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.02593",
        "HTML": "https://arxiv.org/html/2507.02593v1",
        "PDF": "https://arxiv.org/pdf/2507.02593"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses active learning and label variation in machine learning, with no focus on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2407.16206",
      "abstract": "Haptic sciences and technologies benefit greatly from comprehensive datasets that capture tactile stimuli under controlled, systematic conditions. However, existing haptic databases collect data through uncontrolled exploration, which hinders the systematic analysis of how motion parameters (e.g., motion direction and velocity) influence tactile perception. This paper introduces Cluster Haptic Texture Database, a multimodal dataset recorded using a 3-axis machine with an artificial finger to precisely control sliding velocity and direction. The dataset encompasses 118 textured surfaces across 9 material categories, with recordings at 5 velocity levels (20-60 mm/s) and 8 directions. Each surface was tested under 160 conditions, yielding 18,880 synchronized recordings of audio, acceleration, force, position, and visual data. Validation using convolutional neural networks demonstrates classification accuracies of 96% for texture recognition, 88.76% for velocity estimation, and 78.79% for direction estimation, confirming the dataset's utility for machine learning applications. This resource enables research in haptic rendering, texture recognition algorithms, and human tactile perception mechanisms, supporting the development of realistic haptic interfaces for virtual reality systems and robotic applications.",
      "authors": [
        "Michikuni Eguchi",
        "Tomohiro Hayase",
        "Yuichi Hiroi",
        "Takefumi Hiraki"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-07-23T06:18:10+00:00",
          "link": "https://arxiv.org/abs/2407.16206v1",
          "size": "5360kb",
          "version": "v1"
        },
        {
          "date": "2025-07-03T04:46:19+00:00",
          "link": "https://arxiv.org/abs/2407.16206v2",
          "size": "10562kb",
          "version": "v2"
        }
      ],
      "title": "Cluster Haptic Texture Database: Haptic Texture Database with Varied Velocity-Direction Sliding Contacts",
      "links": {
        "Abstract": "https://arxiv.org/abs/2407.16206",
        "HTML": "https://arxiv.org/html/2407.16206v2",
        "PDF": "https://arxiv.org/pdf/2407.16206"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The research is about haptic texture databases and machine learning for haptics, with no connection to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2410.08723",
      "abstract": "Natural language generation (NLG) models have emerged as a focal point of research within natural language processing (NLP), exhibiting remarkable performance in tasks such as text composition and dialogue generation. However, their intricate architectures and extensive model parameters pose significant challenges to interpretability, limiting their applicability in high-stakes decision-making scenarios. To address this issue, human-computer interaction (HCI) and visualization techniques offer promising avenues to enhance the transparency and usability of NLG models by making their decision-making processes more interpretable. In this paper, we provide a comprehensive investigation into the roles, limitations, and impact of HCI and visualization in facilitating human understanding and control over NLG systems. We introduce a taxonomy of interaction methods and visualization techniques, categorizing three major research domains and their corresponding six key tasks in the application of NLG models. Finally, we summarize the shortcomings in the existing work and investigate the key challenges and emerging opportunities in the era of large language models (LLMs).",
      "authors": [
        "Yunchao Wang",
        "Guodao Sun",
        "Zihang Fu",
        "Ronghua Liang"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-10-11T11:23:26+00:00",
          "link": "https://arxiv.org/abs/2410.08723v1",
          "size": "6917kb",
          "version": "v1"
        },
        {
          "date": "2025-03-28T04:38:44+00:00",
          "link": "https://arxiv.org/abs/2410.08723v2",
          "size": "4107kb",
          "version": "v2"
        },
        {
          "date": "2025-07-03T05:25:23+00:00",
          "link": "https://arxiv.org/abs/2410.08723v3",
          "size": "3855kb",
          "version": "v3"
        }
      ],
      "title": "Human-Computer Interaction and Visualization in Natural Language Generation Models: Applications, Challenges, and Opportunities",
      "links": {
        "Abstract": "https://arxiv.org/abs/2410.08723",
        "HTML": "https://arxiv.org/html/2410.08723v3",
        "PDF": "https://arxiv.org/pdf/2410.08723"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses HCI and visualization in NLG models, which could relate to creativity via text and dialogue generation, although creativity is not the central theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01548",
      "abstract": "This paper explores how older adults, particularly aging migrants in urban China, can engage AI-assisted co-creation to express personal narratives that are often fragmented, underrepresented, or difficult to verbalize. Through a pilot workshop combining oral storytelling and the symbolic reconstruction of Hanzi, participants shared memories of migration and recreated new character forms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM), together with physical materials. Supported by human facilitation and a soft AI presence, participants transformed lived experience into visual and tactile expressions without requiring digital literacy. This approach offers new perspectives on human-AI collaboration and aging by repositioning AI not as a content producer but as a supportive mechanism, and by supporting narrative agency within sociotechnical systems.",
      "authors": [
        "Wen Zhan",
        "Ziqun Hua",
        "Peiyue Lin",
        "Yunfei Chen"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T10:00:12+00:00",
          "link": "https://arxiv.org/abs/2507.01548v1",
          "size": "1052kb",
          "version": "v1"
        },
        {
          "date": "2025-07-03T08:45:46+00:00",
          "link": "https://arxiv.org/abs/2507.01548v2",
          "size": "1058kb",
          "version": "v2"
        }
      ],
      "title": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01548",
        "PDF": "https://arxiv.org/pdf/2507.01548"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper explicitly discusses AI-assisted co-creation and narrative expression among elderly migrants, focusing on creativity in storytelling and symbolic reconstruction, making creativity a primary focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2407.06902",
      "abstract": "One of the primary catalysts fueling advances in artificial intelligence (AI) and machine learning (ML) is the availability of massive, curated datasets. A commonly used technique to curate such massive datasets is crowdsourcing, where data are dispatched to multiple annotators. The annotator-produced labels are then fused to serve downstream learning and inference tasks. This annotation process often creates noisy labels due to various reasons, such as the limited expertise, or unreliability of annotators, among others. Therefore, a core objective in crowdsourcing is to develop methods that effectively mitigate the negative impact of such label noise on learning tasks. This feature article introduces advances in learning from noisy crowdsourced labels. The focus is on key crowdsourcing models and their methodological treatments, from classical statistical models to recent deep learning-based approaches, emphasizing analytical insights and algorithmic developments. In particular, this article reviews the connections between signal processing (SP) theory and methods, such as identifiability of tensor and nonnegative matrix factorization, and novel, principled solutions of longstanding challenges in crowdsourcing -- showing how SP perspectives drive the advancements of this field. Furthermore, this article touches upon emerging topics that are critical for developing cutting-edge AI/ML systems, such as crowdsourcing in reinforcement learning with human feedback (RLHF) and direct preference optimization (DPO) that are key techniques for fine-tuning large language models (LLMs).",
      "authors": [
        "Shahana Ibrahim",
        "Panagiotis A. Traganitis",
        "Xiao Fu",
        "Georgios B. Giannakis"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Signal Processing (eess.SP)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2024-07-09T14:34:40+00:00",
          "link": "https://arxiv.org/abs/2407.06902v1",
          "size": "3418kb",
          "version": "v1"
        },
        {
          "date": "2025-07-02T19:06:48+00:00",
          "link": "https://arxiv.org/abs/2407.06902v2",
          "size": "829kb",
          "version": "v2"
        }
      ],
      "title": "Learning From Crowdsourced Noisy Labels: A Signal Processing Perspective",
      "links": {
        "Abstract": "https://arxiv.org/abs/2407.06902",
        "HTML": "https://arxiv.org/html/2407.06902v2",
        "PDF": "https://arxiv.org/pdf/2407.06902"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on learning from noisy crowdsourced labels and does not discuss creativity-related topics."
      },
      "tasks": [],
      "source": "arXiv"
    },
    {
      "id": "2412.18716",
      "abstract": "Mobile Money (MoMo), a technology that allows users to complete financial transactions using a mobile phone without requiring a bank account, is a common method for processing financial transactions in Africa and other developing regions. Users can deposit and withdraw money with the help of human agents. During deposit and withdraw operations, know-your-customer (KYC) processes require agents to access and verify customer information such as name and ID number, which can introduce privacy and security risks. In this work, we design alternative protocols for MoMo deposits/withdrawals that protect users' privacy while enabling KYC checks by redirecting the flow of sensitive information from the agent to the MoMo provider. We evaluate the usability and efficiency of our proposed protocols in a role-play and semi-structured interview study with 32 users and 15 agents in Kenya. We find that users and agents prefer the new protocols, due in part to convenient and efficient verification using biometrics as well as better data privacy and access control. However, our study also surfaced challenges that need to be addressed before these protocols can be deployed.",
      "authors": [
        "Karen Sowon",
        "Collins W. Munyendo",
        "Lily Klucinec",
        "Eunice Maingi",
        "Gerald Suleh",
        "Lorrie Faith Cranor",
        "Giulia Fanti",
        "Conrad Tucker",
        "and Assane Gueye"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-12-25T00:27:13+00:00",
          "link": "https://arxiv.org/abs/2412.18716v1",
          "size": "6973kb",
          "version": "v1"
        },
        {
          "date": "2025-07-02T21:57:06+00:00",
          "link": "https://arxiv.org/abs/2412.18716v2",
          "size": "6146kb",
          "version": "v2"
        }
      ],
      "title": "Design and Evaluation of Privacy-Preserving Protocols for Agent-Facilitated Mobile Money Services in Kenya",
      "links": {
        "Abstract": "https://arxiv.org/abs/2412.18716",
        "PDF": "https://arxiv.org/pdf/2412.18716"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper is focused on privacy-preserving protocols for mobile money services, with no discussion or mention of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2503.08061",
      "abstract": "Realistic Hand manipulation is a key component of immersive virtual reality (VR), yet existing methods often rely on kinematic approach or motion-capture datasets that omit crucial physical attributes such as contact forces and finger torques. Consequently, these approaches prioritize tight, one-size-fits-all grips rather than reflecting users' intended force levels. We present ForceGrip, a deep learning agent that synthesizes realistic hand manipulation motions, faithfully reflecting the user's grip force intention. Instead of mimicking predefined motion datasets, ForceGrip uses generated training scenarios-randomizing object shapes, wrist movements, and trigger input flows-to challenge the agent with a broad spectrum of physical interactions. To effectively learn from these complex tasks, we employ a three-phase curriculum learning framework comprising Finger Positioning, Intention Adaptation, and Dynamic Stabilization. This progressive strategy ensures stable hand-object contact, adaptive force control based on user inputs, and robust handling under dynamic conditions. Additionally, a proximity reward function enhances natural finger motions and accelerates training convergence. Quantitative and qualitative evaluations reveal ForceGrip's superior force controllability and plausibility compared to state-of-the-art methods. Demo videos are available as supplementary material and the code is provided at https://han-dongheun.github.io/ForceGrip.",
      "authors": [
        "DongHeun Han",
        "Byungmin Kim",
        "RoUn Lee",
        "KyeongMin Kim",
        "Hyoseok Hwang",
        "HyeongYeop Kang"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Graphics (cs.GR)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-03-11T05:39:07+00:00",
          "link": "https://arxiv.org/abs/2503.08061v1",
          "size": "3156kb",
          "version": "v1"
        },
        {
          "date": "2025-03-13T06:35:25+00:00",
          "link": "https://arxiv.org/abs/2503.08061v2",
          "size": "3156kb",
          "version": "v2"
        },
        {
          "date": "2025-04-30T14:03:25+00:00",
          "link": "https://arxiv.org/abs/2503.08061v3",
          "size": "4958kb",
          "version": "v3"
        },
        {
          "date": "2025-07-03T08:24:20+00:00",
          "link": "https://arxiv.org/abs/2503.08061v4",
          "size": "5003kb",
          "version": "v4"
        }
      ],
      "title": "ForceGrip: Reference-Free Curriculum Learning for Realistic Grip Force Control in VR Hand Manipulation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.08061",
        "HTML": "https://arxiv.org/html/2503.08061v4",
        "PDF": "https://arxiv.org/pdf/2503.08061"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses synthesizing realistic hand manipulation motions in VR, which may involve creative tasks in terms of developing new interaction techniques. However, creativity is not the primary focus."
      },
      "tasks": [],
      "source": "arXiv"
    },
    {
      "id": "2503.17046",
      "abstract": "Automatic robotic facial expression generation is crucial for human-robot interaction, as handcrafted methods based on fixed joint configurations often yield rigid and unnatural behaviors. Although recent automated techniques reduce the need for manual tuning, they tend to fall short by not adequately bridging the gap between human preferences and model predictions-resulting in a deficiency of nuanced and realistic expressions due to limited degrees of freedom and insufficient perceptual integration. In this work, we propose a novel learning-to-rank framework that leverages human feedback to address this discrepancy and enhanced the expressiveness of robotic faces. Specifically, we conduct pairwise comparison annotations to collect human preference data and develop the Human Affective Pairwise Impressions (HAPI) model, a Siamese RankNet-based approach that refines expression evaluation. Results obtained via Bayesian Optimization and online expression survey on a 35-DOF android platform demonstrate that our approach produces significantly more realistic and socially resonant expressions of Anger, Happiness, and Surprise than those generated by baseline and expert-designed methods. This confirms that our framework effectively bridges the gap between human preferences and model predictions while robustly aligning robotic expression generation with human affective responses.",
      "authors": [
        "Dongsheng Yang",
        "Qianying Liu",
        "Wataru Sato",
        "Takashi Minato",
        "Chaoran Liu",
        "Shin'ya Nishida"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-03-21T11:04:01+00:00",
          "link": "https://arxiv.org/abs/2503.17046v1",
          "size": "17709kb",
          "version": "v1"
        },
        {
          "date": "2025-07-03T14:55:55+00:00",
          "link": "https://arxiv.org/abs/2503.17046v2",
          "size": "17452kb",
          "version": "v2"
        }
      ],
      "title": "HAPI: A Model for Learning Robot Facial Expressions from Human Preferences",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.17046",
        "HTML": "https://arxiv.org/html/2503.17046v2",
        "PDF": "https://arxiv.org/pdf/2503.17046"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper explores the generation of robotic facial expressions influenced by human preferences, potentially implicating creativity in improving expressiveness and design but it is not the main objective."
      },
      "tasks": [
        "Bayesian Optimization",
        "Facial expression generation",
        "Learning-To-Rank"
      ],
      "repo_urls": [
        "https://github.com/KUCognitiveInformaticsLab/PairwiseExpressionAnnotator"
      ],
      "source": "arXiv"
    },
    {
      "id": "2405.17728",
      "abstract": "Workshop courses designed to foster creativity are gaining popularity. However, even experienced faculty teams find it challenging to realize a holistic evaluation that accommodates diverse perspectives. Adequate deliberation is essential to integrate varied assessments, but faculty often lack the time for such exchanges. Deriving an average score without discussion undermines the purpose of a holistic evaluation. Therefore, this paper explores the use of a Large Language Model (LLM) as a facilitator to integrate diverse faculty assessments. Scenario-based experiments were conducted to determine if the LLM could integrate diverse evaluations and explain the underlying pedagogical theories to faculty. The results were noteworthy, showing that the LLM can effectively facilitate faculty discussions. Additionally, the LLM demonstrated the capability to create evaluation criteria by generalizing a single scenario-based experiment, leveraging its already acquired pedagogical domain knowledge.",
      "authors": [
        "Toru Ishida",
        "Tongxi Liu",
        "Hailong Wang and William K. Cheunga"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-05-28T01:07:06+00:00",
          "link": "https://arxiv.org/abs/2405.17728v1",
          "size": "634kb",
          "version": "v1"
        },
        {
          "date": "2024-08-12T00:54:28+00:00",
          "link": "https://arxiv.org/abs/2405.17728v2",
          "size": "638kb",
          "version": "v2"
        }
      ],
      "title": "Facilitating Holistic Evaluations with LLMs: Insights from Scenario-Based Experiments",
      "links": {
        "Abstract": "https://arxiv.org/abs/2405.17728",
        "PDF": "https://arxiv.org/pdf/2405.17728"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper involves creativity in the context of evaluating workshop courses designed to foster creativity using LLMs for faculty evaluations, but creativity is not the primary focus."
      },
      "tasks": [
        "Language Modeling",
        "Language Modelling",
        "Large Language Model"
      ],
      "source": "arXiv"
    },
    {
      "id": "2411.11382",
      "abstract": "Haptic affection plays a crucial role in user experience, particularly in the automotive industry where the tactile quality of components can influence customer satisfaction. This study aims to accurately predict the affective property of a car door by only watching the force or torque profile of it when opening. To this end, a deep learning model is designed to capture the underlying relationships between force profiles and user-defined adjective ratings, providing insights into the door-opening experience. The dataset employed in this research includes force profiles and user adjective ratings collected from six distinct car models, reflecting a diverse set of door-opening characteristics and tactile feedback. The model's performance is assessed using Leave-One-Out Cross-Validation, a method that measures its generalization capability on unseen data. The results demonstrate that the proposed model achieves a high level of prediction accuracy, indicating its potential in various applications related to haptic affection and design optimization in the automotive industry.",
      "authors": [
        "Mudassir Ibrahim Awan",
        "Ahsan Raza",
        "Waseem Hassan",
        "Ki-Uk Kyung",
        "and Seokhee Jeon"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-11-18T08:58:53+00:00",
          "link": "https://arxiv.org/abs/2411.11382v1",
          "size": "4686kb",
          "version": "v1"
        },
        {
          "date": "2025-05-02T05:49:56+00:00",
          "link": "https://arxiv.org/abs/2411.11382v2",
          "size": "4212kb",
          "version": "v2"
        },
        {
          "date": "2025-05-22T08:11:09+00:00",
          "link": "https://arxiv.org/abs/2411.11382v3",
          "size": "4440kb",
          "version": "v3"
        }
      ],
      "title": "Quantifying Haptic Affection of Car Door through Data-Driven Analysis of Force Profile",
      "links": {
        "Abstract": "https://arxiv.org/abs/2411.11382",
        "HTML": "https://arxiv.org/html/2411.11382",
        "PDF": "https://arxiv.org/pdf/2411.11382"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The study focuses on the affective properties of a car door through haptic data analysis, unrelated to creativity."
      },
      "source": "arXiv"
    }
  ],
  "subjects": [
    "Computer Vision and Pattern Recognition (cs.CV)",
    "Physics and Society (physics.soc-ph)",
    "Computation and Language (cs.CL)",
    "Systems and Control (eess.SY)",
    "Cryptography and Security (cs.CR)",
    "Robotics (cs.RO)",
    "Artificial Intelligence (cs.AI)",
    "Plasma Physics (physics.plasm-ph)",
    "Systems and Control (cs.SY)",
    "Emerging Technologies (cs.ET)",
    "Signal Processing (eess.SP)",
    "Machine Learning (stat.ML)",
    "Physics Education (physics.ed-ph)",
    "Computers and Society (cs.CY)",
    "Neural and Evolutionary Computing (cs.NE)",
    "General Finance (q-fin.GN)",
    "Software Engineering (cs.SE)",
    "Human-Computer Interaction (cs.HC)",
    "Graphics (cs.GR)",
    "Machine Learning (cs.LG)"
  ],
  "prompt": {
    "creativity": "\nYou are an expert in information retrieval. I will provide you with a list of research papers from arXiv, specifically in the *cs.HC* (Human-Computer Interaction) category.\n\nYour task is to analyze each paper and determine its relevance to the topic of **Creativity**.\n\n\n### Classify each paper into one of the following relevance levels\n\n- `core` \u2014 Creativity is a **primary focus**\n  - The paper directly studies or simulates creativity, with a clear focus on creativity.\n  - Includes creative tasks, co-creative systems, or creativity evaluation metrics.\n  - The title and abstract explicitly mention creativity, and the research questions are directly related to creativity.\n- `partial` \u2014 Creativity is a **secondary theme**\n  - Part of the paper relates to creativity; it is treated as an analytical dimension or design goal but not the main objective (e.g., user creativity, design support).\n  - Creativity may appear in discussions, experiments, or auxiliary applications.\n  - Creativity is presented as a supporting topic (e.g., evaluation criteria, user feedback).\n- `irrelevant` \u2014 **No clear connection** to creativity\n  - The paper does not address creativity as a topic.\n  - Focuses on unrelated technical content (e.g., compression, security, optimization).\n  - If creativity is mentioned, it is only superficial and lacks substantive content.\n\n\n### Return your results in the following JSON format\n\n```json\n{\n  \"result\": [\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    },\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    }\n  ]\n}\n```\n\nBe concise but specific in your reasoning, referencing key terms or sections when applicable.\n"
  },
  "description": "Data source: https://arxiv.org/list/cs.HC/new",
  "level_tatistics": {
    "partial": 12,
    "irrelevant": 24,
    "core": 1
  },
  "arxiv_update_date": "2025-07-04",
  "updated_at": "2025-07-05 23:55:34"
}