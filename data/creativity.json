{
  "data": [
    {
      "id": "2507.11599",
      "abstract": "Neuroaesthetics is an interdisciplinary field that brings together neuroscience, psychology, and the arts to explore how the human brain perceives and responds to visual beauty. This paper examines the neural mechanisms behind aesthetic experiences, aiming to explain why certain designs or artworks feel emotionally or cognitively \"right.\" By analyzing the interaction between perception, emotion, and cognition, neuroaesthetics reveals how beauty is constructed in the brain and how this understanding can inform fields such as graphic and interface design. This paper offers a clear and accessible overview of core neuroaesthetic principles, making the subject approachable to a wide audience. The findings suggest that impactful design is more than surface-level appeal: well-crafted visual experiences can engage, support, and connect people in meaningful ways.",
      "authors": [
        "Harish Vijayakumar"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-15T17:59:28+00:00",
          "link": "https://arxiv.org/abs/2507.11599v1",
          "size": "270kb",
          "version": "v1"
        }
      ],
      "title": "Neuroaesthetics and the Science of Visual Experience",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.11599",
        "PDF": "https://arxiv.org/pdf/2507.11599"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper discusses neuroaesthetics, which directly involves understanding how the brain perceives visual beauty and informs design. These insights directly contribute to creativity in visual experiences."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.11628",
      "abstract": "An interactive vignette is a popular and immersive visual storytelling approach that invites viewers to role-play a character and influences the narrative in an interactive environment. However, it has not been widely used by everyday storytellers yet due to authoring complexity, which conflicts with the immediacy of everyday storytelling. We introduce DiaryPlay, an AI-assisted authoring system for interactive vignette creation in everyday storytelling. It takes a natural language story as input and extracts the three core elements of an interactive vignette (environment, characters, and events), enabling authors to focus on refining these elements instead of constructing them from scratch. Then, it automatically transforms the single-branch story input into a branch-and-bottleneck structure using an LLM-powered narrative planner, which enables flexible viewer interactions while freeing the author from multi-branching. A technical evaluation (N=16) shows that DiaryPlay-generated character activities are on par with human-authored ones regarding believability. A user study (N=16) shows that DiaryPlay effectively supports authors in creating interactive vignette elements, maintains authorial intent while reacting to viewer interactions, and provides engaging viewing experiences.",
      "authors": [
        "Jiangnan Xu",
        "Haeseul Cha",
        "Gosu Choi",
        "Gyu-cheol Lee",
        "Yeo-Jin Yoon",
        "Zucheul Lee",
        "Konstantinos Papangelis",
        "Dae Hyun Kim",
        "and Juho Kim"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-15T18:05:43+00:00",
          "link": "https://arxiv.org/abs/2507.11628v1",
          "size": "3706kb",
          "version": "v1"
        }
      ],
      "title": "DiaryPlay: AI-Assisted Authoring of Interactive Vignettes for Everyday Storytelling",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.11628",
        "PDF": "https://arxiv.org/pdf/2507.11628"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper introduces an AI-assisted tool for creating interactive vignettes, directly involving creative processes in storytelling and narrative creation, making creativity a primary focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.11677",
      "abstract": "Communicating climate change remains challenging, as climate reports, though rich in data and visualizations, often feel too abstract or technical for the public. Although personalization can enhance communication, most tools still lack the narrative and visualization tailoring needed to connect with individual experiences. We present CLAImate, an AI-enabled prototype that personalizes conversation narratives and localizes visualizations based on users' climate knowledge and geographic location. We evaluated CLAImate through internal verification of factual correctness, a formative study with experts, and a pilot with UK residents. CLAImate achieved 66% SNLI accuracy and 70% FACTSCORE. Visualization experts appreciated its clarity and personalization, and seven out of ten UK participants reported better understanding and local relevance of climate risks with CLAImate. We also discuss design challenges in personalization, accuracy, and scalability, and outline future directions for integrating visualizations in personalized conversational interfaces.",
      "authors": [
        "Mashrur Rashik",
        "Jean-Daniel Fekete",
        "Narges Mahyar"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-15T19:32:04+00:00",
          "link": "https://arxiv.org/abs/2507.11677v1",
          "size": "816kb",
          "version": "v1"
        }
      ],
      "title": "CLAImate: AI-Enabled Climate Change Communication through Personalized and Localized Narrative Visualizations",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.11677",
        "HTML": "https://arxiv.org/html/2507.11677v1",
        "PDF": "https://arxiv.org/pdf/2507.11677"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "While the main focus of the paper is on climate change communication through personalized visualizations, it mentions narrative visualizations which can involve creative aspects. However, creativity is treated as a secondary aspect in enhancing communication rather than being the central focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.11797",
      "abstract": "Understanding how teams coordinate, share work, and negotiate roles in immersive environments is critical for designing effective mixed-reality (MR) applications that support real-time collaboration. However, existing methods either rely on external cameras and offline annotation or focus narrowly on single modalities, limiting their validity and applicability. To address this, we present a novel group interaction sensing toolkit (GIST), a deployable system that passively captures multi-modal interaction data, such as speech, gaze, and spatial proximity from commodity MR headset's sensors and automatically derives both overall static interaction networks and dynamic moment-by-moment behavior patterns. We evaluate GIST with a human subject study with 48 participants across 12 four-person groups performing an open-ended image-sorting task in MR. Our analysis shows strong alignment between the identified behavior modes and shifts in interaction network structure, confirming that momentary changes in speech, gaze, and proximity data are observable through the sensor data.",
      "authors": [
        "Diana Romero",
        "Yasra Chandio",
        "Fatima Anwar",
        "Salma Elmalaki"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Emerging Technologies (cs.ET)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-15T23:21:28+00:00",
          "link": "https://arxiv.org/abs/2507.11797v1",
          "size": "923kb",
          "version": "v1"
        }
      ],
      "title": "GIST: Group Interaction Sensing Toolkit for Mixed Reality",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.11797",
        "HTML": "https://arxiv.org/html/2507.11797v1",
        "PDF": "https://arxiv.org/pdf/2507.11797"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on group interaction sensing in mixed reality environments for collaboration, without any significant connection to creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.11841",
      "abstract": "Affective visualization design is an emerging research direction focused on communicating and influencing emotion through visualization. However, as revealed by previous research, this area is highly interdisciplinary and involves theories and practices from diverse fields and disciplines, thus awaiting analysis from more fine-grained angles. To address this need, this work focuses on a pioneering and relatively mature sub-area, affective geovisualization design, to further the research in this direction and provide more domain-specific insights. Through an analysis of a curated corpus of affective geovisualization designs using the Person-Process-Place (PPP) model from geographic theory, we derived a design taxonomy that characterizes a variety of methods for eliciting and enhancing emotions through geographic visualization. We also identified four underlying high-level design paradigms of affective geovisualization design (e.g., computational, anthropomorphic) that guide distinct approaches to linking geographic information with human experience. By extending existing affective visualization design frameworks with geographic specificity, we provide additional design examples, domain-specific analyses, and insights to guide future research and practices in this underexplored yet highly innovative domain.",
      "authors": [
        "Xingyu Lan",
        "Yutong Yang",
        "Yifan Wang"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T02:15:04+00:00",
          "link": "https://arxiv.org/abs/2507.11841v1",
          "size": "7595kb",
          "version": "v1"
        }
      ],
      "title": "\"Mapping What I Feel\": Understanding Affective Geovisualization Design Through the Lens of People-Place Relationships",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.11841",
        "HTML": "https://arxiv.org/html/2507.11841v1",
        "PDF": "https://arxiv.org/pdf/2507.11841"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper focuses on affective geovisualization, a domain where design involves influencing emotions through visualization. Creativity is a secondary theme, as the designs enhance or elicit emotions using creative visualization methods."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.11848",
      "abstract": "Hybrid rice breeding crossbreeds different rice lines and cultivates the resulting hybrids in fields to select those with desirable agronomic traits, such as higher yields. Recently, genomic selection has emerged as an efficient way for hybrid rice breeding. It predicts the traits of hybrids based on their genes, which helps exclude many undesired hybrids, largely reducing the workload of field cultivation. However, due to the limited accuracy of genomic prediction models, breeders still need to combine their experience with the models to identify regulatory genes that control traits and select hybrids, which remains a time-consuming process. To ease this process, in this paper, we proposed a visual analysis method to facilitate interactive hybrid rice breeding. Regulatory gene identification and hybrid selection naturally ensemble a dual-analysis task. Therefore, we developed a parametric dual projection method with theoretical guarantees to facilitate interactive dual analysis. Based on this dual projection method, we further developed a gene visualization and a hybrid visualization to verify the identified regulatory genes and hybrids. The effectiveness of our method is demonstrated through the quantitative evaluation of the parametric dual projection method, identified regulatory genes and desired hybrids in the case study, and positive feedback from breeders.",
      "authors": [
        "Changjian Chen",
        "Pengcheng Wang",
        "Fei Lyu",
        "Zhuo Tang",
        "Li Yang",
        "Long Wang",
        "Yong Cai",
        "Feng Yu",
        "and Kenli Li"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Quantitative Methods (q-bio.QM)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T02:25:31+00:00",
          "link": "https://arxiv.org/abs/2507.11848v1",
          "size": "6129kb",
          "version": "v1"
        }
      ],
      "title": "Interactive Hybrid Rice Breeding with Parametric Dual Projection",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.11848",
        "HTML": "https://arxiv.org/html/2507.11848v1",
        "PDF": "https://arxiv.org/pdf/2507.11848"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper deals with genomic selection and visualization methods for rice breeding, with no clear focus on creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.11903",
      "abstract": "When designed deliberately, data visualizations can become powerful persuasive tools, influencing viewers' opinions, values, and actions. While researchers have begun studying this issue (e.g., to evaluate the effects of persuasive visualization), we argue that a fundamental mechanism of persuasion resides in rhetorical construction, a perspective inadequately addressed in current visualization research. To fill this gap, we present a focused analysis of octopus maps, a visual genre that has maintained persuasive power across centuries and achieved significant social impact. Employing rhetorical schema theory, we collected and analyzed 90 octopus maps spanning from the 19th century to contemporary times. We closely examined how octopus maps implement their persuasive intents and constructed a design space that reveals how visual metaphors are strategically constructed and what common rhetorical strategies are applied to components such as maps, octopus imagery, and text. Through the above analysis, we also uncover a set of interesting findings. For instance, contrary to the common perception that octopus maps are primarily a historical phenomenon, our research shows that they remain a lively design convention in today's digital age. Additionally, while most octopus maps stem from Western discourse that views the octopus as an evil symbol, some designs offer alternative interpretations, highlighting the dynamic nature of rhetoric across different sociocultural settings. Lastly, drawing from the lessons provided by octopus maps, we discuss the associated ethical concerns of persuasive visualization.",
      "authors": [
        "Daocheng Lin",
        "Yifan Wang",
        "Yutong Yang",
        "Xingyu Lan"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Multimedia (cs.MM)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T04:41:53+00:00",
          "link": "https://arxiv.org/abs/2507.11903v1",
          "size": "19061kb",
          "version": "v1"
        }
      ],
      "title": "Unveiling the Visual Rhetoric of Persuasive Cartography: A Case Study of the Design of Octopus Maps",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.11903",
        "HTML": "https://arxiv.org/html/2507.11903v1",
        "PDF": "https://arxiv.org/pdf/2507.11903"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "While the paper primarily focuses on persuasive cartography, it discusses visual rhetoric and metaphor, which can be related to creative design processes, thereby making creativity a secondary theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.11911",
      "abstract": "Electroencephalogram (EEG) decoding models for brain-computer interfaces (BCIs) struggle with cross-dataset learning and generalization due to channel layout inconsistencies, non-stationary signal distributions, and limited neurophysiological prior integration. To address these issues, we propose a plug-and-play Alignment-Based Frame-Patch Modeling (AFPM) framework, which has two main components: 1) Spatial Alignment, which selects task-relevant channels based on brain-region priors, aligns EEG distributions across domains, and remaps the selected channels to a unified layout; and, 2) Frame-Patch Encoding, which models multi-dataset signals into unified spatiotemporal patches for EEG decoding. Compared to 17 state-of-the-art approaches that need dataset-specific tuning, the proposed calibration-free AFPM achieves performance gains of up to 4.40% on motor imagery and 3.58% on event-related potential tasks. To our knowledge, this is the first calibration-free cross-dataset EEG decoding framework, substantially enhancing the practicalness of BCIs in real-world applications.",
      "authors": [
        "Xiaoqing Chen",
        "Siyang Li",
        "Dongrui Wu"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Information Retrieval (cs.IR)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T04:55:09+00:00",
          "link": "https://arxiv.org/abs/2507.11911v1",
          "size": "2653kb",
          "version": "v1"
        }
      ],
      "title": "AFPM: Alignment-based Frame Patch Modeling for Cross-Dataset EEG Decoding",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.11911",
        "HTML": "https://arxiv.org/html/2507.11911v1",
        "PDF": "https://arxiv.org/pdf/2507.11911"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The focus here is on EEG decoding for brain-computer interfaces, with no mention of creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.11960",
      "abstract": "Approaches to enhancing data quality (DQ) are classified into two main categories: data- and process-driven. However, prior research has predominantly utilized batch data preprocessing within the data-driven framework, which often proves insufficient for optimizing machine learning (ML) model performance and frequently leads to distortions in data characteristics. Existing studies have primarily focused on data preprocessing rather than genuine data quality improvement (DQI). In this paper, we introduce d-DQIVAR, a novel visual analytics system designed to facilitate DQI strategies aimed at improving ML model performance. Our system integrates visual analytics techniques that leverage both data-driven and process-driven approaches. Data-driven techniques tackle DQ issues such as imputation, outlier detection, deletion, format standardization, removal of duplicate records, and feature selection. Process-driven strategies encompass evaluating DQ and DQI procedures by considering DQ dimensions and ML model performance and applying the Kolmogorov-Smirnov test. We illustrate how our system empowers users to harness expert and domain knowledge effectively within a practical workflow through case studies, evaluations, and user studies.",
      "authors": [
        "Hyein Hong",
        "Sangbong Yoo",
        "SeokHwan Choi",
        "Jisue Kim",
        "Seongbum Seo",
        "Haneol Cho",
        "Chansoo Kim",
        "and Yun Jang"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T06:45:08+00:00",
          "link": "https://arxiv.org/abs/2507.11960v1",
          "size": "11470kb",
          "version": "v1"
        }
      ],
      "title": "d-DQIVAR: Data-centric Visual Analytics and Reasoning for Data Quality Improvement",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.11960",
        "HTML": "https://arxiv.org/html/2507.11960v1",
        "PDF": "https://arxiv.org/pdf/2507.11960"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on data quality improvement through visual analytics, primarily targeting machine learning model performance. There is no mention of creativity or creative processes in the content."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.11984",
      "abstract": "Selecting the appropriate dimensionality reduction (DR) technique and determining its optimal hyperparameter settings that maximize the accuracy of the output projections typically involves extensive trial and error, often resulting in unnecessary computational overhead. To address this challenge, we propose a dataset-adaptive approach to DR optimization guided by structural complexity metrics. These metrics quantify the intrinsic complexity of a dataset, predicting whether higher-dimensional spaces are necessary to represent it accurately. Since complex datasets are often inaccurately represented in two-dimensional projections, leveraging these metrics enables us to predict the maximum achievable accuracy of DR techniques for a given dataset, eliminating redundant trials in optimizing DR. We introduce the design and theoretical foundations of these structural complexity metrics. We quantitatively verify that our metrics effectively approximate the ground truth complexity of datasets and confirm their suitability for guiding dataset-adaptive DR workflow. Finally, we empirically show that our dataset-adaptive workflow significantly enhances the efficiency of DR optimization without compromising accuracy.",
      "authors": [
        "Hyeon Jeon",
        "Jeongin Park",
        "Soohyun Lee",
        "Dae Hyun Kim",
        "Sungbok Shin",
        "Jinwook Seo"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T07:32:08+00:00",
          "link": "https://arxiv.org/abs/2507.11984v1",
          "size": "1564kb",
          "version": "v1"
        }
      ],
      "title": "Dataset-Adaptive Dimensionality Reduction",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.11984",
        "HTML": "https://arxiv.org/html/2507.11984v1",
        "PDF": "https://arxiv.org/pdf/2507.11984"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses dimensionality reduction techniques and their optimization for data representation. It does not address creativity or any creative aspects."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.11999",
      "abstract": "Graph querying is the process of retrieving information from graph data using specialized languages (e.g., Cypher), often requiring programming expertise. Visual Graph Querying (VGQ) streamlines this process by enabling users to construct and execute queries via an interactive interface without resorting to complex coding. However, current VGQ tools only allow users to construct simple and specific query graphs, limiting users' ability to interactively express their query intent, especially for underspecified query intent. To address these limitations, we propose Envisage, an interactive visual graph querying system to enhance the expressiveness of VGQ in complex query scenarios by supporting intuitive graph structure construction and flexible parameterized rule specification. Specifically, Envisage comprises four stages: Query Expression allows users to interactively construct graph queries through intuitive operations; Query Verification enables the validation of constructed queries via rule verification and query instantiation; Progressive Query Execution can progressively execute queries to ensure meaningful querying results; and Result Analysis facilitates result exploration and interpretation. To evaluate Envisage, we conducted two case studies and in-depth user interviews with 14 graph analysts. The results demonstrate its effectiveness and usability in constructing, verifying, and executing complex graph queries.",
      "authors": [
        "Xiaolin Wen",
        "Qishuang Fu",
        "Shuangyue Han",
        "Yichen Guo",
        "Joseph K. Liu",
        "Yong Wang"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T07:54:37+00:00",
          "link": "https://arxiv.org/abs/2507.11999v1",
          "size": "5643kb",
          "version": "v1"
        }
      ],
      "title": "Envisage: Towards Expressive Visual Graph Querying",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.11999",
        "HTML": "https://arxiv.org/html/2507.11999v1",
        "PDF": "https://arxiv.org/pdf/2507.11999"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The focus of the paper is on enhancing visual graph querying capabilities, particularly in constructing, verifying, and executing complex queries. Creativity is not a theme or focus in this research."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12204",
      "abstract": "Adolescents' mobile technology use is often regulated through rigid control mechanisms that fail to account for their autonomy and natural usage patterns. Drawing on Taoist philosophy, particularly Wu Wei, Yin-Yang, and Zi Ran, this position paper proposes Tao-Technology, a self-organizing, adaptive regulatory framework. Integrating insights from Reflective Informatics and Information Ecologies, we explore how mobile technology can dynamically adjust to context while fostering self-reflection and meaning-making. This approach shifts from external restrictions to dynamic co-adaptative regulation, ensuring technology governance remains flexible yet structured, supporting adolescents in cultivating a balanced and intentional relationship with digital technology.",
      "authors": [
        "Pengyu Zhu and Janghee Cho"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T13:03:24+00:00",
          "link": "https://arxiv.org/abs/2507.12204v1",
          "size": "208kb",
          "version": "v1"
        }
      ],
      "title": "Tao-Technology for Teen Mobile Use: Harmonizing Adaptation, Autonomy, and Reflection",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12204",
        "HTML": "https://arxiv.org/html/2507.12204v1",
        "PDF": "https://arxiv.org/pdf/2507.12204"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "Creativity is touched upon as the paper discusses meaning-making and a balanced relationship with technology through Tao-Technology. However, it\u2019s more about adaptation and autonomy than creativity as a core focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12212",
      "abstract": "Generative AI does not only replicate human creativity but also reproduces deep-seated cultural biases, making it crucial to critically examine how concepts like ugliness are understood and expressed by these tools. This study investigates how four different generative AI models understand and express ugliness through text and image and explores the biases embedded within these representations. We extracted 13 adjectives associated with ugliness through iterative prompting of a large language model and generated 624 images across four AI models and three prompts. Demographic and socioeconomic attributes within the images were independently coded and thematically analyzed. Our findings show that AI models disproportionately associate ugliness with old white male figures, reflecting entrenched social biases as well as paradoxical biases, where efforts to avoid stereotypical depictions of marginalized groups inadvertently result in the disproportionate projection of negative attributes onto majority groups. Qualitative analysis further reveals that, despite supposed attempts to frame ugliness within social contexts, conventional physical markers such as asymmetry and aging persist as central visual motifs. These findings demonstrate that despite attempts to create more equal representations, generative AI continues to perpetuate inherited and paradoxical biases, underscoring the critical work being done to create ethical AI training paradigms and advance methodologies for more inclusive AI development.",
      "authors": [
        "Garyoung Kim",
        "Huisung Kwon",
        "Seoju Yun",
        "Yu-Won Youn"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T13:16:56+00:00",
          "link": "https://arxiv.org/abs/2507.12212v1",
          "size": "920kb",
          "version": "v1"
        }
      ],
      "title": "Draw an Ugly Person An Exploration of Generative AIs Perceptions of Ugliness",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12212",
        "HTML": "https://arxiv.org/html/2507.12212v1",
        "PDF": "https://arxiv.org/pdf/2507.12212"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper looks into generative AI perceptions, which relates to AI creativity. Although it primarily focuses on biases, it also indirectly addresses how AI understands creative outputs, thus partially related to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12296",
      "abstract": "Despite widespread debunking, many psychological myths remain deeply entrenched. This paper investigates whether Large Language Models (LLMs) mimic human behaviour of myth belief and explores methods to mitigate such tendencies. Using 50 popular psychological myths, we evaluate myth belief across multiple LLMs under different prompting strategies, including retrieval-augmented generation and swaying prompts. Results show that LLMs exhibit significantly lower myth belief rates than humans, though user prompting can influence responses. RAG proves effective in reducing myth belief and reveals latent debiasing potential within LLMs. Our findings contribute to the emerging field of Machine Psychology and highlight how cognitive science methods can inform the evaluation and development of LLM-based systems.",
      "authors": [
        "Bevan Koopman and Guido Zuccon"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T14:49:45+00:00",
          "link": "https://arxiv.org/abs/2507.12296v1",
          "size": "86kb",
          "version": "v1"
        }
      ],
      "title": "Humans are more gullible than LLMs in believing common psychological myths",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12296",
        "HTML": "https://arxiv.org/html/2507.12296v1",
        "PDF": "https://arxiv.org/pdf/2507.12296"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The study examines the belief in psychological myths by LLMs and mitigation strategies. There is no clear emphasis on creativity in this investigation."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12298",
      "abstract": "Eligibility criteria play a critical role in clinical trials by determining the target patient population, which significantly influences the outcomes of medical interventions. However, current approaches for designing eligibility criteria have limitations to support interactive exploration of the large space of eligibility criteria. They also ignore incorporating detailed characteristics from the original electronic health record (EHR) data for criteria refinement. To address these limitations, we proposed TrialCompass, a visual analytics system integrating a novel workflow, which can empower clinicians to iteratively explore the vast space of eligibility criteria through knowledge-driven and outcome-driven approaches. TrialCompass supports history-tracking to help clinicians trace the evolution of their adjustments and decisions when exploring various forms of data (i.e., eligibility criteria, outcome metrics, and detailed characteristics of original EHR data) through these two approaches. This feature can help clinicians comprehend the impact of eligibility criteria on outcome metrics and patient characteristics, which facilitates systematic refinement of eligibility criteria. Using a real-world dataset, we demonstrated the effectiveness of TrialCompass in providing insights into designing eligibility criteria for septic shock and sepsis-associated acute kidney injury. We also discussed the research prospects of applying visual analytics to clinical trials.",
      "authors": [
        "Rui Sheng",
        "Xingbo Wang",
        "Jiachen Wang",
        "Xiaofu Jin",
        "Zhonghua Sheng",
        "Zhenxing Xu",
        "Suraj Rajendran",
        "Huamin Qu",
        "and Fei Wang"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T14:54:16+00:00",
          "link": "https://arxiv.org/abs/2507.12298v1",
          "size": "1869kb",
          "version": "v1"
        }
      ],
      "title": "TrialCompass: Visual Analytics for Enhancing the Eligibility Criteria Design of Clinical Trials",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12298",
        "HTML": "https://arxiv.org/html/2507.12298v1",
        "PDF": "https://arxiv.org/pdf/2507.12298"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper proposes a visual analytics system for clinical trial eligibility design, which revolves around medical data analysis and not creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12334",
      "abstract": "Text is an integral but understudied component of visualization design. Although recent studies have examined how text elements (e.g., titles and annotations) influence comprehension, preferences, and predictions, many questions remain about textual design and use in practice. This paper introduces a framework for understanding text functions in information visualizations, building on and filling gaps in prior classifications and taxonomies. Through an analysis of 120 real-world visualizations and 804 text elements, we identified ten distinct text functions, ranging from identifying data mappings to presenting valenced subtext. We further identify patterns in text usage and conduct a factor analysis, revealing four overarching text-informed design strategies: Attribution and Variables, Annotation-Centric Design, Visual Embellishments, and Narrative Framing. In addition to these factors, we explore features of title rhetoric and text multifunctionality, while also uncovering previously unexamined text functions, such as text replacing visual elements. Our findings highlight the flexibility of text, demonstrating how different text elements in a given design can combine to communicate, synthesize, and frame visual information. This framework adds important nuance and detail to existing frameworks that analyze the diverse roles of text in visualization.",
      "authors": [
        "Chase Stokes",
        "Anjana Arunkumar",
        "Marti A. Hearst",
        "and Lace Padilla"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T15:25:26+00:00",
          "link": "https://arxiv.org/abs/2507.12334v1",
          "size": "2361kb",
          "version": "v1"
        }
      ],
      "title": "An Analysis of Text Functions in Information Visualization",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12334",
        "HTML": "https://arxiv.org/html/2507.12334v1",
        "PDF": "https://arxiv.org/pdf/2507.12334"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on text functions within information visualizations without any explicit or implicit emphasis on creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12337",
      "abstract": "Acquiring medical expertise is a critical component of medical education and professional development. While existing studies focus primarily on constructing medical knowledge bases or developing learning tools based on the structured, private healthcare data, they often lack methods for extracting expertise from unstructured medical texts. These texts constitute a significant portion of medical literature and offer greater flexibility and detail compared to structured data formats. Furthermore, many studies fail to provide explicit analytical and learning pathways in this context.\n  This paper introduces MExplore, an interactive visual analytics system designed to support the acquisition of medical expertise. To address the challenges of the inconsistencies and confidentiality concerns inherent in unstructured medical texts, we propose a workflow that employs a fine-tuned BERT-based model to extract medical entities (MEs) from them. We then present a novel multilevel visual analysis framework that integrates multiple coordinated visualizations, enabling a progressive and interactive exploration of medical knowledge.\n  To assess the effectiveness of MExplore, we conducted three case studies, a user study, and interviews with domain experts. The results indicate that the system significantly enhances the medical expertise acquisition process, providing an effective interactive approach for acquiring and retaining knowledge from medical texts.",
      "authors": [
        "Xiao Pang",
        "Yan Huang",
        "Chang Liu",
        "JiYuan Liu",
        "MingYou Liu"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T15:29:56+00:00",
          "link": "https://arxiv.org/abs/2507.12337v1",
          "size": "3030kb",
          "version": "v1"
        }
      ],
      "title": "MExplore: an entity-based visual analytics approach for medical expertise acquisition",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12337",
        "HTML": "https://arxiv.org/html/2507.12337v1",
        "PDF": "https://arxiv.org/pdf/2507.12337"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper primarily addresses medical expertise acquisition through visual analytics, with no mentions or connections to creativity or creative aspects."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12377",
      "abstract": "We conduct a deconstructive reading of a qualitative interview study with 17 visual data journalists from newsrooms across the globe. We borrow a deconstruction approach from literary critique to explore the instability of meaning in language and reveal implicit beliefs in words and ideas. Through our analysis we surface two sets of opposing implicit beliefs in visual data journalism: objectivity/subjectivity and humanism/mechanism. We contextualize these beliefs through a genealogical analysis, which brings deconstruction theory into practice by providing a historic backdrop for these opposing perspectives. Our analysis shows that these beliefs held within visual data journalism are not self-enclosed but rather a product of external societal forces and paradigm shifts over time. Through this work, we demonstrate how thinking with critical theories such as deconstruction and genealogy can reframe \"success\" in visual data storytelling and diversify visualization research outcomes. These efforts push the ways in which we as researchers produce domain knowledge to examine the sociotechnical issues of today's values towards datafication and data visualization.",
      "authors": [
        "Ke Er Amy Zhang",
        "Jodie Jenkinson",
        "Laura Garrison"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T16:26:44+00:00",
          "link": "https://arxiv.org/abs/2507.12377v1",
          "size": "8945kb",
          "version": "v1"
        }
      ],
      "title": "Deconstructing Implicit Beliefs in Visual Data Journalism: Unstable Meanings Behind Data as Truth & Design for Insight",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12377",
        "HTML": "https://arxiv.org/html/2507.12377v1",
        "PDF": "https://arxiv.org/pdf/2507.12377"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper explores beliefs and meanings in visual data journalism through critical theories, which can relate to creativity in terms of storytelling and framing but not as a primary focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.11597",
      "abstract": "AI is transforming research. It is being leveraged to construct surveys, synthesize data, conduct analysis, and write summaries of the results. While the promise is to create efficiencies and increase quality, the reality is not always as clear cut. Leveraging our framework of Truth, Beauty, and Justice (TBJ) which we use to evaluate AI, machine learning and computational models for effective and ethical use (Taber and Timpone 1997; Timpone and Yang 2024), we consider the potential and limitation of analytic, generative, and agentic AI to augment data scientists or take on tasks traditionally done by human analysts and researchers. While AI can be leveraged to assist analysts in their tasks, we raise some warnings about push-button automation. Just as earlier eras of survey analysis created some issues when the increased ease of using statistical software allowed researchers to conduct analyses they did not fully understand, the new AI tools may create similar but larger risks. We emphasize a human-machine collaboration perspective (Daugherty and Wilson 2018) throughout the data science workflow and particularly call out the vital role that data scientists play under VUCA decision areas. We conclude by encouraging the advance of AI tools to complement data scientists but advocate for continued training and understanding of methods to ensure the substantive value of research is fully achieved by applying, interpreting, and acting upon results most effectively and ethically.",
      "authors": [
        "Richard Timpone and Yongwei Yang"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-15T17:59:06+00:00",
          "link": "https://arxiv.org/abs/2507.11597v1",
          "size": "1275kb",
          "version": "v1"
        }
      ],
      "title": "AI, Humans, and Data Science: Optimizing Roles Across Workflows and the Workforce",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.11597",
        "PDF": "https://arxiv.org/pdf/2507.11597"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses the role of AI in the data science workflow and emphasizes human-machine collaboration, which can entail creative approaches in optimizing roles. However, creativity is not the main focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.11821",
      "abstract": "Neural networks are often benchmarked using standard datasets such as MNIST, FashionMNIST, or other variants of MNIST, which, while accessible, are limited to generic classes such as digits or clothing items. For researchers working on domain-specific tasks, such as classifying trees, food items, or other real-world objects, these data sets are insufficient and irrelevant. Additionally, creating and publishing a custom dataset can be time consuming, legally constrained, or beyond the scope of individual projects. We present MNIST-Gen, an automated, modular, and adaptive framework for generating MNIST-style image datasets tailored to user-specified categories using hierarchical semantic categorization. The system combines CLIP-based semantic understanding with reinforcement learning and human feedback to achieve intelligent categorization with minimal manual intervention. Our hierarchical approach supports complex category structures with semantic characteristics, enabling fine-grained subcategorization and multiple processing modes: individual review for maximum control, smart batch processing for large datasets, and fast batch processing for rapid creation. Inspired by category theory, MNIST-Gen models each data transformation stage as a composable morphism, enhancing clarity, modularity, and extensibility. As proof of concept, we generate and benchmark two novel datasets-\\textit{Tree-MNIST} and \\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing task-specific evaluation data while achieving 85\\% automatic categorization accuracy and 80\\% time savings compared to manual approaches.",
      "authors": [
        "Pouya Shaeri",
        "Arash Karimi",
        "Ariane Middel"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T00:50:09+00:00",
          "link": "https://arxiv.org/abs/2507.11821v1",
          "size": "1980kb",
          "version": "v1"
        }
      ],
      "title": "MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.11821",
        "HTML": "https://arxiv.org/html/2507.11821v1",
        "PDF": "https://arxiv.org/pdf/2507.11821"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses generating datasets using hierarchical semantics and reinforcement learning, which involves creative categorization and transformation processes. Creativity is involved as a secondary theme in the design of a modular dataset generation framework."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.11857",
      "abstract": "This paper is a study of techniques for measuring and predicting visual fidelity. As visual stimuli we use polygonal models, and vary their fidelity with two different model simplification algorithms. We also group the stimuli into two object types: animals and man made artifacts. We examine three different experimental techniques for measuring these fidelity changes: naming times, ratings, and preferences. All the measures were sensitive to the type of simplification and level of simplification. However, the measures differed from one another in their response to object type. We also examine several automatic techniques for predicting these experimental measures, including techniques based on images and on the models themselves. Automatic measures of fidelity were successful at predicting experimental ratings, less successful at predicting preferences, and largely failures at predicting naming times. We conclude with suggestions for use and improvement of the experimental and automatic measures of visual fidelity.",
      "authors": [
        "Benjamin Watson",
        "Alinda Friedman",
        "Aaron McGaffey"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Graphics (cs.GR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T02:52:20+00:00",
          "link": "https://arxiv.org/abs/2507.11857v1",
          "size": "248kb",
          "version": "v1"
        }
      ],
      "title": "Measuring and predicting visual fidelity",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.11857",
        "PDF": "https://arxiv.org/pdf/2507.11857"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on techniques for measuring and predicting visual fidelity, which is not directly related to creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.11892",
      "abstract": "Dynamic Facial Expression Recognition (DFER) aims to identify human emotions from temporally evolving facial movements and plays a critical role in affective computing. While recent vision-language approaches have introduced semantic textual descriptions to guide expression recognition, existing methods still face two key limitations: they often underutilize the subtle emotional cues embedded in generated text, and they have yet to incorporate sufficiently effective mechanisms for filtering out facial dynamics that are irrelevant to emotional expression. To address these gaps, We propose GRACE, Granular Representation Alignment for Cross-modal Emotion recognition that integrates dynamic motion modeling, semantic text refinement, and token-level cross-modal alignment to facilitate the precise localization of emotionally salient spatiotemporal features. Our method constructs emotion-aware textual descriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and highlights expression-relevant facial motion through a motion-difference weighting mechanism. These refined semantic and visual signals are aligned at the token level using entropy-regularized optimal transport. Experiments on three benchmark datasets demonstrate that our method significantly improves recognition performance, particularly in challenging settings with ambiguous or imbalanced emotion classes, establishing new state-of-the-art (SOTA) results in terms of both UAR and WAR.",
      "authors": [
        "Yu Liu",
        "Leyuan Qu",
        "Hanlei Shi",
        "Di Gao",
        "Yuhua Zheng",
        "Taihao Li"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T04:15:06+00:00",
          "link": "https://arxiv.org/abs/2507.11892v1",
          "size": "1055kb",
          "version": "v1"
        }
      ],
      "title": "From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.11892",
        "HTML": "https://arxiv.org/html/2507.11892v1",
        "PDF": "https://arxiv.org/pdf/2507.11892"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper is centered on dynamic emotion recognition from facial expressions and does not address creativity as a research theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.11906",
      "abstract": "Collective human activities like using an Ouija board (or Kokkuri-san) often produce emergent, coherent linguistic outputs unintended by any single participant. While psychological explanations such as the ideomotor effect exist, a computational understanding of how decentralized, implicit linguistic knowledge fuses through shared physical interaction remains elusive. We introduce CoCre-Sam (Collective-Creature Sampling), a framework modeling this phenomenon as collective Langevin dynamics sampling from implicitly fused language models. Each participant is represented as an agent associated with an energy landscape derived from an internal language model reflecting linguistic priors, and agents exert stochastic forces based on local energy gradients. We theoretically prove that the collective motion of the shared pointer (planchette) corresponds to Langevin MCMC sampling from the sum of individual energy landscapes, representing fused collective knowledge. Simulations validate that CoCre-Sam dynamics effectively fuse different models and generate meaningful character sequences, while ablation studies confirm the essential roles of collective interaction and stochasticity. Altogether, CoCre-Sam provides a novel computational mechanism linking individual implicit knowledge, embodied collective action, and emergent linguistic phenomena, grounding these complex interactions in the principles of probabilistic sampling.",
      "authors": [
        "Tadahiro Taniguchi",
        "Masatoshi Nagano",
        "Haruumi Omoto and Yoshiki Hayashi"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Multiagent Systems (cs.MA)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T04:45:23+00:00",
          "link": "https://arxiv.org/abs/2507.11906v1",
          "size": "1356kb",
          "version": "v1"
        }
      ],
      "title": "CoCre-Sam (Kokkuri-san): Modeling Ouija Board as Collective Langevin Dynamics Sampling from Fused Language Models",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.11906",
        "HTML": "https://arxiv.org/html/2507.11906v1",
        "PDF": "https://arxiv.org/pdf/2507.11906"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper investigates collective creative processes using models of communal Ouija board activities, highlighting creativity as a central theme by modeling emergent linguistic outputs."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12009",
      "abstract": "We propose an end-to-end deep neural encoder-decoder model to encode and decode brain activity in response to naturalistic stimuli using functional magnetic resonance imaging (fMRI) data. Leveraging temporally correlated input from consecutive film frames, we employ temporal convolutional layers in our architecture, which effectively allows to bridge the temporal resolution gap between natural movie stimuli and fMRI acquisitions. Our model predicts activity of voxels in and around the visual cortex and performs reconstruction of corresponding visual inputs from neural activity. Finally, we investigate brain regions contributing to visual decoding through saliency maps. We find that the most contributing regions are the middle occipital area, the fusiform area, and the calcarine, respectively employed in shape perception, complex recognition (in particular face perception), and basic visual features such as edges and contrasts. These functions being strongly solicited are in line with the decoder's capability to reconstruct edges, faces, and contrasts. All in all, this suggests the possibility to probe our understanding of visual processing in films using as a proxy the behaviour of deep learning models such as the one proposed in this paper.",
      "authors": [
        "Florian David",
        "Michael Chan",
        "Elenor Morgenroth",
        "Patrik Vuilleumier",
        "Dimitri Van De Ville"
      ],
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T08:08:48+00:00",
          "link": "https://arxiv.org/abs/2507.12009v1",
          "size": "3352kb",
          "version": "v1"
        }
      ],
      "title": "Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with Naturalistic Stimuli",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12009",
        "HTML": "https://arxiv.org/html/2507.12009v1",
        "PDF": "https://arxiv.org/pdf/2507.12009"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The research centers on a deep neural model for decoding fMRI brain activity in response to visual stimuli. It does not involve creativity or exploring creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12108",
      "abstract": "Coordinated online behavior, which spans from beneficial collective actions to harmful manipulation such as disinformation campaigns, has become a key focus in digital ecosystem analysis. Traditional methods often rely on monomodal approaches, focusing on single types of interactions like co-retweets or co-hashtags, or consider multiple modalities independently of each other. However, these approaches may overlook the complex dynamics inherent in multimodal coordination. This study compares different ways of operationalizing the detection of multimodal coordinated behavior. It examines the trade-off between weakly and strongly integrated multimodal models, highlighting the balance between capturing broader coordination patterns and identifying tightly coordinated behavior. By comparing monomodal and multimodal approaches, we assess the unique contributions of different data modalities and explore how varying implementations of multimodality impact detection outcomes. Our findings reveal that not all the modalities provide distinct insights, but that with a multimodal approach we can get a more comprehensive understanding of coordination dynamics. This work enhances the ability to detect and analyze coordinated online behavior, offering new perspectives for safeguarding the integrity of digital platforms.",
      "authors": [
        "Lorenzo Mannocci",
        "Stefano Cresci",
        "Matteo Magnani",
        "Anna Monreale",
        "Maurizio Tesconi"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Social and Information Networks (cs.SI)",
        "Artificial Intelligence (cs.AI)",
        "Computers and Society (cs.CY)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T10:25:45+00:00",
          "link": "https://arxiv.org/abs/2507.12108v1",
          "size": "4418kb",
          "version": "v1"
        }
      ],
      "title": "Multimodal Coordinated Online Behavior: Trade-offs and Strategies",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12108",
        "HTML": "https://arxiv.org/html/2507.12108v1",
        "PDF": "https://arxiv.org/pdf/2507.12108"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper examines coordinated online behavior across different modalities. It does not discuss creativity or related research areas."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12118",
      "abstract": "In recent years, attention has increasingly focused on enhancing user satisfaction with user interfaces, spanning both mobile applications and websites. One fundamental aspect of human-machine interaction is the concept of web usability. In order to assess web usability, the A/B testing technique enables the comparison of data between two designs. Expanding the scope of tests to include the designs being evaluated, in conjunction with the involvement of both real and fictional users, presents a challenge for which few online tools offer support. We propose a methodology for web usability evaluation based on user-centered approaches such as design thinking and linguistic decision-making, named Linguistic Decision-Making for Web Usability Evaluation. This engages people in role-playing scenarios and conducts a number of usability tests, including the widely recognized System Usability Scale. We incorporate the methodology into a decision support system based on A/B testing. We use real users in a case study to assess three Moodle platforms at the University of Guadalajara, Mexico.",
      "authors": [
        "Noe Zerme\\~no",
        "Cristina Zuheros",
        "Lucas Daniel Del Rosso Calache",
        "Francisco Herrera",
        "Rosana Montes"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Software Engineering (cs.SE)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T10:37:29+00:00",
          "link": "https://arxiv.org/abs/2507.12118v1",
          "size": "1486kb",
          "version": "v1"
        }
      ],
      "title": "An Online A/B Testing Decision Support System for Web Usability Assessment Based on a Linguistic Decision-making Methodology: Case of Study a Virtual Learning Environment",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12118",
        "HTML": "https://arxiv.org/html/2507.12118v1",
        "PDF": "https://arxiv.org/pdf/2507.12118"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper is focused on web usability and decision support systems for usability assessment using A/B testing. It does not address creativity directly or indirectly."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12356",
      "abstract": "Gender bias has been widely observed in speech perception tasks, influenced by the fundamental voicing differences between genders. This study reveals a gender bias in the perception of Alzheimer's Disease (AD) speech. In a perception experiment involving 16 Chinese listeners evaluating both Chinese and Greek speech, we identified that male speech was more frequently identified as AD, with this bias being particularly pronounced in Chinese speech. Acoustic analysis showed that shimmer values in male speech were significantly associated with AD perception, while speech portion exhibited a significant negative correlation with AD identification. Although language did not have a significant impact on AD perception, our findings underscore the critical role of gender bias in AD speech perception. This work highlights the necessity of addressing gender bias when developing AD detection models and calls for further research to validate model performance across different linguistic contexts.",
      "authors": [
        "Liu He and Yuanchao Li and Rui Feng and XinRan Han and Yin-Long Liu and Yuwei Yang and Zude Zhu and Jiahong Yuan"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)",
        "Sound (cs.SD)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T15:56:09+00:00",
          "link": "https://arxiv.org/abs/2507.12356v1",
          "size": "2611kb",
          "version": "v1"
        }
      ],
      "title": "Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12356",
        "HTML": "https://arxiv.org/html/2507.12356v1",
        "PDF": "https://arxiv.org/pdf/2507.12356"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The study focuses on gender bias in speech perception related to Alzheimer's Disease detection, lacking any association with creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12370",
      "abstract": "Large Language Models (LLMs) have demonstrated significant capabilities in understanding and generating human language, contributing to more natural interactions with complex systems. However, they face challenges such as ambiguity in user requests processed by LLMs. To address these challenges, this paper introduces and evaluates a multi-agent debate framework designed to enhance detection and resolution capabilities beyond single models. The framework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and Mistral-7B variants) and a dataset with diverse ambiguities. The debate framework markedly enhanced the performance of Llama3-8B and Mistral-7B variants over their individual baselines, with Mistral-7B-led debates achieving a notable 76.7% success rate and proving particularly effective for complex ambiguities and efficient consensus. While acknowledging varying model responses to collaborative strategies, these findings underscore the debate framework's value as a targeted method for augmenting LLM capabilities. This work offers important insights for developing more robust and adaptive language understanding systems by showing how structured debates can lead to improved clarity in interactive systems.",
      "authors": [
        "Ana Davila",
        "Jacinto Colan",
        "Yasuhisa Hasegawa"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T16:15:25+00:00",
          "link": "https://arxiv.org/abs/2507.12370v1",
          "size": "1227kb",
          "version": "v1"
        }
      ],
      "title": "Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12370",
        "HTML": "https://arxiv.org/html/2507.12370v1",
        "PDF": "https://arxiv.org/pdf/2507.12370"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper discusses improving ambiguity detection in language models through structured debate, which is not directly related to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.12443",
      "abstract": "Beyond hallucinations, another problem in program synthesis using LLMs is ambiguity in user intent. We illustrate the ambiguity problem in a networking context for LLM-based incremental configuration synthesis of route-maps and ACLs. These structures frequently overlap in header space, making the relative priority of actions impossible for the LLM to infer without user interaction. Measurements in a large cloud identify complex ACLs with 100's of overlaps, showing ambiguity is a real problem. We propose a prototype system, Clarify, which uses an LLM augmented with a new module called a Disambiguator that helps elicit user intent. On a small synthetic workload, Clarify incrementally synthesizes routing policies after disambiguation and then verifies them. Our treatment of ambiguities is useful more generally when the intent of updates can be correctly synthesized by LLMs, but their integration is ambiguous and can lead to different global behaviors.",
      "authors": [
        "Rajdeep Mondal",
        "Nikolaj Bjorner",
        "Todd Millstein",
        "Alan Tang",
        "George Varghese"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Networking and Internet Architecture (cs.NI)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Programming Languages (cs.PL)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-16T17:29:15+00:00",
          "link": "https://arxiv.org/abs/2507.12443v1",
          "size": "211kb",
          "version": "v1"
        }
      ],
      "title": "LLM-Based Config Synthesis requires Disambiguation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.12443",
        "HTML": "https://arxiv.org/html/2507.12443v1",
        "PDF": "https://arxiv.org/pdf/2507.12443"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on program synthesis and disambiguation of user intent within the context of networking and configuration synthesis, without addressing creativity. It discusses technical solutions to ambiguity in LLM-based systems, which does not relate to creative processes or creativity research."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.10024",
      "abstract": "Design studies aim to create visualization solutions for real-world problems of different application domains. Recently, the emergence of large language models (LLMs) has introduced new opportunities to enhance the design study process, providing capabilities such as creative problem-solving, data handling, and insightful analysis. However, despite their growing popularity, there remains a lack of systematic understanding of how LLMs can effectively assist researchers in visualization-specific design studies. In this paper, we conducted a multi-stage qualitative study to fill this gap, involving 30 design study researchers from diverse backgrounds and expertise levels. Through in-depth interviews and carefully-designed questionnaires, we investigated strategies for utilizing LLMs, the challenges encountered, and the practices used to overcome them. We further compiled and summarized the roles that LLMs can play across different stages of the design study process. Our findings highlight practical implications to inform visualization practitioners, and provide a framework for leveraging LLMs to enhance the design study process in visualization research.",
      "authors": [
        "Shaolun Ruan",
        "Rui Sheng",
        "Xiaolin Wen",
        "Jiachen Wang",
        "Tianyi Zhang",
        "Yong Wang",
        "Tim Dwyer",
        "Jiannan Li"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T08:06:12+00:00",
          "link": "https://arxiv.org/abs/2507.10024v1",
          "size": "19939kb",
          "version": "v1"
        },
        {
          "date": "2025-07-16T08:53:53+00:00",
          "link": "https://arxiv.org/abs/2507.10024v2",
          "size": "19939kb",
          "version": "v2"
        }
      ],
      "title": "Qualitative Study for LLM-assisted Design Study Process: Strategies, Challenges, and Roles",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.10024",
        "HTML": "https://arxiv.org/html/2507.10024v2",
        "PDF": "https://arxiv.org/pdf/2507.10024"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper investigates the role of LLMs in the design study process, highlighting strategies for creative problem-solving and the enhancement of visualization design, which directly relates to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2411.02179",
      "abstract": "High-quality environment lighting is essential for creating immersive mobile augmented reality (AR) experiences. However, achieving visually coherent estimation for mobile AR is challenging due to several key limitations in AR device sensing capabilities, including low camera FoV and limited pixel dynamic ranges. Recent advancements in generative AI, which can generate high-quality images from different types of prompts, including texts and images, present a potential solution for high-quality lighting estimation. Still, to effectively use generative image diffusion models, we must address two key limitations of content quality and slow inference. In this work, we design and implement a generative lighting estimation system called CleAR that can produce high-quality, diverse environment maps in the format of 360{\\deg} HDR images. Specifically, we design a two-step generation pipeline guided by AR environment context data to ensure the output aligns with the physical environment's visual context and color appearance. To improve the estimation robustness under different lighting conditions, we design a real-time refinement component to adjust lighting estimation results on AR devices. Through a combination of quantitative and qualitative evaluations, we show that CleAR outperforms state-of-the-art lighting estimation methods on both estimation accuracy, latency, and robustness, and is rated by 31 participants as producing better renderings for most virtual objects. For example, CleAR achieves 51% to 56% accuracy improvement on virtual object renderings across objects of three distinctive types of materials and reflective properties. CleAR produces lighting estimates of comparable or better quality in just 3.2 seconds -- over 110X faster than state-of-the-art methods.",
      "authors": [
        "Yiqin Zhao",
        "Mallesham Dasari",
        "Tian Guo"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Graphics (cs.GR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-11-04T15:37:18+00:00",
          "link": "https://arxiv.org/abs/2411.02179v1",
          "size": "14752kb",
          "version": "v1"
        },
        {
          "date": "2025-05-05T19:58:02+00:00",
          "link": "https://arxiv.org/abs/2411.02179v2",
          "size": "16061kb",
          "version": "v2"
        },
        {
          "date": "2025-07-16T16:15:30+00:00",
          "link": "https://arxiv.org/abs/2411.02179v3",
          "size": "2816kb",
          "version": "v3"
        }
      ],
      "title": "CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile Augmented Reality",
      "links": {
        "Abstract": "https://arxiv.org/abs/2411.02179",
        "HTML": "https://arxiv.org/html/2411.02179v3",
        "PDF": "https://arxiv.org/pdf/2411.02179"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on lighting estimation for mobile augmented reality and does not address creativity as a research theme."
      },
      "tasks": [
        "Hallucination",
        "Lighting Estimation"
      ],
      "source": "arXiv"
    },
    {
      "id": "2507.10644",
      "abstract": "The concept of the Web of Agents (WoA), which transforms the static, document-centric Web into an environment of autonomous agents acting on users' behalf, has attracted growing interest as large language models (LLMs) become more capable. However, research in this area is still fragmented across different communities. Contemporary surveys catalog the latest LLM-powered frameworks, while the rich histories of Multi-Agent Systems (MAS) and the Semantic Web are often treated as separate, legacy domains. This fragmentation obscures the intellectual lineage of modern systems and hinders a holistic understanding of the field's trajectory. We present the first comprehensive evolutionary overview of the WoA. We show that modern protocols like A2A and the MCP, are direct evolutionary responses to the well-documented limitations of earlier standards like FIPA standards and OWL-based semantic agents. To systematize this analysis, we introduce a four-axis taxonomy (semantic foundation, communication paradigm, locus of intelligence, discovery mechanism). This framework provides a unified analytical lens for comparing agent architectures across all generations, revealing a clear line of descent where others have seen a disconnect. Our analysis identifies a paradigm shift in the 'locus of intelligence': from being encoded in external data (Semantic Web) or the platform (MAS) to being embedded within the agent's core model (LLM). This shift is foundational to modern Agentic AI, enabling the scalable and adaptive systems the WoA has long envisioned. We conclude that while new protocols are essential, they are insufficient for building a robust, open, trustworthy ecosystem. Finally, we argue that the next research frontier lies in solving persistent socio-technical challenges, and we map out a new agenda focused on decentralized identity, economic models, security, and governance for the emerging WoA.",
      "authors": [
        "Tatiana Petrova (1)",
        "Boris Bliznioukov (1)",
        "Aleksandr Puzikov (1)",
        "Radu State (1) ((1) SEDAN SnT",
        "University of Luxembourg",
        "Luxembourg",
        "Luxembourg)"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Cryptography and Security (cs.CR)",
        "Human-Computer Interaction (cs.HC)",
        "Multiagent Systems (cs.MA)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T16:47:19+00:00",
          "link": "https://arxiv.org/abs/2507.10644v1",
          "size": "1312kb",
          "version": "v1"
        },
        {
          "date": "2025-07-16T15:30:42+00:00",
          "link": "https://arxiv.org/abs/2507.10644v2",
          "size": "1139kb",
          "version": "v2"
        }
      ],
      "title": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.10644",
        "HTML": "https://arxiv.org/html/2507.10644v2",
        "PDF": "https://arxiv.org/pdf/2507.10644"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper provides an overview of the evolution of the Web of Agents and does not focus on creativity as a research theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.10786",
      "abstract": "Equipped with artificial intelligence (AI) and advanced sensing capabilities, social robots are gaining interest among consumers in the United States. These robots seem like a natural evolution of traditional smart home devices. However, their extensive data collection capabilities, anthropomorphic features, and capacity to interact with their environment make social robots a more significant security and privacy threat. Increased risks include data linkage, unauthorized data sharing, and the physical safety of users and their homes. It is critical to investigate U.S. users' security and privacy needs and concerns to guide the design of social robots while these devices are still in the early stages of commercialization in the U.S. market. Through 19 semi-structured interviews, we identified significant security and privacy concerns, highlighting the need for transparency, usability, and robust privacy controls to support adoption. For educational applications, participants worried most about misinformation, and in medical use cases, they worried about the reliability of these devices. Participants were also concerned with the data inference that social robots could enable. We found that participants expect tangible privacy controls, indicators of data collection, and context-appropriate functionality.",
      "authors": [
        "Henry Bell",
        "Jabari Kwesi",
        "Hiba Laabadli",
        "Pardis Emami-Naeini"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)",
        "Cryptography and Security (cs.CR)",
        "Emerging Technologies (cs.ET)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-14T20:27:40+00:00",
          "link": "https://arxiv.org/abs/2507.10786v1",
          "size": "82kb",
          "version": "v1"
        },
        {
          "date": "2025-07-16T16:58:46+00:00",
          "link": "https://arxiv.org/abs/2507.10786v2",
          "size": "82kb",
          "version": "v2"
        }
      ],
      "title": "\"Is it always watching? Is it always listening?\" Exploring Contextual Privacy and Security Concerns Toward Domestic Social Robots",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.10786",
        "HTML": "https://arxiv.org/html/2507.10786v2",
        "PDF": "https://arxiv.org/pdf/2507.10786"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on privacy and security concerns related to domestic social robots, without addressing creativity as a research theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.11330",
      "abstract": "Novelty is a crucial criterion in the peer review process for evaluating academic papers. Traditionally, it's judged by experts or measure by unique reference combinations. Both methods have limitations: experts have limited knowledge, and the effectiveness of the combination method is uncertain. Moreover, it's unclear if unique citations truly measure novelty. The large language model (LLM) possesses a wealth of knowledge, while human experts possess judgment abilities that the LLM does not possess. Therefore, our research integrates the knowledge and abilities of LLM and human experts to address the limitations of novelty assessment. One of the most common types of novelty in academic papers is the introduction of new methods. In this paper, we propose leveraging human knowledge and LLM to assist pretrained language models (PLMs, e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we extract sentences related to the novelty of the academic paper from peer review reports and use LLM to summarize the methodology section of the academic paper, which are then used to fine-tune PLMs. In addition, we have designed a text-guided fusion module with novel Sparse-Attention to better integrate human and LLM knowledge. We compared the method we proposed with a large number of baselines. Extensive experiments demonstrate that our method achieves superior performance.",
      "authors": [
        "Wenqing Wu",
        "Chengzhi Zhang",
        "Yi Zhao"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Digital Libraries (cs.DL)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-15T14:03:55+00:00",
          "link": "https://arxiv.org/abs/2507.11330v1",
          "size": "3937kb",
          "version": "v1"
        },
        {
          "date": "2025-07-16T14:26:34+00:00",
          "link": "https://arxiv.org/abs/2507.11330v2",
          "size": "3939kb",
          "version": "v2"
        }
      ],
      "title": "Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.11330",
        "HTML": "https://arxiv.org/html/2507.11330v2",
        "PDF": "https://arxiv.org/pdf/2507.11330"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses novelty evaluation of academic papers, but it focuses on using human and machine intelligence for assessing novelty rather than creativity itself."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.21536",
      "abstract": "With the rapid development of digital technology, AI-driven psychological counseling has gradually become an important research direction in the field of mental health. However, existing models still have deficiencies in dialogue safety, detailed scenario handling, and lightweight deployment. To address these issues, this study proposes PsyLite, a lightweight psychological counseling large language model agent developed based on the base model InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation data fine-tuning and ORPO preference optimization), PsyLite enhances the model's deep-reasoning ability, psychological counseling ability, and safe dialogue ability. After deployment using Ollama and Open WebUI, a custom workflow is created with Pipelines. An innovative conditional RAG is designed to introduce crosstalk humor elements at appropriate times during psychological counseling to enhance user experience and decline dangerous requests to strengthen dialogue safety. Evaluations show that PsyLite outperforms the baseline models in the Chinese general evaluation (CEval), psychological counseling professional evaluation (CPsyCounE), and dialogue safety evaluation (SafeDialBench), particularly in psychological counseling professionalism (CPsyCounE score improvement of 47.6\\%) and dialogue safety (\\safe{} score improvement of 2.4\\%). Additionally, the model uses quantization technology (GGUF q4\\_k\\_m) to achieve low hardware deployment (5GB memory is sufficient for operation), providing a feasible solution for psychological counseling applications in resource-constrained environments.",
      "authors": [
        "Fangjun Ding and Renyu Zhang and Xinyu Feng and Chengye Xie and Zheng Zhang and Yanting Zhang"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-26T17:54:42+00:00",
          "link": "https://arxiv.org/abs/2506.21536v1",
          "size": "3209kb",
          "version": "v1"
        }
      ],
      "title": "PsyLite Technical Report",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.21536",
        "HTML": "https://arxiv.org/html/2506.21536",
        "PDF": "https://arxiv.org/pdf/2506.21536"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper concentrates on AI-driven psychological counseling and model deployment, with no clear connection to creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2504.03966",
      "abstract": "The integration of Large Language Models (LLMs) with Learning Management Systems (LMSs) has the potential to enhance task automation and accessibility in education. However, hallucination where LLMs generate inaccurate or misleading information remains a significant challenge. This study introduces the Dynamic Course Content Integration (DCCI) mechanism, which dynamically retrieves and integrates course content and curriculum from Canvas LMS into the LLM-powered assistant, Ask ME. By employing prompt engineering to structure retrieved content within the LLM's context window, DCCI ensures accuracy, relevance, and contextual alignment, mitigating hallucination. To evaluate DCCI's effectiveness, Ask ME's usability, and broader student perceptions of AI in education, a mixed-methods approach was employed, incorporating user satisfaction ratings and a structured survey. Results from a pilot study indicate high user satisfaction (4.614/5), with students recognizing Ask ME's ability to provide timely and contextually relevant responses for both administrative and course-related inquiries. Additionally, a majority of students agreed that Ask ME's integration with course content in Canvas LMS reduced platform-switching, improving usability, engagement, and comprehension. AI's role in reducing classroom hesitation and fostering self-directed learning and intellectual curiosity was also highlighted. Despite these benefits and positive perception of AI tools, concerns emerged regarding over-reliance on AI, accuracy limitations, and ethical issues such as plagiarism and reduced student-teacher interaction. These findings emphasize the need for strategic AI implementation, ethical safeguards, and a pedagogical framework that prioritizes human-AI collaboration over substitution.",
      "authors": [
        "Kovan Mzwri (1)",
        "M\\'arta Turcs\\'anyi-Szabo (2) ((1) Doctoral School of Informatics",
        "E\\\"otv\\\"os Lor\\'and University",
        "Budapest",
        "Hungary",
        "(2) Department of Media & Educational Technology",
        "Faculty of Informatics",
        "E\\\"otv\\\"os Lor\\'and University",
        "Budapest",
        "Hungary)"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)",
        "Emerging Technologies (cs.ET)",
        "Human-Computer Interaction (cs.HC)",
        "Software Engineering (cs.SE)"
      ],
      "submission_historys": [
        {
          "date": "2025-04-04T22:17:30+00:00",
          "link": "https://arxiv.org/abs/2504.03966v1",
          "size": "8412kb",
          "version": "v1"
        }
      ],
      "title": "Bridging LMS and Generative AI: Dynamic Course Content Integration (DCCI) for Connecting LLMs to Course Content -- The Ask ME Assistant",
      "links": {
        "Abstract": "https://arxiv.org/abs/2504.03966",
        "PDF": "https://arxiv.org/pdf/2504.03966"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "Although the primary focus is on integrating LLMs with LMS for educational purposes, the paper discusses the potential for fostering self-directed learning and intellectual curiosity, which are related to creativity."
      },
      "tasks": [
        "Hallucination",
        "Prompt Engineering"
      ],
      "source": "arXiv"
    }
  ],
  "subjects": [
    "Computer Vision and Pattern Recognition (cs.CV)",
    "Networking and Internet Architecture (cs.NI)",
    "Multimedia (cs.MM)",
    "Computation and Language (cs.CL)",
    "Sound (cs.SD)",
    "Cryptography and Security (cs.CR)",
    "Information Retrieval (cs.IR)",
    "Artificial Intelligence (cs.AI)",
    "Quantitative Methods (q-bio.QM)",
    "Programming Languages (cs.PL)",
    "Emerging Technologies (cs.ET)",
    "Social and Information Networks (cs.SI)",
    "Computers and Society (cs.CY)",
    "Multiagent Systems (cs.MA)",
    "Software Engineering (cs.SE)",
    "Human-Computer Interaction (cs.HC)",
    "Graphics (cs.GR)",
    "Digital Libraries (cs.DL)",
    "Machine Learning (cs.LG)"
  ],
  "prompt": {
    "creativity": "\nYou are an expert in information retrieval. I will provide you with a list of research papers from arXiv, specifically in the *cs.HC* (Human-Computer Interaction) category.\n\nYour task is to analyze each paper and determine its relevance to the topic of **Creativity**.\n\n\n### Classify each paper into one of the following relevance levels\n\n- `core` \u2014 Creativity is a **primary focus**\n  - The paper directly studies or simulates creativity, with a clear focus on creativity.\n  - Includes creative tasks, co-creative systems, or creativity evaluation metrics.\n  - The title and abstract explicitly mention creativity, and the research questions are directly related to creativity.\n- `partial` \u2014 Creativity is a **secondary theme**\n  - Part of the paper relates to creativity; it is treated as an analytical dimension or design goal but not the main objective (e.g., user creativity, design support).\n  - Creativity may appear in discussions, experiments, or auxiliary applications.\n  - Creativity is presented as a supporting topic (e.g., evaluation criteria, user feedback).\n- `irrelevant` \u2014 **No clear connection** to creativity\n  - The paper does not address creativity as a topic.\n  - Focuses on unrelated technical content (e.g., compression, security, optimization).\n  - If creativity is mentioned, it is only superficial and lacks substantive content.\n\n\n### Return your results in the following JSON format\n\n```json\n{\n  \"result\": [\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    },\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    }\n  ]\n}\n```\n\nBe concise but specific in your reasoning, referencing key terms or sections when applicable.\n"
  },
  "description": "Data source: https://arxiv.org/list/cs.HC/new",
  "level_tatistics": {
    "irrelevant": 23,
    "core": 4,
    "partial": 9
  },
  "arxiv_update_date": "2025-07-17",
  "updated_at": "2025-07-17 10:20:33"
}