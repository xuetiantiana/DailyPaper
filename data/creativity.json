{
  "data": [
    {
      "id": "2506.22520",
      "abstract": "This study examines the impact of an Artificial Intelligence tutor teammate (AI) on student curiosity-driven engagement and learning effectiveness during Interactive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics platform. It explores the role of the AI's curiosity-triggering and response behaviors in stimulating and sustaining student curiosity, affecting the frequency and complexity of student-initiated questions. The study further assesses how AI interventions shape student engagement, foster discovery curiosity, and enhance team performance within the IMD learning environment. Using a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI tutor teammate's behavior through a large language model. By employing a mixed-methods exploratory design, a total of 11 high school students participated in four IMD tasks that involved molecular visualization and calculations, which increased in complexity over a 60-minute period. Team performance was evaluated through real-time observation and recordings, whereas team communication was measured by question complexity and AI's curiosity-triggering and response behaviors. Cross Recurrence Quantification Analysis (CRQA) metrics reflected structural alignment in coordination and were linked to communication behaviors. High-performing teams exhibited superior task completion, deeper understanding, and increased engagement. Advanced questions were associated with AI curiosity-triggering, indicating heightened engagement and cognitive complexity. CRQA metrics highlighted dynamic synchronization in student-AI interactions, emphasizing structured yet adaptive engagement to promote curiosity. These proof-of-concept findings suggest that the AI's dual role as a teammate and educator indicates its capacity to provide adaptive feedback, sustaining engagement and epistemic curiosity.",
      "authors": [
        "Mustafa Demir",
        "Jacob Miratsky",
        "Jonathan Nguyen",
        "Chun Kit Chan",
        "Punya Mishra",
        "and Abhishek Singharoy"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Computational Engineering, Finance, and Science (cs.CE)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-26T19:30:25+00:00",
          "link": "https://arxiv.org/abs/2506.22520v1",
          "size": "1193kb",
          "version": "v1"
        }
      ],
      "title": "Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22520",
        "PDF": "https://arxiv.org/pdf/2506.22520"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The study involves an AI tutor to stimulate student curiosity and engagement, indirectly promoting creativity through curiosity-driven learning. Creativity is not the main focus but is a supporting theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22583",
      "abstract": "Level of detail (LOD) is widely used to control visual feedback in interactive applications. LOD control is typically based on perception at threshold - the conditions in which a stimulus first becomes perceivable. Yet most LOD manipulations are quite perceivable and occur well above threshold. Moreover, research shows that supra-threshold perception differs drastically from perception at threshold. In that case, should supra-threshold LOD control also differ from LOD control at threshold?\n  In two experiments, we examine supra-threshold LOD control in the visual periphery and find that indeed, it should differ drastically from LOD control at threshold. Specifically, we find that LOD must support a task-dependent level of reliable perceptibility. Above that level, perceptibility of LOD control manipulations should be minimized, and detail contrast is a better predictor of perceptibility than detail size. Below that level, perceptibility must be maximized, and LOD should be improved as eccentricity rises or contrast drops. This directly contradicts prevailing threshold-based LOD control schemes, and strongly suggests a reexamination of LOD control for foveal display.",
      "authors": [
        "Benjamin Watson",
        "Neff Walker",
        "Larry F Hodges"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Graphics (cs.GR)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-27T19:01:45+00:00",
          "link": "https://arxiv.org/abs/2506.22583v1",
          "size": "524kb",
          "version": "v1"
        }
      ],
      "title": "Supra-threshold control of peripheral LOD",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22583",
        "PDF": "https://arxiv.org/pdf/2506.22583"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "This paper examines level of detail (LOD) control in visual settings and perceptibility, a topic unrelated to creativity and does not discuss creativity concepts or systems."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22597",
      "abstract": "Wayfinding, the ability to recall the environment and navigate through it, is an essential cognitive skill relied upon almost every day in a person's life. A crucial component of wayfinding is the construction of cognitive maps, mental representations of the environments through which a person travels. Age, disease or injury can severely affect cognitive mapping, making assessment of this basic survival skill particularly important to clinicians and therapists. Cognitive mapping has also been the focus of decades of basic research by cognitive psychologists. Both communities have evolved a number of techniques for assessing cognitive mapping ability. We present the Cognitive Map Probe (CMP), a new computerized tool for assessment of cognitive mapping ability that increases consistency and promises improvements in flexibility, accessibility, sensitivity and control. The CMP uses a tangible user interface that affords spatial manipulation. We describe the design of the CMP, and find that it is sensitive to factors known to affect cognitive mapping performance in extensive experimental testing.",
      "authors": [
        "Ehud Sharlin",
        "Benjamin Watson",
        "Steve Sutphen",
        "Lili Liu",
        "Robert Lederer",
        "John Frazer"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-27T19:36:27+00:00",
          "link": "https://arxiv.org/abs/2506.22597v1",
          "size": "220kb",
          "version": "v1"
        }
      ],
      "title": "A tangible user interface for assessing cognitive mapping ability",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22597",
        "PDF": "https://arxiv.org/pdf/2506.22597"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper focuses on cognitive mapping and wayfinding abilities assessed through a tangible user interface. There is no mention of creativity or related concepts."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22674",
      "abstract": "Electric vehicles (EVs) are a promising alternative to fuel vehicles (FVs), given some unique characteristics of EVs, for example, the low air pollution and maintenance cost. However, the increasing prevalence of EVs is accompanied by widespread complaints regarding the high likelihood of motion sickness (MS) induction, especially when compared to FVs, which has become one of the major obstacles to the acceptance and popularity of EVs. Despite the prevalence of such complaints online and among EV users, the association between vehicle type (i.e., EV versus FV) and MS prevalence and severity has not been quantified. Thus, this study aims to investigate the existence of EV-induced MS and explore the potential factors leading to it. A survey study was conducted to collect passengers' MS experience in EVs and FVs in the past one year. In total, 639 valid responses were collected from mainland China. The results show that FVs were associated with a higher frequency of MS, while EVs were found to induce more severe MS symptoms. Further, we found that passengers' MS severity was associated with individual differences (i.e., age, gender, sleep habits, susceptibility to motion-induced MS), in-vehicle activities (i.e., chatting with others and watching in-vehicle displays), and road conditions (i.e., congestion and slope), while the MS frequency was associated with the vehicle ownership and riding frequency. The results from this study can guide the directions of future empirical studies that aim to quantify the inducers of MS in EVs and FVs, as well as the optimization of EVs to reduce MS.",
      "authors": [
        "Weiyin Xie",
        "Chunxi Huang",
        "Jiyao Wang",
        "Dengbo He"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)",
        "Applications (stat.AP)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-27T22:55:55+00:00",
          "link": "https://arxiv.org/abs/2506.22674v1",
          "size": "530kb",
          "version": "v1"
        }
      ],
      "title": "Do Electric Vehicles Induce More Motion Sickness Than Fuel Vehicles? A Survey Study in China",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22674",
        "PDF": "https://arxiv.org/pdf/2506.22674"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "This paper examines motion sickness in electric vs. fuel vehicles. Creativity is not a topic within the paper, focusing instead on health and vehicle comparison aspects."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22741",
      "abstract": "Significant changes in the digital employment landscape, driven by rapid technological advancements and the COVID-19 pandemic, have introduced new opportunities for blind and visually impaired (BVI) individuals in developing countries like India. However, a significant portion of the BVI population in India remains unemployed despite extensive accessibility advancements and job search interventions. Therefore, we conducted semi-structured interviews with 20 BVI persons who were either pursuing or recently sought employment in the digital industry. Our findings reveal that despite gaining digital literacy and extensive training, BVI individuals struggle to meet industry requirements for fulfilling job openings. While they engage in self-reflection to identify shortcomings in their approach and skills, they lack constructive feedback from peers and recruiters. Moreover, the numerous job intervention tools are limited in their ability to meet the unique needs of BVI job seekers. Our results therefore provide key insights that inform the design of future collaborative intervention systems that offer personalized feedback for BVI individuals, effectively guiding their self-reflection process and subsequent job search behaviors, and potentially leading to improved employment outcomes.",
      "authors": [
        "Akshay Nayak Kolgar",
        "Yash Prakash",
        "Sampath Jayarathna",
        "Hae-Na Lee and Vikas Ashok"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-28T03:05:49+00:00",
          "link": "https://arxiv.org/abs/2506.22741v1",
          "size": "1641kb",
          "version": "v1"
        }
      ],
      "title": "Insights in Adaptation: Examining Self-reflection Strategies of Job Seekers with Visual Impairments in India",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22741",
        "HTML": "https://arxiv.org/html/2506.22741v1",
        "PDF": "https://arxiv.org/pdf/2506.22741"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper investigates employment strategies for visually impaired job seekers in India, with a focus on self-reflection and adaptation. Creativity is not discussed as a significant theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22815",
      "abstract": "This position paper aims to rethink the role and design of memory in Large Language Model (LLM)-based agent systems. We observe that while current memory practices have begun to transcend the limitations of single interactions, they remain conceptually grounded in \"bound memory\" in terms of design concept-where memory is treated as local state attached to specific context or entities, forming \"memory silos\" that impede cross-entity collaboration. To overcome this architectural bottleneck, this paper proposes the timely design perspective of \"Memory as a Service\" (MaaS). MaaS advocates decoupling memory from its conventional role as an interaction byproduct and encapsulating it as a modular service that can be independently callable, dynamically composable, and finely governed. At its core, MaaS leverages the duality of memory-its inherently private nature and its potential for public service-to grant memory controlled, on-demand interoperability across entities. This paper introduces a two-dimensional design space defined by entity structure and service type, illustrating how MaaS aligns with current memory practices while naturally extending them to cross-entity collaborative scenarios. Finally, we outline an open research agenda spanning governance, security, and ethical ecosystems, and call upon the broader research community to explore this shift toward service-oriented memory for collaborative agents operating across entity boundaries.",
      "authors": [
        "Haichang Li"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-28T08:33:17+00:00",
          "link": "https://arxiv.org/abs/2506.22815v1",
          "size": "33kb",
          "version": "v1"
        }
      ],
      "title": "Memory as a Service (MaaS): Rethinking Contextual Memory as Service-Oriented Modules for Collaborative Agents",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22815",
        "HTML": "https://arxiv.org/html/2506.22815v1",
        "PDF": "https://arxiv.org/pdf/2506.22815"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper focuses on memory management in agent systems rather than creativity. It proposes a new perspective on memory as a service but does not address creativity in the context of design or analysis."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22841",
      "abstract": "Adjusting transparency is a common method of mitigating occlusion but is often detrimental for understanding the relative depth relationships between objects as well as removes potentially important information from the occluding object. We propose using dichoptic opacity, a novel method for occlusion management that contrasts the transparency of occluders presented to each eye. This allows for better simultaneous understanding of both occluder and occluded. A user study highlights the technique's potential, showing strong user engagement and a clear preference for dichoptic opacity over traditional presentations. While it does not determine optimal transparency values, it reveals promising trends in both percentage and range that merit further investigation.",
      "authors": [
        "George Bell (1)",
        "Alma Cantu (1) ((1) Newcastle University)"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-28T10:41:38+00:00",
          "link": "https://arxiv.org/abs/2506.22841v1",
          "size": "589kb",
          "version": "v1"
        }
      ],
      "title": "Dichoptic Opacity: Managing Occlusion in Stereoscopic Displays via Dichoptic Presentation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22841",
        "HTML": "https://arxiv.org/html/2506.22841v1",
        "PDF": "https://arxiv.org/pdf/2506.22841"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper is concerned with occlusion management in stereoscopic displays. It does not mention creativity or related concepts in its discussion or objectives."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22926",
      "abstract": "Volumetric medical imaging technologies produce detailed 3D representations of anatomical structures. However, effective medical data visualization and exploration pose significant challenges, especially for individuals with limited medical expertise. We introduce a novel XR-based system with two key innovations: (1) a coordinated visualization module integrating Multi-layered Multi-planar Reconstruction with 3D mesh models and (2) a multimodal interaction framework combining hand gestures with LLM-enabled voice commands. We conduct preliminary evaluations, including a 15-participant user study and expert interviews, to demonstrate the system's abilities to enhance spatial understanding and reduce cognitive load. Experimental results show notable improvements in task completion times, usability metrics, and interaction effectiveness enhanced by LLM-driven voice control. While identifying areas for future refinement, our findings highlight the potential of this immersive visualization system to advance medical training and clinical practice. Our demo application and supplemental materials are available for download at: https://osf.io/bpjq5/.",
      "authors": [
        "Qixuan Liu",
        "Shi Qiu",
        "Yinqiao Wang",
        "Xiwen Wu",
        "Kenneth Siu Ho Chok",
        "Chi-Wing Fu",
        "Pheng-Ann Heng"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Graphics (cs.GR)",
        "Multimedia (cs.MM)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-28T15:23:13+00:00",
          "link": "https://arxiv.org/abs/2506.22926v1",
          "size": "5140kb",
          "version": "v1"
        }
      ],
      "title": "Coordinated 2D-3D Visualization of Volumetric Medical Data in XR with Multimodal Interactions",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22926",
        "HTML": "https://arxiv.org/html/2506.22926v1",
        "PDF": "https://arxiv.org/pdf/2506.22926"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper introduces an XR-based system for medical data visualization. The focus is on enhancing spatial understanding and reducing cognitive load, with no mention of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22932",
      "abstract": "The increase of the percentage of elderly population in modern societies dictates the use of emerging technologies as a means of supporting elder members of the society. Within this scope, Extended Reality (XR) technologies pose as a promising technology for improving the daily lives of the elderly population. This paper presents a literature review that describes the most common characteristics of the physical and mental state of the elderly, allowing readers, and specifically XR developers, to understand the main difficulties faced by elderly users of extended reality applications so they can develop accessible, user friendly and engaging applications for the target audience. Furthermore, a review of existing extended reality applications that target the elder population is presented, allowing readers to get acquainted with existing design paradigms that can inspire future developments.",
      "authors": [
        "Zoe Anastasiadou",
        "Andreas Lanitis"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-28T16:04:01+00:00",
          "link": "https://arxiv.org/abs/2506.22932v1",
          "size": "19kb",
          "version": "v1"
        }
      ],
      "title": "Immersive Technologies and Elderly Users: Current use, Limitations and Future Perspectives",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22932",
        "HTML": "https://arxiv.org/html/2506.22932v1",
        "PDF": "https://arxiv.org/pdf/2506.22932"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper reviews XR technology for elderly users, aiming to improve accessibility and usability. It lacks any explicit focus on creativity or its enhancement."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22937",
      "abstract": "Blind and low-vision (BLV) players encounter critical challenges in engaging with video games due to the inaccessibility of visual elements, difficulties in navigating interfaces, and limitations in sending interaction input. Moreover, the development of specialized accessibility features typically requires substantial programming effort and is often implemented on a game-by-game basis. To address these challenges, we introduce \\textit{GamerAstra}, a generalized accessibility framework that leverages a multi-agent design to facilitate access to video games for BLV players. It integrates multi-modal techniques including large language models and vision-language models, enabling interaction with games lacking native accessibility support. The framework further incorporates customizable assistance granularities to support varying degrees of visual impairment and enhances interface navigation through multiple input modalities. The evaluation through technical assessments and user studies indicate that \\textit{GamerAstra} effectively enhances playability and delivers a more immersive gaming experience for BLV players. These findings also underscore potential avenues for advancing intelligent accessibility frameworks in the gaming domain.",
      "authors": [
        "Tianrun Qiu",
        "Changxin Chen",
        "Sizhe Cheng",
        "Yiming Yang",
        "Yixiao Guo",
        "Zhicong Lu",
        "Yuxin Ma"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-28T16:08:08+00:00",
          "link": "https://arxiv.org/abs/2506.22937v1",
          "size": "9679kb",
          "version": "v1"
        }
      ],
      "title": "GamerAstra: Enhancing Video Game Accessibility for Blind and Low-Vision Players through a Multi-Agent AI Framework",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22937",
        "HTML": "https://arxiv.org/html/2506.22937v1",
        "PDF": "https://arxiv.org/pdf/2506.22937"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The focus is on enhancing accessibility in video games for blind and low-vision players, without exploring creativity directly."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22940",
      "abstract": "This paper investigates how collaborative AI systems can enhance user agency in identifying and evaluating misinformation on social media platforms. Traditional methods, such as personal judgment or basic fact-checking, often fall short when faced with emotionally charged or context-deficient content. To address this, we designed and evaluated an interactive interface that integrates collaborative AI features, including real-time explanations, source aggregation, and debate-style interaction. These elements aim to support critical thinking by providing contextual cues and argumentative reasoning in a transparent, user-centered format. In a user study with 14 participants, 79% found the debate mode more effective than standard chatbot interfaces, and the multiple-source view received an average usefulness rating of 4.6 out of 5. Our findings highlight the potential of context-rich, dialogic AI systems to improve media literacy and foster trust in digital information environments. We argue that future tools for misinformation mitigation should prioritize ethical design, explainability, and interactive engagement to empower users in a post-truth era.",
      "authors": [
        "Varun Sangwan",
        "Heidi Makitalo"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Social and Information Networks (cs.SI)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-28T16:13:42+00:00",
          "link": "https://arxiv.org/abs/2506.22940v1",
          "size": "783kb",
          "version": "v1"
        }
      ],
      "title": "Context, Credibility, and Control: User Reflections on AI Assisted Misinformation Tools",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22940",
        "PDF": "https://arxiv.org/pdf/2506.22940"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper focuses on AI tools for misinformation evaluation and media literacy, which does not directly relate to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22941",
      "abstract": "Access to accurate and actionable harm reduction information can directly impact the health outcomes of People Who Use Drugs (PWUD), yet existing online channels often fail to meet their diverse and dynamic needs due to limitations in adaptability, accessibility, and the pervasive impact of stigma. Large Language Models (LLMs) present a novel opportunity to enhance information provision, but their application in such a high-stakes domain is under-explored and presents socio-technical challenges. This paper investigates how LLMs can be responsibly designed to support the information needs of PWUD. Through a qualitative workshop involving diverse stakeholder groups (academics, harm reduction practitioners, and an online community moderator), we explored LLM capabilities, identified potential use cases, and delineated core design considerations. Our findings reveal that while LLMs can address some existing information barriers (e.g., by offering responsive, multilingual, and potentially less stigmatising interactions), their effectiveness is contingent upon overcoming challenges related to ethical alignment with harm reduction principles, nuanced contextual understanding, effective communication, and clearly defined operational boundaries. We articulate design pathways emphasising collaborative co-design with experts and PWUD to develop LLM systems that are helpful, safe, and responsibly governed. This work contributes empirically grounded insights and actionable design considerations for the responsible development of LLMs as supportive tools within the harm reduction ecosystem.",
      "authors": [
        "Kaixuan Wang",
        "Jason T. Jacques",
        "and Chenxin Diao"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-28T16:15:47+00:00",
          "link": "https://arxiv.org/abs/2506.22941v1",
          "size": "4041kb",
          "version": "v1"
        }
      ],
      "title": "Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22941",
        "PDF": "https://arxiv.org/pdf/2506.22941"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "While the paper discusses LLMs and harm reduction, it does not address creativity as a focus or theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22968",
      "abstract": "AI is flattening culture. Evaluations of \"culture\" are showing the myriad ways in which large AI models are homogenizing language and culture, averaging out rich linguistic differences into generic expressions. I call this phenomenon \"softmaxing culture,\" and it is one of the fundamental challenges facing AI evaluations today. Efforts to improve and strengthen evaluations of culture are central to the project of cultural alignment in large AI systems. This position paper argues that machine learning (ML) and human-computer interaction (HCI) approaches to evaluation are limited. I propose two key shifts. First, instead of asking \"what is culture?\" at the start of system evaluations, I propose beginning with the question: \"when is culture?\" Second, while I acknowledge the philosophical claim that cultural universals exist, the challenge is not simply to describe them, but to situate them in relation to their particulars. Taken together, these conceptual shifts invite evaluation approaches that move beyond technical requirements, toward perspectives more responsive to the complexities of culture.",
      "authors": [
        "Daniel Mwesigwa"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-28T17:59:17+00:00",
          "link": "https://arxiv.org/abs/2506.22968v1",
          "size": "13kb",
          "version": "v1"
        }
      ],
      "title": "Against 'softmaxing' culture",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22968",
        "HTML": "https://arxiv.org/html/2506.22968v1",
        "PDF": "https://arxiv.org/pdf/2506.22968"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper addresses cultural evaluation in AI which may have some conceptual overlap with creativity, but it does not specifically focus on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23016",
      "abstract": "The global prevalence of dementia is projected to double by 2050, highlighting the urgent need for scalable diagnostic tools. This study utilizes digital cognitive tasks with eye-tracking data correlated with memory processes to distinguish between Healthy Controls (HC) and Mild Cognitive Impairment (MCI), a precursor to dementia. A deep learning model based on VTNet was trained using eye-tracking data from 44 participants (24 MCI, 20 HCs) who performed a visual memory task. The model utilizes both time series and spatial data derived from eye-tracking. It was modified to incorporate scan paths, heat maps, and image content. These modifications also enabled testing parameters such as image resolution and task performance, analyzing their impact on model performance. The best model, utilizing $700\\times700px$ resolution heatmaps, achieved 68% sensitivity and 76% specificity. Despite operating under more challenging conditions (e.g., smaller dataset size, shorter task duration, or a less standardized task), the model's performance is comparable to an Alzheimer's study using similar methods (70% sensitivity and 73% specificity). These findings contribute to the development of automated diagnostic tools for MCI. Future work should focus on refining the model and using a standardized long-term visual memory task.",
      "authors": [
        "Tom\\'as Silva Santos Rocha",
        "Anastasiia Mikhailova",
        "Moreno I. Coco and Jos\\'e Santos-Victor"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-28T21:20:33+00:00",
          "link": "https://arxiv.org/abs/2506.23016v1",
          "size": "992kb",
          "version": "v1"
        }
      ],
      "title": "Deep Learning in Mild Cognitive Impairment Diagnosis using Eye Movements and Image Content in Visual Memory Tasks",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23016",
        "HTML": "https://arxiv.org/html/2506.23016v1",
        "PDF": "https://arxiv.org/pdf/2506.23016"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "This research focuses on diagnosing mild cognitive impairment through eye movement analysis, without addressing creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23017",
      "abstract": "This paper addresses the critical issue of deceptive design elements prevalent in technology, and their potential impact on children. Recent research highlights the impact of dark patterns on adults and adolescents, while studies involving children are scarce. In an era where children wield greater independence with digital devices, their vulnerability to dark patterns amplifies without early education. Our findings show a significant positive impact of dark pattern education on children's awareness, revealing that heightened awareness considerably alters children's navigation of social media, video games, and streaming platforms. To this end, we developed a gamified application aimed at instructing children on identifying and responding to various dark patterns. Our evaluation results emphasize the critical role of early education in empowering children to recognize and counter deceptive design, thereby cultivating a digitally literate generation capable of making informed choices in the complex landscape of digital technology.",
      "authors": [
        "Noverah Khan",
        "Hira Eiraj Daud",
        "Suleman Shahid"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-28T21:25:40+00:00",
          "link": "https://arxiv.org/abs/2506.23017v1",
          "size": "12849kb",
          "version": "v1"
        }
      ],
      "title": "Mind the Dark: A Gamified Exploration of Deceptive Design Awareness for Children in the Digital Age",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23017",
        "PDF": "https://arxiv.org/pdf/2506.23017"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper focuses on educating children about deceptive design patterns, with no mention of creativity as a theme or objective."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23075",
      "abstract": "Understanding and decoding brain activity from electroencephalography (EEG) signals is a fundamental challenge in neuroscience and AI, with applications in cognition, emotion recognition, diagnosis, and brain-computer interfaces. While recent EEG foundation models advance generalized decoding via unified architectures and large-scale pretraining, they adopt a scale-agnostic dense modeling paradigm inherited from NLP and vision. This design neglects a core property of neural activity: cross-scale spatiotemporal structure. EEG task patterns span a wide range of temporal and spatial scales, from short bursts to slow rhythms, and from localized cortical responses to distributed interactions. Ignoring this diversity leads to suboptimal representations and weak generalization. We propose CSBrain, a Cross-scale Spatiotemporal Brain foundation model for generalized EEG decoding. CSBrain introduces: (i) Cross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale features from localized temporal windows and anatomical brain regions into compact scale-aware tokens; and (ii) Structured Sparse Attention (SSA), which captures cross-window and cross-region dependencies, enhancing scale diversity while removing spurious correlations. CST and SSA are alternately stacked to progressively integrate multi-scale dependencies. Experiments on 11 EEG tasks across 16 datasets show that CSBrain consistently outperforms task-specific and foundation model baselines. These results establish cross-scale modeling as a key inductive bias and position CSBrain as a robust backbone for future brain-AI research.",
      "authors": [
        "Yuchen Zhou",
        "Jiamin Wu",
        "Zichen Ren",
        "Zhouheng Yao",
        "Weiheng Lu",
        "Kunyu Peng",
        "Qihao Zheng",
        "Chunfeng Song",
        "Wanli Ouyang",
        "Chao Gou"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)",
        "Signal Processing (eess.SP)",
        "Neurons and Cognition (q-bio.NC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-29T03:29:34+00:00",
          "link": "https://arxiv.org/abs/2506.23075v1",
          "size": "14483kb",
          "version": "v1"
        }
      ],
      "title": "CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23075",
        "HTML": "https://arxiv.org/html/2506.23075v1",
        "PDF": "https://arxiv.org/pdf/2506.23075"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper deals with EEG decoding technology and spatiotemporal modeling in neuroscience and AI, without addressing creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23116",
      "abstract": "User experience (UX) practices have evolved in stages and are entering a transformative phase (UX 3.0), driven by AI technologies and shifting user needs. Human-centered AI (HCAI) experiences are emerging, necessitating new UX approaches to support UX practices in the AI era. We propose a UX 3.0 paradigm framework to respond and guide UX practices in developing HCAI systems.",
      "authors": [
        "Wei Xu"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-29T06:45:14+00:00",
          "link": "https://arxiv.org/abs/2506.23116v1",
          "size": "406kb",
          "version": "v1"
        }
      ],
      "title": "A User Experience 3.0 (UX 3.0) Paradigm Framework: Designing for Human-Centered AI Experiences",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23116",
        "PDF": "https://arxiv.org/pdf/2506.23116"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "While it discusses user experience in AI settings, the paper does not present creativity as a main or secondary theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23180",
      "abstract": "Improvisation training for actors presents unique challenges, particularly in maintaining narrative coherence and managing cognitive load during performances. Previous research on AI in improvisation performance often predates advances in large language models (LLMs) and relies on human intervention. We introduce ImprovMate, which leverages LLMs as GPTs to automate the generation of narrative stimuli and cues, allowing actors to focus on creativity without keeping track of plot or character continuity. Based on insights from professional improvisers, ImprovMate incorporates exercises that mimic live training, such as abrupt story resolution and reactive thinking exercises, while maintaining coherence via reference tables. By balancing randomness and structured guidance, ImprovMate provides a groundbreaking tool for improv training. Our pilot study revealed that actors might embrace AI techniques if the latter mirrors traditional practices, and appreciate the fresh twist introduced by our approach with the AI-generated cues.",
      "authors": [
        "Riccardo Drago",
        "Yotam Sechayk",
        "Mustafa Doga Dogan",
        "Andrea Sanna",
        "Takeo Igarashi"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-29T10:28:13+00:00",
          "link": "https://arxiv.org/abs/2506.23180v1",
          "size": "1771kb",
          "version": "v1"
        }
      ],
      "title": "ImprovMate: Multimodal AI Assistant for Improv Actor Training",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23180",
        "HTML": "https://arxiv.org/html/2506.23180v1",
        "PDF": "https://arxiv.org/pdf/2506.23180"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper discusses an AI assistant for actor improvisation training, focusing significantly on enhancing and supporting creativity in improvisational practices."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23253",
      "abstract": "We examine \"vibe coding\": an emergent programming paradigm where developers primarily write code by interacting with code-generating large language models rather than writing code directly. We analysed a curated set of videos depicting extended vibe coding sessions with rich think-aloud reflections. Using framework analysis, we investigated programmers' goals, workflows, prompting techniques, debugging approaches, and challenges encountered. We find that vibe coding follows iterative goal satisfaction cycles where developers alternate between prompting AI, evaluating generated code through rapid scanning and application testing, and manual editing. Prompting strategies blend vague, high-level directives with detailed technical specifications. Debugging remains a hybrid process combining AI assistance with manual practices. Critically, vibe coding does not eliminate the need for programming expertise but rather redistributes it toward context management, rapid code evaluation, and decisions about when to transition between AI-driven and manual manipulation of code. Trust in AI tools during vibe coding is dynamic and contextual, developed through iterative verification rather than blanket acceptance. Vibe coding is an evolution of AI-assisted programming that represents an early manifestation of \"material disengagement\", where practitioners orchestrate code production and manipulation, mediated through AI, while maintaining selective and strategic oversight.",
      "authors": [
        "Advait Sarkar",
        "Ian Drosos"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-29T14:19:29+00:00",
          "link": "https://arxiv.org/abs/2506.23253v1",
          "size": "43kb",
          "version": "v1"
        }
      ],
      "title": "Vibe coding: programming through conversation with artificial intelligence",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23253",
        "HTML": "https://arxiv.org/html/2506.23253v1",
        "PDF": "https://arxiv.org/pdf/2506.23253"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "While creativity is not explicitly mentioned, the paper discusses programming paradigms involving AI, which might impact developers' creative workflows and how they use AI to enhance creative problem-solving."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23443",
      "abstract": "Our work aims to develop new assistive technologies that enable blind or low vision (BLV) people to explore and analyze data readily. At present, barriers exist for BLV people to explore and analyze data, restricting access to government, health and personal data, and limiting employment opportunities. This work explores the co-design and development of an innovative system to support data access, with a focus on the use of refreshable tactile displays (RTDs) and conversational agents. The envisaged system will use a combination of tactile graphics and speech to communicate with BLV users, and proactively assist with data analysis tasks. As well as addressing significant equity gaps, our work expects to produce innovations in assistive technology, multimodal interfaces, dialogue systems, and natural language understanding and generation.",
      "authors": [
        "Samuel Reinders",
        "Munazza Zaib",
        "Matthew Butler",
        "Bongshin Lee",
        "Ingrid Zukerman",
        "Lizhen Qu",
        "Kim Marriott"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T00:35:35+00:00",
          "link": "https://arxiv.org/abs/2506.23443v1",
          "size": "1929kb",
          "version": "v1"
        }
      ],
      "title": "Accessible Data Access and Analysis by People who are Blind or Have Low Vision",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23443",
        "HTML": "https://arxiv.org/html/2506.23443v1",
        "PDF": "https://arxiv.org/pdf/2506.23443"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper focuses on assistive technologies for data access for people with visual impairments, with no clear connection to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23457",
      "abstract": "Autonomous personal mobility vehicles (APMVs) are small mobility devices designed for individual automated transportation in shared spaces. In such environments, frequent pedestrian avoidance maneuvers may cause rapid steering adjustments and passive postural responses from passengers, thereby increasing the risk of motion sickness. This study investigated the effects of providing path information on 16 passengers' head movement behavior and motion sickness while riding an APMV. Through a controlled experiment comparing manual driving (MD), autonomous driving without path information (AD w/o path), and autonomous driving with path information (AD w/ path), we found that providing path cues significantly reduced MISC scores and delayed the onset of motion sickness symptoms. In addition, participants were more likely to proactively align their head movements with the direction of vehicle rotation in both MD and AD w/ path conditions. Although a small correlation was observed between the delay in yaw rotation of the passenger's head relative to the vehicle and the occurrence of motion sickness, the underlying physiological mechanism remains to be elucidated.",
      "authors": [
        "Yuya Ide and Hailong Liu and Takahiro Wada"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T01:36:45+00:00",
          "link": "https://arxiv.org/abs/2506.23457v1",
          "size": "3355kb",
          "version": "v1"
        }
      ],
      "title": "Reducing Motion Sickness in Passengers of Autonomous Personal Mobility Vehicles by Presenting a Driving Path",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23457",
        "HTML": "https://arxiv.org/html/2506.23457v1",
        "PDF": "https://arxiv.org/pdf/2506.23457"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The study addresses motion sickness in autonomous vehicles, which is unrelated to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23458",
      "abstract": "Portable and wearable consumer-grade electroencephalography (EEG) devices, like Muse headbands, offer unprecedented mobility for daily brain-computer interface (BCI) applications, including cognitive load detection. However, the exacerbated non-stationarity in portable EEG signals constrains data fidelity and decoding accuracy, creating a fundamental trade-off between portability and performance. To mitigate such limitation, we propose MuseCogNet (Muse-based Cognitive Network), a unified joint learning framework integrating self-supervised and supervised training paradigms. In particular, we introduce an EEG-grounded self-supervised reconstruction loss based on average pooling to capture robust neurophysiological patterns, while cross-entropy loss refines task-specific cognitive discriminants. This joint learning framework resembles the bottom-up and top-down attention in humans, enabling MuseCogNet to significantly outperform state-of-the-art methods on a publicly available Muse dataset and establish an implementable pathway for neurocognitive monitoring in ecological settings.",
      "authors": [
        "Xiaoxiao Yang",
        "Chan Feng",
        "Jiancheng Chen"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T01:42:31+00:00",
          "link": "https://arxiv.org/abs/2506.23458v1",
          "size": "73kb",
          "version": "v1"
        }
      ],
      "title": "Neuro-Informed Joint Learning Enhances Cognitive Workload Decoding in Portable BCIs",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23458",
        "HTML": "https://arxiv.org/html/2506.23458v1",
        "PDF": "https://arxiv.org/pdf/2506.23458"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper is about improving cognitive workload detection using portable EEG devices and does not address creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23545",
      "abstract": "Virtual, Augmented, and eXtended Reality (VR/AR/XR) technologies are increasingly recognized for their applications in training, diagnostics, and psychological research, particularly in high-risk and highly regulated environments. In this panel we discuss how immersive systems enhance human performance across multiple domains, including clinical psychology, space exploration, and medical education. In psychological research and training, XR can offer a controlled yet ecologically valid setting for measuring cognitive and affective processes. In space exploration, we discuss the development of VR-based astronaut training and diagnostic systems, allowing astronauts to perform real-time health assessments. In medical education and rehabilitation, we cover procedural training and patient engagement. From virtual surgical simulations to gamified rehabilitation exercises, immersive environments enhance both learning outcomes and treatment adherence.",
      "authors": [
        "Barbara Karpowicz",
        "Maciej Grzeszczuk",
        "Adam Kuzdrali\\'nski",
        "Monika Kornacka",
        "Aliaksandr Marozau",
        "Wiktor Stawski",
        "Pavlo Zinevych",
        "Grzegorz Marcin W\\'ojcik",
        "Tomasz Kowalewski",
        "Grzegorz Pochwatko",
        "Wies{\\l}aw Kope\\'c"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computational Engineering, Finance, and Science (cs.CE)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T06:31:38+00:00",
          "link": "https://arxiv.org/abs/2506.23545v1",
          "size": "379kb",
          "version": "v1"
        }
      ],
      "title": "Immersive Technologies in Training and Healthcare: From Space Missions to Psychophysiological Research",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23545",
        "HTML": "https://arxiv.org/html/2506.23545v1",
        "PDF": "https://arxiv.org/pdf/2506.23545"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses the application of immersive technologies in various fields. Although not focused on creativity, XR's potential for enhancing training and rehabilitation could indirectly relate to creative approaches in these domains."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23678",
      "abstract": "The output quality of large language models (LLMs) can be improved via \"reasoning\": generating segments of chain-of-thought (CoT) content to further condition the model prior to producing user-facing output. While these chains contain valuable information, they are verbose and lack explicit organization, making them tedious to review. Moreover, they lack opportunities for user feedback, such as to remove unwanted considerations, add desired ones, or clarify unclear assumptions. We introduce Interactive Reasoning, an interaction design that visualizes chain-of-thought outputs as a hierarchy of topics and enables user review and modification. We implement interactive reasoning in Hippo, a prototype for AI-assisted decision making in the face of uncertain trade-offs. In a user study with 16 participants, we find that interactive reasoning in Hippo allows users to quickly identify and interrupt erroneous generations, efficiently steer the model towards customized responses, and better understand both model reasoning and model outputs. Our work contributes to a new paradigm that incorporates user oversight into LLM reasoning processes.",
      "authors": [
        "Rock Yuren Pang",
        "K. J. Kevin Feng",
        "Shangbin Feng",
        "Chu Li",
        "Weijia Shi",
        "Yulia Tsvetkov",
        "Jeffrey Heer",
        "Katharina Reinecke"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T10:00:43+00:00",
          "link": "https://arxiv.org/abs/2506.23678v1",
          "size": "4499kb",
          "version": "v1"
        }
      ],
      "title": "Interactive Reasoning: Visualizing and Controlling Chain-of-Thought Reasoning in Large Language Models",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23678",
        "HTML": "https://arxiv.org/html/2506.23678v1",
        "PDF": "https://arxiv.org/pdf/2506.23678"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "This paper introduces an interactive system for reasoning with language models, allowing user input for modifying output. The focus on interactive reasoning can relate to creativity, particularly in generating customized responses, but creativity is not the primary focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23694",
      "abstract": "The process of requirements analysis requires an understanding of the end users of a system. Thus, expert stakeholders, such as User Experience (UX) designers, usually create various descriptions containing information about the users and their possible needs. In our paper, we investigate to what extent UX novices are able to write such descriptions into user scenarios. We conducted a user study with 60 participants consisting of 30 UX experts and 30 novices who were asked to write a user scenario with or without the help of an LLM-supported writing assistant. Our findings show that LLMs empower laypersons to write reasonable user scenarios and provide first-hand insights for requirements analysis that are comparable to UX experts in terms of structure and clarity, while especially excelling at audience-orientation. We present our qualitative and quantitative findings, including user scenario anatomies, potential influences, and differences in the way participants approached the task.",
      "authors": [
        "Patrick Stadler",
        "Christopher Lazik",
        "Christopher Katins",
        "and Thomas Kosch"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T10:15:44+00:00",
          "link": "https://arxiv.org/abs/2506.23694v1",
          "size": "1020kb",
          "version": "v1"
        }
      ],
      "title": "If You Had to Pitch Your Ideal Software -- Evaluating Large Language Models to Support User Scenario Writing for User Experience Experts and Laypersons",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23694",
        "HTML": "https://arxiv.org/html/2506.23694v1",
        "PDF": "https://arxiv.org/pdf/2506.23694"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper investigates user scenario writing with support from language models, empowering novices to achieve results similar to UX experts. It touches on creativity in terms of scenario writing and user experience, but creativity is not the main theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23815",
      "abstract": "The influence of Artificial Intelligence (AI), and specifically Large Language Models (LLM), on education is continuously increasing. These models are frequently used by students, giving rise to the question whether current forms of assessment are still a valid way to evaluate student performance and comprehension. The theoretical framework developed in this paper is grounded in Constructive Alignment (CA) theory and Bloom's taxonomy for defining learning objectives. We argue that AI influences learning objectives of different Bloom levels in a different way, and assessment has to be adopted accordingly. Furthermore, in line with Bloom's vision, formative and summative assessment should be aligned on whether the use of AI is permitted or not.\n  Although lecturers tend to agree that education and assessment need to be adapted to the presence of AI, a strong bias exists on the extent to which lecturers want to allow for AI in assessment. This bias is caused by a lecturer's familiarity with AI and specifically whether they use it themselves. To avoid this bias, we propose structured guidelines on a university or faculty level, to foster alignment among the staff. Besides that, we argue that teaching staff should be trained on the capabilities and limitations of AI tools. In this way, they are better able to adapt their assessment methods.",
      "authors": [
        "Patrick Stokkink"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T13:02:01+00:00",
          "link": "https://arxiv.org/abs/2506.23815v1",
          "size": "241kb",
          "version": "v1"
        }
      ],
      "title": "The Impact of AI on Educational Assessment: A Framework for Constructive Alignment",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23815",
        "PDF": "https://arxiv.org/pdf/2506.23815"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "This study examines AI's impact on education and assessments, focusing on aligning assessment strategies with AI capabilities. Creativity is not a primary or secondary theme in the discussion or objectives of the paper."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23850",
      "abstract": "This paper introduces a novel architectural framework that integrates Large Language Models (LLMs) with email interfaces to automate administrative tasks, specifically targeting accessibility barriers in enterprise environments. The system connects email communication channels with Optical Character Recognition (OCR) and intelligent automation, enabling non-technical administrative staff to delegate complex form-filling and document processing tasks using familiar email interfaces. By treating the email body as a natural language prompt and attachments as contextual information, the workflow bridges the gap between advanced AI capabilities and practical usability. Empirical evaluation shows that the system can complete complex administrative forms in under 8 seconds of automated processing, with human supervision reducing total staff time by a factor of three to four compared to manual workflows. The top-performing LLM accurately filled 16 out of 29 form fields and reduced the total cost per processed form by 64% relative to manual completion. These findings demonstrate that email-based LLM integration is a viable and cost-effective approach for democratizing advanced automation in organizational settings, supporting widespread adoption without requiring specialized technical knowledge or major workflow changes. This aligns with broader trends in leveraging LLMs to enhance accessibility and automate complex tasks for non-technical users, making technology more inclusive and efficient.",
      "authors": [
        "Andres Navarro",
        "Carlos de Quinto",
        "Jos\\'e Alberto Hern\\'andez"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T13:39:54+00:00",
          "link": "https://arxiv.org/abs/2506.23850v1",
          "size": "2520kb",
          "version": "v1"
        }
      ],
      "title": "Email as the Interface to Generative AI Models: Seamless Administrative Automation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23850",
        "HTML": "https://arxiv.org/html/2506.23850v1",
        "PDF": "https://arxiv.org/pdf/2506.23850"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "This paper focuses on automating administrative tasks using LLMs and email interfaces, emphasizing efficiency and accessibility. Creativity is not mentioned, as the application is more about practical utility and automation rather than creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23952",
      "abstract": "AI systems increasingly support human decision-making across domains of professional, skill-based, and personal activity. While previous work has examined how AI might affect human autonomy globally, the effects of AI on domain-specific autonomy -- the capacity for self-governed action within defined realms of skill or expertise -- remain understudied. We analyze how AI decision-support systems affect two key components of domain-specific autonomy: skilled competence (the ability to make informed judgments within one's domain) and authentic value-formation (the capacity to form genuine domain-relevant values and preferences). By engaging with prior investigations and analyzing empirical cases across medical, financial, and educational domains, we demonstrate how the absence of reliable failure indicators and the potential for unconscious value shifts can erode domain-specific autonomy both immediately and over time. We then develop a constructive framework for autonomy-preserving AI support systems. We propose specific socio-technical design patterns -- including careful role specification, implementation of defeater mechanisms, and support for reflective practice -- that can help maintain domain-specific autonomy while leveraging AI capabilities. This framework provides concrete guidance for developing AI systems that enhance rather than diminish human agency within specialized domains of action.",
      "authors": [
        "Stefan Buijsman",
        "Sarah Carter",
        "Juan Pablo Berm\\'udez"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)",
        "General Economics (econ.GN)",
        "Economics (q-fin.EC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T15:20:10+00:00",
          "link": "https://arxiv.org/abs/2506.23952v1",
          "size": "457kb",
          "version": "v1"
        }
      ],
      "title": "Autonomy by Design: Preserving Human Autonomy in AI Decision-Support",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23952",
        "PDF": "https://arxiv.org/pdf/2506.23952"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The study is about AI's impact on human autonomy in decision-making across various domains, not on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.24057",
      "abstract": "The popularity of accessibility research has grown recently, improving digital inclusion for people with disabilities. However, researchers, including those who have disabilities, have attempted to include people with disabilities in all aspects of design, and they have identified a myriad of practical accessibility barriers posed by tools and methods leveraged by human-computer interaction (HCI) researchers during prototyping. To build a more inclusive technological landscape, we must question the effectiveness of existing prototyping tools and methods, repurpose/retrofit existing resources, and build new tools and methods to support the participation of both researchers and people with disabilities within the prototyping design process of novel technologies. This full-day workshop at CHI 2025 will provide a platform for HCI researchers, designers, and practitioners to discuss barriers and opportunities for creating accessible prototyping and promote hands-on ideation and fabrication exercises aimed at futuring accessible prototyping.",
      "authors": [
        "Patricia Piedade",
        "Peter A Hayton",
        "Cynthia Bennett",
        "Anna R L Carter",
        "Clara Crivellaro",
        "Alan Dix",
        "Jess McGowan",
        "Katta Spiel",
        "Miriam Sturdee",
        "Garreth W. Tigwell",
        "Hugo Nicolau"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T17:03:19+00:00",
          "link": "https://arxiv.org/abs/2506.24057v1",
          "size": "81kb",
          "version": "v1"
        }
      ],
      "title": "Access InContext: Futuring Accessible Prototyping Tools and Methods",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.24057",
        "HTML": "https://arxiv.org/html/2506.24057v1",
        "PDF": "https://arxiv.org/pdf/2506.24057"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The workshop discusses creating accessible prototyping tools and methods aimed at inclusive design, where creativity in prototyping may be an underlying theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.24104",
      "abstract": "Digital twins (DT) are increasingly used in healthcare to model patients, processes, and physiological systems. While recent solutions leverage visualization, visual analytics, and user interaction, these systems rarely incorporate structured service design methodologies. Bridging service design with visual analytics and visualization can be valuable for the healthcare DT community. This paper aims to introduce the service design discipline to visualization researchers by framing this integration gap and suggesting research directions to enhance the real-world applicability of DT solutions.",
      "authors": [
        "Mariia Ershova and Graziano Blasilli"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T17:53:08+00:00",
          "link": "https://arxiv.org/abs/2506.24104v1",
          "size": "56kb",
          "version": "v1"
        }
      ],
      "title": "Bridging Service Design, Visualizations, and Visual Analytics in Healthcare Digital Twins: Challenges, Gaps, and Research Opportunities",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.24104",
        "HTML": "https://arxiv.org/html/2506.24104v1",
        "PDF": "https://arxiv.org/pdf/2506.24104"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper focuses on digital twins in healthcare and the integration of service design, visualization, and visual analytics. Creativity is not a primary or secondary theme in the content presented."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22443",
      "abstract": "Rule-based models offer interpretability but struggle with complex data, while deep neural networks excel in performance yet lack transparency. This work investigates a neuro-symbolic rule learning neural network named RL-Net that learns interpretable rule lists through neural optimization, applied for the first time to radar-based hand gesture recognition (HGR). We benchmark RL-Net against a fully transparent rule-based system (MIRA) and an explainable black-box model (XentricAI), evaluating accuracy, interpretability, and user adaptability via transfer learning. Our results show that RL-Net achieves a favorable trade-off, maintaining strong performance (93.03% F1) while significantly reducing rule complexity. We identify optimization challenges specific to rule pruning and hierarchy bias and propose stability-enhancing modifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical middle ground between transparency and performance. This study highlights the real-world feasibility of neuro-symbolic models for interpretable HGR and offers insights for extending explainable AI to edge-deployable sensing systems.",
      "authors": [
        "Sarah Seifi",
        "Tobias Sukianto",
        "Cecilia Carbonelli",
        "Lorenzo Servadei",
        "Robert Wille"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-11T11:30:48+00:00",
          "link": "https://arxiv.org/abs/2506.22443v1",
          "size": "371kb",
          "version": "v1"
        }
      ],
      "title": "Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22443",
        "HTML": "https://arxiv.org/html/2506.22443v1",
        "PDF": "https://arxiv.org/pdf/2506.22443"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper addresses neuro-symbolic AI for hand gesture recognition and interpretability issues in AI. It does not engage with creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22462",
      "abstract": "Fall detection is critical to support the growing elderly population, projected to reach 2.1 billion by 2050. However, existing methods often face data scarcity challenges or compromise privacy. We propose a novel IoT-based Fall Detection as a Service (FDaaS) framework to assist the elderly in living independently and safely by accurately detecting falls. We design a service-oriented architecture that leverages Ultra-wideband (UWB) radar sensors as an IoT health-sensing service, ensuring privacy and minimal intrusion. We address the challenges of data scarcity by utilizing a Fall Detection Generative Pre-trained Transformer (FD-GPT) that uses augmentation techniques. We developed a protocol to collect a comprehensive dataset of the elderly daily activities and fall events. This resulted in a real dataset that carefully mimics the elderly's routine. We rigorously evaluate and compare various models using this dataset. Experimental results show our approach achieves 90.72% accuracy and 89.33% precision in distinguishing between fall events and regular activities of daily living.",
      "authors": [
        "Abdallah Lakhdari",
        "Jiajie Li",
        "Amani Abusafia",
        "Athman Bouguettaya"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Signal Processing (eess.SP)",
        "Artificial Intelligence (cs.AI)",
        "Computers and Society (cs.CY)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-18T03:28:07+00:00",
          "link": "https://arxiv.org/abs/2506.22462v1",
          "size": "7575kb",
          "version": "v1"
        }
      ],
      "title": "Privacy-aware IoT Fall Detection Services For Aging in Place",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22462",
        "HTML": "https://arxiv.org/html/2506.22462v1",
        "PDF": "https://arxiv.org/pdf/2506.22462"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper focuses on fall detection in aging populations using IoT technology. It deals with privacy and data challenges and does not mention or relate to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22464",
      "abstract": "This paper presents a novel localization algorithm for wireless sensor networks (WSNs) called Golden Ratio Localization (GRL), which leverages the mathematical properties of the golden ratio (phi 1.618) to optimize both node placement and communication range. GRL introduces phi-based anchor node deployment and hop-sensitive weighting using phi-exponents to improve localization accuracy while minimizing energy consumption. Through extensive simulations conducted on a 100 m * 100 m sensor field with 100 nodes and 10 anchors, GRL achieved an average localization error of 2.35 meters, outperforming DV- Hop (3.87 meters) and Centroid (4.95 meters). In terms of energy efficiency, GRL reduced localization energy consumption to 1.12 microJ per node, compared to 1.78 microJ for DV-Hop and 1.45 microJ for Centroid. These results confirm that GRL provides a more balanced and efficient localization approach, making it especially suitable for energy-constrained and large-scale WSN deployments.",
      "authors": [
        "Hitesh Mohapatra"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Networking and Internet Architecture (cs.NI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-18T04:26:16+00:00",
          "link": "https://arxiv.org/abs/2506.22464v1",
          "size": "688kb",
          "version": "v1"
        }
      ],
      "title": "Golden Ratio Assisted Localization for Wireless Sensor Network",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22464",
        "HTML": "https://arxiv.org/html/2506.22464v1",
        "PDF": "https://arxiv.org/pdf/2506.22464"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The research presents a novel localization algorithm for wireless sensor networks, emphasizing mathematical modeling and energy efficiency without involving creativity as a subject."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22476",
      "abstract": "Objective skill assessment in high-stakes procedural environments requires models that not only decode underlying cognitive and motor processes but also generalize across tasks, individuals, and experimental contexts. While prior work has demonstrated the potential of functional near-infrared spectroscopy (fNIRS) for evaluating cognitive-motor performance, existing approaches are often task-specific, rely on extensive preprocessing, and lack robustness to new procedures or conditions. Here, we introduce an interpretable transformer-based foundation model trained on minimally processed fNIRS signals for cross-procedural skill assessment. Pretrained using self-supervised learning on data from laparoscopic surgical tasks and endotracheal intubation (ETI), the model achieves greater than 88% classification accuracy on all tasks, with Matthews Correlation Coefficient exceeding 0.91 on ETI. It generalizes to a novel emergency airway procedure--cricothyrotomy--using fewer than 30 labeled samples and a lightweight (less than 2k parameter) adapter module, attaining an AUC greater than 87%. Interpretability is achieved via a novel channel attention mechanism--developed specifically for fNIRS--that identifies functionally coherent prefrontal sub-networks validated through ablation studies. Temporal attention patterns align with task-critical phases and capture stress-induced changes in neural variability, offering insight into dynamic cognitive states.",
      "authors": [
        "A. Subedi",
        "S. De",
        "L. Cavuoto",
        "S. Schwaitzberg",
        "M. Hackett",
        "J. Norfleet"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Signal Processing (eess.SP)",
        "Emerging Technologies (cs.ET)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)",
        "Neurons and Cognition (q-bio.NC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-21T18:30:58+00:00",
          "link": "https://arxiv.org/abs/2506.22476v1",
          "size": "1075kb",
          "version": "v1"
        }
      ],
      "title": "An Interpretable Transformer-Based Foundation Model for Cross-Procedural Skill Assessment Using Raw fNIRS Signals",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22476",
        "PDF": "https://arxiv.org/pdf/2506.22476"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "While the paper introduces advanced skill assessment methods using fNIRS signals, it is centered around procedural skill assessment and interpretability without linking to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22604",
      "abstract": "Robot end users increasingly require accessible means of specifying tasks for robots to perform. Two common end-user programming paradigms include drag-and-drop interfaces and natural language programming. Although natural language interfaces harness an intuitive form of human communication, drag-and-drop interfaces enable users to meticulously and precisely dictate the key actions of the robot's task. In this paper, we investigate the degree to which both approaches can be combined. Specifically, we construct a large language model (LLM)-based pipeline that accepts natural language as input and produces human-like action sequences as output, specified at a level of granularity that a human would produce. We then compare these generated action sequences to another dataset of hand-specified action sequences. Although our results reveal that larger models tend to outperform smaller ones in the production of human-like action sequences, smaller models nonetheless achieve satisfactory performance.",
      "authors": [
        "David Porfirio",
        "Vincent Hsiao",
        "Morgan Fine-Morris",
        "Leslie Smith",
        "and Laura M. Hiatt"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-27T20:00:51+00:00",
          "link": "https://arxiv.org/abs/2506.22604v1",
          "size": "285kb",
          "version": "v1"
        }
      ],
      "title": "Bootstrapping Human-Like Planning via LLMs",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22604",
        "HTML": "https://arxiv.org/html/2506.22604v1",
        "PDF": "https://arxiv.org/pdf/2506.22604"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The study centers on improving human-like planning in robots using large language models and does not engage with creativity as a concept or goal."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22803",
      "abstract": "Recent advances in deep learning have led to increasingly complex models with deeper layers and more parameters, reducing interpretability and making their decisions harder to understand. While many methods explain black-box reasoning, most lack effective interventions or only operate at sample-level without modifying the model itself. To address this, we propose the Concept Bottleneck Model for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU). CBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable framework to approximate black-box reasoning and communicate conceptual understanding. Detrimental concepts are automatically identified and refined (removed/replaced) based on global gradient contributions. The modified CBM then distills corrected knowledge back into the black-box model, enhancing both interpretability and accuracy. We evaluate CBM-HNMU on various CNN and transformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft, and CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum increase in average accuracy across 1.03%. Source code is available at: https://github.com/XiGuaBo/CBM-HNMU.",
      "authors": [
        "Nuoye Xiong",
        "Anqi Dong",
        "Ning Wang",
        "Cong Hua",
        "Guangming Zhu",
        "Mei Lin",
        "Peiyi Shen",
        "Liang Zhang"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-28T08:11:29+00:00",
          "link": "https://arxiv.org/abs/2506.22803v1",
          "size": "34443kb",
          "version": "v1"
        }
      ],
      "title": "Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22803",
        "HTML": "https://arxiv.org/html/2506.22803v1",
        "PDF": "https://arxiv.org/pdf/2506.22803"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The research presents a model to enhance interpretability in neural networks. It does not address creativity, focusing instead on improving AI performance and understanding."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22893",
      "abstract": "After a very long winter, the Artificial Intelligence (AI) spring is here. Or, so it seems over the last three years. AI has the potential to impact many areas of human life - personal, social, health, education, professional. In this paper, we take a closer look at the potential of AI for Enterprises, where decision-making plays a crucial and repeated role across functions, tasks, and operations. We consider Agents imbued with AI as means to increase decision-productivity of enterprises. We highlight six tenets for Agentic success in enterprises, by drawing attention to what the current, AI-Centric User paradigm misses, in the face of persistent needs of and usefulness for Enterprise Decision-Making. In underscoring a shift to User-Centric AI, we offer six tenets and promote market mechanisms for platforms, aligning the design of AI and its delivery by Agents to the cause of enterprise users.",
      "authors": [
        "Arpit Narechania and Alex Endert and Atanu R Sinha"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-28T14:05:59+00:00",
          "link": "https://arxiv.org/abs/2506.22893v1",
          "size": "481kb",
          "version": "v1"
        }
      ],
      "title": "Agentic Enterprise: AI-Centric User to User-Centric AI",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22893",
        "HTML": "https://arxiv.org/html/2506.22893v1",
        "PDF": "https://arxiv.org/pdf/2506.22893"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "This paper discusses AI's implementation in enterprise decision-making and emphasizes user-centric AI. Creativity is not mentioned, nor is it relevant to the discussion."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23092",
      "abstract": "Many scientific and engineering problems involving multi-physics span a wide range of scales. Understanding the interactions across these scales is essential for fully comprehending such complex problems. However, visualizing multivariate, multiscale data within an integrated view where correlations across space, scales, and fields are easily perceived remains challenging. To address this, we introduce a novel local spatial statistical visualization of flow fields across multiple fields and turbulence scales. Our method leverages the curvelet transform for scale decomposition of fields of interest, a level-set-restricted centroidal Voronoi tessellation to partition the spatial domain into local regions for statistical aggregation, and a set of glyph designs that combines information across scales and fields into a single, or reduced set of perceivable visual representations. Each glyph represents data aggregated within a Voronoi region and is positioned at the Voronoi site for direct visualization in a 3D view centered around flow features of interest. We implement and integrate our method into an interactive visualization system where the glyph-based technique operates in tandem with linked 3D spatial views and 2D statistical views, supporting a holistic analysis. We demonstrate with case studies visualizing turbulent combustion data--multi-scalar compressible flows--and turbulent incompressible channel flow data. This new capability enables scientists to better understand the interactions between multiple fields and length scales in turbulent flows.",
      "authors": [
        "Arisa Cowe",
        "Tyson Neuroth",
        "Qi Wu",
        "Martin Rieth",
        "Jacqueline Chen",
        "Myoungkyu Lee",
        "and Kwan-Liu Ma"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Graphics (cs.GR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-29T05:13:13+00:00",
          "link": "https://arxiv.org/abs/2506.23092v1",
          "size": "23477kb",
          "version": "v1"
        }
      ],
      "title": "Glyph-Based Multiscale Visualization of Turbulent Multi-Physics Statistics",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23092",
        "HTML": "https://arxiv.org/html/2506.23092v1",
        "PDF": "https://arxiv.org/pdf/2506.23092"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper offers techniques for visualizing complex multi-physics data, focusing on technical visualization methods rather than creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23549",
      "abstract": "Effective coordination among artificial agents in dynamic and uncertain environments remains a significant challenge in multi-agent systems. Existing approaches, such as self-play and population-based methods, either generalize poorly to unseen partners or require extensive training. To overcome these limitations, we propose Coordination Transformers (CooT), a novel in-context coordination framework that uses recent interaction histories to adapt to unseen partners rapidly. Unlike previous approaches that primarily aim to increase the diversity of training partners, CooT explicitly focuses on adapting to new partner behaviors by predicting actions aligned with observed partner interactions. Trained on interaction trajectories collected from diverse pairs of agents with complementary behaviors, CooT quickly learns effective coordination strategies without explicit supervision or fine-tuning. Evaluations on the Overcooked benchmark demonstrate that CooT significantly outperforms baseline methods in coordination tasks involving previously unseen partners. Human evaluations further confirm CooT as the most effective collaborative partner, while extensive ablations highlight its robustness, flexibility, and sensitivity to context in multi-agent scenarios.",
      "authors": [
        "Huai-Chih Wang",
        "Hsiang-Chun Chuang",
        "Hsi-Chun Cheng",
        "Dai-Jie Wu",
        "Shao-Hua Sun"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T06:45:39+00:00",
          "link": "https://arxiv.org/abs/2506.23549v1",
          "size": "3127kb",
          "version": "v1"
        }
      ],
      "title": "CooT: Learning to Coordinate In-Context with Coordination Transformers",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23549",
        "HTML": "https://arxiv.org/html/2506.23549v1",
        "PDF": "https://arxiv.org/pdf/2506.23549"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper focuses on coordination among artificial agents using a new framework called Coordination Transformers. It does not address creativity as a topic."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23682",
      "abstract": "A digital security-by-design computer architecture, like CHERI, lets you program without fear of buffer overflows or other memory safety errors, but CHERI also rewrites some of the assumptions about how C works and how fundamental types (such as pointers) are implemented in hardware. We conducted a usability study to examine how developers react to the changes required by CHERI when porting software to run on it. We find that developers struggle with CHERI's display of warnings and errors and a lack of diverse documentation.",
      "authors": [
        "Maysara Alhindi",
        "Joseph Hallett"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Hardware Architecture (cs.AR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T10:04:14+00:00",
          "link": "https://arxiv.org/abs/2506.23682v1",
          "size": "25kb",
          "version": "v1"
        }
      ],
      "title": "Not quite a piece of CHERI-cake: Are new digital security by design architectures usable?",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23682",
        "HTML": "https://arxiv.org/html/2506.23682v1",
        "PDF": "https://arxiv.org/pdf/2506.23682"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper examines the usability of a digital security architecture, specifically CHERI. It deals with developers' reactions to security design architecture changes, not creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23721",
      "abstract": "Ultrasound (US) is widely accessible and radiation-free but has a steep learning curve due to its dynamic nature and non-standard imaging planes. Additionally, the constant need to shift focus between the US screen and the patient poses a challenge. To address these issues, we integrate deep learning (DL)-based semantic segmentation for real-time (RT) automated kidney volumetric measurements, which are essential for clinical assessment but are traditionally time-consuming and prone to fatigue. This automation allows clinicians to concentrate on image interpretation rather than manual measurements. Complementing DL, augmented reality (AR) enhances the usability of US by projecting the display directly into the clinician's field of view, improving ergonomics and reducing the cognitive load associated with screen-to-patient transitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one streams directly via the application programming interface for a wireless setup, while the other supports any US device with video output for broader accessibility. We evaluate RT feasibility and accuracy using the Open Kidney Dataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with MedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model implementations, measurement algorithms, and a Wi-Fi-based streaming solution, enhancing US training and diagnostics, especially in point-of-care settings.",
      "authors": [
        "Gijs Luijten",
        "Roberto Maria Scardigno",
        "Lisle Faray de Paiva",
        "Peter Hoyer",
        "Jens Kleesiek",
        "Domenico Buongiorno",
        "Vitoantonio Bevilacqua",
        "Jan Egger"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Image and Video Processing (eess.IV)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T10:49:54+00:00",
          "link": "https://arxiv.org/abs/2506.23721v1",
          "size": "1835kb",
          "version": "v1"
        }
      ],
      "title": "Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23721",
        "HTML": "https://arxiv.org/html/2506.23721v1",
        "PDF": "https://arxiv.org/pdf/2506.23721"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "This research discusses an augmented reality and deep learning system for kidney imaging and ultrasound. It focuses on technical aspects of medical imaging, not creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23739",
      "abstract": "Ensuring safe and realistic interactions between automated driving systems and vulnerable road users (VRUs) in urban environments requires advanced testing methodologies. This paper presents a test environment that combines a Vehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the feasibility of cyber-physical (CP) testing of vehicle-pedestrian and vehicle-cyclist interactions. Building upon previous work focused on pedestrian localization, we further validate a human pose estimation (HPE) approach through a comparative analysis of real-world (RW) and virtual representations of VRUs. The study examines the perception of full-body motion using a commercial monocular camera-based 3Dskeletal detection AI. The virtual scene is generated in Unreal Engine 5, where VRUs are animated in real time and projected onto a screen to stimulate the camera. The proposed stimulation technique ensures the correct perspective, enabling realistic vehicle perception. To assess the accuracy and consistency of HPE across RW and CP domains, we analyze the reliability of detections as well as variations in movement trajectories and joint estimation stability. The validation includes dynamic test scenarios where human avatars, both walking and cycling, are monitored under controlled conditions. Our results show a strong alignment in HPE between RW and CP test conditions for stable motion patterns, while notable inaccuracies persist under dynamic movements and occlusions, particularly for complex cyclist postures. These findings contribute to refining CP testing approaches for evaluating next-generation AI-based vehicle perception and to enhancing interaction models of automated vehicles and VRUs in CP environments.",
      "authors": [
        "Lisa Marie Otto",
        "Michael Kaiser",
        "Daniel Seebacher",
        "Steffen M\\\"uller"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Computational Engineering, Finance, and Science (cs.CE)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T11:27:22+00:00",
          "link": "https://arxiv.org/abs/2506.23739v1",
          "size": "829kb",
          "version": "v1"
        }
      ],
      "title": "Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23739",
        "PDF": "https://arxiv.org/pdf/2506.23739"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "This paper focuses on the validation of AI-based 3D human pose estimation in a cyber-physical environment, specifically for vehicle and VRU interactions. It does not address creativity, as it is primarily about technical testing and validation processes in AI-driven vehicle perception."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23774",
      "abstract": "Computer-aided teacher training is a state-of-the-art method designed to enhance teachers' professional skills effectively while minimising concerns related to costs, time constraints, and geographical limitations. We investigate the potential of large language models (LLMs) in teacher education, using a case of teaching hate incidents management in schools. To this end, we create a multi-agent LLM-based system that mimics realistic situations of hate, using a combination of retrieval-augmented prompting and persona modelling. It is designed to identify and analyse hate speech patterns, predict potential escalation, and propose effective intervention strategies. By integrating persona modelling with agentic LLMs, we create contextually diverse simulations of hate incidents, mimicking real-life situations. The system allows teachers to analyse and understand the dynamics of hate incidents in a safe and controlled environment, providing valuable insights and practical knowledge to manage such situations confidently in real life. Our pilot evaluation demonstrates teachers' enhanced understanding of the nature of annotator disagreements and the role of context in hate speech interpretation, leading to the development of more informed and effective strategies for addressing hate in classroom settings.",
      "authors": [
        "Ewelina Gajewska",
        "Michal Wawer",
        "Katarzyna Budzynska",
        "Jaros{\\l}aw A. Chudziak"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T12:18:13+00:00",
          "link": "https://arxiv.org/abs/2506.23774v1",
          "size": "483kb",
          "version": "v1"
        }
      ],
      "title": "Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate Incidents Management",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23774",
        "HTML": "https://arxiv.org/html/2506.23774v1",
        "PDF": "https://arxiv.org/pdf/2506.23774"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper discusses the use of LLMs for educational purposes, specifically for managing hate incidents. While it involves some form of simulation, it focuses on educational strategy and teacher training, not on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23826",
      "abstract": "Human Digital Twins (HDTs) have traditionally been conceptualized as data-driven models designed to support decision-making across various domains. However, recent advancements in conversational AI open new possibilities for HDTs to function as authentic, interactive digital counterparts of individuals. This paper introduces a novel HDT system architecture that integrates large language models with dynamically updated personal data, enabling it to mirror an individual's conversational style, memories, and behaviors. To achieve this, our approach implements context-aware memory retrieval, neural plasticity-inspired consolidation, and adaptive learning mechanisms, creating a more natural and evolving digital persona. The resulting system does not only replicate an individual's unique conversational style depending on who they are speaking with, but also enriches responses with dynamically captured personal experiences, opinions, and memories. While this marks a significant step toward developing authentic virtual counterparts, it also raises critical ethical concerns regarding privacy, accountability, and the long-term implications of persistent digital identities. This study contributes to the field of HDTs by describing our novel system architecture, demonstrating its capabilities, and discussing future directions and emerging challenges to ensure the responsible and ethical development of HDTs.",
      "authors": [
        "Llu\\'is C. Coll",
        "Martin W. Lauer-Schmaltz",
        "Philip Cash",
        "John P. Hansen",
        "Anja Maier"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Emerging Technologies (cs.ET)",
        "Artificial Intelligence (cs.AI)",
        "Computers and Society (cs.CY)",
        "Human-Computer Interaction (cs.HC)",
        "Information Retrieval (cs.IR)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T13:18:31+00:00",
          "link": "https://arxiv.org/abs/2506.23826v1",
          "size": "1963kb",
          "version": "v1"
        }
      ],
      "title": "Towards the \"Digital Me\": A vision of authentic Conversational Agents powered by personal Human Digital Twins",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23826",
        "HTML": "https://arxiv.org/html/2506.23826v1",
        "PDF": "https://arxiv.org/pdf/2506.23826"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper presents a novel system architecture for Human Digital Twins using conversational AI. While creativity is not the main focus, the system's capacity for simulating personal style and interactions hints at creative processes through adaptive learning and persona mirroring."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.23851",
      "abstract": "The integration of cloud computing in education can revolutionise learning in advanced (Australia & South Korea) and middle-income (Ghana & Nigeria) countries, while offering scalable, cost-effective and equitable access to adaptive learning systems. This paper explores how cloud computing and adaptive learning technologies are deployed across different socio-economic and infrastructure contexts. The study identifies enabling factors and systematic challenges, providing insights into how cloud-based education can be tailored to bridge the digital and educational divide globally.",
      "authors": [
        "Israel Fianyi",
        "Soonja Yeom",
        "Ju-Hyun Shin"
      ],
      "license": "http://creativecommons.org/publicdomain/zero/1.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Emerging Technologies (cs.ET)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T13:43:28+00:00",
          "link": "https://arxiv.org/abs/2506.23851v1",
          "size": "392kb",
          "version": "v1"
        }
      ],
      "title": "Comparative Studies: Cloud-Enabled Adaptive Learning System for Scalable Education in Sub-Saharan",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.23851",
        "PDF": "https://arxiv.org/pdf/2506.23851"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper focuses on cloud-enabled adaptive learning systems and their deployment across socio-economic contexts with no mention or exploration of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.24039",
      "abstract": "Zero-shot and prompt-based technologies capitalized on using frequently occurring images to transform visual reasoning tasks, which explains why such technologies struggle with valuable yet scarce scientific image sets. In this work, we propose Zenesis, a comprehensive no-code interactive platform designed to minimize barriers posed by data readiness for scientific images. We develop lightweight multi-modal adaptation techniques that enable zero-shot operation on raw scientific data, along with human-in-the-loop refinement and heuristic-based temporal enhancement options. We demonstrate the performance of our approach through comprehensive comparison and validation on challenging Focused Ion Beam Scanning Electron Microscopy (FIB-SEM) data of catalyst-loaded membranes. Zenesis significantly outperforms baseline methods, achieving an average accuracy of 0.947, an Intersection over Union (IOU) of 0.858, and a Dice score of 0.923 for amorphous catalyst samples and accuracy of 0.987, an IOU of 0.857, and a Dice score of 0.923 for crystalline samples. These results mark a substantial improvement over traditional methods like Otsu thresholding and even advanced models like Segment Anything Model (SAM) when used in isolation. Our results demonstrate that Zenesis is a powerful tool for scientific applications, particularly in fields where high-quality annotated datasets are unavailable, accelerating accurate analysis of experimental imaging.",
      "authors": [
        "Shubhabrata Mukherjee",
        "Jack Lang",
        "Obeen Kwon",
        "Iryna Zenyuk",
        "Valerie Brogden",
        "Adam Weber",
        "and Daniela Ushizima"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T16:45:23+00:00",
          "link": "https://arxiv.org/abs/2506.24039v1",
          "size": "8574kb",
          "version": "v1"
        }
      ],
      "title": "Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.24039",
        "HTML": "https://arxiv.org/html/2506.24039v1",
        "PDF": "https://arxiv.org/pdf/2506.24039"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The focus is on zero-shot segmentation of scientific images and optimization of analysis processes, not on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.24046",
      "abstract": "New endoscopists require a large volume of expert-proctored colonoscopies to attain minimal competency. Developing multi-fingered, synchronized control of a colonoscope requires significant time and exposure to the device. Current training methods inhibit this development by relying on tool hand-off for expert demonstrations. There is a need for colonoscopy training tools that enable in-hand expert guidance in real-time. We present a new concept of a tandem training system that uses a telemanipulated preceptor colonoscope to guide novice users as they perform a colonoscopy. This system is capable of dual-control and can automatically toggle between expert and novice control of a standard colonoscope's angulation control wheels. Preliminary results from a user study with novice and expert users show the effectiveness of this device as a skill acquisition tool. We believe that this device has the potential to accelerate skill acquisition for colonoscopy and, in the future, enable individualized instruction and responsive teaching through bidirectional actuation.",
      "authors": [
        "Olivia Richards",
        "Keith L. Obstein",
        "Nabil Simaan"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T16:50:52+00:00",
          "link": "https://arxiv.org/abs/2506.24046v1",
          "size": "423kb",
          "version": "v1"
        }
      ],
      "title": "Exploring Accelerated Skill Acquisition via Tandem Training for Colonoscopy",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.24046",
        "PDF": "https://arxiv.org/pdf/2506.24046"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "This paper discusses a training system for colonoscopy skill acquisition which does not relate to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2503.18243",
      "abstract": "Emotion regulation is a crucial skill for managing emotions in everyday life, yet finding a constructive and accessible method to support these processes remains challenging due to their cognitive demands. In this study, we explore how regular interactions with a social robot, conducted in a structured yet familiar environment within university halls and departments, can provide effective support for emotion regulation through cognitive reappraisal. Twenty-one students participated in a five-session study at a university hall or department, where the robot, powered by a large language model (GPT-3.5), facilitated structured conversations, encouraging the students to reinterpret emotionally charged situations they shared with the robot. Quantitative and qualitative results indicate significant improvements in emotion self-regulation, with participants reporting better understanding and control of their emotions. The intervention led to significant changes in constructive emotion regulation tendencies and positive effects on mood and sentiment after each session. The findings also demonstrate that repeated interactions with the robot encouraged greater emotional expressiveness, including longer speech disclosures, increased use of affective language, and heightened facial arousal. Notably, expressiveness followed structured patterns aligned with the reappraisal process, with expression peaking during key reappraisal moments, particularly when participants were prompted to reinterpret negative experiences. The qualitative feedback further highlighted how the robot fostered introspection and provided a supportive space for discussing emotions, enabling participants to confront long-avoided emotional challenges. These findings demonstrate the potential of robots to effectively assist in emotion regulation in familiar environments, offering both emotional support and cognitive guidance.",
      "authors": [
        "Guy Laban",
        "Julie Wang",
        "Hatice Gunes"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "2025-03-23T23:47:46+00:00",
          "link": "https://arxiv.org/abs/2503.18243v1",
          "size": "17948kb",
          "version": "v1"
        },
        {
          "date": "2025-06-30T16:08:56+00:00",
          "link": "https://arxiv.org/abs/2503.18243v2",
          "size": "7703kb",
          "version": "v2"
        }
      ],
      "title": "A Robot-Led Intervention for Emotion Regulation: From Expression to Reappraisal",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.18243",
        "HTML": "https://arxiv.org/html/2503.18243v2",
        "PDF": "https://arxiv.org/pdf/2503.18243"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The study involves social robots that assist in emotion regulation which can indirectly support creativity by fostering emotional expressiveness and introspection."
      },
      "source": "arXiv"
    },
    {
      "id": "2504.17677",
      "abstract": "The rise of AI, especially Large Language Models, presents challenges and opportunities to integrate such technology into the classroom. AI has the potential to revolutionize education by helping teaching staff with various tasks, such as personalizing their teaching methods, but it also raises concerns, for example, about the degradation of student-teacher interactions and user privacy. Based on interviews with teaching staff, this paper introduces INSIGHT, a proof of concept to combine various AI tools to assist teaching staff and students in the process of solving exercises. INSIGHT has a modular design that allows it to be integrated into various higher education courses. We analyze students' questions to an LLM by extracting keywords, which we use to dynamically build an FAQ from students' questions and provide new insights for the teaching staff to use for more personalized face-to-face support. Future work could build upon INSIGHT by using the collected data to provide adaptive learning and adjust content based on student progress and learning styles to offer a more interactive and inclusive learning experience.",
      "authors": [
        "Jarne Thys",
        "Sebe Vanbrabant",
        "Davy Vanacken",
        "Gustavo Rovelo Ruiz"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-04-24T15:47:20+00:00",
          "link": "https://arxiv.org/abs/2504.17677v1",
          "size": "1131kb",
          "version": "v1"
        },
        {
          "date": "2025-06-30T17:30:39+00:00",
          "link": "https://arxiv.org/abs/2504.17677v2",
          "size": "1055kb",
          "version": "v2"
        }
      ],
      "title": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models",
      "links": {
        "Abstract": "https://arxiv.org/abs/2504.17677",
        "HTML": "https://arxiv.org/html/2504.17677v2",
        "PDF": "https://arxiv.org/pdf/2504.17677"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "INSIGHT focuses on educational applications of AI to support teaching and does not explicitly aim to enhance or study creativity."
      },
      "tasks": [],
      "source": "arXiv"
    },
    {
      "id": "2506.07193",
      "abstract": "Eye tracking technology is frequently utilized to diagnose eye and neurological disorders, assess sleep and fatigue, study human visual perception, and enable novel gaze-based interaction methods. However, traditional eye tracking methodologies are constrained by bespoke hardware that is often cumbersome to wear, complex to apply, and demands substantial computational resources. To overcome these limitations, we investigated Electrooculography (EOG) eye tracking using 14 electrodes positioned around the ears, integrated into a custom-built headphone form factor device. In a controlled experiment, 16 participants tracked stimuli designed to induce smooth pursuits and saccades. Data analysis identified optimal electrode pairs for vertical and horizontal eye movement tracking, benchmarked against gold-standard EOG and camera-based methods. The electrode montage nearest the eyes yielded the best horizontal results. Horizontal smooth pursuits via earEOG showed high correlation with gold-standard measures ($r_{\\mathrm{EOG}} = 0.81, p = 0.01$; $r_{\\mathrm{CAM}} = 0.56, p = 0.02$), while vertical pursuits were weakly correlated ($r_{\\mathrm{EOG}} = 0.28, p = 0.04$; $r_{\\mathrm{CAM}} = 0.35, p = 0.05$). Voltage deflections when performing saccades showed strong correlation in the horizontal direction ($r_{\\mathrm{left}} = 0.99, p = 0.0$; $r_{\\mathrm{right}} = 0.99, p = 0.0$) but low correlation in the vertical direction ($r_{\\mathrm{up}} = 0.6, p = 0.23$; $r_{\\mathrm{down}} = 0.19, p = 0.73$). Overall, horizontal earEOG demonstrated strong performance, indicating its potential effectiveness, while vertical earEOG results were poor, suggesting limited feasibility in our current setup.",
      "authors": [
        "Tobias King",
        "Michael Knierim",
        "Philipp Lepold",
        "Christopher Clarke",
        "Hans Gellersen",
        "Michael Beigl",
        "Tobias R\\\"oddiger"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-08T15:28:51+00:00",
          "link": "https://arxiv.org/abs/2506.07193v1",
          "size": "5476kb",
          "version": "v1"
        },
        {
          "date": "2025-06-30T09:54:18+00:00",
          "link": "https://arxiv.org/abs/2506.07193v2",
          "size": "5473kb",
          "version": "v2"
        }
      ],
      "title": "earEOG via Periauricular Electrodes to Facilitate Eye Tracking in a Natural Headphone Form Factor",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.07193",
        "HTML": "https://arxiv.org/html/2506.07193v2",
        "PDF": "https://arxiv.org/pdf/2506.07193"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper focuses on eye-tracking technology and Electrooculography using headphones. There is no discussion or mention of creativity in the context of the study."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.19107",
      "abstract": "With the proliferation of large language model (LLM) applications since 2022, their use in education has sparked both excitement and concern. Recent studies consistently highlight students' (mis)use of LLMs can hinder learning outcomes. This work aims to teach students how to effectively prompt LLMs to improve their learning. We first proposed pedagogical prompting, a theoretically-grounded new concept to elicit learning-oriented responses from LLMs. To move from concept design to a proof-of-concept learning intervention in real educational settings, we selected early undergraduate CS education (CS1/CS2) as the example context. We began with a formative survey study with instructors (N=36) teaching early-stage undergraduate-level CS courses to inform the instructional design based on classroom needs. Based on their insights, we designed and developed a learning intervention through an interactive system with scenario-based instruction to train pedagogical prompting skills. Finally, we evaluated its instructional effectiveness through a user study with CS novice students (N=22) using pre/post-tests. Through mixed methods analyses, our results indicate significant improvements in learners' LLM-based pedagogical help-seeking skills, along with positive attitudes toward the system and increased willingness to use pedagogical prompts in the future. Our contributions include (1) a theoretical framework of pedagogical prompting; (2) empirical insights into current instructor attitudes toward pedagogical prompting; and (3) a learning intervention design with an interactive learning tool and scenario-based instruction leading to promising results on teaching LLM-based help-seeking. Our approach is scalable for broader implementation in classrooms and has the potential to be integrated into tools like ChatGPT as an on-boarding experience to encourage learning-oriented use of generative AI.",
      "authors": [
        "Ruiwei Xiao",
        "Xinying Hou",
        "Runlong Ye",
        "Majeed Kazemitabaar",
        "Nicholas Diana",
        "Michael Liut",
        "John Stamper"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-23T20:39:17+00:00",
          "link": "https://arxiv.org/abs/2506.19107v1",
          "size": "17055kb",
          "version": "v1"
        },
        {
          "date": "2025-06-28T18:15:32+00:00",
          "link": "https://arxiv.org/abs/2506.19107v2",
          "size": "16717kb",
          "version": "v2"
        }
      ],
      "title": "Improving Student-AI Interaction Through Pedagogical Prompting: An Example in Computer Science Education",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19107",
        "HTML": "https://arxiv.org/html/2506.19107v2",
        "PDF": "https://arxiv.org/pdf/2506.19107"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses the use of large language models in education and describes a pedagogical approach to improve interactions, which may indirectly relate to creativity in educational methods. Creativity as an educational tool is a secondary theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2206.00535",
      "abstract": "Deepfakes can fuel online misinformation. As deepfakes get harder to recognize with the naked eye, human users become more reliant on deepfake detection models to help them decide whether a video is real or fake. Currently, models yield a prediction for a video's authenticity, but do not integrate a method for alerting a human user. We introduce a framework for amplifying artifacts in deepfake videos to make them more detectable by people. We propose a novel, semi-supervised Artifact Attention module, which is trained on human responses to create attention maps that highlight video artifacts, and magnify them to create a novel visual indicator we call \"Deepfake Caricatures\". In a user study, we demonstrate that Caricatures greatly increase human detection, across video presentation times and user engagement levels. We also introduce a deepfake detection model that incorporates the Artifact Attention module to increase its accuracy and robustness. Overall, we demonstrate the success of a human-centered approach to designing deepfake mitigation methods.",
      "authors": [
        "Camilo Fosco",
        "Emilie Josephs",
        "Alex Andonian and Aude Oliva"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)",
        "Social and Information Networks (cs.SI)"
      ],
      "submission_historys": [
        {
          "date": "2022-06-01T14:43:49+00:00",
          "link": "https://arxiv.org/abs/2206.00535v1",
          "size": "6137kb",
          "version": "v1"
        },
        {
          "date": "2022-06-02T14:43:19+00:00",
          "link": "https://arxiv.org/abs/2206.00535v2",
          "size": "7330kb",
          "version": "v2"
        },
        {
          "date": "2023-04-10T17:14:43+00:00",
          "link": "https://arxiv.org/abs/2206.00535v3",
          "size": "11478kb",
          "version": "v3"
        },
        {
          "date": "2025-06-29T04:43:18+00:00",
          "link": "https://arxiv.org/abs/2206.00535v4",
          "size": "5589kb",
          "version": "v4"
        }
      ],
      "title": "Deepfake Caricatures: Amplifying attention to artifacts increases deepfake detection by humans and machines",
      "links": {
        "Abstract": "https://arxiv.org/abs/2206.00535",
        "HTML": "https://arxiv.org/html/2206.00535v4",
        "PDF": "https://arxiv.org/pdf/2206.00535"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper focuses on deepfake detection and user interaction rather than creativity as a main subject. Creativity is not discussed or implied as a theme."
      },
      "tasks": [
        "DeepFake Detection",
        "Face Swapping",
        "Human Detection",
        "Misinformation"
      ],
      "source": "arXiv"
    },
    {
      "id": "2409.01754",
      "abstract": "From the invention of writing and the printing press, to television and social media, human history is punctuated by major innovations in communication technology, which fundamentally altered how ideas spread and reshaped our culture. Recent chatbots powered by generative artificial intelligence constitute a novel medium that encodes cultural patterns in their neural representations and disseminates them in conversations with hundreds of millions of people. Understanding whether these patterns transmit into human language, and ultimately shape human culture, is a fundamental question. While fully quantifying the causal impact of a chatbot like ChatGPT on human culture is very challenging, lexicographic shift in human spoken communication may offer an early indicator of such broad phenomenon. Here, we apply econometric causal inference techniques to 740,249 hours of human discourse from 360,445 YouTube academic talks and 771,591 conversational podcast episodes across multiple disciplines. We detect a measurable and abrupt increase in the use of words preferentially generated by ChatGPT, such as delve, comprehend, boast, swift, and meticulous, after its release. These findings suggest a scenario where machines, originally trained on human data and subsequently exhibiting their own cultural traits, can, in turn, measurably reshape human culture. This marks the beginning of a closed cultural feedback loop in which cultural traits circulate bidirectionally between humans and machines. Our results motivate further research into the evolution of human-machine culture, and raise concerns over the erosion of linguistic and cultural diversity, and the risks of scalable manipulation.",
      "authors": [
        "Hiromu Yakura",
        "Ezequiel Lopez-Lopez",
        "Levin Brinkmann",
        "Ignacio Serna",
        "Prateek Gupta",
        "Ivan Soraperra",
        "Iyad Rahwan"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-09-03T10:01:51+00:00",
          "link": "https://arxiv.org/abs/2409.01754v1",
          "size": "2346kb",
          "version": "v1"
        },
        {
          "date": "2025-06-30T14:43:32+00:00",
          "link": "https://arxiv.org/abs/2409.01754v2",
          "size": "8008kb",
          "version": "v2"
        }
      ],
      "title": "Empirical evidence of Large Language Model's influence on human spoken communication",
      "links": {
        "Abstract": "https://arxiv.org/abs/2409.01754",
        "HTML": "https://arxiv.org/html/2409.01754v2",
        "PDF": "https://arxiv.org/pdf/2409.01754"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses the influence of language models on culture and communication, potentially relating to creative expression and cultural impacts. Creativity is a secondary theme."
      },
      "tasks": [
        "Diversity"
      ],
      "source": "arXiv"
    },
    {
      "id": "2411.15240",
      "abstract": "Pretrained foundation models and transformer architectures have driven the success of large language models (LLMs) and other modern AI breakthroughs. However, similar advancements in health data modeling remain limited due to the need for innovative adaptations. Wearable movement data offers a valuable avenue for exploration, as it's a core feature in nearly all commercial smartwatches, well established in clinical and mental health research, and the sequential nature of the data shares similarities to language. We introduce the Pretrained Actigraphy Transformer (PAT), the first open source foundation model designed for time-series wearable movement data. Leveraging transformer-based architectures and novel techniques, such as patch embeddings, and pretraining on data from 29,307 participants in a national U.S. sample, PAT achieves state-of-the-art performance in several mental health prediction tasks. PAT is also lightweight and easily interpretable, making it a robust tool for mental health research.\n  GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/",
      "authors": [
        "Franklin Y. Ruan",
        "Aiwei Zhang",
        "Jenny Y. Oh",
        "SouYoung Jin",
        "Nicholas C. Jacobson"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Quantitative Methods (q-bio.QM)"
      ],
      "submission_historys": [
        {
          "date": "2024-11-22T01:58:35+00:00",
          "link": "https://arxiv.org/abs/2411.15240v1",
          "size": "2614kb",
          "version": "v1"
        },
        {
          "date": "2024-11-26T06:11:42+00:00",
          "link": "https://arxiv.org/abs/2411.15240v2",
          "size": "2067kb",
          "version": "v2"
        },
        {
          "date": "2025-01-14T04:10:46+00:00",
          "link": "https://arxiv.org/abs/2411.15240v3",
          "size": "2126kb",
          "version": "v3"
        },
        {
          "date": "2025-06-28T20:08:42+00:00",
          "link": "https://arxiv.org/abs/2411.15240v4",
          "size": "2265kb",
          "version": "v4"
        }
      ],
      "title": "Foundation Models for Wearable Movement Data in Mental Health Research",
      "links": {
        "Abstract": "https://arxiv.org/abs/2411.15240",
        "PDF": "https://arxiv.org/pdf/2411.15240"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper focuses on the use of pretrained models for wearable movement data in mental health, with no specific mention or exploration of creativity."
      },
      "tasks": [],
      "repo_urls": [
        "https://github.com/njacobsonlab/pretrained-actigraphy-transformer"
      ],
      "source": "arXiv"
    },
    {
      "id": "2506.00924",
      "abstract": "This paper introduces a dual-layer framework for network operator-side quality of experience (QoE) assessment that integrates both objective network modeling and subjective user perception extracted from live-streaming platforms. On the objective side, we develop a machine learning model trained on mean opinion scores (MOS) computed via the ITU-T P.1203 reference implementation, allowing accurate prediction of user-perceived video quality using only network parameters such as packet loss, delay, jitter, and throughput without reliance on video content or client-side instrumentation. On the subjective side, we present a semantic filtering and scoring pipeline that processes user comments from live streams to extract performance-related feedback. A large language model is used to assign scalar MOS scores to filtered comments in a deterministic and reproducible manner. To support scalable and interpretable analysis, we construct a labeled dataset of 47,894 live-stream comments, of which about 34,000 are identified as QoE-relevant through multi-layer semantic filtering. Each comment is enriched with simulated Internet Service Provider attribution and temporally aligned using synthetic timestamps in 5-min intervals. The resulting dataset enables operator-level aggregation and time-series analysis of user-perceived quality. A delta MOS metric is proposed to measure each Internet service provider's deviation from platform-wide sentiment, allowing detection of localized degradations even in the absence of direct network telemetry. A controlled outage simulation confirms the framework's effectiveness in identifying service disruptions through comment-based trends alone. The system provides each operator with its own subjective MOS and the global platform average per interval, enabling real-time interpretation of performance deviations and comparison with objective network-based QoE estimates.",
      "authors": [
        "Parsa Hassani Shariat Panahi",
        "Amir Hossein Jalilvand",
        "and M. Hassan Najafi"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Networking and Internet Architecture (cs.NI)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-01T09:31:55+00:00",
          "link": "https://arxiv.org/abs/2506.00924v1",
          "size": "551kb",
          "version": "v1"
        },
        {
          "date": "2025-06-30T05:21:31+00:00",
          "link": "https://arxiv.org/abs/2506.00924v2",
          "size": "3383kb",
          "version": "v2"
        }
      ],
      "title": "Bridging Subjective and Objective QoE: Operator-Level Aggregation Using LLM-Based Comment Analysis and Network MOS Comparison",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.00924",
        "HTML": "https://arxiv.org/html/2506.00924v2",
        "PDF": "https://arxiv.org/pdf/2506.00924"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper is about Quality of Experience assessment in network operations, without any explicit relation to creativity."
      },
      "tasks": [
        "Large Language Model",
        "Time Series Analysis"
      ],
      "source": "arXiv"
    },
    {
      "id": "2506.21490",
      "abstract": "Achieving seamless coordination between AI agents and humans is crucial for real-world applications, yet it remains a significant open challenge. Hanabi is a cooperative card game featuring imperfect information, constrained communication, theory of mind requirements, and coordinated action -- making it an ideal testbed for human-AI coordination. However, its use for human-AI interaction has been limited by the challenges of human evaluation. In this work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to overcome the constraints of costly and difficult-to-reproduce human evaluations. We develop \\textit{human proxy agents} on a large-scale human dataset that serve as robust, cheap, and reproducible human-like evaluation partners in AH2AC2. To encourage the development of data-efficient methods, we open-source a dataset of 3,079 games, deliberately limiting the amount of available human gameplay data. We present baseline results for both two- and three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy agents through a controlled evaluation system rather than releasing them publicly. The code is available at \\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.",
      "authors": [
        "Tin Dizdarevi\\'c",
        "Ravi Hammond",
        "Tobias Gessler",
        "Anisoara Calinescu",
        "Jonathan Cook",
        "Matteo Gallici",
        "Andrei Lupu",
        "Darius Muglich",
        "Johannes Forkel",
        "Jakob Nicolaus Foerster"
      ],
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Multiagent Systems (cs.MA)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-26T17:19:52+00:00",
          "link": "https://arxiv.org/abs/2506.21490v1",
          "size": "1683kb",
          "version": "v1"
        },
        {
          "date": "2025-06-29T10:25:50+00:00",
          "link": "https://arxiv.org/abs/2506.21490v2",
          "size": "1683kb",
          "version": "v2"
        }
      ],
      "title": "Ad-Hoc Human-AI Coordination Challenge",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.21490",
        "HTML": "https://arxiv.org/html/2506.21490v2",
        "PDF": "https://arxiv.org/pdf/2506.21490"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper introduces human-AI coordination challenges using the game Hanabi. It does not discuss creativity as a focus or theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2408.15256",
      "abstract": "Past ontology requirements engineering (ORE) has primarily relied on manual methods, such as interviews and collaborative forums, to gather user requirements from domain experts, especially in large projects. Current OntoChat offers a framework for ORE that utilises large language models (LLMs) to streamline the process through four key functions: user story creation, competency question (CQ) extraction, CQ filtration and analysis, and ontology testing support. In OntoChat, users are expected to prompt the chatbot to generate user stories. However, preliminary evaluations revealed that they struggle to do this effectively. To address this issue, we experimented with a research method called participatory prompting, which involves researcher-mediated interactions to help users without deep knowledge of LLMs use the chatbot more effectively. This participatory prompting user study produces pre-defined prompt templates based on user queries, focusing on creating and refining personas, goals, scenarios, sample data, and data resources for user stories. These refined user stories will subsequently be converted into CQs.",
      "authors": [
        "Yihang Zhao",
        "Bohui Zhang",
        "Xi Hu",
        "Shuyin Ouyang",
        "Jongmo Kim",
        "Nitisha Jain",
        "Jacopo de Berardinis",
        "Albert Mero\\~no-Pe\\~nuela",
        "Elena Simperl"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2024-08-09T19:21:14+00:00",
          "link": "https://arxiv.org/abs/2408.15256v1",
          "size": "701kb",
          "version": "v1"
        },
        {
          "date": "2024-08-29T09:34:48+00:00",
          "link": "https://arxiv.org/abs/2408.15256v2",
          "size": "1328kb",
          "version": "v2"
        },
        {
          "date": "2024-09-18T16:09:40+00:00",
          "link": "https://arxiv.org/abs/2408.15256v3",
          "size": "1328kb",
          "version": "v3"
        }
      ],
      "title": "Improving Ontology Requirements Engineering with OntoChat and Participatory Prompting",
      "links": {
        "Abstract": "https://arxiv.org/abs/2408.15256",
        "HTML": "https://arxiv.org/html/2408.15256",
        "PDF": "https://arxiv.org/pdf/2408.15256"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The focus is on ontology requirements engineering and user interactions with language models. Creativity is not a theme in the study."
      },
      "tasks": [
        "Chatbot"
      ],
      "repo_urls": [
        "https://github.com/king-s-knowledge-graph-lab/ontochat"
      ],
      "source": "arXiv"
    },
    {
      "id": "2503.15127",
      "abstract": "Social robot navigation is an evolving research field that aims to find efficient strategies to safely navigate dynamic environments populated by humans. A critical challenge in this domain is the accurate modeling of human motion, which directly impacts the design and evaluation of navigation algorithms. This paper presents a comparative study of two popular categories of human motion models used in social robot navigation, namely velocity-based models and force-based models. A system-theoretic representation of both model types is presented, which highlights their common feedback structure, although with different state variables. Several navigation policies based on reinforcement learning are trained and tested in various simulated environments involving pedestrian crowds modeled with these approaches. A comparative study is conducted to assess performance across multiple factors, including human motion model, navigation policy, scenario complexity and crowd density. The results highlight advantages and challenges of different approaches to modeling human behavior, as well as their role during training and testing of learning-based navigation policies. The findings offer valuable insights and guidelines for selecting appropriate human motion models when designing socially-aware robot navigation systems.",
      "authors": [
        "Tommaso Van Der Meer",
        "Andrea Garulli",
        "Antonio Giannitrapani",
        "Renato Quartullo"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Robotics (cs.RO)",
        "Systems and Control (cs.SY)",
        "Systems and Control (eess.SY)"
      ],
      "submission_historys": [
        {
          "date": "2025-03-19T11:41:22+00:00",
          "link": "https://arxiv.org/abs/2503.15127v1",
          "size": "874kb",
          "version": "v1"
        }
      ],
      "title": "A Comparative Study of Human Motion Models in Reinforcement Learning Algorithms for Social Robot Navigation",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.15127",
        "HTML": "https://arxiv.org/html/2503.15127",
        "PDF": "https://arxiv.org/pdf/2503.15127"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "This paper discusses human motion models and reinforcement learning for social robot navigation, without addressing creativity."
      },
      "repo_urls": [
        "https://github.com/TommasoVandermeer/Social-Navigation-PyEnvs"
      ],
      "source": "arXiv"
    },
    {
      "id": "2409.02691",
      "abstract": "We explore the integration of large language models (LLMs) into visual analytics (VA) systems to transform their capabilities through intuitive natural language interactions. We survey current research directions in this emerging field, examining how LLMs are integrated into data management, language interaction, visualisation generation, and language generation processes. We highlight the new possibilities that LLMs bring to VA, especially how they can change VA processes beyond the usual use cases. We especially highlight building new visualisation-language models, allowing access of a breadth of domain knowledge, multimodal interaction, and opportunities with guidance. Finally, we carefully consider the prominent challenges of using current LLMs in VA tasks. Our discussions in this paper aim to guide future researchers working on LLM-assisted VA systems and help them navigate common obstacles when developing these systems.",
      "authors": [
        "Maeve Hutchinson",
        "Radu Jianu",
        "Aidan Slingsby and Pranava Madhyastha"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2024-09-04T13:24:03+00:00",
          "link": "https://arxiv.org/abs/2409.02691v1",
          "size": "198kb",
          "version": "v1"
        }
      ],
      "title": "LLM-Assisted Visual Analytics: Opportunities and Challenges",
      "links": {
        "Abstract": "https://arxiv.org/abs/2409.02691",
        "HTML": "https://arxiv.org/html/2409.02691",
        "PDF": "https://arxiv.org/pdf/2409.02691"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper explores the integration of language models into visual analytics, without an emphasis on creativity. Creativity is not addressed."
      },
      "tasks": [
        "Management",
        "multimodal interaction",
        "Navigate",
        "Text Generation"
      ],
      "source": "arXiv"
    },
    {
      "id": "2403.15861",
      "abstract": "This research investigates User Experience (UX) issues in dataset search, targeting Google Dataset Search and data.europa.eu. It focuses on 6 areas within UX: Initial Interaction, Search Process, Dataset Exploration, Filtering and Sorting, Dataset Actions, and Assistance and Feedback. The evaluation method combines 'The Pandemic Puzzle' user task, think-aloud methods, and demographic and post-task questionnaires. 29 strengths and 63 weaknesses were collected from 19 participants involved in roles within technology firm or academia. While certain insights are specific to particular platforms, most are derived from features commonly observed in dataset search platforms across a variety of fields, implying that our findings are broadly applicable. Observations from commonly found features in dataset search platforms across various fields have led to the development of 10 new design prototypes. Unlike literature retrieval, dataset retrieval involves a significant focus on metadata accessibility and quality, each element of which can impact decision-making. To address issues like reading fatigue from metadata presentation, inefficient methods for results searching, filtering, and selection, along with other unresolved user-centric issues on current platforms. These prototypes concentrate on enhancing metadata-related features. They include a redesigned homepage, an improved search bar, better sorting options, an enhanced search result display, a metadata comparison tool, and a navigation guide. Our aim is to improve usability for a wide range of users, including both developers and researchers.",
      "authors": [
        "Yihang Zhao",
        "Albert Mero\\~no-Pe\\~nuela",
        "Elena Simperl"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-03-23T14:57:34+00:00",
          "link": "https://arxiv.org/abs/2403.15861v1",
          "size": "4219kb",
          "version": "v1"
        },
        {
          "date": "2024-08-04T20:29:22+00:00",
          "link": "https://arxiv.org/abs/2403.15861v2",
          "size": "3263kb",
          "version": "v2"
        }
      ],
      "title": "User Experience In Dataset Search",
      "links": {
        "Abstract": "https://arxiv.org/abs/2403.15861",
        "HTML": "https://arxiv.org/html/2403.15861",
        "PDF": "https://arxiv.org/pdf/2403.15861"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper is about user experience in dataset search and improving metadata-related features. There is no linkage to creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.20884",
      "abstract": "''TikTok, Do Your Thing'' is a viral trend where users attempt to identify strangers they see in public via information crowd-sourcing. The trend started as early as 2021 and users typically engage with it for romantic purposes (similar to a ''Missed Connections'' personal advertisement). This practice includes acts of surveillance and identification in the public sphere, although by peers rather than governments or corporations. To understand users' reactions to this trend we conducted a qualitative analysis of 60 TikTok videos and 1,901 user comments. Of the 60 videos reviewed, we find 19 individuals were successfully identified. We also find that while there were comments expressing disapproval (n=310), more than double the number expressed support (n=883). Supportive comments demonstrated genuine interest and empathy, reflecting evolving conceptions of community and algorithmic engagement. On the other hand, disapproving comments highlighted concerns about inappropriate relationships, stalking, consent, and gendered double standards. We discuss these insights in relation to the normalization of interpersonal surveillance, online stalking, and as an evolution of social surveillance to offer a new perspective on user perceptions surrounding interpersonal surveillance and identification in the public sphere.",
      "authors": [
        "Meira Gilbert",
        "Miranda Wei",
        "Lindah Kotut"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-25T23:13:43+00:00",
          "link": "https://arxiv.org/abs/2506.20884v1",
          "size": "1215kb",
          "version": "v1"
        }
      ],
      "title": "\"TikTok, Do Your Thing\": User Reactions to Social Surveillance in the Public Sphere",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20884",
        "PDF": "https://arxiv.org/pdf/2506.20884"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "none-irrelevant",
        "reason": "The paper examines social surveillance practices on TikTok with no relation to creativity, instead focusing on privacy, ethics, and user reactions."
      },
      "source": "arXiv"
    }
  ],
  "subjects": [
    "Computer Vision and Pattern Recognition (cs.CV)",
    "Networking and Internet Architecture (cs.NI)",
    "Multimedia (cs.MM)",
    "Computational Engineering, Finance, and Science (cs.CE)",
    "Computation and Language (cs.CL)",
    "Image and Video Processing (eess.IV)",
    "Systems and Control (eess.SY)",
    "Cryptography and Security (cs.CR)",
    "Robotics (cs.RO)",
    "Information Retrieval (cs.IR)",
    "Artificial Intelligence (cs.AI)",
    "Neurons and Cognition (q-bio.NC)",
    "Applications (stat.AP)",
    "Quantitative Methods (q-bio.QM)",
    "Systems and Control (cs.SY)",
    "Emerging Technologies (cs.ET)",
    "Signal Processing (eess.SP)",
    "Social and Information Networks (cs.SI)",
    "Computers and Society (cs.CY)",
    "Multiagent Systems (cs.MA)",
    "Hardware Architecture (cs.AR)",
    "Human-Computer Interaction (cs.HC)",
    "Graphics (cs.GR)",
    "Machine Learning (cs.LG)",
    "General Economics (econ.GN)",
    "Economics (q-fin.EC)"
  ],
  "prompt": {
    "creativity": "\nYou are an expert in information retrieval. I will provide you with a list of research papers from arXiv, specifically in the *cs.HC* (Human-Computer Interaction) category.\n\nYour task is to analyze each paper and determine its relevance to the topic of **Creativity**.\n\n\n### Classify each paper into one of the following relevance levels\n\n- `core` \u2014 Creativity is a **primary focus**\n  - The paper directly studies or simulates creativity, with a clear focus on creativity.\n  - Includes creative tasks, co-creative systems, or creativity evaluation metrics.\n  - The title and abstract explicitly mention creativity, and the research questions are directly related to creativity.\n- `partial` \u2014 Creativity is a **secondary theme**\n  - Part of the paper relates to creativity; it is treated as an analytical dimension or design goal but not the main objective (e.g., user creativity, design support).\n  - Creativity may appear in discussions, experiments, or auxiliary applications.\n  - Creativity is presented as a supporting topic (e.g., evaluation criteria, user feedback).\n- `none-irrelevant` \u2014 **No clear connection** to creativity\n  - The paper does not address creativity as a topic.\n  - Focuses on unrelated technical content (e.g., compression, security, optimization).\n  - If creativity is mentioned, it is only superficial and lacks substantive content.\n\n\n### Return your results in the following JSON format\n\n```json\n{\n  \"result\": [\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | none-irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    },\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | none-irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    }\n  ]\n}\n```\n\nBe concise but specific in your reasoning, referencing key terms or sections when applicable.\n"
  },
  "description": "Data source: https://arxiv.org/list/cs.HC/new",
  "level_tatistics": {
    "none-irrelevant": 50,
    "partial": 10,
    "core": 1
  },
  "arxiv_update_date": "2025-07-01"
}