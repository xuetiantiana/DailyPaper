{
  "data": [
    {
      "id": "2507.16819",
      "abstract": "We examined eye and head movements to gain insights into skill development in clinical settings. A total of 24 practitioners participated in simulated baby delivery training sessions. We calculated key metrics, including pupillary response rate, fixation duration, or angular velocity. Our findings indicate that eye and head tracking can effectively differentiate between trained and untrained practitioners, particularly during labor tasks. For example, head-related features achieved an F1 score of 0.85 and AUC of 0.86, whereas pupil-related features achieved F1 score of 0.77 and AUC of 0.85. The results lay the groundwork for computational models that support implicit skill assessment and training in clinical settings by using commodity eye-tracking glasses as a complementary device to more traditional evaluation methods such as subjective scores.",
      "authors": [
        "Kayhan Latifzadeh",
        "Luis A. Leiva",
        "Klen \\v{C}opi\\v{c} Pucihar",
        "Matja\\v{z} Kljun",
        "Iztok Devetak",
        "Lili Steblovnik"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "2025-05-12T08:27:05+00:00",
          "link": "https://arxiv.org/abs/2507.16819v1",
          "size": "8780kb",
          "version": "v1"
        }
      ],
      "title": "Assessing Medical Training Skills via Eye and Head Movements",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.16819",
        "HTML": "https://arxiv.org/html/2507.16819v1",
        "PDF": "https://arxiv.org/pdf/2507.16819"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper investigates eye and head movements for skill assessment in medical training. It focuses on skill differentiation and training, with no mention of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17024",
      "abstract": "A growing body of work on visualization affordances highlights how specific design choices shape reader takeaways from information visualizations. However, mapping the relationship between design choices and reader conclusions often requires labor-intensive crowdsourced studies, generating large corpora of free-response text for analysis. To address this challenge, we explored alternative scalable research methodologies to assess chart affordances. We test four elicitation methods from human-subject studies: free response, visualization ranking, conclusion ranking, and salience rating, and compare their effectiveness in eliciting reader interpretations of line charts, dot plots, and heatmaps. Overall, we find that while no method fully replicates affordances observed in free-response conclusions, combinations of ranking and rating methods can serve as an effective proxy at a broad scale. The two ranking methodologies were influenced by participant bias towards certain chart types and the comparison of suggested conclusions. Rating conclusion salience could not capture the specific variations between chart types observed in the other methods. To supplement this work, we present a case study with GPT-4o, exploring the use of large language models (LLMs) to elicit human-like chart interpretations. This aligns with recent academic interest in leveraging LLMs as proxies for human participants to improve data collection and analysis efficiency. GPT-4o performed best as a human proxy for the salience rating methodology but suffered from severe constraints in other areas. Overall, the discrepancies in affordances we found between various elicitation methodologies, including GPT-4o, highlight the importance of intentionally selecting and combining methods and evaluating trade-offs.",
      "authors": [
        "Chase Stokes",
        "Kylie Lin",
        "and Cindy Xiong Bearfield"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-22T21:27:16+00:00",
          "link": "https://arxiv.org/abs/2507.17024v1",
          "size": "13412kb",
          "version": "v1"
        }
      ],
      "title": "Write, Rank, or Rate: Comparing Methods for Studying Visualization Affordances",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17024",
        "HTML": "https://arxiv.org/html/2507.17024v1",
        "PDF": "https://arxiv.org/pdf/2507.17024"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses visualization affordances and methodologies to assess reader interpretations. While not centered on creativity, visualizations can be a tool for creative expression and the study of affordances may influence creative outcomes in designing visual information."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17139",
      "abstract": "We present a first study of the effects of frame time variations, in both deviation around mean frame times and period of fluctuation, on task performance in a virtual environment (VE). Chosen are open and closed loop tasks that are typical for current applications or likely to be prominent in future ones. The results show that at frame times in the range deemed acceptable for many applications, fairly large deviations in amplitude over a fairly wide range of periods do not significantly affect task performance. However, at a frame time often considered a minimum for immersive VR, frame time variations do produce significant effects on closed loop task performance. The results will be of use to designers of VEs and immersive applications, who often must control frame time variations due to large fluctuations of complexity (graphical and otherwise) in the VE.",
      "authors": [
        "Benjamin Watson",
        "Victoria Spaulding",
        "Neff Walker",
        "William Ribarsky"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Emerging Technologies (cs.ET)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T02:22:51+00:00",
          "link": "https://arxiv.org/abs/2507.17139v1",
          "size": "137kb",
          "version": "v1"
        }
      ],
      "title": "Evaluation of the effects of frame time variation on VR task performance",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17139",
        "PDF": "https://arxiv.org/pdf/2507.17139"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper focuses on the effects of frame time variations on VR task performance, which is relevant to VR system design and optimization but does not address creativity as a research theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17209",
      "abstract": "Modern scientific discovery faces growing challenges in integrating vast and heterogeneous knowledge critical to breakthroughs in biomedicine and drug development. Traditional hypothesis-driven research, though effective, is constrained by human cognitive limits, the complexity of biological systems, and the high cost of trial-and-error experimentation. Deep learning models, especially graph neural networks (GNNs), have accelerated prediction generation, but the sheer volume of outputs makes manual selection for validation unscalable. Large language models (LLMs) offer promise in filtering and hypothesis generation, yet suffer from hallucinations and lack grounding in structured knowledge, limiting their reliability. To address these issues, we propose HypoChainer, a collaborative visualization framework that integrates human expertise, LLM-driven reasoning, and knowledge graphs (KGs) to enhance hypothesis generation and validation. HypoChainer operates in three stages: First, exploration and contextualization -- experts use retrieval-augmented LLMs (RAGs) and dimensionality reduction to navigate large-scale GNN predictions, assisted by interactive explanations. Second, hypothesis chain formation -- experts iteratively examine KG relationships around predictions and semantically linked entities, refining hypotheses with LLM and KG suggestions. Third, validation prioritization -- refined hypotheses are filtered based on KG-supported evidence to identify high-priority candidates for experimentation, with visual analytics further strengthening weak links in reasoning. We demonstrate HypoChainer's effectiveness through case studies in two domains and expert interviews, highlighting its potential to support interpretable, scalable, and knowledge-grounded scientific discovery.",
      "authors": [
        "Haoran Jiang",
        "Shaohan Shi",
        "Yunjie Yao",
        "Chang Jiang",
        "Quan Li"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T05:02:54+00:00",
          "link": "https://arxiv.org/abs/2507.17209v1",
          "size": "5350kb",
          "version": "v1"
        }
      ],
      "title": "HypoChainer: A Collaborative System Combining LLMs and Knowledge Graphs for Hypothesis-Driven Scientific Discovery",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17209",
        "PDF": "https://arxiv.org/pdf/2507.17209"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper proposes a system for scientific discovery that involves hypothesis generation and validation using LLMs and knowledge graphs. Although its primary focus is on scientific discovery, creativity is a secondary theme, as it involves creative reasoning and hypothesis formulation processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17218",
      "abstract": "Communicating the complexity of oceanic phenomena-such as hypoxia and acidification-poses a persistent challenge for marine science. Despite advances in sensing technologies and computational models, conventional formats like static visualizations and text-based reports often fall short in conveying the dynamics of ocean changes. To address this gap, we present OceanVive, an immersive and interactive visualization system that transforms complex ocean datasets into navigable spatial narratives. OceanVive incorporates an exploratory panel on a table-sized tablet for managing immersive content on a large screen and integrates adaptive visual encodings, contextual storytelling, and intuitive navigation pathways to support effective communication. We validate the system through expert interviews, demonstrating its potential to enhance science communication and promote deeper public understanding.",
      "authors": [
        "Yang Ouyang",
        "Yuchen Wu",
        "Xiyuan Wang",
        "Laixin Xie",
        "Weicong Cheng",
        "Jianping Gan",
        "Quan Li",
        "Xiaojuan Ma"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T05:29:27+00:00",
          "link": "https://arxiv.org/abs/2507.17218v1",
          "size": "6208kb",
          "version": "v1"
        }
      ],
      "title": "OceanVive: An Immersive Visualization System for Communicating Complex Oceanic Phenomena",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17218",
        "HTML": "https://arxiv.org/html/2507.17218v1",
        "PDF": "https://arxiv.org/pdf/2507.17218"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "OceanVive aims at enhancing the communication of complex oceanic phenomena through immersive visualization. While it involves visualization and communication techniques, it does not explicitly discuss or focus on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17226",
      "abstract": "Generative AI is disrupting computing education. Most interventions focus on teaching GenAI use rather than helping students understand how AI changes their programming process. We designed and deployed a novel comparative video reflection assignment adapting the Describe, Examine, then Articulate Learning (DEAL) framework. In an introductory software engineering course, students recorded themselves programming during their team project two times: first without, then with using generative AI. Students then analyzed their own videos using a scaffolded set of reflection questions, including on their programming process and human, internet, and AI help-seeking. We conducted a qualitative thematic analysis of the reflections, finding students developed insights about planning, debugging, and help-seeking behaviors that transcended AI use. Students reported learning to slow down and understand before writing or generating code, recognized patterns in their problem-solving approaches, and articulated specific process improvements. Students also learned and reflected on AI limits and downsides, and strategies to use AI more critically, including better prompting but also to benefit their learning instead of just completing tasks. Unexpectedly, the comparative reflection also scaffolded reflection on programming not involving AI use, and even led to students spontaneously setting future goals to adopt video and other regular reflection. This work demonstrates structured reflection on programming session videos can develop metacognitive skills essential for programming with and without generative AI and also lifelong learning in our evolving field.",
      "authors": [
        "Sarah \"Magz\" Fernandez",
        "Greg L Nelson"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T05:47:46+00:00",
          "link": "https://arxiv.org/abs/2507.17226v1",
          "size": "39kb",
          "version": "v1"
        }
      ],
      "title": "A \"watch your replay videos\" reflection assignment on comparing programming without versus with generative AI: learning about programming, critical AI use and limitations, and reflection",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17226",
        "HTML": "https://arxiv.org/html/2507.17226v1",
        "PDF": "https://arxiv.org/pdf/2507.17226"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "This paper addresses the use of reflection assignments to develop metacognitive skills and learn about AI's impacts on programming processes. Creativity is a secondary theme as the reflections encourage students to develop critical and creative thinking about their programming and learning processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17230",
      "abstract": "Students continue their education when they feel their learning is meaningful and relevant for their future careers. Computing educators now face the challenge of preparing students for careers increasingly shaped by generative AI (GenAI) with the goals of supporting their learning, motivation, ethics, and career development. Our longitudinal qualitative study of students in a GenAI-integrated creative media course shows how this is a \"wicked\" problem: progress on one goal can then impede progress on other goals. Students developed concerning patterns despite extensive instruction in critical and ethical GenAI use including prompt engineering, ethics and bias, and industry panels on GenAI's career impact. We present an analysis of two students' experiences to showcase this complexity. Increasing GenAI use skills can lower ethics; for example, Pat started from purposefully avoiding GenAI use, to dependency. He described himself as a \"notorious cheater\" who now uses GenAi to \"get all the right answers\" while acknowledging he's learning less. Increasing ethical awareness can lower the learning of GenAI use skills; for example, Jay's newfound environmental concerns led to self-imposed usage limits that impeded skill development, and new serious fears that GenAI would eliminate creative careers they had been passionate about. Increased GenAI proficiency, a potential career skill, did not improve their career confidence. These findings suggest that supporting student development in the GenAI era is a \"wicked\" problem requiring multi-dimensional evaluation and design, rather than optimizing learning, GenAI skills, ethics, or career motivation individually.",
      "authors": [
        "Clara Scalzer",
        "Saurav Pokhrel",
        "Sara Hunt",
        "Greg L Nelson"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T05:53:54+00:00",
          "link": "https://arxiv.org/abs/2507.17230v1",
          "size": "43kb",
          "version": "v1"
        }
      ],
      "title": "Designing for Learning with Generative AI is a Wicked Problem: An Illustrative Longitudinal Qualitative Case Series",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17230",
        "HTML": "https://arxiv.org/html/2507.17230v1",
        "PDF": "https://arxiv.org/pdf/2507.17230"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "This paper discusses a GenAI-integrated creative media course, indicating that creativity is not the main focus but ties into GenAI's use in creative media education. Creativity is a secondary theme, discussed in the context of GenAI's impact on learning and careers."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17242",
      "abstract": "Brain-computer interface (BCI) technology establishes a direct communication pathway between the brain and external devices. Current visual BCI systems suffer from insufficient information transfer rates (ITRs) for practical use. Spatial information, a critical component of visual perception, remains underexploited in existing systems because the limited spatial resolution of recording methods hinders the capture of the rich spatiotemporal dynamics of brain signals. This study proposed a frequency-phase-space fusion encoding method, integrated with 256-channel high-density electroencephalogram (EEG) recordings, to develop high-speed BCI systems. In the classical frequency-phase encoding 40-target BCI paradigm, the 256-66, 128-32, and 64-21 electrode configurations brought theoretical ITR increases of 83.66%, 79.99%, and 55.50% over the traditional 64-9 setup. In the proposed frequency-phase-space encoding 200-target BCI paradigm, these increases climbed to 195.56%, 153.08%, and 103.07%. The online BCI system achieved an average actual ITR of 472.7 bpm. This study demonstrates the essential role and immense potential of high-density EEG in decoding the spatiotemporal information of visual stimuli.",
      "authors": [
        "Gege Ming (1)",
        "Weihua Pei (2 and 3)",
        "Sen Tian (4)",
        "Xiaogang Chen (5)",
        "Xiaorong Gao (1)",
        "Yijun Wang (2",
        "3 and 6) ((1) Department of Biomedical Engineering",
        "Tsinghua University",
        "(2) Laboratory of Solid-State Optoelectronics Information Technology",
        "Institute of Semiconductors",
        "Chinese Academy of Sciences",
        "(3) School of Future Technology",
        "University of Chinese Academy of Sciences",
        "(4) Suzhou Nianji Intelligent Technology Co.",
        "Ltd.",
        "(5) Institute of Biomedical Engineering",
        "Chinese Academy of Medical Sciences and Peking Union Medical College",
        "(6) Chinese Institute for Brain Research)"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Signal Processing (eess.SP)",
        "Neurons and Cognition (q-bio.NC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T06:18:39+00:00",
          "link": "https://arxiv.org/abs/2507.17242v1",
          "size": "7712kb",
          "version": "v1"
        }
      ],
      "title": "High-Density EEG Enables the Fastest Visual Brain-Computer Interfaces",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17242",
        "PDF": "https://arxiv.org/pdf/2507.17242"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on improving visual brain-computer interfaces using high-density EEG, with no mention of creativity. Its primary aim is to enhance information transfer rates in BCIs, unrelated to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17248",
      "abstract": "Interacting with real-world objects in Mixed Reality (MR) often proves difficult when they are crowded, distant, or partially occluded, hindering straightforward selection and manipulation. We observe that these difficulties stem from performing interaction directly on physical objects, where input is tightly coupled to their physical constraints. Our key insight is to decouple interaction from these constraints by introducing proxies-abstract representations of real-world objects. We embody this concept in Reality Proxy, a system that seamlessly shifts interaction targets from physical objects to their proxies during selection. Beyond facilitating basic selection, Reality Proxy uses AI to enrich proxies with semantic attributes and hierarchical spatial relationships of their corresponding physical objects, enabling novel and previously cumbersome interactions in MR - such as skimming, attribute-based filtering, navigating nested groups, and complex multi object selections - all without requiring new gestures or menu systems. We demonstrate Reality Proxy's versatility across diverse scenarios, including office information retrieval, large-scale spatial navigation, and multi-drone control. An expert evaluation suggests the system's utility and usability, suggesting that proxy-based abstractions offer a powerful and generalizable interaction paradigm for future MR systems.",
      "authors": [
        "Xiaoan Liu",
        "Difan Jia",
        "Xianhao Carton Liu",
        "Mar Gonzalez-Franco",
        "Chen Zhu-Tian"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Graphics (cs.GR)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T06:34:58+00:00",
          "link": "https://arxiv.org/abs/2507.17248v1",
          "size": "15905kb",
          "version": "v1"
        }
      ],
      "title": "Reality Proxy: Fluid Interactions with Real-World Objects in MR via Abstract Representations",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17248",
        "HTML": "https://arxiv.org/html/2507.17248v1",
        "PDF": "https://arxiv.org/pdf/2507.17248"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "Although the focus is on fluid interactions in MR via abstract representations, it indirectly addresses creativity through innovative interaction paradigms that could inspire creative applications in mixed reality environments."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17320",
      "abstract": "Discrete event sequences serve as models for numerous real-world datasets, including publications over time, project milestones, and medication dosing during patient treatments. These event sequences typically exhibit bursty behavior, where events cluster together in rapid succession, interspersed with periods of inactivity. Standard timeline charts with linear time axes fail to adequately represent such data, resulting in cluttered regions during event bursts while leaving other areas unutilized. We introduce EventLines, a novel technique that dynamically adjusts the time scale to match the underlying event distribution, enabling more efficient use of screen space. To address the challenges of non-linear time scaling, EventLines employs the time axis's visual representation itself to communicate the varying scale. We present findings from a crowdsourced graphical perception study that examines how different time scale representations influence temporal perception.",
      "authors": [
        "Yuet Ling Wong",
        "Niklas Elmqvist"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T08:34:34+00:00",
          "link": "https://arxiv.org/abs/2507.17320v1",
          "size": "613kb",
          "version": "v1"
        }
      ],
      "title": "EventLines: Time Compression for Discrete Event Timelines",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17320",
        "HTML": "https://arxiv.org/html/2507.17320v1",
        "PDF": "https://arxiv.org/pdf/2507.17320"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on a novel visualization technique for time compression in discrete event timelines, with no mention of creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17430",
      "abstract": "Integrating technology with the distinctive characteristics of craftsmanship has become a key issue in the field of digital craftsmanship. This paper introduces Layered Interactions, a design approach that seamlessly merges Human-Computer Interaction (HCI) technologies with traditional lacquerware craftsmanship. By leveraging the multi-layer structure and material properties of lacquerware, we embed interactive circuits and integrate programmable hardware within the layers, creating tangible interfaces that support diverse interactions. This method enhances the adaptability and practicality of traditional crafts in modern digital contexts. Through the development of a lacquerware toolkit, along with user experiments and semi-structured interviews, we demonstrate that this approach not only makes technology more accessible to traditional artisans but also enhances the materiality and emotional qualities of interactive interfaces. Additionally, it fosters mutual learning and collaboration between artisans and technologists. Our research introduces a cross-disciplinary perspective to the HCI community, broadening the material and design possibilities for interactive interfaces.",
      "authors": [
        "Yan Dong",
        "Hanjie Yu",
        "Yanran Chen",
        "Zipeng Zhang",
        "Qiong Wu"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T11:41:43+00:00",
          "link": "https://arxiv.org/abs/2507.17430v1",
          "size": "25336kb",
          "version": "v1"
        }
      ],
      "title": "Layered Interactions: Exploring Non-Intrusive Digital Craftsmanship Design Through Lacquer Art Interfaces",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17430",
        "HTML": "https://arxiv.org/html/2507.17430v1",
        "PDF": "https://arxiv.org/pdf/2507.17430"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper explores the integration of HCI technologies with traditional craftsmanship, emphasizing creative processes through novel interface design and cross-disciplinary collaboration. These aspects indicate a strong focus on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17524",
      "abstract": "Electroencephalography(EEG) based emotion recognition holds great promise for affective brain-computer interfaces (aBCIs), yet practical deployment remains challenging due to substantial inter-subject variability and the lack of labeled data in target domains. To overcome these limitations, we present a novel unsupervised Semantic-Dynamic Consistency domain adaptation network for fully label-free cross-subject EEG emotion recognition. First, we introduce a Same-Subject Same-Trial Mixup strategy that generates augmented samples via intra-trial interpolation, enhancing data diversity while explicitly preserving individual identity to mitigate label ambiguity. Second, we construct a dynamic distribution alignment module in reproducing kernel Hilbert space (RKHS), jointly aligning marginal and conditional distributions through multi-objective kernel mean embedding, and leveraging a confidence-aware pseudo-labeling strategy to ensure stable adaptation. Third, we propose a dual-domain similarity consistency learning mechanism that enforces cross-domain structural constraints based on latent pairwise similarities, enabling semantic boundary learning without relying on temporal synchronization or label priors. To validate the effectiveness and robustness of the proposed SDC-Net, extensive experiments are conducted on three widely used EEG benchmark datasets: SEED, SEED-IV, and Faced. Comparative results against existing unsupervised domain adaptation methods demonstrate that SDC-Net achieves state-of-the-art performance in emotion recognition under both cross-subject and cross-session conditions. This advancement significantly improves the accuracy and generalization capability of emotion decoding, and lays a solid foundation for real-world applications of personalized affective brain-computer interfaces (aBCIs). The source code will be released at https://github.com/XuanSuTrum/SDC-Net.",
      "authors": [
        "Jiahao Tang and Youjun Li and Xiangting Fan and Yangxuan Zheng and Siyuan Lu and Xueping Li and Peng Fang and Chenxi Li and Zi-Gang Huang"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T14:04:43+00:00",
          "link": "https://arxiv.org/abs/2507.17524v1",
          "size": "9653kb",
          "version": "v1"
        }
      ],
      "title": "SDC-Net: A Domain Adaptation Framework with Semantic-Dynamic Consistency for Cross-Subject EEG Emotion Recognition",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17524",
        "HTML": "https://arxiv.org/html/2507.17524v1",
        "PDF": "https://arxiv.org/pdf/2507.17524"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on emotion recognition using EEG data and domain adaptation but does not mention creativity or creative processes. It centers on technical advancements for emotion recognition, which is unrelated to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17543",
      "abstract": "The rapid growth of messaging scams creates an escalating challenge for user security and financial safety. In this paper, we present the Anticipate, Simulate, Reason (ASR) framework, a generative AI method that enables users to proactively identify and comprehend scams within instant messaging platforms. Using large language models, ASR predicts scammer responses, creates realistic scam conversations, and delivers real-time, interpretable support to end-users. We develop ScamGPT-J, a domain-specific language model fine-tuned on a new, high-quality dataset of scam conversations covering multiple scam types. Thorough experimental evaluation shows that the ASR framework substantially enhances scam detection, particularly in challenging contexts such as job scams, and uncovers important demographic patterns in user vulnerability and perceptions of AI-generated assistance. Our findings reveal a contradiction where those most at risk are often least receptive to AI support, emphasizing the importance of user-centered design in AI-driven fraud prevention. This work advances both the practical and theoretical foundations for interpretable, human-centered AI systems in combating evolving digital threats.",
      "authors": [
        "Xue Wen Tan",
        "Kenneth See",
        "Stanley Kok"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T14:20:09+00:00",
          "link": "https://arxiv.org/abs/2507.17543v1",
          "size": "39165kb",
          "version": "v1"
        }
      ],
      "title": "Anticipate, Simulate, Reason (ASR): A Comprehensive Generative AI Framework for Combating Messaging Scams",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17543",
        "HTML": "https://arxiv.org/html/2507.17543v1",
        "PDF": "https://arxiv.org/pdf/2507.17543"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "Although this paper discusses a generative AI framework, it primarily deals with combating messaging scams and security issues. Creativity is not a focus or theme in the presented work."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17597",
      "abstract": "As surgery embraces digital transformation--integrating sophisticated imaging, advanced algorithms, and robotics to support and automate complex sub-tasks--human judgment of system correctness remains a vital safeguard for patient safety. This shift introduces new \"operator-type\" roles tasked with verifying complex algorithmic outputs, particularly at critical junctures of the procedure, such as the intermediary check before drilling or implant placement. A prime example is 2D/3D registration, a key enabler of image-based surgical navigation that aligns intraoperative 2D images with preoperative 3D data. Although registration algorithms have advanced significantly, they occasionally yield inaccurate results. Because even small misalignments can lead to revision surgery or irreversible surgical errors, there is a critical need for robust quality assurance. Current visualization-based strategies alone have been found insufficient to enable humans to reliably detect 2D/3D registration misalignments. In response, we propose the first artificial intelligence (AI) framework trained specifically for 2D/3D registration quality verification, augmented by explainability features that clarify the model's decision-making. Our explainable AI (XAI) approach aims to enhance informed decision-making for human operators by providing a second opinion together with a rationale behind it. Through algorithm-centric and human-centered evaluations, we systematically compare four conditions: AI-only, human-only, human-AI, and human-XAI. Our findings reveal that while explainability features modestly improve user trust and willingness to override AI errors, they do not exceed the standalone AI in aggregate performance. Nevertheless, future work extending both the algorithmic design and the human-XAI collaboration elements holds promise for more robust quality assurance of 2D/3D registration.",
      "authors": [
        "Sue Min Cho",
        "Alexander Do",
        "Russell H. Taylor",
        "Mathias Unberath"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T15:28:57+00:00",
          "link": "https://arxiv.org/abs/2507.17597v1",
          "size": "727kb",
          "version": "v1"
        }
      ],
      "title": "Explainable AI for Collaborative Assessment of 2D/3D Registration Quality",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17597",
        "HTML": "https://arxiv.org/html/2507.17597v1",
        "PDF": "https://arxiv.org/pdf/2507.17597"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper addresses explainable AI in the context of surgical procedures and 2D/3D registration quality. There is no mention of creativity, and the focus is on quality assurance and explainability in medical settings."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17688",
      "abstract": "Mindfulness training is widely recognized for its benefits in reducing depression, anxiety, and loneliness. With the rise of smartphone-based mindfulness apps, digital meditation has become more accessible, but sustaining long-term user engagement remains a challenge. This paper explores whether respiration biosignal feedback and mindfulness skill estimation enhance system usability and skill development. We develop a smartphone's accelerometer-based respiration tracking algorithm, eliminating the need for additional wearables. Unlike existing methods, our approach accurately captures slow breathing patterns typical of mindfulness meditation. Additionally, we introduce the first quantitative framework to estimate mindfulness skills-concentration, sensory clarity, and equanimity-based on accelerometer-derived respiration data. We develop and test our algorithms on 261 mindfulness sessions in both controlled and real-world settings. A user study comparing an experimental group receiving biosignal feedback with a control group using a standard app shows that respiration feedback enhances system usability. Our respiration tracking model achieves a mean absolute error (MAE) of 1.6 breaths per minute, closely aligning with ground truth data, while our mindfulness skill estimation attains F1 scores of 80-84% in tracking skill progression. By integrating respiration tracking and mindfulness estimation into a commercial app, we demonstrate the potential of smartphone sensors to enhance digital mindfulness training.",
      "authors": [
        "Mohammad Nur Hossain Khan",
        "David creswell",
        "Jordan Albert",
        "Patrick O'Connell",
        "Shawn Fallon",
        "Mathew Polowitz",
        "Xuhai \"orson\" Xu",
        "and Bashima islam"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T16:52:42+00:00",
          "link": "https://arxiv.org/abs/2507.17688v1",
          "size": "2216kb",
          "version": "v1"
        }
      ],
      "title": "Mindfulness Meditation and Respiration: Accelerometer-Based Respiration Rate and Mindfulness Progress Estimation to Enhance App Engagement and Mindfulness Skills",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17688",
        "PDF": "https://arxiv.org/pdf/2507.17688"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "This paper explores mindfulness and skill estimation, which can tangentially relate to creativity through enhancing mental clarity and focus. However, creativity is not directly discussed, making it a secondary theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17734",
      "abstract": "Creating aesthetically pleasing data visualizations remains challenging for users without design expertise or familiarity with visualization tools. To address this gap, we present DataWink, a system that enables users to create custom visualizations by adapting high-quality examples. Our approach combines large multimodal models (LMMs) to extract data encoding from existing SVG-based visualization examples, featuring an intermediate representation of visualizations that bridges primitive SVG and visualization programs. Users may express adaptation goals to a conversational agent and control the visual appearance through widgets generated on demand. With an interactive interface, users can modify both data mappings and visual design elements while maintaining the original visualization's aesthetic quality. To evaluate DataWink, we conduct a user study (N=12) with replication and free-form exploration tasks. As a result, DataWink is recognized for its learnability and effectiveness in personalized authoring tasks. Our results demonstrate the potential of example-driven approaches for democratizing visualization creation.",
      "authors": [
        "Liwenhan Xie",
        "Yanna Lin",
        "Can Liu",
        "Huamin Qu",
        "Xinhuan Shu"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T17:50:34+00:00",
          "link": "https://arxiv.org/abs/2507.17734v1",
          "size": "8276kb",
          "version": "v1"
        }
      ],
      "title": "DataWink: Reusing and Adapting SVG-based Visualization Examples with Large Multimodal Models",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17734",
        "HTML": "https://arxiv.org/html/2507.17734v1",
        "PDF": "https://arxiv.org/pdf/2507.17734"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper focuses on democratizing the creation of aesthetically pleasing data visualizations, which is a creative process. It involves the adaptation of high-quality examples to allow custom visualization creation by users without design expertise, indicating a core focus on creativity in visual design."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17174",
      "abstract": "Despite the widespread use of Uniform Manifold Approximation and Projection (UMAP), the impact of its stochastic optimization process on the results remains underexplored. We observed that it often produces unstable results where the projections of data points are determined mostly by chance rather than reflecting neighboring structures. To address this limitation, we introduce (r,d)-stability to UMAP: a framework that analyzes the stochastic positioning of data points in the projection space. To assess how stochastic elements, specifically initial projection positions and negative sampling, impact UMAP results, we introduce \"ghosts\", or duplicates of data points representing potential positional variations due to stochasticity. We define a data point's projection as (r,d)-stable if its ghosts perturbed within a circle of radius r in the initial projection remain confined within a circle of radius d for their final positions. To efficiently compute the ghost projections, we develop an adaptive dropping scheme that reduces a runtime up to 60% compared to an unoptimized baseline while maintaining approximately 90% of unstable points. We also present a visualization tool that supports the interactive exploration of the (r,d)-stability of data points. Finally, we demonstrate the effectiveness of our framework by examining the stability of projections of real-world datasets and present usage guidelines for the effective use of our framework.",
      "authors": [
        "Myeongwon Jung",
        "Takanori Fujiwara",
        "and Jaemin Jo"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Graphics (cs.GR)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T03:40:53+00:00",
          "link": "https://arxiv.org/abs/2507.17174v1",
          "size": "7850kb",
          "version": "v1"
        }
      ],
      "title": "GhostUMAP2: Measuring and Analyzing (r,d)-Stability of UMAP",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17174",
        "HTML": "https://arxiv.org/html/2507.17174v1",
        "PDF": "https://arxiv.org/pdf/2507.17174"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses stability in UMAP, an algorithm for data visualization. While it covers optimization and analysis, it does not address creativity-related aspects."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17264",
      "abstract": "Prompting foundation models (FMs) like large language models (LLMs) have enabled new AI-powered software features (e.g., text summarization) that previously were only possible by fine-tuning FMs. Now, developers are embedding prompts in software, known as prompt programs. The process of prompt programming requires the developer to make many changes to their prompt. Yet, the questions developers ask to update their prompt is unknown, despite the answers to these questions affecting how developers plan their changes. With the growing number of research and commercial prompt programming tools, it is unclear whether prompt programmers' needs are being adequately addressed. We address these challenges by developing a taxonomy of 25 tasks prompt programmers do and 51 questions they ask, measuring the importance of each task and question. We interview 16 prompt programmers, observe 8 developers make prompt changes, and survey 50 developers. We then compare the taxonomy with 48 research and commercial tools. We find that prompt programming is not well-supported: all tasks are done manually, and 16 of the 51 questions -- including a majority of the most important ones -- remain unanswered. Based on this, we outline important opportunities for prompt programming tools.",
      "authors": [
        "Jenny T. Liang",
        "Chenyang Yang",
        "Agnia Sergeyuk",
        "Travis D. Breaux",
        "Brad A. Myers"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Software Engineering (cs.SE)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T07:01:44+00:00",
          "link": "https://arxiv.org/abs/2507.17264v1",
          "size": "215kb",
          "version": "v1"
        }
      ],
      "title": "Understanding Prompt Programming Tasks and Questions",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17264",
        "PDF": "https://arxiv.org/pdf/2507.17264"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper relates to prompt programming tasks, a creative process of crafting prompts for AI models. Creativity is not the main focus but is implicit in developing and refining prompts for novel AI-powered features."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17265",
      "abstract": "We present a novel visualization-driven illumination model for density plots, a new technique to enhance density plots by effectively revealing the detailed structures in high- and medium-density regions and outliers in low-density regions, while avoiding artifacts in the density field's colors. When visualizing large and dense discrete point samples, scatterplots and dot density maps often suffer from overplotting, and density plots are commonly employed to provide aggregated views while revealing underlying structures. Yet, in such density plots, existing illumination models may produce color distortion and hide details in low-density regions, making it challenging to look up density values, compare them, and find outliers. The key novelty in this work includes (i) a visualization-driven illumination model that inherently supports density-plot-specific analysis tasks and (ii) a new image composition technique to reduce the interference between the image shading and the color-encoded density values. To demonstrate the effectiveness of our technique, we conducted a quantitative study, an empirical evaluation of our technique in a controlled study, and two case studies, exploring twelve datasets with up to two million data point samples.",
      "authors": [
        "Xin Chen",
        "Yunhai Wang",
        "Huaiwei Bao",
        "Kecheng Lu",
        "Jaemin Jo",
        "Chi-Wing Fu",
        "Jean-Daniel Fekete"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Graphics (cs.GR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T07:02:13+00:00",
          "link": "https://arxiv.org/abs/2507.17265v1",
          "size": "8643kb",
          "version": "v1"
        }
      ],
      "title": "Visualization-Driven Illumination for Density Plots",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17265",
        "HTML": "https://arxiv.org/html/2507.17265v1",
        "PDF": "https://arxiv.org/pdf/2507.17265"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper centers on visualization techniques for enhancing density plots, with no emphasis on creativity. Its focus is on improving data visualization for analysis rather than creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17401",
      "abstract": "Affordances - i.e. possibilities for action that an environment or objects in it provide - are important for robots operating in human environments to perceive. Existing approaches train such capabilities on annotated static images or shapes. This work presents a novel dataset for affordance learning of common household tasks. Unlike previous approaches, our dataset consists of video sequences demonstrating the tasks from first- and third-person perspectives, along with metadata about the affordances that are manifested in the task, and is aimed towards training perception systems to recognize affordance manifestations. The demonstrations were collected from several participants and in total record about seven hours of human activity. The variety of task performances also allows studying preparatory maneuvers that people may perform for a task, such as how they arrange their task space, which is also relevant for collaborative service robots.",
      "authors": [
        "Rachel Ringe",
        "Mihai Pomarlan",
        "Nikolaos Tsiogkas",
        "Stefano De Giorgis",
        "Maria Hedblom",
        "Rainer Malaka"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T10:57:59+00:00",
          "link": "https://arxiv.org/abs/2507.17401v1",
          "size": "664kb",
          "version": "v1"
        }
      ],
      "title": "The Wilhelm Tell Dataset of Affordance Demonstrations",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17401",
        "HTML": "https://arxiv.org/html/2507.17401v1",
        "PDF": "https://arxiv.org/pdf/2507.17401"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses affordance learning for robots using a novel dataset of household tasks, without addressing creativity as a component of the research."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17481",
      "abstract": "Artificial intelligence has deeply permeated numerous fields, especially the design area which relies on technology as a tool for innovation. This change naturally extends to the field of design education, which is closest to design practice. This has led to further exploration of the impact of AI on college-level education in the design discipline. This study aims to examine how current design educators perceive the role of AI in college-level design education, their perspectives on integrating AI into teaching and research, and their concerns regarding its potential challenges in design education and research. Through qualitative, semi-structured, in-depth interviews with seven faculties in U.S. design colleges, the findings reveal that AI, as a tool and source of information, has become an integral part of design education. AI- derived functionalities are increasingly utilized in design software, and educators are actively incorporating AI as a theoretical framework in their teaching. Educators can guide students in using AI tools, but only if they first acquire a strong foundation in basic design principles and skills. This study also indicates the importance of promoting a cooperative relationship between design educators and AI. At the same time, educators express anticipation for advancements in ethical standards, authenticity, and the resolution of copyright issues related to AI.",
      "authors": [
        "Lizhu Zhang and Cecilia X. Wang"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T13:02:37+00:00",
          "link": "https://arxiv.org/abs/2507.17481v1",
          "size": "560kb",
          "version": "v1"
        }
      ],
      "title": "AI in Design Education at College Level-Educators' Perspectives and Challenges",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17481",
        "PDF": "https://arxiv.org/pdf/2507.17481"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper examines how AI is integrated into design education and its role in innovation, which touches on fostering creativity in students, though it is not the central theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17518",
      "abstract": "Digital Twins (DTs) are gaining prominence in cybersecurity for their ability to replicate complex IT (Information Technology), OT (Operational Technology), and IoT (Internet of Things) infrastructures, allowing for real time monitoring, threat analysis, and system simulation. This study investigates how integrating DTs with penetration testing tools and Large Language Models (LLMs) can enhance cybersecurity education and operational readiness. By simulating realistic cyber environments, this approach offers a practical, interactive framework for exploring vulnerabilities and defensive strategies. At the core of this research is the Red Team Knife (RTK), a custom penetration testing toolkit aligned with the Cyber Kill Chain model. RTK is designed to guide learners through key phases of cyberattacks, including reconnaissance, exploitation, and response within a DT powered ecosystem. The incorporation of Large Language Models (LLMs) further enriches the experience by providing intelligent, real-time feedback, natural language threat explanations, and adaptive learning support during training exercises. This combined DT LLM framework is currently being piloted in academic settings to develop hands on skills in vulnerability assessment, threat detection, and security operations. Initial findings suggest that the integration significantly improves the effectiveness and relevance of cybersecurity training, bridging the gap between theoretical knowledge and real-world application. Ultimately, the research demonstrates how DTs and LLMs together can transform cybersecurity education to meet evolving industry demands.",
      "authors": [
        "Vita Santa Barletta",
        "Vito Bavaro",
        "Miriana Calvano",
        "Antonio Curci",
        "Antonio Piccinno",
        "Davide Pio Posa"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Artificial Intelligence (cs.AI)",
        "Computers and Society (cs.CY)",
        "Human-Computer Interaction (cs.HC)",
        "Software Engineering (cs.SE)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T13:55:35+00:00",
          "link": "https://arxiv.org/abs/2507.17518v1",
          "size": "1300kb",
          "version": "v1"
        }
      ],
      "title": "Enabling Cyber Security Education through Digital Twins and Generative AI",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17518",
        "HTML": "https://arxiv.org/html/2507.17518v1",
        "PDF": "https://arxiv.org/pdf/2507.17518"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper primarily focuses on enhancing cybersecurity education using Digital Twins and Generative AI, without any direct connection to creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17718",
      "abstract": "With the rise of voice-enabled artificial intelligence (AI) systems, quantitative survey researchers have access to a new data-collection mode: AI telephone surveying. By using AI to conduct phone interviews, researchers can scale quantitative studies while balancing the dual goals of human-like interactivity and methodological rigor. Unlike earlier efforts that used interactive voice response (IVR) technology to automate these surveys, voice AI enables a more natural and adaptive respondent experience as it is more robust to interruptions, corrections, and other idiosyncrasies of human speech.\n  We built and tested an AI system to conduct quantitative surveys based on large language models (LLM), automatic speech recognition (ASR), and speech synthesis technologies. The system was specifically designed for quantitative research, and strictly adhered to research best practices like question order randomization, answer order randomization, and exact wording.\n  To validate the system's effectiveness, we deployed it to conduct two pilot surveys with the SSRS Opinion Panel and followed-up with a separate human-administered survey to assess respondent experiences. We measured three key metrics: the survey completion rates, break-off rates, and respondent satisfaction scores. Our results suggest that shorter instruments and more responsive AI interviewers may contribute to improvements across all three metrics studied.",
      "authors": [
        "Danny D. Leybzon",
        "Shreyas Tirumala",
        "Nishant Jain",
        "Summer Gillen",
        "Michael Jackson",
        "Cameron McPhee",
        "Jennifer Schmidt"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T17:30:14+00:00",
          "link": "https://arxiv.org/abs/2507.17718v1",
          "size": "2753kb",
          "version": "v1"
        }
      ],
      "title": "AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17718",
        "HTML": "https://arxiv.org/html/2507.17718v1",
        "PDF": "https://arxiv.org/pdf/2507.17718"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper explores AI systems in the context of telephone surveying for data collection. The focus is on improving survey methodologies, with no relation to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.17744",
      "abstract": "Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \\method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \\sekai to train \\method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on https://github.com/stdstu12/YUME. Yume will update monthly to achieve its original goal. Project page: https://stdstu12.github.io/YUME-Project/.",
      "authors": [
        "Xiaofeng Mao",
        "Shaoheng Lin",
        "Zhen Li",
        "Chuanhao Li",
        "Wenshuo Peng",
        "Tong He",
        "Jiangmiao Pang",
        "Mingmin Chi",
        "Yu Qiao",
        "Kaipeng Zhang"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-23T17:57:09+00:00",
          "link": "https://arxiv.org/abs/2507.17744v1",
          "size": "2986kb",
          "version": "v1"
        }
      ],
      "title": "Yume: An Interactive World Generation Model",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.17744",
        "HTML": "https://arxiv.org/html/2507.17744v1",
        "PDF": "https://arxiv.org/pdf/2507.17744"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "While the paper discusses creating interactive, realistic, and dynamic worlds from media inputs, which could enhance creativity in digital content creation, creativity is not the central theme. The focus is more on technical advancements in world generation and user interaction, making creativity a secondary theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2408.08068",
      "abstract": "Informal Knowledge Sharing (KS) is vital for end-user programmers to gain expertise. To better understand how personal (self-efficacy), social (reputational gains, trust between colleagues), and software-related (codification effort) variables influence spreadsheet KS intention, we conducted a multiple regressions analysis based on survey data from spreadsheet users (n=100) in administrative and finance roles. We found that high levels of spreadsheet self-efficacy and a perception that sharing would result in reputational gains predicted higher KS intention, but individuals who found knowledge codification effortful showed lower KS intention. We also observed that regardless of occupation, users tended to report a lower sense of self-efficacy in their general spreadsheet proficiency, despite also reporting high self-efficacy in spreadsheet use for job-related contexts. Our findings suggest that acknowledging and designing for these social and personal variables can help avoid situations where experienced individuals refrain unnecessarily from sharing, with implications for spreadsheet design.",
      "authors": [
        "Qing (Nancy) Xia",
        "Advait Sarkar",
        "Duncan P. Brumby",
        "Anna Cox"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-08-15T10:31:46+00:00",
          "link": "https://arxiv.org/abs/2408.08068v1",
          "size": "143kb",
          "version": "v1"
        },
        {
          "date": "2025-07-23T13:12:15+00:00",
          "link": "https://arxiv.org/abs/2408.08068v2",
          "size": "143kb",
          "version": "v2"
        }
      ],
      "title": "The Paradox of Spreadsheet Self-Efficacy: Social Incentives for Informal Knowledge Sharing in End-User Programming",
      "links": {
        "Abstract": "https://arxiv.org/abs/2408.08068",
        "HTML": "https://arxiv.org/html/2408.08068v2",
        "PDF": "https://arxiv.org/pdf/2408.08068"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper examines informal knowledge sharing in end-user programming with a focus on variables influencing spreadsheet KS intention. It does not engage with creativity-related themes."
      },
      "source": "arXiv"
    },
    {
      "id": "2409.14659",
      "abstract": "Visual content on social media plays a key role in entertainment and information sharing, yet some images gain more engagement than others. We propose that image memorability - the ability to be remembered - may predict viral potential. Using 1,247 Reddit image posts across three timepoints, we assessed memorability with neural network ResMem and correlated the predicted memorability scores with virality metrics. Memorable images are consistently associated with more comments, even after controlling for image categories with ResNet-152. Semantic analysis revealed that memorable images relate to more neutral-affect comments, suggesting a distinct pathway to virality from emotional contents. Additionally, visual consistency analysis showed that memorable posts inspired diverse, externally-associated comments. By analyzing ResMem's layers, we found that semantic distinctiveness was key to both memorability and virality even after accounting for image category effects. This study highlights memorability as a unique correlate of social media virality, offering insights into how visual features and human cognitive behavioral interactions are associated with online engagement.",
      "authors": [
        "Shikang Peng",
        "Wilma A. Bainbridge"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computational Engineering, Finance, and Science (cs.CE)",
        "Social and Information Networks (cs.SI)"
      ],
      "submission_historys": [
        {
          "date": "2024-09-23T02:00:34+00:00",
          "link": "https://arxiv.org/abs/2409.14659v1",
          "size": "1801kb",
          "version": "v1"
        },
        {
          "date": "2025-07-22T22:35:07+00:00",
          "link": "https://arxiv.org/abs/2409.14659v2",
          "size": "1934kb",
          "version": "v2"
        }
      ],
      "title": "Image memorability predicts social media virality and externally-associated commenting",
      "links": {
        "Abstract": "https://arxiv.org/abs/2409.14659",
        "PDF": "https://arxiv.org/pdf/2409.14659"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The study on image memorability's prediction of social media virality could have implications for creative content creation, making it a secondary theme related to creativity."
      },
      "repo_urls": [
        "https://github.com/shikangpeng/memoMedia"
      ],
      "source": "arXiv"
    },
    {
      "id": "2501.06348",
      "abstract": "Understanding the motivations underlying the human inclination to automate tasks is vital to developing truly helpful robots integrated into daily life. Accordingly, we ask: are individuals more inclined to automate chores based on the time they consume or the feelings experienced while performing them? This study explores these preferences and whether they vary across different social groups (i.e., gender category and income level). Leveraging data from the BEHAVIOR-1K dataset, the American Time-Use Survey, and the American Time-Use Survey Well-Being Module, we investigate the relationship between the desire for automation, time spent on daily activities, and their associated feelings - Happiness, Meaningfulness, Sadness, Painfulness, Stressfulness, or Tiredness. Our key findings show that, despite common assumptions, time spent does not strongly relate to the desire for automation for the general population. For the feelings analyzed, only happiness and pain are key indicators. Significant differences by gender and economic level also emerged: Women prefer to automate stressful activities, whereas men prefer to automate those that make them unhappy; mid-income individuals prioritize automating less enjoyable and meaningful activities, while low and high-income show no significant correlations. We hope our research helps motivate technologies to develop robots that match the priorities of potential users, moving domestic robotics toward more socially relevant solutions. We open-source all the data, including an online tool that enables the community to replicate our analysis and explore additional trends at https://robin-lab.cs.utexas.edu/why-automate-this/.",
      "authors": [
        "Ruchira Ray",
        "Leona Pang",
        "Sanjana Srivastava",
        "Li Fei-Fei",
        "Samantha Shorey",
        "Roberto Mart\\'in-Mart\\'in"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "2025-01-10T21:20:11+00:00",
          "link": "https://arxiv.org/abs/2501.06348v1",
          "size": "2403kb",
          "version": "v1"
        },
        {
          "date": "2025-07-20T16:18:12+00:00",
          "link": "https://arxiv.org/abs/2501.06348v2",
          "size": "1860kb",
          "version": "v2"
        },
        {
          "date": "2025-07-22T17:44:37+00:00",
          "link": "https://arxiv.org/abs/2501.06348v3",
          "size": "1860kb",
          "version": "v3"
        }
      ],
      "title": "Why Automate This? Exploring Correlations between Desire for Robotic Automation, Invested Time and Well-Being",
      "links": {
        "Abstract": "https://arxiv.org/abs/2501.06348",
        "PDF": "https://arxiv.org/pdf/2501.06348"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper examines the motivations for automating tasks based on time and well-being but does not address creativity-related themes or processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2501.08518",
      "abstract": "Seasickness poses a widespread problem that adversely impacts both passenger comfort and the operational efficiency of maritime crews. Although attention shift has been proposed as a potential method to alleviate symptoms of motion sickness, its efficacy remains to be rigorously validated, especially in maritime environments. In this study, we develop an AI-driven brain-computer interface (BCI) to realize sustained and practical attention shift by incorporating tasks such as breath counting. Forty-three participants completed a real-world nautical experiment consisting of a real-feedback session, a resting session, and a pseudo-feedback session. Notably, 81.39\\% of the participants reported that the BCI intervention was effective. EEG analysis revealed that the proposed system can effectively regulate motion sickness EEG signatures, such as an decrease in total band power, along with an increase in theta relative power and a decrease in beta relative power. Furthermore, an indicator of attentional focus, the theta/beta ratio, exhibited a significant reduction during the real-feedback session, providing further evidence to support the effectiveness of the BCI in shifting attention. Collectively, this study presents a novel nonpharmacological, portable, and effective approach for seasickness intervention, which has the potential to open up a brand-new application domain for BCIs.",
      "authors": [
        "Xiaoyu Bao and Kailin Xu and Jiawei Zhu and Haiyun Huang and Kangning Li and Qiyun Huang and Yuanqing Li"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Signal Processing (eess.SP)",
        "Quantitative Methods (q-bio.QM)"
      ],
      "submission_historys": [
        {
          "date": "2025-01-15T02:06:29+00:00",
          "link": "https://arxiv.org/abs/2501.08518v1",
          "size": "14355kb",
          "version": "v1"
        },
        {
          "date": "2025-07-23T08:01:28+00:00",
          "link": "https://arxiv.org/abs/2501.08518v2",
          "size": "2744kb",
          "version": "v2"
        }
      ],
      "title": "Alleviating Seasickness through Brain-Computer Interface-based Attention Shift",
      "links": {
        "Abstract": "https://arxiv.org/abs/2501.08518",
        "HTML": "https://arxiv.org/html/2501.08518v2",
        "PDF": "https://arxiv.org/pdf/2501.08518"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper discusses alleviating seasickness through brain-computer interfaces but does not relate to creativity."
      },
      "tasks": [
        "Brain Computer Interface",
        "EEG"
      ],
      "source": "arXiv"
    },
    {
      "id": "2507.14792",
      "abstract": "Information processing tasks involve complex cognitive mechanisms that are shaped by various factors, including individual goals, prior experience, and system environments. Understanding such behaviors requires a sophisticated and personalized data capture of how one interacts with modern information systems (e.g., web search engines). Passive sensors, such as wearables, capturing physiological and behavioral data, have the potential to provide solutions in this context. This paper presents a novel dataset, SenseSeek, designed to evaluate the effectiveness of consumer-grade sensors in a complex information processing scenario: searching via systems (e.g., search engines), one of the common strategies users employ for information seeking. The SenseSeek dataset comprises data collected from 20 participants, 235 trials of the stimulated search process, 940 phases of stages in the search process, including the realization of Information Need (IN), Query Formulation (QF), Query Submission by Typing (QS-T) or Speaking (QS-S), and Relevance Judgment by Reading (RJ-R) or Listening (RJ-L). The data includes Electrodermal Activities (EDA), Electroencephalogram (EEG), PUPIL, GAZE, and MOTION data, which were captured using consumer-grade sensors. It also contains 258 features extracted from the sensor data, the gaze-annotated screen recordings, and task responses. We validate the usefulness of the dataset by providing baseline analysis on the impacts of different cognitive intents and interaction modalities on the sensor data, and effectiveness of the data in discriminating the search stages. To our knowledge, SenseSeek is the first dataset that characterizes the multiple stages involved in information seeking with physiological signals collected from multiple sensors. We hope this dataset can serve as a reference for future research on information-seeking behaviors.",
      "authors": [
        "Kaixin Ji",
        "Danula Hettiachchi",
        "Falk Scholer",
        "Flora D. Salim",
        "Damiano Spina"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-20T02:35:47+00:00",
          "link": "https://arxiv.org/abs/2507.14792v1",
          "size": "1884kb",
          "version": "v1"
        },
        {
          "date": "2025-07-23T03:42:07+00:00",
          "link": "https://arxiv.org/abs/2507.14792v2",
          "size": "1777kb",
          "version": "v2"
        }
      ],
      "title": "SenseSeek Dataset: Multimodal Sensing to Study Information Seeking Behaviors",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.14792",
        "HTML": "https://arxiv.org/html/2507.14792v2",
        "PDF": "https://arxiv.org/pdf/2507.14792"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper is about multimodal sensing and information-seeking behaviors. It presents sensor data for analyzing cognitive mechanisms without discussing creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.16258",
      "abstract": "Autonomous mobility systems increasingly operate in environments shared with animals, from urban pets to wildlife. However, their design has largely focused on human interaction, with limited understanding of how non-human species perceive, respond to, or are affected by these systems. Motivated by research in Animal-Computer Interaction (ACI) and more-than-human design, this study investigates animal interactions with autonomous mobility through a multi-method approach combining a scoping review (45 articles), online ethnography (39 YouTube videos and 11 Reddit discussions), and expert interviews (8 participants). Our analysis surfaces five key areas of concern: Physical Impact (e.g., collisions, failures to detect), Behavioural Effects (e.g., avoidance, stress), Accessibility Concerns (particularly for service animals), Ethics and Regulations, and Urban Disturbance. We conclude with design and policy directions aimed at supporting multispecies coexistence in the age of autonomous systems. This work underscores the importance of incorporating non-human perspectives to ensure safer, more inclusive futures for all species.",
      "authors": [
        "Tram Thi Minh Tran",
        "Xinyan Yu",
        "Marius Hoggenmueller",
        "Callum Parker",
        "Paul Schmitt",
        "Julie Stephany Berrio Perez",
        "Stewart Worrall",
        "Martin Tomitsch"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-22T06:15:56+00:00",
          "link": "https://arxiv.org/abs/2507.16258v1",
          "size": "197kb",
          "version": "v1"
        },
        {
          "date": "2025-07-23T01:48:01+00:00",
          "link": "https://arxiv.org/abs/2507.16258v2",
          "size": "197kb",
          "version": "v2"
        }
      ],
      "title": "Animal Interaction with Autonomous Mobility Systems: Designing for Multi-Species Coexistence",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.16258",
        "HTML": "https://arxiv.org/html/2507.16258v2",
        "PDF": "https://arxiv.org/pdf/2507.16258"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper examines animal interactions with autonomous mobility systems and does not involve discussions related to creativity or creative processes."
      },
      "source": "arXiv"
    },
    {
      "id": "2501.06488",
      "abstract": "Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting, effectively creates photorealistic scenes from sparse viewpoints, typically evaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However, these full-reference methods, which compare synthesized views to reference views, may not fully capture the perceptual quality of neurally synthesized scenes (NSS), particularly due to the limited availability of dense reference views. Furthermore, the challenges in acquiring human perceptual labels hinder the creation of extensive labeled datasets, risking model overfitting and reduced generalizability. To address these issues, we propose NVS-SQA, a NSS quality assessment method to learn no-reference quality representations through self-supervision without reliance on human labels. Traditional self-supervised learning predominantly relies on the \"same instance, similar representation\" assumption and extensive datasets. However, given that these conditions do not apply in NSS quality assessment, we employ heuristic cues and quality scores as learning objectives, along with a specialized contrastive pair preparation process to improve the effectiveness and efficiency of learning. The results show that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e., on average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second best) and even exceeds 16 full-reference methods across all evaluation metrics (i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).",
      "authors": [
        "Qiang Qu",
        "Yiran Shen",
        "Xiaoming Chen",
        "Yuk Ying Chung",
        "Weidong Cai",
        "Tongliang Liu"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Multimedia (cs.MM)",
        "Image and Video Processing (eess.IV)"
      ],
      "submission_historys": [
        {
          "date": "2025-01-11T09:12:43+00:00",
          "link": "https://arxiv.org/abs/2501.06488v1",
          "size": "33342kb",
          "version": "v1"
        },
        {
          "date": "2025-07-23T02:32:10+00:00",
          "link": "https://arxiv.org/abs/2501.06488v2",
          "size": "33453kb",
          "version": "v2"
        }
      ],
      "title": "NVS-SQA: Exploring Self-Supervised Quality Representation Learning for Neurally Synthesized Scenes without References",
      "links": {
        "Abstract": "https://arxiv.org/abs/2501.06488",
        "HTML": "https://arxiv.org/html/2501.06488v2",
        "PDF": "https://arxiv.org/pdf/2501.06488"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on quality assessment for neurally synthesized scenes without references, which does not involve creativity as a research theme."
      },
      "tasks": [
        "NeRF",
        "Representation Learning",
        "Self-Supervised Learning",
        "SSIM"
      ],
      "repo_urls": [
        "https://github.com/vincentqqu/nvs-sqa"
      ],
      "source": "arXiv"
    },
    {
      "id": "2502.13294",
      "abstract": "The implementation of responsible AI in an organization is inherently complex due to the involvement of multiple stakeholders, each with their unique set of goals and responsibilities across the entire AI lifecycle. These responsibilities are often ambiguously defined and assigned, leading to confusion, miscommunication, and inefficiencies. Even when responsibilities are clearly defined and assigned to specific roles, the corresponding AI actors lack effective tools to support their execution.\n  Toward closing these gaps, we present a systematic review and comprehensive meta-analysis of the current state of responsible AI tools, focusing on their alignment with specific stakeholder roles and their responsibilities in various AI lifecycle stages. We categorize over 220 tools according to AI actors and stages they address. Our findings reveal significant imbalances across the stakeholder roles and lifecycle stages addressed. The vast majority of available tools have been created to support AI designers and developers specifically during data-centric and statistical modeling stages while neglecting other roles such as institutional leadership, deployers, end-users, and impacted communities, and stages such as value proposition and deployment. The uneven distribution we describe here highlights critical gaps that currently exist in responsible AI governance research and practice. Our analysis reveals that despite the myriad of frameworks and tools for responsible AI, it remains unclear \\emph{who} within an organization and \\emph{when} in the AI lifecycle a tool applies. Furthermore, existing tools are rarely validated, leaving critical gaps in their usability and effectiveness. These gaps provide a starting point for researchers and practitioners to create more effective and holistic approaches to responsible AI development and governance.",
      "authors": [
        "Blaine Kuehnert",
        "Rachel M. Kim",
        "Jodi Forlizzi",
        "Hoda Heidari"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-02-18T21:31:31+00:00",
          "link": "https://arxiv.org/abs/2502.13294v1",
          "size": "1547kb",
          "version": "v1"
        },
        {
          "date": "2025-07-23T14:52:43+00:00",
          "link": "https://arxiv.org/abs/2502.13294v2",
          "size": "197kb",
          "version": "v2"
        }
      ],
      "title": "The \"Who\", \"What\", and \"How\" of Responsible AI Governance: A Systematic Review and Meta-Analysis of (Actor, Stage)-Specific Tools",
      "links": {
        "Abstract": "https://arxiv.org/abs/2502.13294",
        "HTML": "https://arxiv.org/html/2502.13294v2",
        "PDF": "https://arxiv.org/pdf/2502.13294"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper addresses responsible AI governance and tools, which does not involve creativity as a research theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.04260",
      "abstract": "Practitioners building online services and tools often turn to online forums such as Reddit, Law Stack Exchange, and Stack Overflow for legal guidance to ensure compliance with the GDPR. The legal information presented in these forums directly impacts present-day industry practitioner's decisions. Online forums can serve as gateways that, depending on the accuracy and quality of the answers provided, may either support or undermine the protection of privacy and data protection fundamental rights. However, there is a need for deeper investigation into practitioners' decision-making processes and their understanding of legal compliance when seeking for legal information online.\n  Using GDPR's ``legitimate interests'' legal ground for processing personal data as a case study, we investigate how practitioners use online forums to identify common areas of confusion in applying legitimate interests in practice, and evaluate how legally sound online forum responses are.\n  Our analysis found that applying the legal basis of legitimate interest is complex for practitioners, with important implications for how the GDPR is implemented in practice. The legal analysis showed that crowdsourced legal information tends to be legally sound, though sometimes incomplete. We outline recommendations to improve the quality of online forums by ensuring that responses are more legally sound and comprehensive, enabling practitioners to apply legitimate interests effectively in practice and uphold the GDPR.",
      "authors": [
        "Lin Kyi",
        "Cristiana Santos",
        "Sushil Ammanaghatta Shivakumar",
        "Franziska Roesner and Asia Biega"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-02T20:16:01+00:00",
          "link": "https://arxiv.org/abs/2506.04260v1",
          "size": "125kb",
          "version": "v1"
        },
        {
          "date": "2025-07-23T10:21:00+00:00",
          "link": "https://arxiv.org/abs/2506.04260v2",
          "size": "128kb",
          "version": "v2"
        }
      ],
      "title": "Turning to Online Forums for Legal Information: A Case Study of GDPR's Legitimate Interests",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.04260",
        "HTML": "https://arxiv.org/html/2506.04260v2",
        "PDF": "https://arxiv.org/pdf/2506.04260"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The study investigates the use of online forums for legal information on GDPR, without any focus on creativity-related aspects."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.16622",
      "abstract": "Effectively engaging the public with science is vital for fostering trust and understanding in our scientific community. Yet, with an ever-growing volume of information, science communicators struggle to anticipate how audiences will perceive and interact with scientific news. In this paper, we introduce a computational framework that models public perception across twelve dimensions, such as newsworthiness, importance, and surprisingness. Using this framework, we create a large-scale science news perception dataset with 10,489 annotations from 2,101 participants from diverse US and UK populations, providing valuable insights into public responses to scientific information across domains. We further develop NLP models that predict public perception scores with a strong performance. Leveraging the dataset and model, we examine public perception of science from two perspectives: (1) Perception as an outcome: What factors affect the public perception of scientific information? (2) Perception as a predictor: Can we use the estimated perceptions to predict public engagement with science? We find that individuals' frequency of science news consumption is the driver of perception, whereas demographic factors exert minimal influence. More importantly, through a large-scale analysis and carefully designed natural experiment on Reddit, we demonstrate that the estimated public perception of scientific information has direct connections with the final engagement pattern. Posts with more positive perception scores receive significantly more comments and upvotes, which is consistent across different scientific information and for the same science, but are framed differently. Overall, this research underscores the importance of nuanced perception modeling in science communication, offering new pathways to predict public interest and engagement with scientific content.",
      "authors": [
        "Jiaxin Pei",
        "Dustin Wright",
        "Isabelle Augenstein and David Jurgens"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Computers and Society (cs.CY)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-19T21:49:28+00:00",
          "link": "https://arxiv.org/abs/2506.16622v1",
          "size": "461kb",
          "version": "v1"
        },
        {
          "date": "2025-07-22T18:13:52+00:00",
          "link": "https://arxiv.org/abs/2506.16622v2",
          "size": "461kb",
          "version": "v2"
        }
      ],
      "title": "Modeling Public Perceptions of Science in Media",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.16622",
        "HTML": "https://arxiv.org/html/2506.16622v2",
        "PDF": "https://arxiv.org/pdf/2506.16622"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on modeling public perceptions of science in media, discussing how perceived scientific information affects public engagement. Creativity is not a theme discussed in this paper."
      },
      "tasks": [],
      "source": "arXiv"
    },
    {
      "id": "2409.08577",
      "abstract": "This study is the first to explore the interplay between haptic interaction and avatar representation in Shared Virtual Environments (SVEs). Specifically, how these factors shape users' sense of social presence during dyadic collaborations, while assessing potential effects on task performance. In a series of experiments, participants performed the collaborative task with haptic interaction under four avatar representation conditions: avatars of both participant and partner were displayed, only the participant's avatar was displayed, only the partner's avatar was displayed, and no avatars were displayed. The study finds that avatar representation, especially of the partner, significantly enhances the perception of social presence, which haptic interaction alone does not fully achieve. However, neither the presence nor the type of avatar representation impacts the task performance or participants' force effort of the task, suggesting that haptic interaction provides sufficient interaction cues for the execution of the task. These results underscore the significance of integrating both visual and haptic modalities to optimize remote collaboration experiences in virtual environments, ensuring effective communication and a strong sense of social presence.",
      "authors": [
        "Genki Sasaki",
        "Hiroshi Igarashi"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "2024-09-13T06:52:30+00:00",
          "link": "https://arxiv.org/abs/2409.08577v1",
          "size": "4535kb",
          "version": "v1"
        },
        {
          "date": "2025-05-28T06:11:58+00:00",
          "link": "https://arxiv.org/abs/2409.08577v2",
          "size": "3527kb",
          "version": "v2"
        }
      ],
      "title": "Exploring Remote Collaborative Tasks: The Impact of Avatar Representation on Dyadic Haptic Interactions in Shared Virtual Environments",
      "links": {
        "Abstract": "https://arxiv.org/abs/2409.08577",
        "HTML": "https://arxiv.org/html/2409.08577",
        "PDF": "https://arxiv.org/pdf/2409.08577"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper explores remote collaboration and how avatar representation influences social presence, which can indirectly relate to creativity in designing collaborative virtual environments but does not focus on creativity directly."
      },
      "source": "arXiv"
    },
    {
      "id": "2405.08906",
      "abstract": "Educational games enhance learning experiences by integrating touchscreens, making interactions more engaging and intuitive for learners. However, the cognitive impacts of educational gameplay input modalities, such as the hand and stylus technique, are unclear. We compared the experience of using hands vs. a stylus for touchscreens while playing an educational game by analyzing oxygenated hemoglobin collected by functional Near-Infrared Spectroscopy and self-reported measures. In addition, we measured the hand vs. the stylus modalities of the task and calculated the relative neural efficiency and relative neural involvement using the mental demand and the quiz score. Our findings show that the hand condition had a significantly lower neural involvement, yet higher neural efficiency than the stylus condition. This result suggests the requirement of less cognitive effort while using the hand. Additionally, the self-reported measures show significant differences, and the results suggest that hand-based input is more intuitive, less cognitively demanding, and less frustrating. Conversely, the use of a stylus required higher cognitive effort due to the cognitive balance of controlling the pen and answering questions. These findings highlight the importance of designing educational games that allow learners to engage with the system while minimizing cognitive effort.",
      "authors": [
        "Shayla Sharmin",
        "Elham Bakhshipour",
        "Behdokht Kiafar",
        "Md Fahim Abrar",
        "Pinar Kullu",
        "Nancy Getchell",
        "Roghayeh Leila Barmaki"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2024-05-14T18:47:04+00:00",
          "link": "https://arxiv.org/abs/2405.08906v1",
          "size": "8440kb",
          "version": "v1"
        },
        {
          "date": "2024-11-11T18:12:09+00:00",
          "link": "https://arxiv.org/abs/2405.08906v2",
          "size": "4544kb",
          "version": "v2"
        },
        {
          "date": "2024-11-15T23:17:21+00:00",
          "link": "https://arxiv.org/abs/2405.08906v3",
          "size": "4544kb",
          "version": "v3"
        },
        {
          "date": "2025-02-03T19:12:29+00:00",
          "link": "https://arxiv.org/abs/2405.08906v4",
          "size": "13721kb",
          "version": "v4"
        },
        {
          "date": "2025-05-29T18:41:07+00:00",
          "link": "https://arxiv.org/abs/2405.08906v5",
          "size": "6261kb",
          "version": "v5"
        },
        {
          "date": "2025-07-18T14:17:15+00:00",
          "link": "https://arxiv.org/abs/2405.08906v6",
          "size": "6212kb",
          "version": "v6"
        }
      ],
      "title": "Functional Near-Infrared Spectroscopy (fNIRS) Analysis of Interaction Techniques in Touchscreen-Based Educational Gaming",
      "links": {
        "Abstract": "https://arxiv.org/abs/2405.08906",
        "HTML": "https://arxiv.org/html/2405.08906",
        "PDF": "https://arxiv.org/pdf/2405.08906"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "While the primary focus is on cognitive impacts and neural efficiencies of different interaction modalities in educational games, the design of these games involves considerations that could support user creativity and engagement."
      },
      "source": "arXiv"
    },
    {
      "id": "2308.16053",
      "abstract": "With the increasing adoption of digitization, more and more historical visualizations created hundreds of years ago are accessible in digital libraries online. It provides a unique opportunity for visualization and history research. Meanwhile, there is no large-scale digital collection dedicated to historical visualizations. The visualizations are scattered in various collections, which hinders retrieval. In this study, we curate the first large-scale dataset dedicated to historical visualizations. Our dataset comprises 13K historical visualization images with corresponding processed metadata from seven digital libraries. In curating the dataset, we propose a workflow to scrape and process heterogeneous metadata. We develop a semi-automatic labeling approach to distinguish visualizations from other artifacts. Our dataset can be accessed with OldVisOnline, a system we have built to browse and label historical visualizations. We discuss our vision of usage scenarios and research opportunities with our dataset, such as textual criticism for historical visualizations. Drawing upon our experience, we summarize recommendations for future efforts to improve our dataset.",
      "authors": [
        "Yu Zhang",
        "Ruike Jiang",
        "Liwenhan Xie",
        "Yuheng Zhao",
        "Can Liu",
        "Tianhong Ding",
        "Siming Chen",
        "Xiaoru Yuan"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Digital Libraries (cs.DL)"
      ],
      "submission_historys": [
        {
          "date": "2023-08-30T14:19:31+00:00",
          "link": "https://arxiv.org/abs/2308.16053v1",
          "size": "20124kb",
          "version": "v1"
        }
      ],
      "title": "OldVisOnline: Curating a Dataset of Historical Visualizations",
      "links": {
        "Abstract": "https://arxiv.org/abs/2308.16053",
        "PDF": "https://arxiv.org/pdf/2308.16053"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on curating a dataset of historical visualizations for research purposes. It does not involve creativity in terms of creative processes, tasks, or systems."
      },
      "repo_urls": [
        "https://github.com/oldvis/gallery"
      ],
      "source": "arXiv"
    }
  ],
  "subjects": [
    "Computer Vision and Pattern Recognition (cs.CV)",
    "Multimedia (cs.MM)",
    "Computational Engineering, Finance, and Science (cs.CE)",
    "Computation and Language (cs.CL)",
    "Image and Video Processing (eess.IV)",
    "Cryptography and Security (cs.CR)",
    "Robotics (cs.RO)",
    "Artificial Intelligence (cs.AI)",
    "Neurons and Cognition (q-bio.NC)",
    "Quantitative Methods (q-bio.QM)",
    "Emerging Technologies (cs.ET)",
    "Signal Processing (eess.SP)",
    "Social and Information Networks (cs.SI)",
    "Computers and Society (cs.CY)",
    "Software Engineering (cs.SE)",
    "Human-Computer Interaction (cs.HC)",
    "Graphics (cs.GR)",
    "Digital Libraries (cs.DL)",
    "Machine Learning (cs.LG)"
  ],
  "prompt": {
    "creativity": "\nYou are an expert in information retrieval. I will provide you with a list of research papers from arXiv, specifically in the *cs.HC* (Human-Computer Interaction) category.\n\nYour task is to analyze each paper and determine its relevance to the topic of **Creativity**.\n\n\n### Classify each paper into one of the following relevance levels\n\n- `core` \u2014 Creativity is a **primary focus**\n  - The paper directly studies or simulates creativity, with a clear focus on creativity.\n  - Includes creative tasks, co-creative systems, or creativity evaluation metrics.\n  - The title and abstract explicitly mention creativity, and the research questions are directly related to creativity.\n- `partial` \u2014 Creativity is a **secondary theme**\n  - Part of the paper relates to creativity; it is treated as an analytical dimension or design goal but not the main objective (e.g., user creativity, design support).\n  - Creativity may appear in discussions, experiments, or auxiliary applications.\n  - Creativity is presented as a supporting topic (e.g., evaluation criteria, user feedback).\n- `irrelevant` \u2014 **No clear connection** to creativity\n  - The paper does not address creativity as a topic.\n  - Focuses on unrelated technical content (e.g., compression, security, optimization).\n  - If creativity is mentioned, it is only superficial and lacks substantive content.\n\n\n### Return your results in the following JSON format\n\n```json\n{\n  \"result\": [\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    },\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    }\n  ]\n}\n```\n\nBe concise but specific in your reasoning, referencing key terms or sections when applicable.\n"
  },
  "description": "Data source: https://arxiv.org/list/cs.HC/new",
  "level_tatistics": {
    "irrelevant": 23,
    "partial": 12,
    "core": 2
  },
  "arxiv_update_date": "2025-07-24",
  "updated_at": "2025-07-24 10:16:09"
}