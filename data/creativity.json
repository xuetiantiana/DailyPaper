{
  "data": [
    {
      "id": "2507.01081",
      "abstract": "Trauma prevalence is vast globally. Evidence-based digital treatments can help, but most require human guidance. Human guides provide tailored instructions and responsiveness to internal cognitive states, but limit scalability. Can generative AI and neurotechnology provide a scalable alternative? Here we test ANTIDOTE, combining AI guidance and pupillometry to automatically deliver and monitor an evidence-based digital treatment, specifically the Imagery Competing Task Intervention (ICTI), to reduce intrusive memories after psychological trauma. One hundred healthy volunteers were exposed to videos of traumatic events and randomly assigned to an intervention or active control condition. As predicted, intervention participants reported significantly fewer intrusive memories over the following week. Post-hoc assessment against clinical rubrics confirmed the AI guide delivered the intervention successfully. Additionally, pupil size tracked intervention engagement and predicted symptom reduction, providing a candidate biomarker of intervention effectiveness. These findings open a path toward rigorous AI-guided digital interventions that can scale to trauma prevalence.",
      "authors": [
        "Megan T. deBettencourt",
        "Sruthi Sakthivel",
        "Emily A. Holmes",
        "Mark Chevillet"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T17:59:01+00:00",
          "link": "https://arxiv.org/abs/2507.01081v1",
          "size": "1198kb",
          "version": "v1"
        }
      ],
      "title": "AI-guided digital intervention with physiological monitoring reduces intrusive memories after experimental trauma",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01081",
        "PDF": "https://arxiv.org/pdf/2507.01081"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on AI-guided digital interventions for trauma reduction and does not address creativity as a topic or objective. The discussion is centered around intervention effectiveness and scalability, not involving creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01121",
      "abstract": "Reproductive well-being is shaped by intersecting cultural, religious, gendered, and political contexts, yet current technologies often reflect narrow, Western-centric assumptions. In this literature review, we synthesize findings from 147 peer-reviewed papers published between 2015 and 2025 across HCI, CSCW and social computing, ICTD, digital and public health, and AI for well-being scholarship to map the evolving reproductive well-being landscape. We identify three thematic waves that focused on early access and education, cultural sensitivity and privacy, and AI integration with policy-aware design, and highlight how technologies support or constrain diverse reproductive experiences. Our analysis reveals critical gaps in inclusivity, with persistent exclusions of men and non-binary users, migrants, and users in the Global South. Additionally, we surfaced the significant absence of literature on the role of stakeholders (e.g., husband and family members, household maids and cleaning helping hands, midwife, etc.) in the reproductive well-being space. Drawing on the findings from the literature, we propose the ReWA framework to support reproductive well-being for all agendas through six design orientations associated with: location, culture, and history; polyvocality and agency; rationality, temporality, distributive roles, and methodology.",
      "authors": [
        "Hafsah Mahzabin Chowdhury",
        "Sharifa Sultana"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T18:33:30+00:00",
          "link": "https://arxiv.org/abs/2507.01121v1",
          "size": "2115kb",
          "version": "v1"
        }
      ],
      "title": "From Literature to ReWA: Discussing Reproductive Well-being in HCI",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01121",
        "HTML": "https://arxiv.org/html/2507.01121v1",
        "PDF": "https://arxiv.org/pdf/2507.01121"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper discusses reproductive well-being and related technologies but does not address creativity as a core or secondary focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01134",
      "abstract": "Game-Based Learning has proven to be an effective method for enhancing engagement with educational material. However, gaining a deeper understanding of player strategies remains challenging. Sequential game-state and action-based tracking tools often gather extensive data that can be difficult to interpret as long-term strategy. This data presents unique problems to visualization, as it can be fairly natural, noisy data but is constrained within synthetic, controlled environments, leading to issues such as overplotting which can make interpretation complicated. We propose an animated visual encoding tool that utilizes kinetic visualization to address these issues. This tool enables researchers to construct animated data narratives through the configuration of parameter interpolation curves and blending layers. Finally, we demonstrate the usefulness of the tool while addressing specific interests as outlined by a domain expert collaborator.",
      "authors": [
        "Braden Roper",
        "William Thompson",
        "Chris Weaver"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T18:52:09+00:00",
          "link": "https://arxiv.org/abs/2507.01134v1",
          "size": "26548kb",
          "version": "v1"
        }
      ],
      "title": "Animated Visual Encoding and Layer Blending for Identification of Educational Game Strategies",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01134",
        "HTML": "https://arxiv.org/html/2507.01134v1",
        "PDF": "https://arxiv.org/pdf/2507.01134"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The research involves animated visual encoding for understanding educational game strategies, which may relate to creativity in the context of educational design and data visualization. However, creativity is not the core focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01166",
      "abstract": "Identification of affective and attentional states of individuals within groups is difficult to obtain without disrupting the natural flow of collaboration. Recent work from our group used a retrospect cued recall paradigm where participants spoke about their cognitive-affective states while they viewed videos of their groups. We then collected additional participants where their reports were constrained to a subset of pre-identified cognitive-affective states. In this latter case, participants either self reported or reported in response to probes. Here, we present an initial analysis of the frequency and temporal distribution of participant reports, and how the distributions of labels changed across the two collections. Our approach has implications for the educational data mining community in tracking cognitive-affective states in collaborative learning more effectively and in developing improved adaptive learning systems that can detect and respond to cognitive-affective states.",
      "authors": [
        "Sifatul Anindho",
        "Videep Venkatesha",
        "Nathaniel Blanchard"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T20:04:20+00:00",
          "link": "https://arxiv.org/abs/2507.01166v1",
          "size": "2123kb",
          "version": "v1"
        }
      ],
      "title": "A Methodological Framework for Capturing Cognitive-Affective States in Collaborative Learning",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01166",
        "HTML": "https://arxiv.org/html/2507.01166v1",
        "PDF": "https://arxiv.org/pdf/2507.01166"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper discusses capturing cognitive-affective states in collaborative learning without a clear connection to creativity. It focuses on educational data mining and adaptive learning systems rather than creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01209",
      "abstract": "Professional visualization design has become an increasingly important area of inquiry, yet much of the field's discourse remains anchored in researcher-centered contexts. Studies of design practice often focus on individual designers' decisions and reflections, offering limited insight into the collaborative and systemic dimensions of professional work. In this paper, we propose a systems-level reframing of design judgment grounded in the coordination and adaptation that sustain progress amid uncertainty, constraint, and misalignment. Drawing on sustained engagement across multiple empirical studies--including ethnographic observation of design teams and qualitative studies of individual practitioners--we identify recurring episodes in which coherence was preserved not by selecting an optimal option, but by repairing alignment, adjusting plans, and reframing goals. We interpret these dynamics through the lens of Joint Cognitive Systems, which provide tools for analyzing how judgment emerges as a distributed capacity within sociotechnical activity. This perspective surfaces often-invisible work in visualization design and offers researchers a new conceptual vocabulary for studying how design activity is sustained in practice.",
      "authors": [
        "Paul C. Parsons",
        "Arran Ridley"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T22:08:01+00:00",
          "link": "https://arxiv.org/abs/2507.01209v1",
          "size": "946kb",
          "version": "v1"
        }
      ],
      "title": "Judgment as Coordination: A Joint Systems View of Visualization Design Practice",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01209",
        "HTML": "https://arxiv.org/html/2507.01209v1",
        "PDF": "https://arxiv.org/pdf/2507.01209"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses visualization design with a focus on collaboration and problem-solving. Creativity is a secondary theme as design judgment involves adapting and reframing goals, which includes some creative practices."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01274",
      "abstract": "Traditional simulator-based training for maritime professionals is critical for ensuring safety at sea but often depends on subjective trainer assessments of technical skills, behavioral focus, communication, and body language, posing challenges such as subjectivity, difficulty in measuring key features, and cognitive limitations. Addressing these issues, this study develops an AI-driven framework to enhance maritime training by objectively assessing trainee performance through visual focus tracking, speech recognition, and stress detection, improving readiness for high-risk scenarios. The system integrates AI techniques, including visual focus determination using eye tracking, pupil dilation analysis, and computer vision; communication analysis through a maritime-specific speech-to-text model and natural language processing; communication correctness using large language models; and mental stress detection via vocal pitch. Models were evaluated on data from simulated maritime scenarios with seafarers exposed to controlled high-stress events. The AI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for maritime speech recognition, and ~90% for stress detection, surpassing existing benchmarks. The system provides insights into visual attention, adherence to communication checklists, and stress levels under demanding conditions. This study demonstrates how AI can transform maritime training by delivering objective performance analytics, enabling personalized feedback, and improving preparedness for real-world operational challenges.",
      "authors": [
        "Vishakha Lall",
        "Yisi Liu"
      ],
      "license": "http://creativecommons.org/publicdomain/zero/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T01:19:32+00:00",
          "link": "https://arxiv.org/abs/2507.01274v1",
          "size": "630kb",
          "version": "v1"
        }
      ],
      "title": "AI Meets Maritime Training: Precision Analytics for Enhanced Safety and Performance",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01274",
        "PDF": "https://arxiv.org/pdf/2507.01274"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The study focuses on using AI to improve maritime training efficiency and safety. It does not address creativity or involve creative tasks."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01436",
      "abstract": "Despite the ubiquity of visualization examples published on the web, retargeting existing custom chart implementations to new datasets remains difficult, time-intensive, and tedious. The adaptation process assumes author familiarity with both the implementation of the example as well as how the new dataset might need to be transformed to fit into the example code. With recent advances in Large Language Models (LLMs), automatic adaptation of code can be achieved from high-level user prompts, reducing the barrier for visualization retargeting. To better understand how LLMs can assist retargeting and its potential limitations, we characterize and evaluate the performance of LLM assistance across multiple datasets and charts of varying complexity, categorizing failures according to type and severity. In our evaluation, we compare two approaches: (1) directly instructing the LLM model to fully generate and adapt code by treating code as text inputs and (2) a more constrained program synthesis pipeline where the LLM guides the code construction process by providing structural information (e.g., visual encodings) based on properties of the example code and data. We find that both approaches struggle when new data has not been appropriately transformed, and discuss important design recommendations for future retargeting systems.",
      "authors": [
        "Luke S. Snyder",
        "Chenglong Wang",
        "Steven Drucker"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T07:43:43+00:00",
          "link": "https://arxiv.org/abs/2507.01436v1",
          "size": "1603kb",
          "version": "v1"
        }
      ],
      "title": "Challenges & Opportunities with LLM-Assisted Visualization Retargeting",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01436",
        "HTML": "https://arxiv.org/html/2507.01436v1",
        "PDF": "https://arxiv.org/pdf/2507.01436"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper examines visualization retargeting using LLMs, which may involve some creative aspects in how visual data is adapted and transformed, but creativity is not the core focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01471",
      "abstract": "Researchers have been using simulation-based methods for drone-assisted inspection training. Multiple brain regions are associated with information processes and decision-making, and the connectivity of these regions may further influence inspectors' performance. However, researchers do not understand the pathways of the information flows when drone pilots process the maintenance and manipulation of information, which may affect the efficiency of tacit knowledge transfer. This study aims to reveal the causal connection between participants' brain regions using an electroencephalogram and dynamic causal modeling when processing drone-assisted building energy audit tasks using different display modalities. The results showed similar single-direction connectivity patterns for the different simulation groups. The results also showed similar patterns between brain regions related to visual inspection performance before and after training. These findings highlight the nature of brain asymmetries and may be utilized in measuring cognitive states and designing adaptive automation in the knowledge transfer of drone-based inspection.",
      "authors": [
        "Pengkun Liu",
        "Jackson Greene",
        "Jiali Huang",
        "Pingbo Tang",
        "Yu Hou"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T08:34:13+00:00",
          "link": "https://arxiv.org/abs/2507.01471v1",
          "size": "3127kb",
          "version": "v1"
        }
      ],
      "title": "Analysis of Drone-Assisted Building Inspection Training in VR vs 2D Monitor Display: an EEG Study",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01471",
        "PDF": "https://arxiv.org/pdf/2507.01471"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper is about drone-assisted inspection training using different display modalities, with no clear relevance to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01548",
      "abstract": "This paper explores how older adults, particularly aging migrants in urban China, can engage AI-assisted co-creation to express personal narratives that are often fragmented, underrepresented, or difficult to verbalize. Through a pilot workshop combining oral storytelling and the symbolic reconstruction of Hanzi, participants shared memories of migration and recreated new character forms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM), together with physical materials. Supported by human facilitation and a soft AI presence, participants transformed lived experience into visual and tactile expressions without requiring digital literacy. This approach offers new perspectives on human-AI collaboration and aging by repositioning AI not as a content producer but as a supportive mechanism, and by supporting narrative agency within sociotechnical systems.",
      "authors": [
        "Wen Zhan",
        "Ziqun Hua",
        "Peiyue Lin",
        "Yunfei Chen"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T10:00:12+00:00",
          "link": "https://arxiv.org/abs/2507.01548v1",
          "size": "1052kb",
          "version": "v1"
        }
      ],
      "title": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01548",
        "PDF": "https://arxiv.org/pdf/2507.01548"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper centers on AI-assisted co-creation workshops, focusing on creative expression through storytelling and glyph design, directly engaging with creativity as the main topic."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01690",
      "abstract": "Academic well-being is deeply influenced by peer-support networks, yet they remain informal, inequitable, and unsustainable, often relying on personal connections and social capital rather than structured, inclusive systems. Additionally, institutional well-being responses frequently focus on student populations, neglecting the emotional labour of faculty and staff, reinforcing an exclusionary academic culture. Drawing on HCI methodologies, participatory design, and care ethics, this workshop will provide a space for rethinking how academic communities can support inclusive networks. Through pre-workshop engagement, co-design activities, and reflection, participants will examine systemic gaps in networks and explore ways to embed care, equity, and sustainability into academic peer-support frameworks -- from informal, exclusionary models to structured, inclusive care-based ecosystems. At the end of the workshop, participants will co-develop design strategies for integrating care and resilience in academic ecosystems, resources for designing equitable support systems, and a peer network invested and committed to fostering a supportive academic community.",
      "authors": [
        "Beatriz Severes and Ana O. Henriques and Rory Clark and Paulo Bala and Anna Carter and Rua Mae Williams and Geraldine Fitzpatrick"
      ],
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T13:17:38+00:00",
          "link": "https://arxiv.org/abs/2507.01690v1",
          "size": "47kb",
          "version": "v1"
        }
      ],
      "title": "Designing for Community Care: Reimagining Support for Equity & Well-being in Academia",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01690",
        "HTML": "https://arxiv.org/html/2507.01690v1",
        "PDF": "https://arxiv.org/pdf/2507.01690"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on rethinking academic community support systems for equity and well-being, with no mention or discussion of creativity as a theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01719",
      "abstract": "There is justifiable interest in leveraging conversational AI (CAI) for health across the majority world, but to be effective, CAI must respond appropriately within culturally and linguistically diverse contexts. Therefore, we need ways to address the fact that current LLMs exclude many lived experiences globally. Various advances are underway which focus on top-down approaches and increasing training data. In this paper, we aim to complement these with a bottom-up locally-grounded approach based on qualitative data collected during participatory workshops in Latin America. Our goal is to construct a rich and human-centred understanding of: a) potential areas of cultural misalignment in digital health; b) regional perspectives on chatbots for health and c)strategies for creating culturally-appropriate CAI; with a focus on the understudied Latin American context. Our findings show that academic boundaries on notions of culture lose meaning at the ground level and technologies will need to engage with a broader framework; one that encapsulates the way economics, politics, geography and local logistics are entangled in cultural experience. To this end, we introduce a framework for 'Pluriversal Conversational AI for Health' which allows for the possibility that more relationality and tolerance, rather than just more data, may be called for.",
      "authors": [
        "Dorian Peters",
        "Fernanda Espinoza",
        "Marco da Re",
        "Guido Ivetta",
        "Luciana Benotti",
        "Rafael A. Calvo"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T13:48:25+00:00",
          "link": "https://arxiv.org/abs/2507.01719v1",
          "size": "546kb",
          "version": "v1"
        }
      ],
      "title": "Towards culturally-appropriate conversational AI for health in the majority world: An exploratory study with citizens and professionals in Latin America",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01719",
        "PDF": "https://arxiv.org/pdf/2507.01719"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper explores culturally-appropriate CAI for health in Latin American contexts. Creativity is not addressed in the objectives, discussion, or framework."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01776",
      "abstract": "The integration of machine learning (ML) into spatial design holds immense potential for optimizing space utilization, enhancing functionality, and streamlining design processes. ML can automate tasks, predict performance outcomes, and tailor spaces to user preferences. However, the emotional, cultural, and aesthetic dimensions of design remain crucial for creating spaces that truly resonate with users-elements that ML alone cannot address. The key challenge lies in harmonizing data-driven efficiency with the nuanced, subjective aspects of design. This paper proposes a human-machine collaboration framework to bridge this gap. An effective framework should recognize that while ML enhances design efficiency through automation and prediction, it must be paired with human creativity to ensure spaces are emotionally engaging and culturally relevant. Human designers contribute intuition, empathy, and cultural insight, guiding ML-generated solutions to align with users' emotional and cultural needs. Additionally, we explore how various ML models can be integrated with human-centered design principles. These models can automate design generation and optimization, while human designers refine the outputs to ensure emotional resonance and aesthetic appeal. Through case studies in office and residential design, we illustrate how this framework fosters both creativity and cultural relevance. By merging ML with human creativity, spatial design can achieve a balance of efficiency and emotional impact, resulting in environments that are both functional and deeply human.",
      "authors": [
        "Yuxuan Yang"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Multimedia (cs.MM)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T15:01:51+00:00",
          "link": "https://arxiv.org/abs/2507.01776v1",
          "size": "9968kb",
          "version": "v1"
        }
      ],
      "title": "Human-Machine Collaboration-Guided Space Design: Combination of Machine Learning Models and Humanistic Design Concepts",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01776",
        "HTML": "https://arxiv.org/html/2507.01776v1",
        "PDF": "https://arxiv.org/pdf/2507.01776"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "This paper discusses the integration of machine learning with human creativity for space design. It focuses explicitly on creativity as part of the human-machine collaboration framework, exploring how human creativity complements ML in design tasks."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01862",
      "abstract": "Domain specific chatbot applications often involve multi step interactions, such as refining search filters, selecting multiple items, or performing comparisons. Traditional graphical user interfaces (GUIs) handle these workflows by providing explicit \"Submit\" (commit data) and \"Reset\" (discard data) actions, allowing back-end systems to track user intent unambiguously. In contrast, conversational agents rely on subtle language cues, which can lead to confusion and incomplete context management. This paper proposes modeling these GUI inspired metaphors acknowledgment (submit like) and context switching (reset-like) as explicit tasks within large language model (LLM) prompts. By capturing user acknowledgment, reset actions, and chain of thought (CoT) reasoning as structured session data, we preserve clarity, reduce user confusion, and align domain-specific chatbot interactions with back-end logic. We demonstrate our approach in hotel booking and customer management scenarios, highlighting improvements in multi-turn task coherence, user satisfaction, and efficiency.",
      "authors": [
        "Sanjay Krishna Anbalagan",
        "Xinrui Nie",
        "Umesh Mohan",
        "Vijay Kumar Kanamarlapudi",
        "Anughna Kommalapati and Xiaodan Zhao"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T16:24:50+00:00",
          "link": "https://arxiv.org/abs/2507.01862v1",
          "size": "288kb",
          "version": "v1"
        }
      ],
      "title": "Bridging UI Design and chatbot Interactions: Applying Form-Based Principles to Conversational Agents",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01862",
        "PDF": "https://arxiv.org/pdf/2507.01862"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper addresses improvements in chatbot interface design through form-based principles. It does not mention creativity or involve it as a concept."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01944",
      "abstract": "This paper discusses Tangible User Interfaces (TUIs) and their potential impact on cognitive assessment and cognitive training. We believe that TUIs, and particularly a subset that we dub spatial TUIs, can extend human computer interaction beyond some of its current limitations. Spatial TUIs exploit human innate spatial and tactile ability in an intuitive and direct manner, affording interaction paradigms that are practically impossible using current interface technology. As proof-of-concept we examine implementations in the field of cognitive assessment and training. In this paper we use Cognitive Cubes, a novel TUI we developed, as an applied test bed for our beliefs, presenting promising experimental results for cognitive assessment of spatial ability, and possibly for training purposes.",
      "authors": [
        "Ehud Sharlin",
        "Yuichi Itoh",
        "Benjamin Watson",
        "Yoshifumi Kitamura",
        "Steve Sutphen",
        "Lili Liu",
        "Fumio Kishino"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T17:54:23+00:00",
          "link": "https://arxiv.org/abs/2507.01944v1",
          "size": "226kb",
          "version": "v1"
        }
      ],
      "title": "Spatial tangible user interfaces for cognitive assessment and training",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01944",
        "PDF": "https://arxiv.org/pdf/2507.01944"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses TUIs for cognitive training, including spatial and tactile interaction capabilities that may support creative tasks, but creativity is not the main focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01022",
      "abstract": "This study presents an exploratory evaluation of Music Generation Systems (MGS) within contemporary music production workflows by examining eight open-source systems. The evaluation framework combines technical insights with practical experimentation through criteria specifically designed to investigate the practical and creative affordances of the systems within the iterative, non-linear nature of music production. Employing a single-evaluator methodology as a preliminary phase, this research adopts a mixed approach utilizing qualitative methods to form hypotheses subsequently assessed through quantitative metrics. The selected systems represent architectural diversity across both symbolic and audio-based music generation approaches, spanning composition, arrangement, and sound design tasks. The investigation addresses limitations of current MGS in music production, challenges and opportunities for workflow integration, and development potential as collaborative tools while maintaining artistic authenticity. Findings reveal these systems function primarily as complementary tools enhancing rather than replacing human expertise. They exhibit limitations in maintaining thematic and structural coherence that emphasize the indispensable role of human creativity in tasks demanding emotional depth and complex decision-making. This study contributes a structured evaluation framework that considers the iterative nature of music creation. It identifies methodological refinements necessary for subsequent comprehensive evaluations and determines viable areas for AI integration as collaborative tools in creative workflows. The research provides empirically-grounded insights to guide future development in the field.",
      "authors": [
        "Shayan Dadman",
        "Bernt Arild Bremdal",
        "Andreas Bergsland"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Audio and Speech Processing (eess.AS)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)",
        "Multimedia (cs.MM)",
        "Sound (cs.SD)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-11T13:07:47+00:00",
          "link": "https://arxiv.org/abs/2507.01022v1",
          "size": "998kb",
          "version": "v1"
        }
      ],
      "title": "Workflow-Based Evaluation of Music Generation Systems",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01022",
        "HTML": "https://arxiv.org/html/2507.01022v1",
        "PDF": "https://arxiv.org/pdf/2507.01022"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "core",
        "reason": "The paper directly addresses the evaluation of Music Generation Systems within creative workflows, a central topic in creative processes. It emphasizes the role of AI as a tool for enhancing creative output and discusses creativity evaluation within music production."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01061",
      "abstract": "The integration of Large Language Models (LLMs) into social science experiments represents a transformative approach to understanding human-AI interactions and their societal impacts. We introduce Epitome, the world's first open experimental platform dedicated to the deep integration of artificial intelligence and social science. Rooted in theoretical foundations from management, communication studies, sociology, psychology, and ethics, Epitome focuses on the interactive impacts of AI on individuals, organizations, and society during its real-world deployment. It constructs a theoretical support system through cross-disciplinary experiments. The platform offers a one-stop comprehensive experimental solution spanning \"foundation models-complex application development-user feedback\" through seven core modules, while embedding the classical \"control-comparison-comparative causal logic\" of social science experiments into multilevel human-computer interaction environments, including dialogues, group chats, and multi-agent virtual scenarios. With its canvas-style, user-friendly interface, Epitome enables researchers to easily design and run complex experimental scenarios, facilitating systematic investigations into the social impacts of AI and exploration of integrated solutions.To demonstrate its capabilities, we replicated three seminal social science experiments involving LLMs, showcasing Epitome's potential to streamline complex experimental designs and produce robust results, suitable for publishing in the top selective journals. Our findings highlight the platform's utility in enhancing the efficiency and quality of human-AI interactions, providing valuable insights into the societal implications of AI technologies. Epitome thus offers a powerful tool for advancing interdisciplinary research at the intersection of AI and social science, with potential applications in policy-making, ...",
      "authors": [
        "Jingjing Qu",
        "Kejia Hu",
        "Jun Zhu",
        "Wenhao Li",
        "Teng Wang",
        "Zhiyun Chen",
        "Yulei Ye",
        "Chaochao Lu",
        "Aimin Zhou",
        "Xiangfeng Wang",
        "James Evan"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-30T09:06:16+00:00",
          "link": "https://arxiv.org/abs/2507.01061v1",
          "size": "3494kb",
          "version": "v1"
        }
      ],
      "title": "Epitome: Pioneering an Experimental Platform for AI-Social Science Integration",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01061",
        "HTML": "https://arxiv.org/html/2507.01061v1",
        "PDF": "https://arxiv.org/pdf/2507.01061"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "Epitome focuses on integrating AI with social sciences to study human-AI interactions. While not directly about creativity, the platform's interdisciplinary approach and study design capabilities can support creative experimentation in research methodologies."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01111",
      "abstract": "Current control strategies for powered lower limb prostheses often lack awareness of the environment and the user's intended interactions with it. This limitation becomes particularly apparent in complex terrains. Obstacle negotiation, a critical scenario exemplifying such challenges, requires both real-time perception of obstacle geometry and responsiveness to user intention about when and where to step over or onto, to dynamically adjust swing trajectories. We propose a novel control strategy that fuses environmental awareness and human cooperativeness: an on-board depth camera detects obstacles ahead of swing phase, prompting an elevated early-swing trajectory to ensure clearance, while late-swing control defers to natural biomechanical cues from the user. This approach enables intuitive stepping strategies without requiring unnatural movement patterns. Experiments with three non-amputee participants demonstrated 100 percent success across more than 150 step-overs and 30 step-ons with randomly placed obstacles of varying heights (4-16 cm) and distances (15-70 cm). By effectively addressing obstacle navigation -- a gateway challenge for complex terrain mobility -- our system demonstrates adaptability to both environmental constraints and user intentions, with promising applications across diverse locomotion scenarios.",
      "authors": [
        "Haosen Xing",
        "Haoran Ma",
        "Sijin Zhang",
        "and Hartmut Geyer"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T18:15:50+00:00",
          "link": "https://arxiv.org/abs/2507.01111v1",
          "size": "5592kb",
          "version": "v1"
        }
      ],
      "title": "Environment-Aware and Human-Cooperative Swing Control for Lower-Limb Prostheses in Diverse Obstacle Scenarios",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01111",
        "HTML": "https://arxiv.org/html/2507.01111v1",
        "PDF": "https://arxiv.org/pdf/2507.01111"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper is about control strategies for lower-limb prostheses and environmental awareness. It does not mention or relate to creativity or creative processes within its scope."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01168",
      "abstract": "There is growing interest in explainable recommender systems that provide recommendations along with explanations for the reasoning behind them. When evaluating recommender systems, most studies focus on overall recommendation performance. Only a few assess the quality of the explanations. Explanation quality is often evaluated through user studies that subjectively gather users' opinions on representative explanatory factors that shape end-users' perspective towards the results, not about the explanation contents itself. We aim to fill this gap by developing an objective metric to evaluate Veracity: the information quality of explanations. Specifically, we decompose Veracity into two dimensions: Fidelity and Attunement. Fidelity refers to whether the explanation includes accurate information about the recommended item. Attunement evaluates whether the explanation reflects the target user's preferences. By applying signal detection theory, we first determine decision outcomes for each dimension and then combine them to calculate a sensitivity, which serves as the final Veracity value. To assess the effectiveness of the proposed metric, we set up four cases with varying levels of information quality to validate whether our metric can accurately capture differences in quality. The results provided meaningful insights into the effectiveness of our proposed metric.",
      "authors": [
        "Yeonbin Son and Matthew L. Bolton"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Information Retrieval (cs.IR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T20:11:17+00:00",
          "link": "https://arxiv.org/abs/2507.01168v1",
          "size": "929kb",
          "version": "v1"
        }
      ],
      "title": "Towards a Signal Detection Based Measure for Assessing Information Quality of Explainable Recommender Systems",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01168",
        "HTML": "https://arxiv.org/html/2507.01168v1",
        "PDF": "https://arxiv.org/pdf/2507.01168"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on evaluating the quality of explanations in recommender systems using signal detection theory, with no direct mention or exploration of creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01196",
      "abstract": "Foundation Models have demonstrated significant success across various domains in Artificial Intelligence (AI), yet their capabilities for brainwave modeling remain unclear. In this paper, we comprehensively evaluate current Large Brainwave Foundation Models (LBMs) through systematic fine-tuning experiments across multiple Brain-Computer Interface (BCI) benchmark tasks, including memory tasks and sleep stage classification. Our extensive analysis shows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%) over traditional deep architectures while requiring significantly more parameters (millions vs thousands), raising important questions about their efficiency and applicability in BCI contexts. Moreover, through detailed ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce trainable parameters without performance degradation, while demonstrating that architectural and training inefficiencies limit LBMs' current capabilities. Our experiments span both full model fine-tuning and parameter-efficient adaptation techniques, providing insights into optimal training strategies for BCI applications. We pioneer the application of LoRA to LBMs, revealing that performance benefits generally emerge when adapting multiple neural network components simultaneously. These findings highlight the critical need for domain-specific development strategies to advance LBMs, suggesting that current architectures may require redesign to fully leverage the potential of foundation models in brainwave analysis.",
      "authors": [
        "Na Lee",
        "Konstantinos Barmpas",
        "Yannis Panagakis",
        "Dimitrios Adamos",
        "Nikolaos Laskaris and Stefanos Zafeiriou"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T21:21:42+00:00",
          "link": "https://arxiv.org/abs/2507.01196v1",
          "size": "380kb",
          "version": "v1"
        }
      ],
      "title": "Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01196",
        "HTML": "https://arxiv.org/html/2507.01196v1",
        "PDF": "https://arxiv.org/pdf/2507.01196"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper evaluates brainwave foundation models for BCI tasks. It discusses model efficiency but does not relate to creativity or creative processing."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01206",
      "abstract": "As modern computing advances, new interaction paradigms have emerged, particularly in Augmented Reality (AR), which overlays virtual interfaces onto physical objects. This evolution poses challenges in machine perception, especially for tasks like 3D object pose estimation in complex, dynamic environments. Our project addresses critical issues in human-robot interaction within mobile AR, focusing on non-intrusive, spatially aware interfaces. We present URSA, an LLM-driven immersive AR system developed for NASA's 2023-2024 SUITS challenge, targeting future spaceflight needs such as the Artemis missions. URSA integrates three core technologies: a head-mounted AR device (e.g., HoloLens) for intuitive visual feedback, voice control powered by large language models for hands-free interaction, and robot tracking algorithms that enable accurate 3D localization in dynamic settings. To enhance precision, we leverage digital twin localization technologies, using datasets like DTTD-Mobile and specialized hardware such as the ZED2 camera for real-world tracking under noise and occlusion. Our system enables real-time robot control and monitoring via an AR interface, even in the absence of ground-truth sensors--vital for hazardous or remote operations. Key contributions include: (1) a non-intrusive AR interface with LLM-based voice input; (2) a ZED2-based dataset tailored for non-rigid robotic bodies; (3) a Local Mission Control Console (LMCC) for mission visualization; (4) a transformer-based 6DoF pose estimator (DTTDNet) optimized for depth fusion and real-time tracking; and (5) end-to-end integration for astronaut mission support. This work advances digital twin applications in robotics, offering scalable solutions for both aerospace and industrial domains.",
      "authors": [
        "Kathy Zhuang",
        "Zixun Huang",
        "Yukun Song",
        "Rui Li",
        "Yinuo Zhou",
        "Allen Y. Yang"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Robotics (cs.RO)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-01T21:54:35+00:00",
          "link": "https://arxiv.org/abs/2507.01206v1",
          "size": "15526kb",
          "version": "v1"
        }
      ],
      "title": "2024 NASA SUITS Report: LLM-Driven Immersive Augmented Reality User Interface for Robotics and Space Exploration",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01206",
        "HTML": "https://arxiv.org/html/2507.01206v1",
        "PDF": "https://arxiv.org/pdf/2507.01206"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The focus is on augmenting reality with AR interfaces for space exploration. Although innovative, it does not discuss creativity as a theme."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01282",
      "abstract": "The recent boom of large language models (LLMs) has re-ignited the hope that artificial intelligence (AI) systems could aid medical diagnosis. Yet despite dazzling benchmark scores, LLM assistants have yet to deliver measurable improvements at the bedside. This scoping review aims to highlight the areas where AI is limited to make practical contributions in the clinical setting, specifically in dementia diagnosis and care.\n  Standalone machine-learning models excel at pattern recognition but seldom provide actionable, interpretable guidance, eroding clinician trust. Adjacent use of LLMs by physicians did not result in better diagnostic accuracy or speed. Key limitations trace to the data-driven paradigm: black-box outputs which lack transparency, vulnerability to hallucinations, and weak causal reasoning. Hybrid approaches that combine statistical learning with expert rule-based knowledge, and involve clinicians throughout the process help bring back interpretability. They also fit better with existing clinical workflows, as seen in examples like PEIRS and ATHENA-CDS.\n  Future decision-support should prioritise explanatory coherence by linking predictions to clinically meaningful causes. This can be done through neuro-symbolic or hybrid AI that combines the language ability of LLMs with human causal expertise. AI researchers have addressed this direction, with explainable AI and neuro-symbolic AI being the next logical steps in further advancement in AI. However, they are still based on data-driven knowledge integration instead of human-in-the-loop approaches. Future research should measure success not only by accuracy but by improvements in clinician understanding, workflow fit, and patient outcomes. A better understanding of what helps improve human-computer interactions is greatly needed for AI systems to become part of clinical practice.",
      "authors": [
        "Matthew JY Kang",
        "Wenli Yang",
        "Monica R Roberts",
        "Byeong Ho Kang",
        "Charles B Malpas"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T01:43:06+00:00",
          "link": "https://arxiv.org/abs/2507.01282v1",
          "size": "442kb",
          "version": "v1"
        }
      ],
      "title": "Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01282",
        "PDF": "https://arxiv.org/pdf/2507.01282"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on interpretable AI systems for dementia care, with no mention or exploration of creativity-related topics."
      },
      "source": "arXiv"
    },
    {
      "id": "2507.01431",
      "abstract": "Grading handwritten, open-ended responses remains a major bottleneck in large university STEM courses. We introduce Pensieve (https://www.pensieve.co), an AI-assisted grading platform that leverages large language models (LLMs) to transcribe and evaluate student work, providing instructors with rubric-aligned scores, transcriptions, and confidence ratings. Unlike prior tools that focus narrowly on specific tasks like transcription or rubric generation, Pensieve supports the entire grading pipeline-from scanned student submissions to final feedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and has graded more than 300,000 student responses. We present system details and empirical results across four core STEM disciplines: Computer Science, Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces grading time by an average of 65%, while maintaining a 95.4% agreement rate with instructor-assigned grades for high-confidence predictions.",
      "authors": [
        "Yoonseok Yang",
        "Minjune Kim",
        "Marlon Rondinelli",
        "Keren Shao"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-07-02T07:33:19+00:00",
          "link": "https://arxiv.org/abs/2507.01431v1",
          "size": "19486kb",
          "version": "v1"
        }
      ],
      "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading",
      "links": {
        "Abstract": "https://arxiv.org/abs/2507.01431",
        "HTML": "https://arxiv.org/html/2507.01431v1",
        "PDF": "https://arxiv.org/pdf/2507.01431"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper describes an AI platform for grading STEM assessments, which doesn't involve creativity as a theme or study focus."
      },
      "source": "arXiv"
    },
    {
      "id": "2211.03324",
      "abstract": "Human civilization has witnessed transformative technological milestones, from ancient fire lighting to the internet era. This chapter delves into the invasive brain machine interface (BMI), a pioneering technology poised to be a defining chapter in our progress. Beyond aiding medical conditions, invasive BMI promises far reaching impacts across diverse technologies and aspects of life. The exploration begins by unraveling the biological and engineering principles essential for BMI implementation. The chapter comprehensively analyzes potential applications, methodologies for detecting and decoding brain signals, and options for stimulating signals within the human brain. It concludes with a discussion on the multifaceted challenges and opportunities for the continued development of invasive BMI. This chapter not only provides a profound understanding of the foundational elements of invasive BMI but also serves as a guide through its applications, intricacies, and potential societal implications. Navigating neurobiology, engineering innovations, and the evolving landscape of human AI symbiosis, the chapter sheds light on the promises and hurdles that define the future of invasive BMI.",
      "authors": [
        "Rezwan Firuzi",
        "Ayub Bokani",
        "Jahan Hassan",
        "Hamed Ahmadyani",
        "Mohammad Foad Abdi",
        "Dana Naderi",
        "and Diako Ebrahimi"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2022-11-07T06:07:58+00:00",
          "link": "https://arxiv.org/abs/2211.03324v1",
          "size": "8750kb",
          "version": "v1"
        },
        {
          "date": "2024-01-22T02:14:49+00:00",
          "link": "https://arxiv.org/abs/2211.03324v2",
          "size": "0kb",
          "version": "v2"
        },
        {
          "date": "2025-07-02T04:59:16+00:00",
          "link": "https://arxiv.org/abs/2211.03324v3",
          "size": "1398kb",
          "version": "v3"
        }
      ],
      "title": "Decoding Neural Signals: Invasive BMI Review",
      "links": {
        "Abstract": "https://arxiv.org/abs/2211.03324",
        "PDF": "https://arxiv.org/pdf/2211.03324"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on invasive brain-machine interfaces, discussing biological and engineering principles and potential applications but does not address creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2412.16825",
      "abstract": "Differential Privacy (DP) has emerged as a pivotal approach for safeguarding individual privacy in data analysis, yet its practical adoption is often hindered by challenges in the implementation and communication of DP. This paper presents a comprehensive systematization of existing research studies around the usability of DP, synthesizing insights from studies on both the practical use of DP tools and strategies for conveying DP parameters that determine privacy protection levels, such as epsilon($\\varepsilon$). By reviewing and analyzing these studies, we identify core usability challenges, best practices, and critical gaps in current DP tools that affect adoption across diverse user groups, including developers, data analysts, and non-technical stakeholders. Our analysis highlights actionable insights and pathways for future research that emphasizes user-centered design and clear communication, fostering the development of more accessible DP tools that meet practical needs and support broader adoption.",
      "authors": [
        "Onyinye Dibia",
        "Prianka Bhattacharjee",
        "Brad Stenger",
        "Steven Baldasty",
        "Mako Bates",
        "Ivoline C. Ngong",
        "Yuanyuan Feng",
        "Joseph P. Near"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "date": "2024-12-22T02:21:57+00:00",
          "link": "https://arxiv.org/abs/2412.16825v1",
          "size": "115kb",
          "version": "v1"
        },
        {
          "date": "2025-07-02T01:29:35+00:00",
          "link": "https://arxiv.org/abs/2412.16825v2",
          "size": "108kb",
          "version": "v2"
        }
      ],
      "title": "SoK: Usability Studies in Differential Privacy",
      "links": {
        "Abstract": "https://arxiv.org/abs/2412.16825",
        "HTML": "https://arxiv.org/html/2412.16825v2",
        "PDF": "https://arxiv.org/pdf/2412.16825"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on usability studies in the context of Differential Privacy and does not address creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2504.13901",
      "abstract": "Mild cognitive impairment (MCI) may affect up to 20 % of people over 65 years old. Global incidence of MCI is increasing, and technology is being explored for early intervention. Theories of technology adoption predict that useful and easy to use solutions will have higher rates of adoption, however, these models do not specifically consider older people with cognitive impairments, or the unique human computer interaction challenges posed by MCI. We collated opinions from older people with MCI about technology solutions proposed for them, found in 83 articles published between Jan 2014 and May 2024, and found in nine databases. Inductive, thematic analysis of feedback identified five themes (i) purpose and need, (ii) solution design and ease of use, (iii) self-impression, (iv) lifestyle, and (v) interaction modality. Solutions are perceived as useful, even though gaps in functional support exist, however, they are not perceived as entirely easy to use, due to issues related to usability and user experience. Devices which are light, portable, common and have large screens, are preferred, as is multimodal interaction, in particular speech, visual/text and touch. This review recommends future work to (i) improve usability and user experience, (ii) enhance personalisation, (iii) better understand interaction preferences and effectiveness, (iv) enable options for multimodal interaction, and (v) more seamlessly integrate solutions into users lifestyles.",
      "authors": [
        "Snezna B Schmidt",
        "Stephen Isbel",
        "Blooma John",
        "Ram Subramanian",
        "Nathan M DCunha"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-04-08T05:30:10+00:00",
          "link": "https://arxiv.org/abs/2504.13901v1",
          "size": "616kb",
          "version": "v1"
        },
        {
          "date": "2025-04-22T04:22:10+00:00",
          "link": "https://arxiv.org/abs/2504.13901v2",
          "size": "0kb",
          "version": "v2"
        },
        {
          "date": "2025-05-20T08:17:36+00:00",
          "link": "https://arxiv.org/abs/2504.13901v3",
          "size": "745kb",
          "version": "v3"
        },
        {
          "date": "2025-07-01T22:52:52+00:00",
          "link": "https://arxiv.org/abs/2504.13901v4",
          "size": "632kb",
          "version": "v4"
        }
      ],
      "title": "Examining Technology Perspectives of Older Adults with Mild Cognitive Impairment: A Scoping Review",
      "links": {
        "Abstract": "https://arxiv.org/abs/2504.13901",
        "PDF": "https://arxiv.org/pdf/2504.13901"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The focus is on technology perspectives of older adults with mild cognitive impairment, with no clear emphasis on creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2505.03440",
      "abstract": "We propose manvr3d, a novel VR-ready platform for interactive human-in-the-loop cell tracking. We utilize VR controllers and eye-tracking hardware to facilitate rapid ground truth generation and proofreading for deep learning-based cell tracking models. Life scientists reconstruct the developmental history of organisms on the cellular level by analyzing 3D time-lapse microscopy images acquired at high spatio-temporal resolution. The reconstruction of such cell lineage trees traditionally involves tracking individual cells through all recorded time points, manually annotating their positions, and then linking them over time to create complete trajectories. Deep learning-based algorithms accelerate this process, yet depend heavily on manually-annotated high-quality ground truth data and curation. Visual representation of the image data in this process still relies primarily on 2D renderings, which greatly limits spatial understanding and navigation. In this work, we bridge the gap between deep learning-based cell tracking software and 3D/VR visualization to create a human-in-the-loop cell tracking system. We lift the incremental annotation, training and proofreading loop of the deep learning model into the 3rd dimension and apply natural user interfaces like hand gestures and eye tracking to accelerate the cell tracking workflow for life scientists.",
      "authors": [
        "Samuel Pantze",
        "Jean-Yves Tinevez",
        "Matthew McGinity",
        "Ulrik G\\\"unther"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2025-05-06T11:27:01+00:00",
          "link": "https://arxiv.org/abs/2505.03440v1",
          "size": "2882kb",
          "version": "v1"
        },
        {
          "date": "2025-05-07T09:53:00+00:00",
          "link": "https://arxiv.org/abs/2505.03440v2",
          "size": "4107kb",
          "version": "v2"
        },
        {
          "date": "2025-07-02T08:27:44+00:00",
          "link": "https://arxiv.org/abs/2505.03440v3",
          "size": "2052kb",
          "version": "v3"
        }
      ],
      "title": "manvr3d: A Platform for Human-in-the-loop Cell Tracking in Virtual Reality",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.03440",
        "HTML": "https://arxiv.org/html/2505.03440v3",
        "PDF": "https://arxiv.org/pdf/2505.03440"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper discusses a human-in-the-loop system that uses VR for cell tracking, facilitating spatial understanding and navigation. While creativity isn't the main focus, the innovative use of VR and interactive technologies could indirectly support creative exploration in scientific data analysis."
      },
      "repo_urls": [
        "https://github.com/scenerygraphics/manvr3d"
      ],
      "source": "arXiv"
    },
    {
      "id": "2506.21319",
      "abstract": "Current multimodal large language models (MLLMs), while effective in natural image understanding, struggle with visualization understanding due to their inability to decode the data-to-visual mapping and extract structured information. To address these challenges, we propose SimVec, a novel simplified vector format that encodes chart elements such as mark type, position, and size. The effectiveness of SimVec is demonstrated by using MLLMs to reconstruct chart information from SimVec formats. Then, we build a new visualization dataset, SimVecVis, to enhance the performance of MLLMs in visualization understanding, which consists of three key dimensions: bitmap images of charts, their SimVec representations, and corresponding data-centric question-answering (QA) pairs with explanatory chain-of-thought (CoT) descriptions. We finetune state-of-the-art MLLMs (e.g., MiniCPM and Qwen-VL), using SimVecVis with different dataset dimensions. The experimental results show that it leads to substantial performance improvements of MLLMs with good spatial perception capabilities (e.g., MiniCPM) in data-centric QA tasks. Our dataset and source code are available at: https://github.com/VIDA-Lab/SimVecVis.",
      "authors": [
        "Can Liu and Chunlin Da and Xiaoxiao Long and Yuxiao Yang and Yu Zhang and Yong Wang"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-26T14:35:59+00:00",
          "link": "https://arxiv.org/abs/2506.21319v1",
          "size": "5645kb",
          "version": "v1"
        },
        {
          "date": "2025-07-01T10:11:25+00:00",
          "link": "https://arxiv.org/abs/2506.21319v2",
          "size": "5583kb",
          "version": "v2"
        },
        {
          "date": "2025-07-02T09:58:58+00:00",
          "link": "https://arxiv.org/abs/2506.21319v3",
          "size": "5583kb",
          "version": "v3"
        }
      ],
      "title": "SimVecVis: A Dataset for Enhancing MLLMs in Visualization Understanding",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.21319",
        "HTML": "https://arxiv.org/html/2506.21319v3",
        "PDF": "https://arxiv.org/pdf/2506.21319"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper focuses on enhancing visualization understanding using MLLMs and a new dataset SimVecVis. There is no direct mention of creativity themes."
      },
      "source": "arXiv"
    },
    {
      "id": "2506.22941",
      "abstract": "Access to accurate and actionable harm reduction information can directly impact the health outcomes of People Who Use Drugs (PWUD), yet existing online channels often fail to meet their diverse and dynamic needs due to limitations in adaptability, accessibility, and the pervasive impact of stigma. Large Language Models (LLMs) present a novel opportunity to enhance information provision, but their application in such a high-stakes domain is under-explored and presents socio-technical challenges. This paper investigates how LLMs can be responsibly designed to support the information needs of PWUD. Through a qualitative workshop involving diverse stakeholder groups (academics, harm reduction practitioners, and an online community moderator), we explored LLM capabilities, identified potential use cases, and delineated core design considerations. Our findings reveal that while LLMs can address some existing information barriers (e.g., by offering responsive, multilingual, and potentially less stigmatising interactions), their effectiveness is contingent upon overcoming challenges related to ethical alignment with harm reduction principles, nuanced contextual understanding, effective communication, and clearly defined operational boundaries. We articulate design pathways emphasising collaborative co-design with experts and PWUD to develop LLM systems that are helpful, safe, and responsibly governed. This work contributes empirically grounded insights and actionable design considerations for the responsible development of LLMs as supportive tools within the harm reduction ecosystem.",
      "authors": [
        "Kaixuan Wang",
        "Jason T. Jacques",
        "and Chenxin Diao"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "2025-06-28T16:15:47+00:00",
          "link": "https://arxiv.org/abs/2506.22941v1",
          "size": "4041kb",
          "version": "v1"
        },
        {
          "date": "2025-07-02T13:02:17+00:00",
          "link": "https://arxiv.org/abs/2506.22941v2",
          "size": "4078kb",
          "version": "v2"
        }
      ],
      "title": "Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.22941",
        "PDF": "https://arxiv.org/pdf/2506.22941"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper addresses the use of LLMs for harm reduction practice in online settings. Creativity is not a focus; the paper is more concerned with ethical and responsive design considerations in a socio-technical context."
      },
      "source": "arXiv"
    },
    {
      "id": "2308.02515",
      "abstract": "Classification of motor imagery (MI) using non-invasive electroencephalographic (EEG) signals is a critical objective as it is used to predict the intention of limb movements of a subject. In recent research, convolutional neural network (CNN) based methods have been widely utilized for MI-EEG classification. The challenges of training neural networks for MI-EEG signals classification include low signal-to-noise ratio, non-stationarity, non-linearity, and high complexity of EEG signals. The features computed by CNN-based networks on the highly noisy MI-EEG signals contain irrelevant information. Subsequently, the feature maps of the CNN-based network computed from the noisy and irrelevant features contain irrelevant information. Thus, many non-contributing features often mislead the neural network training and degrade the classification performance. Hence, a novel feature reweighting approach is proposed to address this issue. The proposed method gives a noise reduction mechanism named feature reweighting module that suppresses irrelevant temporal and channel feature maps. The feature reweighting module of the proposed method generates scores that reweight the feature maps to reduce the impact of irrelevant information. Experimental results show that the proposed method significantly improved the classification of MI-EEG signals of Physionet EEG-MMIDB and BCI Competition IV 2a datasets by a margin of 9.34% and 3.82%, respectively, compared to the state-of-the-art methods.",
      "authors": [
        "Taveena Lotey",
        "Prateek Keserwani",
        "Debi Prosad Dogra",
        "and Partha Pratim Roy"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Machine Learning (cs.LG)",
        "Human-Computer Interaction (cs.HC)",
        "Signal Processing (eess.SP)"
      ],
      "submission_historys": [
        {
          "date": "2023-07-29T14:22:10+00:00",
          "link": "https://arxiv.org/abs/2308.02515v1",
          "size": "644kb",
          "version": "v1"
        },
        {
          "date": "2025-07-02T05:29:39+00:00",
          "link": "https://arxiv.org/abs/2308.02515v2",
          "size": "1639kb",
          "version": "v2"
        }
      ],
      "title": "Feature Reweighting for EEG-based Motor Imagery Classification",
      "links": {
        "Abstract": "https://arxiv.org/abs/2308.02515",
        "HTML": "https://arxiv.org/html/2308.02515v2",
        "PDF": "https://arxiv.org/pdf/2308.02515"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "The paper is about feature reweighting in EEG-based motor imagery classification, focused on improving classification performance, with no connection to creativity."
      },
      "tasks": [
        "Classification",
        "EEG",
        "Motor Imagery"
      ],
      "source": "arXiv"
    },
    {
      "id": "2311.10918",
      "abstract": "This paper includes a review of current state of the art 6d pose estimation methods, as well as a discussion of which pose estimation method should be used in two types of architectural design scenarios. Taking the latest pose estimation research Gen6d as an example, we make a qualitative assessment of the current openset methods in terms of application level, prediction speed, resistance to occlusion, accuracy, resistance to environmental interference, etc. In addition, we try to combine 6D pose estimation and building wind environment assessment to create tangible architectural design approach, we discuss the limitations of the method and point out the direction in which 6d pose estimation is eager to progress in this scenario.",
      "authors": [
        "Zixun Huang"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "2023-11-18T00:15:53+00:00",
          "link": "https://arxiv.org/abs/2311.10918v1",
          "size": "6312kb",
          "version": "v1"
        },
        {
          "date": "2025-07-01T21:53:55+00:00",
          "link": "https://arxiv.org/abs/2311.10918v2",
          "size": "5304kb",
          "version": "v2"
        }
      ],
      "title": "Jenga Stacking Based on 6D Pose Estimation for Architectural Form Finding Process",
      "links": {
        "Abstract": "https://arxiv.org/abs/2311.10918",
        "HTML": "https://arxiv.org/html/2311.10918v2",
        "PDF": "https://arxiv.org/pdf/2311.10918"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper evaluates pose estimation methods which have implications for architectural design scenarios, a topic where creativity is a secondary theme in form finding processes."
      },
      "tasks": [
        "6D Pose Estimation",
        "Form",
        "Pose Estimation"
      ],
      "source": "arXiv"
    },
    {
      "id": "2403.08700",
      "abstract": "Obstetric ultrasound image quality is crucial for accurate diagnosis and monitoring of fetal health. However, acquiring high-quality standard planes is difficult, influenced by the sonographer's expertise and factors like the maternal BMI or fetus dynamics. In this work, we explore diffusion-based counterfactual explainable AI to generate realistic, high-quality standard planes from low-quality non-standard ones. Through quantitative and qualitative evaluation, we demonstrate the effectiveness of our approach in generating plausible counterfactuals of increased quality. This shows future promise for enhancing training of clinicians by providing visual feedback and potentially improving standard plane quality and acquisition for downstream diagnosis and monitoring.",
      "authors": [
        "Paraskevas Pegios",
        "Manxi Lin",
        "Nina Weng",
        "Morten Bo S{\\o}ndergaard Svendsen",
        "Zahra Bashir",
        "Siavash Bigdeli",
        "Anders Nymark Christensen",
        "Martin Tolsgaard and Aasa Feragen"
      ],
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "subjects": [
        "Image and Video Processing (eess.IV)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2024-03-13T17:04:56+00:00",
          "link": "https://arxiv.org/abs/2403.08700v1",
          "size": "861kb",
          "version": "v1"
        },
        {
          "date": "2025-07-02T09:00:19+00:00",
          "link": "https://arxiv.org/abs/2403.08700v2",
          "size": "447kb",
          "version": "v2"
        }
      ],
      "title": "Diffusion-based Iterative Counterfactual Explanations for Fetal Ultrasound Image Quality Assessment",
      "links": {
        "Abstract": "https://arxiv.org/abs/2403.08700",
        "HTML": "https://arxiv.org/html/2403.08700v2",
        "PDF": "https://arxiv.org/pdf/2403.08700"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This research focuses on image quality assessment for fetal ultrasound using counterfactual explanations, with no relation to creativity."
      },
      "tasks": [
        "counterfactual",
        "Image Quality Assessment"
      ],
      "source": "arXiv"
    },
    {
      "id": "2502.09282",
      "abstract": "Remote sensing image captioning aims to generate descriptive text from remote sensing images, typically employing an encoder-decoder framework. In this setup, a convolutional neural network (CNN) extracts feature representations from the input image, which then guide the decoder in a sequence-to-sequence caption generation process. Although much research has focused on refining the decoder, the quality of image representations from the encoder remains crucial for accurate captioning. This paper introduces a novel approach that integrates features from two distinct CNN based encoders, capturing complementary information to enhance caption generation. Additionally, we propose a weighted averaging technique to combine the outputs of all GRUs in the stacked decoder. Furthermore, a comparison-based beam search strategy is incorporated to refine caption selection. The results demonstrate that our fusion-based approach, along with the enhanced stacked decoder, significantly outperforms both the transformer-based state-of-the-art model and other LSTM-based baselines.",
      "authors": [
        "Swadhin Das",
        "Raksha Sharma"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-02-13T12:54:13+00:00",
          "link": "https://arxiv.org/abs/2502.09282v1",
          "size": "2218kb",
          "version": "v1"
        },
        {
          "date": "2025-07-02T07:55:33+00:00",
          "link": "https://arxiv.org/abs/2502.09282v2",
          "size": "1127kb",
          "version": "v2"
        }
      ],
      "title": "FE-LWS: Refined Image-Text Representations via Decoder Stacking and Fused Encodings for Remote Sensing Image Captioning",
      "links": {
        "Abstract": "https://arxiv.org/abs/2502.09282",
        "HTML": "https://arxiv.org/html/2502.09282v2",
        "PDF": "https://arxiv.org/pdf/2502.09282"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper relates to image-text representation for remote sensing captioning, where creativity is a secondary consideration in generating descriptive text."
      },
      "tasks": [
        "Caption Generation",
        "Decoder",
        "Descriptive",
        "Image Captioning"
      ],
      "source": "arXiv"
    },
    {
      "id": "2505.02329",
      "abstract": "Algorithmic management (AM)'s impact on worker well-being has led to calls for regulation. However, little is known about the effectiveness and challenges in real-world AM regulation across the regulatory process -- rule operationalization, software use, and enforcement. Our multi-stakeholder study addresses this gap within workplace scheduling, one of the few AM domains with implemented regulations. We interviewed 38 stakeholders across the regulatory process: regulators, defense attorneys, worker advocates, managers, and workers. Our findings suggest that the efficacy of AM regulation is influenced by: (i) institutional constraints that challenge efforts to encode law into AM software, (ii) on-the-ground use of AM software that shapes its ability to facilitate compliance, (iii) mismatches between software and regulatory contexts that hinder enforcement, and (iv) unique concerns that software introduces when used to regulate AM. These findings underscore the importance of a sociotechnical approach to AM regulation, which considers organizational and collaborative contexts alongside the inherent attributes of software. We offer future research directions and implications for technology policy and design.",
      "authors": [
        "Jonathan Lynn",
        "Rachel Y. Kim",
        "Sicun Gao",
        "Daniel Schneider",
        "Sachin S. Pandya",
        "Min Kyung Lee"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "subjects": [
        "Computers and Society (cs.CY)",
        "Human-Computer Interaction (cs.HC)",
        "Software Engineering (cs.SE)"
      ],
      "submission_historys": [
        {
          "date": "2025-05-05T02:56:28+00:00",
          "link": "https://arxiv.org/abs/2505.02329v1",
          "size": "5159kb",
          "version": "v1"
        },
        {
          "date": "2025-05-26T03:26:37+00:00",
          "link": "https://arxiv.org/abs/2505.02329v2",
          "size": "2884kb",
          "version": "v2"
        },
        {
          "date": "2025-07-01T20:12:44+00:00",
          "link": "https://arxiv.org/abs/2505.02329v3",
          "size": "2884kb",
          "version": "v3"
        }
      ],
      "title": "Regulating Algorithmic Management: A Multi-Stakeholder Study of Challenges in Aligning Software and the Law for Workplace Scheduling",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.02329",
        "HTML": "https://arxiv.org/html/2505.02329v3",
        "PDF": "https://arxiv.org/pdf/2505.02329"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper analyzes algorithmic management in workplace scheduling regulations, with no direct connection to creativity."
      },
      "source": "arXiv"
    },
    {
      "id": "2502.09203",
      "abstract": "Due to large intra-subject and inter-subject variabilities of electroencephalogram (EEG) signals, EEG-based brain-computer interfaces (BCIs) usually need subject-specific calibration to tailor the decoding algorithm for each new subject, which is time-consuming and user-unfriendly, hindering their real-world applications. Transfer learning (TL) has been extensively used to expedite the calibration, by making use of EEG data from other subjects/sessions. An important consideration in TL for EEG-based BCIs is to reduce the data distribution discrepancies among different subjects/sessions, to avoid negative transfer. Euclidean alignment (EA) was proposed in 2020 to address this challenge. Numerous experiments from 13 different BCI paradigms demonstrated its effectiveness and efficiency. This paper revisits EA, explaining its procedure and correct usage, introducing its applications and extensions, and pointing out potential new research directions. It should be very helpful to BCI researchers, especially those who are working on EEG signal decoding.",
      "authors": [
        "Dongrui Wu"
      ],
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "2025-02-13T11:43:43+00:00",
          "link": "https://arxiv.org/abs/2502.09203v1",
          "size": "3750kb",
          "version": "v1"
        },
        {
          "date": "2025-05-03T14:58:21+00:00",
          "link": "https://arxiv.org/abs/2502.09203v2",
          "size": "5170kb",
          "version": "v2"
        }
      ],
      "title": "Revisiting Euclidean Alignment for Transfer Learning in EEG-Based Brain-Computer Interfaces",
      "links": {
        "Abstract": "https://arxiv.org/abs/2502.09203",
        "HTML": "https://arxiv.org/html/2502.09203",
        "PDF": "https://arxiv.org/pdf/2502.09203"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "irrelevant",
        "reason": "This paper discusses transfer learning for EEG-based brain-computer interfaces, focusing on calibration efficiency, without addressing creative aspects."
      },
      "tasks": [
        "EEG",
        "Electroencephalogram (EEG)",
        "Transfer Learning"
      ],
      "source": "arXiv"
    },
    {
      "id": "2407.11218",
      "abstract": "Robots are becoming more capable and can autonomously perform tasks such as navigating between locations. However, human oversight remains crucial. This study compared two touchless methods for directing mobile robots: voice control and gesture control, to investigate the efficiency of the methods and the preference of users. We tested these methods in two conditions: one in which participants remained stationary and one in which they walked freely alongside the robot. We hypothesized that walking alongside the robot would result in higher intuitiveness ratings and improved task performance, based on the idea that walking promotes spatial alignment and reduces the effort required for mental rotation. In a 2x2 within-subject design, 218 participants guided the quadruped robot Spot along a circuitous route with multiple 90-degree turns using rotate left, rotate right, and walk forward commands. After each trial, participants rated the intuitiveness of the command mapping, while post-experiment interviews were used to gather the participants' preferences. Results showed that voice control combined with walking with Spot was the most favored and intuitive, whereas gesture control while standing caused confusion for left/right commands. Nevertheless, 29% of participants preferred gesture control, citing increased task engagement and visual congruence as reasons. An odometry-based analysis revealed that participants often followed behind Spot, particularly in the gesture control condition, when they were allowed to walk. In conclusion, voice control with walking produced the best outcomes. Improving physical ergonomics and adjusting gesture types could make gesture control more effective.",
      "authors": [
        "Renchi Zhang",
        "Jesse van der Linden",
        "Dimitra Dodou",
        "Harleigh Seyffert",
        "Yke Bauke Eisma",
        "Joost C. F. de Winter"
      ],
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "2024-07-15T20:07:33+00:00",
          "link": "https://arxiv.org/abs/2407.11218v1",
          "size": "12227kb",
          "version": "v1"
        },
        {
          "date": "2024-07-17T07:48:45+00:00",
          "link": "https://arxiv.org/abs/2407.11218v2",
          "size": "12227kb",
          "version": "v2"
        },
        {
          "date": "2025-01-13T09:23:41+00:00",
          "link": "https://arxiv.org/abs/2407.11218v3",
          "size": "9981kb",
          "version": "v3"
        },
        {
          "date": "2025-04-04T16:25:04+00:00",
          "link": "https://arxiv.org/abs/2407.11218v4",
          "size": "31989kb",
          "version": "v4"
        },
        {
          "date": "2025-04-14T08:01:06+00:00",
          "link": "https://arxiv.org/abs/2407.11218v5",
          "size": "11885kb",
          "version": "v5"
        }
      ],
      "title": "Walk along: An Experiment on Controlling the Mobile Robot 'Spot' with Voice and Gestures",
      "links": {
        "Abstract": "https://arxiv.org/abs/2407.11218",
        "PDF": "https://arxiv.org/pdf/2407.11218"
      },
      "relevance": {
        "keyword": "creativity",
        "level": "partial",
        "reason": "The paper explores human-robot interaction through voice and gesture controls, where the creativity aspect might relate to user engagement and interaction design, making it a secondary theme."
      },
      "source": "arXiv"
    }
  ],
  "subjects": [
    "Computer Vision and Pattern Recognition (cs.CV)",
    "Multimedia (cs.MM)",
    "Computation and Language (cs.CL)",
    "Sound (cs.SD)",
    "Image and Video Processing (eess.IV)",
    "Cryptography and Security (cs.CR)",
    "Robotics (cs.RO)",
    "Information Retrieval (cs.IR)",
    "Artificial Intelligence (cs.AI)",
    "Signal Processing (eess.SP)",
    "Audio and Speech Processing (eess.AS)",
    "Computers and Society (cs.CY)",
    "Software Engineering (cs.SE)",
    "Human-Computer Interaction (cs.HC)",
    "Machine Learning (cs.LG)"
  ],
  "prompt": {
    "creativity": "\nYou are an expert in information retrieval. I will provide you with a list of research papers from arXiv, specifically in the *cs.HC* (Human-Computer Interaction) category.\n\nYour task is to analyze each paper and determine its relevance to the topic of **Creativity**.\n\n\n### Classify each paper into one of the following relevance levels\n\n- `core` \u2014 Creativity is a **primary focus**\n  - The paper directly studies or simulates creativity, with a clear focus on creativity.\n  - Includes creative tasks, co-creative systems, or creativity evaluation metrics.\n  - The title and abstract explicitly mention creativity, and the research questions are directly related to creativity.\n- `partial` \u2014 Creativity is a **secondary theme**\n  - Part of the paper relates to creativity; it is treated as an analytical dimension or design goal but not the main objective (e.g., user creativity, design support).\n  - Creativity may appear in discussions, experiments, or auxiliary applications.\n  - Creativity is presented as a supporting topic (e.g., evaluation criteria, user feedback).\n- `irrelevant` \u2014 **No clear connection** to creativity\n  - The paper does not address creativity as a topic.\n  - Focuses on unrelated technical content (e.g., compression, security, optimization).\n  - If creativity is mentioned, it is only superficial and lacks substantive content.\n\n\n### Return your results in the following JSON format\n\n```json\n{\n  \"result\": [\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    },\n    {\n      \"id\": \"paper id\",\n      \"level\": \"core | partial | irrelevant\",\n      \"reason\": \"Brief justification based on the paper content\"\n    }\n  ]\n}\n```\n\nBe concise but specific in your reasoning, referencing key terms or sections when applicable.\n"
  },
  "description": "Data source: https://arxiv.org/list/cs.HC/new",
  "level_tatistics": {
    "partial": 9,
    "irrelevant": 23,
    "core": 3
  },
  "arxiv_update_date": "2025-07-03",
  "updated_at": "2025-07-03 10:09:27"
}