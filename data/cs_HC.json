{
  "data": [
    {
      "id": "2506.18962",
      "abstract": "Decoding human brain activity from electroencephalography (EEG) signals is a central challenge at the intersection of neuroscience and artificial intelligence, enabling diverse applications in mental state assessment, clinical monitoring, and human-machine interaction. Recent efforts have extensively explored EEG-based brain foundation models for generalized brain decoding, employing large-scale training on multiple datasets. However, most of these attempts struggle with generalizability and fail to achieve satisfactory performance without task-specific tuning due to pronounced inherent heterogeneity among decoding tasks. To address these challenges, we present UniMind, a general-purpose EEG foundation model for unified multi-task brain decoding by uniquely unleashing the power of large language models to comprehend complex neural patterns. UniMind offers several advantages. First, we design a Neuro-Language Connector to bridge the modality gap between neural signals and large language models, distilling and transforming the spatiotemporal neural patterns of EEG data into representations understandable by language models. Second, a Task-aware Query Selection module is proposed to inject task-awareness into the cross-modal alignment by dynamically generating task-adaptive query tokens, enabling learning of task-relevant neural patterns across diverse tasks. Extensive experiments across ten datasets demonstrate that UniMind substantially outperforms state-of-the-art multi-task decoding models, with an average gain of 12 percent, while also offering valuable neuroscientific insights into neural functional correlations across tasks. The code will be made publicly available.",
      "authors": [
        "Weiheng Lu",
        "Chunfeng Song",
        "Jiamin Wu",
        "Pengyu Zhu",
        "Yuchen Zhou",
        "Weijian Mai",
        "Qihao Zheng",
        "Wanli Ouyang"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18962",
        "HTML": "https://arxiv.org/html/2506.18962",
        "PDF": "https://arxiv.org/pdf/2506.18962"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 17:58:17 GMT",
          "size": "5359kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "UniMind: Unleashing the Power of LLMs for Unified Multi-Task Brain Decoding",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The focus is on brain decoding using large language models, offering no explicit relation to creativity."
      }
    },
    {
      "id": "2506.19017",
      "abstract": "The climate is warming rapidly, and atmospheric concentrations of greenhouse gases (GHGs) are at their highest levels ever recorded. As a result of these climate changes, caused mainly by human activities, disasters have increased fivefold over the past 50 years, causing death and economic loss. Civic engagement and awareness are essential to mitigate climate change and its impacts. In this work, we proposed a user interface that makes users aware of the environmental impact of the food products they buy when shopping. A user-centered scenario-based design was followed in the development of the interface. Gamification elements were added to increase civic participation in climate action.",
      "authors": [
        "Lorenzo Porcelli and Francesco Palmieri"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19017",
        "HTML": "https://arxiv.org/html/2506.19017",
        "PDF": "https://arxiv.org/pdf/2506.19017"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 18:12:16 GMT",
          "size": "332kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Raise Awareness of the Environmental Impacts of Retail Food Products: A User-Centered Scenario-Based Approach",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper concerns environmental awareness and gamification in a retail context, without relevance to creativity."
      }
    },
    {
      "id": "2506.19107",
      "abstract": "With the proliferation of large language model (LLM) applications since 2022, their use in education has sparked both excitement and concern. Recent studies consistently highlight students' (mis)use of LLMs can hinder learning outcomes. This work aims to teach students how to effectively prompt LLMs to improve their learning. We first proposed pedagogical prompting, a theoretically-grounded new concept to elicit learning-oriented responses from LLMs. To move from concept design to a proof-of-concept learning intervention in real educational settings, we selected early undergraduate CS education (CS1/CS2) as the example context. We began with a formative survey study with instructors (N=36) teaching early-stage undergraduate-level CS courses to inform the instructional design based on classroom needs. Based on their insights, we designed and developed a learning intervention through an interactive system with scenario-based instruction to train pedagogical prompting skills. Finally, we evaluated its instructional effectiveness through a user study with CS novice students (N=22) using pre/post-tests. Through mixed methods analyses, our results indicate significant improvements in learners' LLM-based pedagogical help-seeking skills, along with positive attitudes toward the system and increased willingness to use pedagogical prompts in the future. Our contributions include (1) a theoretical framework of pedagogical prompting; (2) empirical insights into current instructor attitudes toward pedagogical prompting; and (3) a learning intervention design with an interactive learning tool and scenario-based instruction leading to promising results on teaching LLM-based help-seeking. Our approach is scalable for broader implementation in classrooms and has the potential to be integrated into tools like ChatGPT as an on-boarding experience to encourage learning-oriented use of generative AI.",
      "authors": [
        "Ruiwei Xiao",
        "Xinying Hou",
        "Runlong Ye",
        "Majeed Kazemitabaar",
        "Nicholas Diana",
        "Michael Liut",
        "John Stamper"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19107",
        "HTML": "https://arxiv.org/html/2506.19107",
        "PDF": "https://arxiv.org/pdf/2506.19107"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 20:39:17 GMT",
          "size": "17055kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Improving Student-AI Interaction Through Pedagogical Prompting: An Example in Computer Science Education",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the paper focuses on improving student interaction with AI through pedagogical prompting, it indirectly involves creativity in the context of designing effective prompts for learning. However, creativity is not the main focus."
      }
    },
    {
      "id": "2506.19210",
      "abstract": "Cerebral Visual Impairment (CVI) is the set to be the leading cause of vision impairment, yet remains underrepresented in assistive technology research. Unlike ocular conditions, CVI affects higher-order visual processing-impacting object recognition, facial perception, and attention in complex environments. This paper presents a co-design study with two adults with CVI investigating how smart glasses, i.e. head-mounted extended reality displays, can support understanding and interaction with the immediate environment. Guided by the Double Diamond design framework, we conducted a two-week diary study, two ideation workshops, and ten iterative development sessions using the Apple Vision Pro. Our findings demonstrate that smart glasses can meaningfully address key challenges in locating objects, reading text, recognising people, engaging in conversations, and managing sensory stress. With the rapid advancement of smart glasses and increasing recognition of CVI as a distinct form of vision impairment, this research addresses a timely and under-explored intersection of technology and need.",
      "authors": [
        "Bhanuka Gamage",
        "Nicola McDowell",
        "Dijana Kovacic",
        "Leona Holloway",
        "Thanh-Toan Do",
        "Nicholas Price",
        "Arthur Lowery",
        "Kim Marriott"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19210",
        "HTML": "https://arxiv.org/html/2506.19210",
        "PDF": "https://arxiv.org/pdf/2506.19210"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 00:40:20 GMT",
          "size": "3668kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Smart Glasses for CVI: Co-Designing Extended Reality Solutions to Support Environmental Perception by People with Cerebral Visual Impairment",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The research centers on co-designing smart glasses for people with Cerebral Visual Impairment to aid in environmental perception. Creativity is not a topic of discussion."
      }
    },
    {
      "id": "2506.19268",
      "abstract": "We present HARPT, a large-scale annotated corpus of mobile health app store reviews aimed at advancing research in user privacy and trust. The dataset comprises over 480,000 user reviews labeled into seven categories that capture critical aspects of trust in applications, trust in providers and privacy concerns. Creating HARPT required addressing multiple complexities, such as defining a nuanced label schema, isolating relevant content from large volumes of noisy data, and designing an annotation strategy that balanced scalability with accuracy. This strategy integrated rule-based filtering, iterative manual labeling with review, targeted data augmentation, and weak supervision using transformer-based classifiers to accelerate coverage. In parallel, a carefully curated subset of 7,000 reviews was manually annotated to support model development and evaluation. We benchmark a broad range of classification models, demonstrating that strong performance is achievable and providing a baseline for future research. HARPT is released as a public resource to support work in health informatics, cybersecurity, and natural language processing.",
      "authors": [
        "Timoteo Kelly",
        "Abdulkadir Korkmaz",
        "Samuel Mallet",
        "Connor Souders",
        "Sadra Aliakbarpour",
        "Praveen Rao"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19268",
        "HTML": "https://arxiv.org/html/2506.19268",
        "PDF": "https://arxiv.org/pdf/2506.19268"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Cryptography and Security (cs.CR)",
        "Emerging Technologies (cs.ET)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 02:59:14 GMT",
          "size": "213kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "HARPT: A Corpus for Analyzing Consumers' Trust and Privacy Concerns in Mobile Health Apps",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper is focused on privacy and trust concerns in mobile health apps, with no mention of creativity."
      }
    },
    {
      "id": "2506.19307",
      "abstract": "Presbyopia, a common age-related vision condition affecting most people as they age, often remains inadequately understood by those unaffected. To help bridge the gap between abstract accessibility knowledge and a more grounded appreciation of perceptual challenges, this study presents OpticalAging, an optical see-through simulation approach. Unlike VR-based methods, OpticalAging uses dynamically controlled tunable lenses to simulate the first-person visual perspective of presbyopia's distance-dependent blur during real-world interaction, aiming to enhance awareness. While acknowledging critiques regarding simulation's limitations in fully capturing lived experience, we position this tool as a complement to user-centered methods. Our user study (N = 19, 18-35 years old) provides validation: quantitative measurements show statistically significant changes in near points across three age modes (40s, 50s, 60s), while qualitative results suggest increases in reported understanding and empathy among participants. The integration of our tool into a design task showcases its potential applicability within age-inclusive design workflows when used critically alongside direct user engagement.",
      "authors": [
        "Qing Zhang",
        "Zixiong Su",
        "Yoshihito Kondoh",
        "Kazunori Asada",
        "Thad Starner",
        "Kai Kunze",
        "Yuta Itoh",
        "Jun Rekimoto"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19307",
        "HTML": "https://arxiv.org/html/2506.19307",
        "PDF": "https://arxiv.org/pdf/2506.19307"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 04:43:12 GMT",
          "size": "11184kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "OpticalAging: Real-time Presbyopia Simulation for Inclusive Design via Tunable Lenses",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The focus is on simulation to enhance understanding and empathy related to presbyopia, with no linkage to creativity."
      }
    },
    {
      "id": "2506.19364",
      "abstract": "The integration of Generative AI (GenAI) into education has raised concerns about over-reliance and superficial learning, particularly in writing tasks in higher education. This study explores whether a theory-driven learning analytics dashboard (LAD) can enhance human-AI collaboration in the academic writing task by improving writing knowledge gains, fostering self-regulated learning (SRL) skills and building different human-AI dialogue characteristics. Grounded in Zimmerman's SRL framework, the LAD provided real-time feedback on learners' goal-setting, writing processes and reflection, while monitoring the quality of learner-AI interactions. A quasi-experiment was conducted involving 52 postgraduate students divided into an experimental group (EG) using the LAD to a control group (CG) without it in a human-AI collaborative writing task. Pre- and post- knowledge tests, questionnaires measuring SRL and cognitive load, and students' dialogue data with GenAI were collected and analyzed. Results showed that the EG achieved significantly higher writing knowledge gains and improved SRL skills, particularly in self-efficacy and cognitive strategies. However, the EG also reported increased test anxiety and cognitive load, possibly due to heightened metacognitive awareness. Epistemic Network Analysis revealed that the EG engaged in more reflective, evaluative interactions with GenAI, while the CG focused on more transactional and information-seeking exchanges. These findings contribute to the growing body of literature on the educational use of GenAI and highlight the importance of designing interventions that complement GenAI tools, ensuring that technology enhances rather than undermines the learning process.",
      "authors": [
        "Angxuan Chen",
        "Jingjing Lian",
        "Xinran Kuang",
        "Jiyou Jia"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19364",
        "HTML": "https://arxiv.org/html/2506.19364",
        "PDF": "https://arxiv.org/pdf/2506.19364"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 06:51:13 GMT",
          "size": "1085kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Can theory-driven learning analytics dashboard enhance human-AI collaboration in writing learning? Insights from an empirical experiment",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "Explores AI collaboration in writing, which might involve elements of creative writing or approach, although creativity is not the primary focus."
      }
    },
    {
      "id": "2506.19430",
      "abstract": "The automated analysis of human behaviour provides many opportunities for the creation of interactive systems and the post-experiment investigations for user studies. Commodity depth cameras offer reasonable body tracking accuracy at a low price point, without the need for users to wear or hold any extra equipment. The resulting systems typically perform body tracking through a dedicated machine learning model, but they can be enhanced with additional AI components providing extra capabilities. This leads to opportunities but also challenges, for example regarding the orchestration of such AI components and the engineering of the resulting tracking pipeline. In this paper, we discuss these elements, based on our experience with the creation of a remote collaboration system across distant wall-sized displays, that we built using existing and readily available building blocks, including AI-based recognition models.",
      "authors": [
        "Adrien Coppens and Val\\'erie Maquil"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19430",
        "HTML": "https://arxiv.org/html/2506.19430",
        "PDF": "https://arxiv.org/pdf/2506.19430"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 08:55:06 GMT",
          "size": "1387kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Integrating AIs With Body Tracking Technology for Human Behaviour Analysis: Challenges and Opportunities",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "Although the paper discusses AI integration and challenges in human behavior analysis, it only indirectly relates to creativity as it might touch on creative system design."
      }
    },
    {
      "id": "2506.19495",
      "abstract": "Empathy is widely recognized as a vital attribute for effective collaboration and communication in the workplace, yet developing empathic skills and fostering it among colleagues remains a challenge. This study explores the potential of a collaborative digital storytelling platform - In Your Shoes - designed to promote empathic listening and interpersonal understanding through the structured exchange of personal narratives. A one-week intervention was conducted with employees from multiple organizations using the platform. Employing a mixed methods approach, we assessed quantitative changes in empathy using the Empathy Quotient (EQ) and qualitatively analyzed participant experiences through grounded theory. While quantitative analysis revealed no statistically significant shift in dispositional empathy, qualitative findings suggested the tool facilitated situational empathy, prompted self-reflection, improved emotional resonance, and enhanced workplace relationships. Participants reported feelings of psychological safety, connection, and, in some cases, therapeutic benefits from sharing and responding to stories. These results highlight the promise of asynchronous, structured narrative-based digital tools for supporting empathic engagement in professional settings, offering insights for the design of emotionally intelligent workplace technologies.",
      "authors": [
        "Russell Beale",
        "Eugenia Sergueeva"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19495",
        "HTML": "https://arxiv.org/html/2506.19495",
        "PDF": "https://arxiv.org/pdf/2506.19495"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 10:32:15 GMT",
          "size": "1424kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "5 Days, 5 Stories: Using Technology to Promote Empathy in the Workplace",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper is explicitly focused on using digital storytelling to promote empathy, a process closely linked to creative expression and narrative creation."
      }
    },
    {
      "id": "2506.19519",
      "abstract": "Virtual reality (VR) can enrich neuropsychological testing, yet the ergonomic trade-offs of its input modes remain under-examined. Seventy-seven healthy volunteers-young (19-29 y) and middle-aged (27-56 y)-completed a VR Trail-Making Test with three pointing methods: eye-tracking, head-gaze, and a six-degree-of-freedom hand controller. Completion time, spatial accuracy, and error counts for the simple (Trail A) and alternating (Trail B) sequences were analysed in 3 x 2 x 2 mixed-model ANOVAs; post-trial scales captured usability (SUS), user experience (UEQ-S), and acceptability. Age dominated behaviour: younger adults were reliably faster, more precise, and less error-prone. Against this backdrop, input modality mattered. Eye-tracking yielded the best spatial accuracy and shortened Trail A time relative to manual control; head-gaze matched eye-tracking on Trail A speed and became the quickest, least error-prone option on Trail B. Controllers lagged on every metric. Subjective ratings were high across the board, with only a small usability dip in middle-aged low-gamers. Overall, gaze-based ray-casting clearly outperformed manual pointing, but optimal choice depended on task demands: eye-tracking maximised spatial precision, whereas head-gaze offered calibration-free enhanced speed and error-avoidance under heavier cognitive load. TMT-VR appears to be accurate, engaging, and ergonomically adaptable assessment, yet it requires age-specific-stratified norms.",
      "authors": [
        "Panagiotis Kourtesis",
        "Evgenia Giatzoglou",
        "Panagiotis Vorias",
        "Katerina Alkisti Gounari",
        "Eleni Orfanidou and Chrysanthi Nega"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19519",
        "HTML": "https://arxiv.org/html/2506.19519",
        "PDF": "https://arxiv.org/pdf/2506.19519"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 11:10:09 GMT",
          "size": "1325kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Examination of Eye-Tracking, Head-Gaze, and Controller-Based Ray-casting in TMT-VR: Performance and Usability Across Adulthood",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "This paper examines VR testing methods for neuropsychological assessments without any focus on creativity."
      }
    },
    {
      "id": "2506.19524",
      "abstract": "Healthcare professionals (HCPs) face increasing levels of stress and burnout. Technological wellbeing interventions provide accessible and flexible support for HCPs. While most studies have focused on mobile- and web-based programs, alternative technologies like virtual reality (VR), augmented reality (AR), tangible interfaces, and embodied technologies are emerging as engaging and effective tools for wellbeing interventions. However, there is still a lack of research on how such technologies are perceived among HCPs. This study explored HCPs' perceptions and preferences for various types of wellbeing technologies, by conducting a 2-phase co-design study involving 26 HCPs in idea generation, concept evaluation, prototype testing, and design iteration. From our findings, HCPs highly valued the potential of technologies to support mental health with immersive, embodied, and collective experiences. Furthermore, we provided design recommendations for wellbeing technologies for HCPs that sustain user engagement by meeting their needs for autonomy, competence, and relatedness in the experiences.",
      "authors": [
        "Zheyuan Zhang",
        "Jingjing Sun",
        "Dorian Peters",
        "Rafael A. Calvo"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19524",
        "HTML": "https://arxiv.org/html/2506.19524",
        "PDF": "https://arxiv.org/pdf/2506.19524"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 11:22:46 GMT",
          "size": "31257kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Beyond Wellbeing Apps: Co-Designing Immersive, Embodied, and Collective Digital Wellbeing Interventions for Healthcare Professionals",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper focuses on wellbeing technologies for healthcare professionals, where creativity is involved as part of the co-design process, but it's not the main focus of the study."
      }
    },
    {
      "id": "2506.19611",
      "abstract": "This position paper situates AR beauty filters within the broader debate on Body Politics in HCI. We argue that these filters are not neutral tools but technologies of governance that reinforce racialized, gendered, and ableist beauty standards. Through naming conventions, algorithmic bias, and platform governance, they impose aesthetic norms while concealing their influence. To address these challenges, we advocate for transparency-driven interventions and a critical rethinking of algorithmic aesthetics and digital embodiment.",
      "authors": [
        "Miriam Doh",
        "Corinna Canali",
        "Nuria Oliver"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19611",
        "HTML": "https://arxiv.org/html/2506.19611",
        "PDF": "https://arxiv.org/pdf/2506.19611"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 13:30:50 GMT",
          "size": "935kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Filters of Identity: AR Beauty and the Algorithmic Politics of the Digital Body",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper discusses AR beauty filters and their impact on digital body governance, focusing on algorithmic bias and identity, with no emphasis on creativity."
      }
    },
    {
      "id": "2506.19644",
      "abstract": "Diversity in image generation is essential to ensure fair representations and support creativity in ideation. Hence, many text-to-image models have implemented diversification mechanisms. Yet, after a few iterations of generation, a lack of diversity becomes apparent, because each user has their own diversity goals (e.g., different colors, brands of cars), and there are diverse attributions to be specified. To support user-driven diversity control, we propose Varif.ai that employs text-to-image and Large Language Models to iteratively i) (re)generate a set of images, ii) verify if user-specified attributes have sufficient coverage, and iii) vary existing or new attributes. Through an elicitation study, we uncovered user needs for diversity in image generation. A pilot validation showed that Varif.ai made achieving diverse image sets easier. In a controlled evaluation with 20 participants, Varif.ai proved more effective than baseline methods across various scenarios. Thus, this supports user control of diversity in image generation for creative ideation and scalable image generation.",
      "authors": [
        "M. Michelessa",
        "J. Ng",
        "C. Hurter",
        "B. Y. Lim"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19644",
        "HTML": "https://arxiv.org/html/2506.19644",
        "PDF": "https://arxiv.org/pdf/2506.19644"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 14:08:48 GMT",
          "size": "9439kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Varif.ai to Vary and Verify User-Driven Diversity in Scalable Image Generation",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper explicitly addresses creativity in ideation, focusing on controlling diversity in image generation, which contributes directly to creative processes."
      }
    },
    {
      "id": "2506.18941",
      "abstract": "Lucrative career prospects and creative opportunities often attract students to enroll in computer science majors and pursue advanced studies in the field. Consequently, there has been a significant surge in enrollment in computer science courses, resulting in large class sizes that can range from hundreds to even thousands of students. A common challenge in such large classrooms is the lack of engagement between students and both the instructor and the learning material. However, with advancements in technology and improvements in large language models (LLMs), there is a considerable opportunity to utilize LLM-based AI models, such as conversational artificial intelligence (CAI), to enhance student engagement with learning content in large classes. To explore the potential of CAI to support engagement, especially with learning content, we designed an activity in a software Engineering course (with a large class size) where students used CAI for an in-class activity. We conducted a within-subject investigation in a large classroom at a US university where we compared student engagement during an in-class activity that used CAI tool vs. one without CAI tool. The CAI tool we used was ChatGPT due to its widespread popularity and familiarity. Our results indicate that CAI (ChatGPT) has the potential to support engagement with learning content during in-class activities, especially in large class sizes. We further discuss the implications of our findings.",
      "authors": [
        "Neha Rani",
        "Sharan Majumder",
        "Ishan Bhardwaj",
        "Pedro Guillermo Feijoo Garcia"
      ],
      "last_revised_date": "2025/06/22",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18941",
        "HTML": "https://arxiv.org/html/2506.18941",
        "PDF": "https://arxiv.org/pdf/2506.18941"
      },
      "subjects": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)",
        "Emerging Technologies (cs.ET)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Sun, 22 Jun 2025 19:30:47 GMT",
          "size": "2277kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Can AI support student engagement in classroom activities in higher education?",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "Although the paper mentions creative opportunities in education, the focus is on AI-enhanced student engagement rather than creativity."
      }
    },
    {
      "id": "2506.19079",
      "abstract": "Foundation Models (FMs) are rapidly transforming Affective Computing (AC), with Vision Language Models (VLMs) now capable of recognising emotions in zero shot settings. This paper probes a critical but underexplored question: what visual cues do these models rely on to infer affect, and are these cues psychologically grounded or superficially learnt? We benchmark varying scale VLMs on a teeth annotated subset of AffectNet dataset and find consistent performance shifts depending on the presence of visible teeth. Through structured introspection of, the best-performing model, i.e., GPT-4o, we show that facial attributes like eyebrow position drive much of its affective reasoning, revealing a high degree of internal consistency in its valence-arousal predictions. These patterns highlight the emergent nature of FMs behaviour, but also reveal risks: shortcut learning, bias, and fairness issues especially in sensitive domains like mental health and education.",
      "authors": [
        "Iosif Tsangko",
        "Andreas Triantafyllopoulos",
        "Adem Abdelmoula",
        "Adria Mallol-Ragolta",
        "Bjoern W. Schuller"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19079",
        "HTML": "https://arxiv.org/html/2506.19079",
        "PDF": "https://arxiv.org/pdf/2506.19079"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 19:56:30 GMT",
          "size": "826kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Reading Smiles: Proxy Bias in Foundation Models for Facial Emotion Recognition",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper focuses on facial emotion recognition and the biases in foundation models, with no mention of creativity or related concepts."
      }
    },
    {
      "id": "2506.19202",
      "abstract": "Roboticists often design with the assumption that assistive robots should be fully autonomous. However, it remains unclear whether users prefer highly autonomous robots, as prior work in assistive robotics suggests otherwise. High robot autonomy can reduce the user's sense of agency, which represents feeling in control of one's environment. How much control do users, in fact, want over the actions of robots used for in-home assistance? We investigate how robot autonomy levels affect users' sense of agency and the autonomy level they prefer in contexts with varying risks. Our study asked participants to rate their sense of agency as robot users across four distinct autonomy levels and ranked their robot preferences with respect to various household tasks. Our findings revealed that participants' sense of agency was primarily influenced by two factors: (1) whether the robot acts autonomously, and (2) whether a third party is involved in the robot's programming or operation. Notably, an end-user programmed robot highly preserved users' sense of agency, even though it acts autonomously. However, in high-risk settings, e.g., preparing a snack for a child with allergies, they preferred robots that prioritized their control significantly more. Additional contextual factors, such as trust in a third party operator, also shaped their preferences.",
      "authors": [
        "Claire Yang",
        "Heer Patel",
        "Max Kleiman-Weiner",
        "Maya Cakmak"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19202",
        "HTML": "https://arxiv.org/html/2506.19202",
        "PDF": "https://arxiv.org/pdf/2506.19202"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 00:06:58 GMT",
          "size": "1454kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Preserving Sense of Agency: User Preferences for Robot Autonomy and User Control across Household Tasks",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "This paper investigates user preferences in robot autonomy and control during household tasks, focusing on agency rather than creativity."
      }
    },
    {
      "id": "2506.19280",
      "abstract": "Human-Computer Interaction (HCI) has evolved significantly to incorporate emotion recognition capabilities, creating unprecedented opportunities for adaptive and personalized user experiences. This paper explores the integration of emotion detection into calendar applications, enabling user interfaces to dynamically respond to users' emotional states and stress levels, thereby enhancing both productivity and engagement. We present and evaluate two complementary approaches to emotion detection: a biometric-based method utilizing heart rate (HR) data extracted from electrocardiogram (ECG) signals processed through Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) neural networks to predict the emotional dimensions of Valence, Arousal, and Dominance; and a behavioral method analyzing computer activity through multiple machine learning models to classify emotions based on fine-grained user interactions such as mouse movements, clicks, and keystroke patterns. Our comparative analysis, from real-world datasets, reveals that while both approaches demonstrate effectiveness, the computer activity-based method delivers superior consistency and accuracy, particularly for mouse-related interactions, which achieved approximately 90\\% accuracy. Furthermore, GRU networks outperformed LSTM models in the biometric approach, with Valence prediction reaching 84.38\\% accuracy.",
      "authors": [
        "Feiting Yang",
        "Antoine Moevus",
        "Steve L\\'evesque"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19280",
        "HTML": "https://arxiv.org/html/2506.19280",
        "PDF": "https://arxiv.org/pdf/2506.19280"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 03:21:46 GMT",
          "size": "310kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Emotion Detection on User Front-Facing App Interfaces for Enhanced Schedule Optimization: A Machine Learning Approach",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper deals with emotion detection and adaptive user experiences, which may relate indirectly to creativity in terms of innovative solutions for personalized interfaces."
      }
    },
    {
      "id": "2506.19352",
      "abstract": "Ensuring persona fidelity in large language models (LLMs) is essential for maintaining coherent and engaging human-AI interactions. However, LLMs often exhibit Out-of-Character (OOC) behavior, where generated responses deviate from an assigned persona, leading to inconsistencies that affect model reliability. Existing evaluation methods typically assign single scores to entire responses, struggling to capture subtle persona misalignment, particularly in long-form text generation. To address this limitation, we propose an atomic-level evaluation framework that quantifies persona fidelity at a finer granularity. Our three key metrics measure the degree of persona alignment and consistency within and across generations. Our approach enables a more precise and realistic assessment of persona fidelity by identifying subtle deviations that real users would encounter. Through our experiments, we demonstrate that our framework effectively detects persona inconsistencies that prior methods overlook. By analyzing persona fidelity across diverse tasks and personality types, we reveal how task structure and persona desirability influence model adaptability, highlighting challenges in maintaining consistent persona expression.",
      "authors": [
        "Jisu Shin",
        "Juhyun Oh",
        "Eunsu Kim",
        "Hoyun Song",
        "Alice Oh"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19352",
        "HTML": "https://arxiv.org/html/2506.19352",
        "PDF": "https://arxiv.org/pdf/2506.19352"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 06:33:10 GMT",
          "size": "469kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The research concentrates on persona fidelity in language models, without discussing creativity."
      }
    },
    {
      "id": "2506.19415",
      "abstract": "3D Gaussian Splatting represents a breakthrough in the field of novel view synthesis. It establishes Gaussians as core rendering primitives for highly accurate real-world environment reconstruction. Recent advances have drastically increased the size of scenes that can be created. In this work, we present a method for rendering large and complex 3D Gaussian Splatting scenes using virtual memory. By leveraging well-established virtual memory and virtual texturing techniques, our approach efficiently identifies visible Gaussians and dynamically streams them to the GPU just in time for real-time rendering. Selecting only the necessary Gaussians for both storage and rendering results in reduced memory usage and effectively accelerates rendering, especially for highly complex scenes. Furthermore, we demonstrate how level of detail can be integrated into our proposed method to further enhance rendering speed for large-scale scenes. With an optimized implementation, we highlight key practical considerations and thoroughly evaluate the proposed technique and its impact on desktop and mobile devices.",
      "authors": [
        "Jonathan Haberl and Philipp Fleck and Clemens Arth"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19415",
        "HTML": "https://arxiv.org/html/2506.19415",
        "PDF": "https://arxiv.org/pdf/2506.19415"
      },
      "subjects": [
        "Graphics (cs.GR)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 08:31:33 GMT",
          "size": "6141kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Virtual Memory for 3D Gaussian Splatting",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper focuses on technical advancements in 3D rendering and virtual memory usage, with no mention or connection to creativity."
      }
    },
    {
      "id": "2506.19484",
      "abstract": "Large Language Models (LLMs) are rapidly transforming education by enabling rich conversational learning experiences. This article provides a comprehensive review of how LLM-based conversational agents are being used in higher education, with extensions to secondary and lifelong learning contexts. We synthesize existing literature on LLMs in education and theories of conversational and dialogic pedagogy - including Vygotsky's sociocultural learning (scaffolding and the Zone of Proximal Development), the Socratic method, and Laurillard's conversational framework - and examine how prompting strategies and retrieval-augmented generation (RAG) can align LLM behaviors with these pedagogical theories, and how it can support personalized, adaptive learning. We map educational theories to LLM capabilities, highlighting where LLM-driven dialogue supports established learning principles and where it challenges or falls short of traditional pedagogical assumptions. Notable gaps in applying prior theories to LLMs are identified, such as the models tendency to provide direct answers instead of fostering co-construction of knowledge, and the need to account for the constant availability and broad but non-human expertise of LLM tutors. In response, we propose practical strategies to better align LLM interactions with sound pedagogy - for example, designing prompts that encourage Socratic questioning, scaffolded guidance, and student reflection, as well as integrating retrieval mechanisms to ensure accuracy and contextual relevance. Our aim is to bridge the gap between educational theory and the emerging practice of AI-driven conversational learning, offering insights and tools for making LLM-based dialogues more educationally productive and theory-aligned.",
      "authors": [
        "Russell Beale"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19484",
        "HTML": "https://arxiv.org/html/2506.19484",
        "PDF": "https://arxiv.org/pdf/2506.19484"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 10:19:09 GMT",
          "size": "159kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper relates to educational approaches using AI, mentioning creative pedagogical strategies, but does not focus primarily on creativity."
      }
    },
    {
      "id": "2506.19757",
      "abstract": "Context: Developer experience (DX) plays a key role in developers' performance and their continued involvement in a software ecosystem (SECO) platform. While researchers and practitioners have recognized several factors affecting DX in SECO platforms, a clear roadmap of the most influential factors is still missing. This is particularly important given the direct impact on developers' interest in SECO and their ongoing engagement with the common technological platform. Goal: This work aims to identify key DX factors and understand how they influence third-party developers' decisions to adopt and keep contributing to a SECO. Methods: We conducted a systematic mapping study (SMS), analyzing 29 studies to assess the state-of-the-art of DX in SECO. Additionally, we conducted a Delphi study to evaluate the influence of 27 DX factors (identified in our SMS) from the perspective of 21 third-party developers to adopt and keep contributing to a SECO. Results: The factors that most strongly influence developers' adoption and ongoing contributions to a SECO are: financial costs for using the platform, desired technical resources for development, low barriers to entry into the applications market, and more financial gains. Conclusion: DX is essential for the success and sustainability of SECO. Our set of DX factors provides valuable insights and recommendations for researchers and practitioners to address key DX concerns from the perspective of third-party developers.",
      "authors": [
        "Rodrigo Oliveira Zacarias and L\\'eo Carvalho Ramos Antunes and M\\'arcio de Oliveira Barros and Rodrigo Pereira dos Santos and Patricia Lago"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19757",
        "HTML": "https://arxiv.org/html/2506.19757",
        "PDF": "https://arxiv.org/pdf/2506.19757"
      },
      "subjects": [
        "Software Engineering (cs.SE)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 16:17:24 GMT",
          "size": "372kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Exploring Developer Experience Factors in Software Ecosystems",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper is about developer experience in software ecosystems, discussing factors like financial costs and technical resources, without any relation to creativity."
      }
    },
    {
      "id": "2401.08405",
      "abstract": "In an era of AI's growing capabilities and influences, recent advancements are reshaping HCI and CSCW's view of AI. Playful interactions emerged as an important way for users to make sense of the ever-changing AI technologies, yet remained underexamined. We target this gap by investigating playful interactions exhibited by users of a popular AI technology, ChatGPT. Through a thematic analysis of 372 user-generated posts on the ChatGPT subreddit, we found that more than half (54\\%) of user discourse revolved around playful interactions. The analysis further allowed us to construct a preliminary framework to describe these interactions, categorizing them into six types: reflecting, jesting, imitating, challenging, tricking, and contriving; each included sub-categories. This study contributes to HCI and CSCW by identifying the diverse ways users engage in playful interactions with AI. It examines how these interactions can help users understand AI's agency, shape human-AI relationships, and provide insights for designing AI systems.",
      "authors": [
        "Mohammad Ronagh Nikghalb",
        "Jinghui Cheng"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2401.08405",
        "HTML": "https://arxiv.org/html/2401.08405",
        "PDF": "https://arxiv.org/pdf/2401.08405"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 16 Jan 2024 14:44:13 GMT",
          "size": "35kb",
          "version": "v1"
        },
        {
          "date": "Mon, 22 Jul 2024 16:44:14 GMT",
          "size": "56kb",
          "version": "v2"
        },
        {
          "date": "Tue, 15 Oct 2024 02:57:10 GMT",
          "size": "54kb",
          "version": "v3"
        },
        {
          "date": "Tue, 24 Jun 2025 16:45:18 GMT",
          "size": "98kb",
          "version": "v4"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Interrogating AI: Characterizing Emergent Playful Interactions with ChatGPT",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses playful interactions with AI, which can be a component of creativity, but creativity is not the main focus."
      },
      "tasks": []
    },
    {
      "id": "2404.15615",
      "abstract": "Emotion decoding using Electroencephalography (EEG)-based affective brain-computer interfaces (aBCIs) plays a crucial role in affective computing but is limited by challenges such as EEG's non-stationarity, individual variability, and the high cost of large labeled datasets. While deep learning methods are effective, they require extensive computational resources and large data volumes, limiting their practical application. To overcome these issues, we propose Manifold-based Domain Adaptation with Dynamic Distribution (M3D), a lightweight, non-deep transfer learning framework. M3D consists of four key modules: manifold feature transformation, dynamic distribution alignment, classifier learning, and ensemble learning. The data is mapped to an optimal Grassmann manifold space, enabling dynamic alignment of source and target domains. This alignment is designed to prioritize both marginal and conditional distributions, improving adaptation efficiency across diverse datasets. In classifier learning, the principle of structural risk minimization is applied to build robust classification models. Additionally, dynamic distribution alignment iteratively refines the classifier. The ensemble learning module aggregates classifiers from different optimization stages to leverage diversity and enhance prediction accuracy. M3D is evaluated on two EEG emotion recognition datasets using two validation protocols (cross-subject single-session and cross-subject cross-session) and a clinical EEG dataset for Major Depressive Disorder (MDD). Experimental results show that M3D outperforms traditional non-deep learning methods with a 4.47% average improvement and achieves deep learning-level performance with reduced data and computational requirements, demonstrating its potential for real-world aBCI applications.",
      "authors": [
        "Ting Luo",
        "Jing Zhang",
        "Yingwei Qiu",
        "Li Zhang",
        "Yaohua Hu",
        "Zhuliang Yu",
        "Zhen Liang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2404.15615",
        "HTML": "https://arxiv.org/html/2404.15615",
        "PDF": "https://arxiv.org/pdf/2404.15615"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 24 Apr 2024 03:08:25 GMT",
          "size": "2280kb",
          "version": "v1"
        },
        {
          "date": "Sat, 25 Jan 2025 09:51:27 GMT",
          "size": "171kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 08:07:48 GMT",
          "size": "3152kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "M3D: Manifold-based Domain Adaptation with Dynamic Distribution for Non-Deep Transfer Learning in Cross-subject and Cross-session EEG-based Emotion Recognition",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "This paper focuses on EEG-based emotion recognition and domain adaptation, with no mention of creativity."
      },
      "tasks": [
        "Deep Learning",
        "Domain Adaptation",
        "EEG",
        "EEG Emotion Recognition",
        "Emotion Recognition",
        "Ensemble Learning",
        "Robust classification",
        "Transfer Learning"
      ]
    },
    {
      "id": "2501.02084",
      "abstract": "Spatial scheduling of electrode activation (\"rastering\") is essential for safely operating high-density retinal implants, yet its perceptual consequences remain poorly understood. This study systematically evaluates the impact of raster patterns, or spatial arrangements of sequential electrode activation, on performance and perceived difficulty in simulated prosthetic vision (SPV). By addressing this gap, we aimed to identify patterns that optimize functional vision in retinal implants. Sighted participants completed letter recognition and motion discrimination tasks under four raster patterns (horizontal, vertical, checkerboard, and random) using an immersive SPV system. The simulations emulated epiretinal implant perception and employed psychophysically validated models of electrode activation, phosphene appearance, nonlinear spatial summation, and temporal dynamics, ensuring realistic representation of prosthetic vision. Performance accuracy and self-reported difficulty were analyzed to assess the effects of raster patterning. The checkerboard pattern consistently outperformed other raster patterns, yielding significantly higher accuracy and lower difficulty ratings across both tasks. The horizontal and vertical patterns introduced biases aligned with apparent motion artifacts, while the checkerboard minimized such effects. Random patterns resulted in the lowest performance, underscoring the importance of structured activation. Notably, checkerboard matched performance in the \"No Raster\" condition, despite conforming to groupwise safety constraints. This is the first quantitative, task-based evaluation of raster patterns in SPV. Checkerboard-style scheduling enhances perceptual clarity without increasing computational load, offering a low-overhead, clinically relevant strategy for improving usability in next-generation retinal prostheses.",
      "authors": [
        "Justin M. Kasowski",
        "Apurv Varshney",
        "Roksana Sadeghi",
        "Michael Beyeler"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2501.02084",
        "HTML": "https://arxiv.org/html/2501.02084",
        "PDF": "https://arxiv.org/pdf/2501.02084"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Fri, 03 Jan 2025 20:03:17 GMT",
          "size": "13400kb",
          "version": "v1"
        },
        {
          "date": "Sun, 18 May 2025 02:13:27 GMT",
          "size": "8841kb",
          "version": "v2"
        },
        {
          "date": "Mon, 23 Jun 2025 22:43:42 GMT",
          "size": "8787kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Simulated prosthetic vision confirms checkerboard as an effective raster pattern for epiretinal implants",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper focuses on optimizing raster patterns for retinal implants and does not relate to creativity."
      }
    },
    {
      "id": "2501.15276",
      "abstract": "Artificial intelligence is reshaping creative domains, yet its co-creative processes, especially in group settings with novice users, remain under explored. To bridge this gap, we conducted a case study in a college-level course where nine undergraduate students were tasked with creating three original music tracks using AI tools over 10 weeks. The study spanned the entire creative journey from ideation to releasing these songs on Spotify. Participants leveraged AI for music and lyric production, cover art, and distribution. Our findings highlight how AI transforms creative workflows: accelerating ideation but compressing the traditional preparation stage, and requiring novices to navigate a challenging idea selection and validation phase. We also identified a new \"collaging and refinement\" stage, where participants creatively combined diverse AI-generated outputs into cohesive works. Furthermore, AI influenced group social dynamics and role division among human creators. Based on these insights, we propose the Human-AI Co-Creation Stage Model and the Human-AI Agency Model, offering new perspectives on collaborative co-creation with AI.",
      "authors": [
        "Yue Fu",
        "Michele Newman",
        "Lewis Going",
        "Qiuzi Feng",
        "Jin Ha Lee"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2501.15276",
        "HTML": "https://arxiv.org/html/2501.15276",
        "PDF": "https://arxiv.org/pdf/2501.15276"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Sat, 25 Jan 2025 17:00:17 GMT",
          "size": "10798kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 09:33:17 GMT",
          "size": "4953kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Exploring the Collaborative Co-Creation Process with AI: A Case Study in Novice Music Production",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper explicitly examines the co-creation process with AI in music production, making creativity central to its study."
      },
      "tasks": [
        "Navigate"
      ]
    },
    {
      "id": "2503.16484",
      "abstract": "Episodic Future Thinking (EFT) involves vividly imagining personal future events and experiences in detail. It has shown promise as an intervention to reduce delay discounting-the tendency to devalue delayed rewards in favor of immediate gratification- and to promote behavior change in a range of maladaptive health behaviors. We present EFTeacher, an AI chatbot powered by the GPT-4-Turbo large language model, designed to generate EFT cues for users with lifestyle-related conditions. To evaluate the feasibility and usability of EFTeacher, we conducted a mixed-methods study that included usability assessments, user evaluations based on content characteristics questionnaires, and semi-structured interviews. Qualitative findings indicate that participants perceived EFTeacher as communicative and supportive through an engaging dialogue. The chatbot facilitated imaginative thinking and reflection on future goals. Participants appreciated its adaptability and personalization features, though some noted challenges such as repetitive dialogue and verbose responses. Our findings underscore the potential of large language model-based chatbots in EFT interventions targeting maladaptive health behaviors.",
      "authors": [
        "Sareh Ahmadi",
        "Michelle Rockwell",
        "Megan Stuart",
        "Nicki Rohani",
        "Allison Tegge",
        "Xuan Wang",
        "Jeffrey Stein",
        "Edward A. Fox"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.16484",
        "HTML": "https://arxiv.org/html/2503.16484",
        "PDF": "https://arxiv.org/pdf/2503.16484"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Sat, 08 Mar 2025 01:10:04 GMT",
          "size": "487kb",
          "version": "v1"
        },
        {
          "date": "Mon, 23 Jun 2025 18:56:13 GMT",
          "size": "420kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "AI-Facilitated Episodic Future Thinking For Adults with Obesity",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the paper involves imaginative thinking, it focuses on using an AI chatbot for health interventions rather than creativity."
      },
      "tasks": [
        "Chatbot",
        "Language Modeling",
        "Language Modelling",
        "Large Language Model"
      ]
    },
    {
      "id": "2506.08892",
      "abstract": "The human-robot interaction (HRI) field has recognized the importance of enabling robots to interact with teams. Human teams rely on effective communication for successful collaboration in time-sensitive environments. Robots can play a role in enhancing team coordination through real-time assistance. Despite significant progress in human-robot teaming research, there remains an essential gap in how robots can effectively communicate with action teams using multimodal interaction cues in time-sensitive environments. This study addresses this knowledge gap in an experimental in-lab study to investigate how multimodal robot communication in action teams affects workload and human perception of robots. We explore team collaboration in a medical training scenario where a robotic crash cart (RCC) provides verbal and non-verbal cues to help users remember to perform iterative tasks and search for supplies. Our findings show that verbal cues for object search tasks and visual cues for task reminders reduce team workload and increase perceived ease of use and perceived usefulness more effectively than a robot with no feedback. Our work contributes to multimodal interaction research in the HRI field, highlighting the need for more human-robot teaming research to understand best practices for integrating collaborative robots in time-sensitive environments such as in hospitals, search and rescue, and manufacturing applications.",
      "authors": [
        "Tauhid Tanjim",
        "Jonathan St. George",
        "Kevin Ching",
        "and Angelique Taylor"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.08892",
        "HTML": "https://arxiv.org/html/2506.08892",
        "PDF": "https://arxiv.org/pdf/2506.08892"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 10 Jun 2025 15:19:26 GMT",
          "size": "12607kb",
          "version": "v1"
        },
        {
          "date": "Thu, 12 Jun 2025 02:41:38 GMT",
          "size": "4517kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 07:33:18 GMT",
          "size": "4493kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Help or Hindrance: Understanding the Impact of Robot Communication in Action Teams",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper focuses on multimodal robot communication in teams, particularly in medical settings. It does not mention or relate to creativity."
      }
    },
    {
      "id": "2506.14677",
      "abstract": "Existing end-to-end sign-language animation systems suffer from low naturalness, limited facial/body expressivity, and no user control. We propose a human-centered, real-time speech-to-sign animation framework that integrates (1) a streaming Conformer encoder with an autoregressive Transformer-MDN decoder for synchronized upper-body and facial motion generation, (2) a transparent, editable JSON intermediate representation empowering deaf users and experts to inspect and modify each sign segment, and (3) a human-in-the-loop optimization loop that refines the model based on user edits and ratings. Deployed on Unity3D, our system achieves a 13 ms average frame-inference time and a 103 ms end-to-end latency on an RTX 4070. Our key contributions include the design of a JSON-centric editing mechanism for fine-grained sign-level personalization and the first application of an MDN-based feedback loop for continuous model adaptation. This combination establishes a generalizable, explainable AI paradigm for user-adaptive, low-latency multimodal systems. In studies with 20 deaf signers and 5 professional interpreters, we observe a +13 point SUS improvement, 6.7 point reduction in cognitive load, and significant gains in naturalness and trust (p $<$ .001) over baselines. This work establishes a scalable, explainable AI paradigm for accessible sign-language technologies.",
      "authors": [
        "Yingchao Li"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.14677",
        "HTML": "https://arxiv.org/html/2506.14677",
        "PDF": "https://arxiv.org/pdf/2506.14677"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 17 Jun 2025 16:08:48 GMT",
          "size": "1230kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 13:11:34 GMT",
          "size": "909kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Human-Centered Editable Speech-to-Sign-Language Generation via Streaming Conformer-Transformer and Resampling Hook",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper presents a sign-language generation system driven by user feedback and control, focusing on accessibility and user-centered design rather than creativity."
      },
      "tasks": [
        "Decoder",
        "Motion Generation"
      ]
    },
    {
      "id": "2506.15525",
      "abstract": "As generative AI (GenAI) emerges as a transformative force, clear understanding of high school students' perspectives is essential for GenAI's meaningful integration in high school environments. In this work, we draw insights from a participatory design workshop where we engaged 17 high school students -- a group rarely involved in prior research in this area -- through the design of novel GenAI tools and school policies addressing their key concerns. Students identified challenges and developed solutions outlining their ideal features in GenAI tools, appropriate school use, and regulations. These centered around the problem spaces of combating bias & misinformation, tackling crime & plagiarism, preventing over-reliance on AI, and handling false accusations of academic dishonesty. Building on our participants' underrepresented perspectives, we propose new guidelines targeted at educational technology designers for development of GenAI technologies in high schools. We also argue for further incorporation of student voices in development of AI policies in their schools.",
      "authors": [
        "Isabella Pu",
        "Prerna Ravi",
        "Linh Dieu Dinh",
        "Chelsea Joe",
        "Caitlin Ogoe",
        "Zixuan Li",
        "Cynthia Breazeal",
        "Anastasia K. Ostrowski"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.15525",
        "HTML": "https://arxiv.org/html/2506.15525",
        "PDF": "https://arxiv.org/pdf/2506.15525"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 18 Jun 2025 14:58:50 GMT",
          "size": "19724kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 12:16:23 GMT",
          "size": "19724kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "\"How can we learn and use AI at the same time?\": Participatory Design of GenAI with High School Students",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the paper involves participatory design and generative AI, which can be related to creativity, the primary focus is on GenAI tool integration and policy development in education, not directly on creativity."
      }
    },
    {
      "id": "2506.18269",
      "abstract": "This study introduces Co-Persona, a methodological framework bridging large-scale social media analysis with authentic user understanding through systematic integration of Large Language Models and expert validation. Through a case study of B.Co, a Chinese manufacturer, we investigated Co-Persona application in bedside lamp development. Our methodology analyzed over 38 million posts from Xiao Hongshu, employing multi-stage data processing combining advanced NLP with expert validation. Analysis revealed five user personas derived from bedtime behaviors: Health Aficionados, Night Owls, Interior Decorators, Child-care Workers, and Workaholics-each showing unique pre-sleep activities and product preferences. Findings demonstrate Co-Persona enhances manufacturers' ability to process large datasets while maintaining user understanding. The methodology provides structured approaches for targeted marketing and product strategies. Research contributes to theoretical understanding of data-driven persona development and practical applications in consumer-driven innovation. Code and data available at https://github.com/INFPa/LLMwithPersona.",
      "authors": [
        "Min Yin and Haoyu Liu and Boyi Lian and Chunlei Chai"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18269",
        "HTML": "https://arxiv.org/html/2506.18269",
        "PDF": "https://arxiv.org/pdf/2506.18269"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 03:54:30 GMT",
          "size": "2629kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 09:12:31 GMT",
          "size": "1173kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Co-persona: Leveraging LLMs and Expert Collaboration to Understand User Personas through Social Media Data Analysis",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper involves user personas and product innovation, which might indirectly relate to creative processes in marketing and development."
      }
    },
    {
      "id": "2407.02157",
      "abstract": "Dynamic Facial Expression Recognition (DFER) is crucial for understanding human behavior. However, current methods exhibit limited performance mainly due to the scarcity of high-quality data, the insufficient utilization of facial dynamics, and the ambiguity of expression semantics, etc. To this end, we propose a novel framework, named Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs (FineCLIPER), incorporating the following novel designs: 1) To better distinguish between similar facial expressions, we extend the class labels to textual descriptions from both positive and negative aspects, and obtain supervision by calculating the cross-modal similarity based on the CLIP model; 2) Our FineCLIPER adopts a hierarchical manner to effectively mine useful cues from DFE videos. Specifically, besides directly embedding video frames as input (low semantic level), we propose to extract the face segmentation masks and landmarks based on each frame (middle semantic level) and utilize the Multi-modal Large Language Model (MLLM) to further generate detailed descriptions of facial changes across frames with designed prompts (high semantic level). Additionally, we also adopt Parameter-Efficient Fine-Tuning (PEFT) to enable efficient adaptation of large pre-trained models (i.e., CLIP) for this task. Our FineCLIPER achieves SOTA performance on the DFEW, FERV39k, and MAFW datasets in both supervised and zero-shot settings with few tunable parameters. Project Page: https://haroldchen19.github.io/FineCLIPER-Page/",
      "authors": [
        "Haodong Chen",
        "Haojian Huang",
        "Junhao Dong",
        "Mingzhe Zheng",
        "Dian Shao"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2407.02157",
        "HTML": "https://arxiv.org/html/2407.02157",
        "PDF": "https://arxiv.org/pdf/2407.02157"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 02 Jul 2024 10:55:43 GMT",
          "size": "3607kb",
          "version": "v1"
        },
        {
          "date": "Tue, 23 Jul 2024 10:08:52 GMT",
          "size": "4017kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 07:42:09 GMT",
          "size": "3328kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper is centered on facial expression recognition and does not address creativity."
      },
      "tasks": [
        "Dynamic Facial Expression Recognition",
        "Facial Expression Recognition",
        "Language Modelling",
        "Large Language Model",
        "parameter-efficient fine-tuning"
      ]
    },
    {
      "id": "2410.23478",
      "abstract": "Recent years in NLP have seen the continued development of domain-specific information extraction tools for scientific documents, alongside the release of increasingly multimodal pretrained transformer models. While the opportunity for scientists outside of NLP to evaluate and apply such systems to their own domains has never been clearer, these models are difficult to compare: they accept different input formats, are often black-box and give little insight into processing failures, and rarely handle PDF documents, the most common format of scientific publication. In this work, we present Collage, a tool designed for rapid prototyping, visualization, and evaluation of different information extraction models on scientific PDFs. Collage allows the use and evaluation of any HuggingFace token classifier, several LLMs, and multiple other task-specific models out of the box, and provides extensible software interfaces to accelerate experimentation with new models. Further, we enable both developers and users of NLP-based tools to inspect, debug, and better understand modeling pipelines by providing granular views of intermediate states of processing. We demonstrate our system in the context of information extraction to assist with literature review in materials science.",
      "authors": [
        "Sireesh Gururaja",
        "Yueheng Zhang",
        "Guannan Tang",
        "Tianhao Zhang",
        "Kevin Murphy",
        "Yu-Tsen Yi",
        "Junwon Seo",
        "Anthony Rollett",
        "Emma Strubell"
      ],
      "last_revised_date": "2025/06/22",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2410.23478",
        "HTML": "https://arxiv.org/html/2410.23478",
        "PDF": "https://arxiv.org/pdf/2410.23478"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 30 Oct 2024 22:00:34 GMT",
          "size": "9693kb",
          "version": "v1"
        },
        {
          "date": "Sun, 22 Jun 2025 18:50:21 GMT",
          "size": "17414kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Collage: Decomposable Rapid Prototyping for Information Extraction on Scientific PDFs",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The focus is on information extraction from scientific PDFs, without a connection to creativity."
      },
      "tasks": [],
      "repo_urls": [
        "https://github.com/gsireesh/ht-max"
      ]
    },
    {
      "id": "2412.00814",
      "abstract": "We introduce VR-Doh, an open-source, hands-on 3D modeling system that enables intuitive creation and manipulation of elastoplastic objects in Virtual Reality (VR). By customizing the Material Point Method (MPM) for real-time simulation of hand-induced large deformations and enhancing 3D Gaussian Splatting for seamless rendering, VR-Doh provides an interactive and immersive 3D modeling experience. Users can naturally sculpt, deform, and edit objects through both contact- and gesture-based hand-object interactions. To achieve real-time performance, our system incorporates localized simulation techniques, particle-level collision handling, and the decoupling of physical and appearance representations, ensuring smooth and responsive interactions. VR-Doh supports both object creation and editing, enabling diverse modeling tasks such as designing food items, characters, and interlocking structures, all resulting in simulation-ready assets. User studies with both novice and experienced participants highlight the system's intuitive design, immersive feedback, and creative potential. Compared to existing geometric modeling tools, VR-Doh offers enhanced accessibility and natural interaction, making it a powerful tool for creative exploration in VR.",
      "authors": [
        "Zhaofeng Luo",
        "Zhitong Cui",
        "Shijian Luo",
        "Mengyu Chu",
        "Minchen Li"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2412.00814",
        "HTML": "https://arxiv.org/html/2412.00814",
        "PDF": "https://arxiv.org/pdf/2412.00814"
      },
      "subjects": [
        "Graphics (cs.GR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Sun, 01 Dec 2024 14:04:59 GMT",
          "size": "25711kb",
          "version": "v1"
        },
        {
          "date": "Sun, 26 Jan 2025 12:19:12 GMT",
          "size": "18720kb",
          "version": "v2"
        },
        {
          "date": "Tue, 06 May 2025 16:16:01 GMT",
          "size": "5831kb",
          "version": "v3"
        },
        {
          "date": "Wed, 07 May 2025 03:24:16 GMT",
          "size": "5831kb",
          "version": "v4"
        },
        {
          "date": "Tue, 24 Jun 2025 16:24:09 GMT",
          "size": "4718kb",
          "version": "v5"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "VR-Doh: Hands-on 3D Modeling in Virtual Reality",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "VR-Doh is directly related to creativity, as it involves 3D modeling and sculpting in VR, allowing for creative expression."
      }
    },
    {
      "id": "2506.08890",
      "abstract": "Healthcare workers (HCWs) encounter challenges in hospitals, such as retrieving medical supplies quickly from crash carts, which could potentially result in medical errors and delays in patient care. Robotic crash carts (RCCs) have shown promise in assisting healthcare teams during medical tasks through guided object searches and task reminders. Limited exploration has been done to determine what communication modalities are most effective and least disruptive to patient care in real-world settings. To address this gap, we conducted a between-subjects experiment comparing the RCC's verbal and non-verbal communication of object search with a standard crash cart in resuscitation scenarios to understand the impact of robot communication on workload and attitudes toward using robots in the workplace. Our findings indicate that verbal communication significantly reduced mental demand and effort compared to visual cues and with a traditional crash cart. Although frustration levels were slightly higher during collaborations with the robot compared to a traditional cart, these research insights provide valuable implications for human-robot teamwork in high-stakes environments.",
      "authors": [
        "Tauhid Tanjim",
        "Promise Ekpo",
        "Huajie Cao",
        "Jonathan St. George",
        "Kevin Ching",
        "Hee Rin Lee",
        "and Angelique Taylor"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.08890",
        "HTML": "https://arxiv.org/html/2506.08890",
        "PDF": "https://arxiv.org/pdf/2506.08890"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 10 Jun 2025 15:17:33 GMT",
          "size": "15239kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 07:24:29 GMT",
          "size": "4171kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Human-Robot Teaming Field Deployments: A Comparison Between Verbal and Non-verbal Communication",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper is about communication modalities for robot-assisted tasks in healthcare, unrelated to creativity."
      }
    },
    {
      "id": "2506.09160",
      "abstract": "As AI chatbots become increasingly integrated in education, students are turning to these systems for guidance, feedback, and information. However, the anthropomorphic characteristics of these chatbots create ambiguity regarding whether students develop trust toward them as they would a human peer or instructor, based in interpersonal trust, or as they would any other piece of technology, based in technology trust. This ambiguity presents theoretical challenges, as interpersonal trust models may inappropriately ascribe human intentionality and morality to AI, while technology trust models were developed for non-social technologies, leaving their applicability to anthropomorphic systems unclear. To address this gap, we investigate how human-like and system-like trusting beliefs comparatively influence students' perceived enjoyment, trusting intention, behavioral intention to use, and perceived usefulness of an AI chatbot - factors associated with students' engagement and learning outcomes. Through partial least squares structural equation modeling, we found that human-like and system-like trust significantly influenced student perceptions, with varied effects. Human-like trust more strongly predicted trusting intention, while system-like trust better predicted behavioral intention and perceived usefulness. Both had similar effects on perceived enjoyment. Given the partial explanatory power of each type of trust, we propose that students develop a distinct form of trust with AI chatbots (human-AI trust) that differs from human-human and human-technology models of trust. Our findings highlight the need for new theoretical frameworks specific to human-AI trust and offer practical insights for fostering appropriately calibrated trust, which is critical for the effective adoption and pedagogical impact of AI in education.",
      "authors": [
        "Griffin Pitts",
        "Sanaz Motamedi"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.09160",
        "HTML": "https://arxiv.org/html/2506.09160",
        "PDF": "https://arxiv.org/pdf/2506.09160"
      },
      "subjects": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)",
        "Emerging Technologies (cs.ET)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 10 Jun 2025 18:15:40 GMT",
          "size": "1011kb",
          "version": "v1"
        },
        {
          "date": "Thu, 12 Jun 2025 07:06:57 GMT",
          "size": "1013kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 05:15:49 GMT",
          "size": "1032kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Understanding Human-AI Trust in Education",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper investigates human-AI trust in educational settings, focusing on trust dynamics rather than creativity as a primary topic."
      },
      "tasks": [
        "Chatbot"
      ]
    },
    {
      "id": "2506.17364",
      "abstract": "This work investigates the use of multimodal biometrics to detect distractions caused by smartphone use during tasks that require sustained attention, with a focus on computer-based online learning. Although the methods are applicable to various domains, such as autonomous driving, we concentrate on the challenges learners face in maintaining engagement amid internal (e.g., motivation), system-related (e.g., course design) and contextual (e.g., smartphone use) factors. Traditional learning platforms often lack detailed behavioral data, but Multimodal Learning Analytics (MMLA) and biosensors provide new insights into learner attention. We propose an AI-based approach that leverages physiological signals and head pose data to detect phone use. Our results show that single biometric signals, such as brain waves or heart rate, offer limited accuracy, while head pose alone achieves 87%. A multimodal model combining all signals reaches 91% accuracy, highlighting the benefits of integration. We conclude by discussing the implications and limitations of deploying these models for real-time support in online learning environments.",
      "authors": [
        "Alvaro Becerra",
        "Roberto Daza",
        "Ruth Cobos",
        "Aythami Morales",
        "Mutlu Cukurova",
        "Julian Fierrez"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.17364",
        "HTML": "https://arxiv.org/html/2506.17364",
        "PDF": "https://arxiv.org/pdf/2506.17364"
      },
      "subjects": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Fri, 20 Jun 2025 11:37:19 GMT",
          "size": "801kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 13:38:12 GMT",
          "size": "802kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "AI-based Multimodal Biometrics for Detecting Smartphone Distractions: Application to Online Learning",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The study is about detecting distractions via multimodal biometrics in online learning. It does not address creativity or creative processes."
      }
    },
    {
      "id": "2506.17570",
      "abstract": "Virtual reality (VR) has recently proliferated significantly, consisting of headsets or head-mounted displays (HMDs) and hand controllers for an embodied and immersive experience. The VR device is usually embedded with different kinds of IoT sensors, such as cameras, microphones, communication sensors, etc. However, VR security has not been scrutinized from a physical hardware point of view, especially electromagnetic emanations (EM) that are automatically and unintentionally emitted from the VR headset. This paper presents VReaves, a system that can eavesdrop on the electromagnetic emanation side channel of a VR headset for VR app identification and activity recognition. To do so, we first characterize the electromagnetic emanations from the embedded IoT sensors (e.g., cameras and microphones) in the VR headset through a signal processing pipeline and further propose machine learning models to identify the VR app and recognize the VR app activities. Our experimental evaluation with commercial off-the-shelf VR devices demonstrates the efficiency of VR app identification and activity recognition via electromagnetic emanation side channel.",
      "authors": [
        "Wei Sun",
        "Minghong Fang",
        "Mengyuan Li"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.17570",
        "HTML": "https://arxiv.org/html/2506.17570",
        "PDF": "https://arxiv.org/pdf/2506.17570"
      },
      "subjects": [
        "Networking and Internet Architecture (cs.NI)",
        "Cryptography and Security (cs.CR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Sat, 21 Jun 2025 03:53:24 GMT",
          "size": "7412kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 03:33:43 GMT",
          "size": "7412kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "VReaves: Eavesdropping on Virtual Reality App Identity and Activity via Electromagnetic Side Channels",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper focuses on VR app security and electromagnetic side channels, with no mention of creativity."
      }
    }
  ],
  "subjects": [
    "Computer Vision and Pattern Recognition (cs.CV)",
    "Networking and Internet Architecture (cs.NI)",
    "Computation and Language (cs.CL)",
    "Cryptography and Security (cs.CR)",
    "Robotics (cs.RO)",
    "Artificial Intelligence (cs.AI)",
    "Emerging Technologies (cs.ET)",
    "Computers and Society (cs.CY)",
    "Software Engineering (cs.SE)",
    "Human-Computer Interaction (cs.HC)",
    "Graphics (cs.GR)",
    "Machine Learning (cs.LG)"
  ],
  "prompt": {
    "creativity": "\nYou are an expert in information retrieval. I will provide you with a list of research papers from arXiv, specifically in the *cs.HC* (Human-Computer Interaction) category.\n\nYour task is to analyze each paper and determine its relevance to the topic of **Creativity**.\n\nClassify each paper into one of the following relevance levels:\n\n* `\"strong\"`: The paper is explicitly focused on creativity.\n* `\"weak\"`: The paper mentions or touches on creativity, but it is not the main focus.\n* `\"none\"`: The paper is not related to creativity.\n\nReturn your results in the following JSON format:\n\n```json\n{\n  \"result\": [\n    {\n      \"id\": \"paper id\",\n      \"level\": \"strong | weak | none\",\n      \"reason\": \"Brief justification based on the paper content\"\n    },\n    {\n      \"id\": \"paper id\",\n      \"level\": \"strong | weak | none\",\n      \"reason\": \"Brief justification based on the paper content\"\n    }\n  ]\n}\n```\n\nBe concise but specific in your reasoning, referencing key terms or sections when applicable.\n"
  },
  "description": "Data source: https://arxiv.org/list/cs/new"
}