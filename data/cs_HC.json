{
  "data": [
    {
      "id": "2506.19995",
      "abstract": "Augmentative and alternative communication (AAC) is a field of research and practice that works with people who have a communication disability. One form AAC can take is a high-tech tool, such as a software-based communication system. Like all user interfaces, these systems must be designed and it is critical to include AAC users in the design process for their systems. A participatory design approach can include AAC users in the design process, but modifications may be necessary to make these methods more accessible. We present a two-part design process we are investigating for improving the participatory design for high-tech AAC systems. We discuss our plans to refine the accessibility of this process based on participant feedback.",
      "authors": [
        "Blade Frisch",
        "Keith Vertanen"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19995",
        "HTML": "https://arxiv.org/html/2506.19995",
        "PDF": "https://arxiv.org/pdf/2506.19995"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 20:28:27 GMT",
          "size": "148kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Refining Participatory Design for AAC Users",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the paper involves participatory design, which can foster creativity in design processes, creativity is not the central theme."
      }
    },
    {
      "id": "2506.20055",
      "abstract": "Online(-only) friendships have become increasingly common in daily lives post-COVID despite debates around their mental health benefits and equivalence to ''real'' relationships. Previous research has reflected a need to understand how online friends engage beyond individual platforms, and the lack of platform-agnostic inquiry limits our ability to fully understand the dynamics of online friendship. We employed an activity-grounded analysis of 25 interviews on lived experiences of close online friendship spanning multiple years. Our findings present unique challenges and strategies in online friendships, such as stigma from real-life circles, an ambivalent relationship with online communities, and counter-theoretical reappropriations of communication technology. This study contributes to HCI research in online communities and social interface design by refocusing prior impressions of strong vs. weak-ties in online social spaces and foregrounding time-stable interactions in design for relationship maintenance through technology. Our work also promotes critical reflection on biased perspectives towards technology-mediated practices and consideration of online friends as an invisible marginalized community.",
      "authors": [
        "Seraphina Yong",
        "Ashlee Milton",
        "Evan Suma Rosenberg",
        "Stevie Chancellor",
        "Svetlana Yarosh"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20055",
        "HTML": "https://arxiv.org/html/2506.20055",
        "PDF": "https://arxiv.org/pdf/2506.20055"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 23:30:49 GMT",
          "size": "2254kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "\"I'm Petting the Laptop, Which Has You Inside It\": Reflecting on Lived Experiences of Online Friendship",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper focuses on online friendship dynamics and technology-mediated relationship maintenance, not creativity."
      }
    },
    {
      "id": "2506.20062",
      "abstract": "AI-powered code assistants are widely used to generate code completions, significantly boosting developer productivity. However, these tools typically present suggestions without explaining their rationale, leaving their decision-making process inscrutable. This opacity hinders developers' ability to critically evaluate the output, form accurate mental models, and build calibrated trust in the system. To address this, we introduce CopilotLens, a novel interactive framework that reframes code completion from a simple suggestion into a transparent, explainable event. CopilotLens operates as an explanation layer that reveals the AI agent's \"thought process\" through a dynamic two-level interface, surfacing everything from its reconstructed high-level plans to the specific codebase context influencing the code. This paper presents the design and rationale of CopilotLens, offering a concrete framework for building future agentic code assistants that prioritize clarity of reasoning over speed of suggestion, thereby fostering deeper comprehension and more robust human-AI collaboration.",
      "authors": [
        "Runlong Ye",
        "Zeling Zhang",
        "Boushra Almazroua",
        "Michael Liut"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20062",
        "HTML": "https://arxiv.org/html/2506.20062",
        "PDF": "https://arxiv.org/pdf/2506.20062"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 23:50:03 GMT",
          "size": "3294kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Beyond Autocomplete: Designing CopilotLens Towards Transparent and Explainable AI Coding Agents",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses a tool aiding in coding with AI by enhancing transparency and explanation, which can potentially foster creative problem-solving but does not focus on creativity directly."
      }
    },
    {
      "id": "2506.20091",
      "abstract": "Recent advances in multi-agentic systems (e.g. AutoGen, OpenAI Swarm) allow users to interact with a group of specialised AI agents rather than a single general-purpose agent. Despite the promise of this new paradigm, the HCI community has yet to fully examine the opportunities, risks, and user-centred challenges it introduces. We contribute to research on multi-agentic systems by exploring their architectures and key features through a human-centred lens. While literature and use cases remain limited, we build on existing tools and frameworks available to developers to identify a set of overarching challenges, e.g. orchestration and conflict resolution, that can guide future research in HCI. We illustrate these challenges through examples, offer potential design considerations, and provide research opportunities to spark interdisciplinary conversation. Our work lays the groundwork for future exploration and offers a research agenda focused on user-centred design in multi-agentic systems.",
      "authors": [
        "Sarah Sch\\\"ombs",
        "Yan Zhang",
        "Jorge Goncalves",
        "Wafa Johal"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20091",
        "HTML": "https://arxiv.org/html/2506.20091",
        "PDF": "https://arxiv.org/pdf/2506.20091"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 02:28:39 GMT",
          "size": "610kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "From Conversation to Orchestration: HCI Challenges and Opportunities in Interactive Multi-Agentic Systems",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "Focuses on HCI challenges and opportunities in multi-agentic systems, not directly related to creativity."
      }
    },
    {
      "id": "2506.20156",
      "abstract": "The core challenge in learning has shifted from knowledge acquisition to effective Self-Regulated Learning (SRL): planning, monitoring, and reflecting on one's learning. Existing digital tools, however, inadequately support metacognitive reflection. Spaced Repetition Systems (SRS) use de-contextualized review, overlooking the role of context, while Personal Knowledge Management (PKM) tools require high manual maintenance.\n  To address these challenges, this paper introduces \"Insight Recall,\" a novel paradigm that conceptualizes the context-triggered retrieval of personal past insights as a metacognitive scaffold to promote SRL. We formalize this paradigm using the Just-in-Time Adaptive Intervention (JITAI) framework and implement a prototype system, Irec, to demonstrate its feasibility. At its core, Irec uses a dynamic knowledge graph of the user's learning history. When a user faces a new problem, a hybrid retrieval engine recalls relevant personal \"insights.\" Subsequently, a large language model (LLM) performs a deep similarity assessment to filter and present the most relevant scaffold in a just-in-time manner. To reduce cognitive load, Irec features a human-in-the-loop pipeline for LLM-based knowledge graph construction. We also propose an optional \"Guided Inquiry\" module, where users can engage in a Socratic dialogue with an expert LLM, using the current problem and recalled insights as context. The contribution of this paper is a solid theoretical framework and a usable system platform for designing next-generation intelligent learning systems that enhance metacognition and self-regulation.",
      "authors": [
        "Xuefei Hou",
        "Xizhao Tan"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20156",
        "HTML": "https://arxiv.org/html/2506.20156",
        "PDF": "https://arxiv.org/pdf/2506.20156"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)",
        "Information Retrieval (cs.IR)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 06:23:39 GMT",
          "size": "23kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "Irec: A Metacognitive Scaffolding for Self-Regulated Learning through Just-in-Time Insight Recall: A Conceptual Framework and System Prototype",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "This paper discusses self-regulated learning and metacognitive scaffolding, which may indirectly support creative learning strategies but does not focus on creativity as its main theme."
      }
    },
    {
      "id": "2506.20207",
      "abstract": "Mobile Augmented Reality (AR) applications leverage various sensors to provide immersive user experiences. However, their reliance on diverse data sources introduces significant privacy challenges. This paper investigates user perceptions and understanding of privacy permissions in mobile AR apps through an analysis of existing applications and an online survey of 120 participants. Findings reveal common misconceptions, including confusion about how permissions relate to specific AR functionalities (e.g., location and measurement of physical distances), and misinterpretations of permission labels (e.g., conflating camera and gallery access). We identify a set of actionable implications for designing more usable and transparent privacy mechanisms tailored to mobile AR technologies, including contextual explanations, modular permission requests, and clearer permission labels. These findings offer actionable guidance for developers, researchers, and policymakers working to enhance privacy frameworks in mobile AR.",
      "authors": [
        "Viktorija Paneva",
        "Verena Winterhalter",
        "Franziska Augustinowski",
        "Florian Alt"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20207",
        "HTML": "https://arxiv.org/html/2506.20207",
        "PDF": "https://arxiv.org/pdf/2506.20207"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 07:52:12 GMT",
          "size": "1060kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "User Understanding of Privacy Permissions in Mobile Augmented Reality: Perceptions and Misconceptions",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The study focuses on privacy perceptions in mobile AR apps rather than creativity."
      }
    },
    {
      "id": "2506.20291",
      "abstract": "Conversational Recommender Systems (CRSs) have garnered attention as a novel approach to delivering personalized recommendations through multi-turn dialogues. This review developed a taxonomy framework to systematically categorize relevant publications into four groups: dataset construction, algorithm design, system evaluation, and empirical studies, providing a comprehensive analysis of simulation methods in CRSs research. Our analysis reveals that simulation methods play a key role in tackling CRSs' main challenges. For example, LLM-based simulation methods have been used to create conversational recommendation data, enhance CRSs algorithms, and evaluate CRSs. Despite several challenges, such as dataset bias, the limited output flexibility of LLM-based simulations, and the gap between text semantic space and behavioral semantics, persist due to the complexity in Human-Computer Interaction (HCI) of CRSs, simulation methods hold significant potential for advancing CRS research. This review offers a thorough summary of the current research landscape in this domain and identifies promising directions for future inquiry.",
      "authors": [
        "Haoran Zhang",
        "Xin Zhao",
        "Jinze Chen",
        "Junpeng Guo"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20291",
        "HTML": "https://arxiv.org/html/2506.20291",
        "PDF": "https://arxiv.org/pdf/2506.20291"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Information Retrieval (cs.IR)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 09:53:35 GMT",
          "size": "153kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "A Literature Review on Simulation in Conversational Recommender Systems",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The literature review on conversational recommender systems emphasizes simulation methods and challenges in CRSs, not creativity."
      }
    },
    {
      "id": "2506.20377",
      "abstract": "The impact of culture on how people express distress in online support communities is increasingly a topic of interest within Computer Supported Cooperative Work (CSCW) and Human-Computer Interaction (HCI). In the United States, distinct cultures have emerged from each of the two dominant political parties, forming a primary lens by which people navigate online and offline worlds. We examine whether partisan culture may play a role in how U.S. Republican and Democrat users of online mental health support communities express distress. We present a large-scale observational study of 2,184,356 posts from 8,916 statistically matched Republican, Democrat, and unaffiliated online support community members. We utilize methods from causal inference to statistically match partisan users along covariates that correspond with demographic attributes and platform use, in order to create comparable cohorts for analysis. We then leverage methods from natural language processing to understand how partisan expressions of distress compare between these sets of closely matched opposing partisans, and between closely matched partisans and typical support community members. Our data spans January 2013 to December 2022, a period of both rising political polarization and mental health concerns. We find that partisan culture does play into expressions of distress, underscoring the importance of considering partisan cultural differences in the design of online support community platforms.",
      "authors": [
        "Sachin R. Pendse",
        "Ben Rochford",
        "Neha Kumar",
        "Munmun De Choudhury"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20377",
        "HTML": "https://arxiv.org/html/2506.20377",
        "PDF": "https://arxiv.org/pdf/2506.20377"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 12:44:10 GMT",
          "size": "1168kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "The Role of Partisan Culture in Mental Health Language Online",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "This paper investigates the role of partisan culture in online mental health language, not related to creativity."
      }
    },
    {
      "id": "2506.20463",
      "abstract": "Educators and learners worldwide are embracing the rise of Generative Artificial Intelligence (GenAI) as it reshapes higher education. However, GenAI also raises significant privacy and security concerns, as models and privacy-sensitive user data, such as student records, may be misused by service providers. Unfortunately, end-users often have little awareness of or control over how these models operate. To address these concerns, universities are developing institutional policies to guide GenAI use while safeguarding security and privacy. This work examines these emerging policies and guidelines, with a particular focus on the often-overlooked privacy and security dimensions of GenAI integration in higher education, alongside other academic values. Through a qualitative analysis of GenAI usage guidelines from universities across 12 countries, we identify key challenges and opportunities institutions face in providing effective privacy and security protections, including the need for GenAI safeguards tailored specifically to the academic context.",
      "authors": [
        "Bei Yi Ng",
        "Jiarui Li",
        "Xinyuan Tong",
        "Kevin Ye",
        "Gauthami Yenne",
        "Varun Chandrasekaran",
        "Jingjie Li"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20463",
        "HTML": "https://arxiv.org/html/2506.20463",
        "PDF": "https://arxiv.org/pdf/2506.20463"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 14:12:18 GMT",
          "size": "576kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "Analyzing Security and Privacy Challenges in Generative AI Usage Guidelines for Higher Education",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper focuses on security and privacy challenges in Generative AI within higher education, without any mention or implication of creativity."
      }
    },
    {
      "id": "2506.20595",
      "abstract": "The ubiquity of technologies like ChatGPT has raised concerns about their impact on student writing, particularly regarding reduced learner agency and superficial engagement with content. While standalone chat-based LLMs often produce suboptimal writing outcomes, evidence suggests that purposefully designed AI writing support tools can enhance the writing process. This paper investigates how different AI support approaches affect writers' sense of agency and depth of knowledge transformation. Through a randomized control trial with 90 undergraduate students, we compare three conditions: (1) a chat-based LLM writing assistant, (2) an integrated AI writing tool to support diverse subprocesses, and (3) a standard writing interface (control). Our findings demonstrate that, among AI-supported conditions, students using the integrated AI writing tool exhibited greater agency over their writing process and engaged in deeper knowledge transformation overall. These results suggest that thoughtfully designed AI writing support targeting specific aspects of the writing process can help students maintain ownership of their work while facilitating improved engagement with content.",
      "authors": [
        "Momin N. Siddiqui",
        "Roy Pea",
        "Hari Subramonyam"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20595",
        "HTML": "https://arxiv.org/html/2506.20595",
        "PDF": "https://arxiv.org/pdf/2506.20595"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 16:34:09 GMT",
          "size": "2040kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "AI in the Writing Process: How Purposeful AI Support Fosters Student Writing",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses AI tools' effects on student writing, which involves creative aspects of writing but focuses more on agency and engagement rather than creativity explicitly."
      }
    },
    {
      "id": "2506.19899",
      "abstract": "Social engineering attacks using email, commonly known as phishing, are a critical cybersecurity threat. Phishing attacks often lead to operational incidents and data breaches. As a result, many organizations allocate a substantial portion of their cybersecurity budgets to phishing awareness training, driven in part by compliance requirements. However, the effectiveness of this training remains in dispute. Empirical evidence of training (in)effectiveness is essential for evidence-based cybersecurity investment and policy development. Despite recent measurement studies, two critical gaps remain in the literature:\n  (1) we lack a validated measure of phishing lure difficulty, and\n  (2) there are few comparisons of different types of training in real-world business settings.\n  To fill these gaps, we conducted a large-scale study ($N = 12{,}511$) of phishing effectiveness at a US-based financial technology (``fintech'') firm. Our two-factor design compared the effect of treatments (lecture-based, interactive, and control groups) on subjects' susceptibility to phishing lures of varying complexity (using the NIST Phish Scale). The NIST Phish Scale successfully predicted behavior (click rates: 7.0\\% easy to 15.0\\% hard emails, p $<$ 0.001), but training showed no significant main effects on clicks (p = 0.450) or reporting (p = 0.417). Effect sizes remained below 0.01, indicating little practical value in any of the phishing trainings we deployed. Our results add to the growing evidence that phishing training is ineffective, reinforcing the importance of phishing defense-in-depth and the merit of changes to processes and technology to reduce reliance on humans, as well as rebuking the training costs necessitated by regulatory requirements.",
      "authors": [
        "Andrew T. Rozema",
        "James C. Davis"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19899",
        "HTML": "https://arxiv.org/html/2506.19899",
        "PDF": "https://arxiv.org/pdf/2506.19899"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 17:57:10 GMT",
          "size": "706kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Anti-Phishing Training Does Not Work: A Large-Scale Empirical Assessment of Multi-Modal Training Grounded in the NIST Phish Scale",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper assesses the effectiveness of anti-phishing training and is unrelated to creativity."
      }
    },
    {
      "id": "2506.20212",
      "abstract": "With the advent of Industry 5.0, manufacturers are increasingly prioritizing worker well-being alongside mass customization. Stress-aware Human-Robot Collaboration (HRC) plays a crucial role in this paradigm, where robots must adapt their behavior to human mental states to improve collaboration fluency and safety. This paper presents a novel framework that integrates Federated Learning (FL) to enable personalized mental state evaluation while preserving user privacy. By leveraging physiological signals, including EEG, ECG, EDA, EMG, and respiration, a multimodal model predicts an operator's stress level, facilitating real-time robot adaptation. The FL-based approach allows distributed on-device training, ensuring data confidentiality while improving model generalization and individual customization. Results demonstrate that the deployment of an FL approach results in a global model with performance in stress prediction accuracy comparable to a centralized training approach. Moreover, FL allows for enhancing personalization, thereby optimizing human-robot interaction in industrial settings, while preserving data privacy. The proposed framework advances privacy-preserving, adaptive robotics to enhance workforce well-being in smart manufacturing.",
      "authors": [
        "Andrea Bussolan",
        "Oliver Avram",
        "Andrea Pignata",
        "Gianvito Urgese",
        "Stefano Baraldo",
        "Anna Valente"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20212",
        "HTML": "https://arxiv.org/html/2506.20212",
        "PDF": "https://arxiv.org/pdf/2506.20212"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 07:55:59 GMT",
          "size": "14314kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "Personalized Mental State Evaluation in Human-Robot Interaction using Federated Learning",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper focuses on stress-aware human-robot collaboration and personalized mental state evaluation, with no explicit mention of creativity."
      }
    },
    {
      "id": "2506.20268",
      "abstract": "Detecting miscommunication in human-robot interaction is a critical function for maintaining user engagement and trust. While humans effortlessly detect communication errors in conversations through both verbal and non-verbal cues, robots face significant challenges in interpreting non-verbal feedback, despite advances in computer vision for recognizing affective expressions. This research evaluates the effectiveness of machine learning models in detecting miscommunications in robot dialogue. Using a multi-modal dataset of 240 human-robot conversations, where four distinct types of conversational failures were systematically introduced, we assess the performance of state-of-the-art computer vision models. After each conversational turn, users provided feedback on whether they perceived an error, enabling an analysis of the models' ability to accurately detect robot mistakes. Despite using state-of-the-art models, the performance barely exceeds random chance in identifying miscommunication, while on a dataset with more expressive emotional content, they successfully identified confused states. To explore the underlying cause, we asked human raters to do the same. They could also only identify around half of the induced miscommunications, similarly to our model. These results uncover a fundamental limitation in identifying robot miscommunications in dialogue: even when users perceive the induced miscommunication as such, they often do not communicate this to their robotic conversation partner. This knowledge can shape expectations of the performance of computer vision models and can help researchers to design better human-robot conversations by deliberately eliciting feedback where needed.",
      "authors": [
        "Ruben Janssens",
        "Jens De Bock",
        "Sofie Labat",
        "Eva Verhelst",
        "Veronique Hoste",
        "Tony Belpaeme"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20268",
        "HTML": "https://arxiv.org/html/2506.20268",
        "PDF": "https://arxiv.org/pdf/2506.20268"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 09:25:04 GMT",
          "size": "1151kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "This research addresses miscommunication detection in human-robot interaction, without discussing creativity."
      }
    },
    {
      "id": "2506.20400",
      "abstract": "Multi-agent-based simulations (MABS) of electric vehicle (EV) home charging ecosystems generate large, complex, and stochastic time-series datasets that capture interactions between households, grid infrastructure, and energy markets. These interactions can lead to unexpected system-level events, such as transformer overloads or consumer dissatisfaction, that are difficult to detect and explain through static post-processing. This paper presents a modular, Python-based dashboard framework, built using Dash by Plotly, that enables efficient, multi-level exploration and root-cause analysis of emergent behavior in MABS outputs. The system features three coordinated views (System Overview, System Analysis, and Consumer Analysis), each offering high-resolution visualizations such as time-series plots, spatial heatmaps, and agent-specific drill-down tools. A case study simulating full EV adoption with smart charging in a Danish residential network demonstrates how the dashboard supports rapid identification and contextual explanation of anomalies, including clustered transformer overloads and time-dependent charging failures. The framework facilitates actionable insight generation for researchers and distribution system operators, and its architecture is adaptable to other distributed energy resources and complex energy systems.",
      "authors": [
        "Kristoffer Christensen",
        "Bo N{\\o}rregaard J{\\o}rgensen and Zheng Grace Ma"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20400",
        "HTML": "https://arxiv.org/html/2506.20400",
        "PDF": "https://arxiv.org/pdf/2506.20400"
      },
      "subjects": [
        "Multiagent Systems (cs.MA)",
        "Computational Engineering, Finance, and Science (cs.CE)",
        "Human-Computer Interaction (cs.HC)",
        "Systems and Control (cs.SY)",
        "Systems and Control (eess.SY)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 13:14:49 GMT",
          "size": "1161kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "A Visualization Framework for Exploring Multi-Agent-Based Simulations Case Study of an Electric Vehicle Home Charging Ecosystem",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The study is about a visualization framework for multi-agent simulations in EV home charging, focusing on technical insights rather than creativity."
      }
    },
    {
      "id": "2506.20664",
      "abstract": "As Large Language Models (LLMs) gain agentic abilities, they will have to navigate complex multi-agent scenarios, interacting with human users and other agents in cooperative and competitive settings. This will require new reasoning skills, chief amongst them being theory of mind (ToM), or the ability to reason about the \"mental\" states of other agents. However, ToM and other multi-agent abilities in LLMs are poorly understood, since existing benchmarks suffer from narrow scope, data leakage, saturation, and lack of interactivity. We thus propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM drawing inspiration from cognitive science, computational pragmatics and multi-agent reinforcement learning. It is designed to be as easy as possible in all other dimensions, eliminating confounding factors commonly found in other benchmarks. To our knowledge, it is also the first platform for designing interactive ToM experiments.\n  We validate the benchmark design through comprehensive empirical evaluations of frontier LLMs, robustness studies, and human-AI cross-play experiments. We find that LLM game-playing abilities lag behind humans and simple word-embedding baselines. We then create variants of two classic cognitive science experiments within Decrypto to evaluate three key ToM abilities. Surprisingly, we find that state-of-the-art reasoning models are significantly worse at those tasks than their older counterparts. This demonstrates that Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and paves the path towards better artificial agents.",
      "authors": [
        "Andrei Lupu",
        "Timon Willi",
        "Jakob Foerster"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20664",
        "HTML": "https://arxiv.org/html/2506.20664",
        "PDF": "https://arxiv.org/pdf/2506.20664"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)",
        "Multiagent Systems (cs.MA)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 17:55:27 GMT",
          "size": "2440kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper is about multi-agent reasoning and theory of mind, with no connection to creativity."
      }
    },
    {
      "id": "2404.17582",
      "abstract": "As crowdsourcing emerges as an efficient and cost-effective method for obtaining labels for machine learning datasets, it is important to assess the quality of crowd-provided data, so as to improve analysis performance and reduce biases in subsequent machine learning tasks. Given the lack of ground truth in most cases of crowdsourcing, we refer to data quality as annotators' consistency and credibility. Unlike the simple scenarios where Kappa coefficient and intraclass correlation coefficient usually can apply, online crowdsourcing requires dealing with more complex situations. We introduce a systematic method for evaluating data quality and detecting spamming threats via variance decomposition, and we classify spammers into three categories based on their different behavioral patterns. A spammer index is proposed to assess entire data consistency, and two metrics are developed to measure crowd workers' credibility by utilizing the Markov chain and generalized random effects models. Furthermore, we showcase the practicality of our techniques and their advantages by applying them on a face verification task with both simulation and real-world data collected from two crowdsourcing platforms.",
      "authors": [
        "Yang Ba",
        "Michelle V. Mancenido",
        "Erin K. Chiou",
        "and Rong Pan"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2404.17582",
        "HTML": "https://arxiv.org/html/2404.17582",
        "PDF": "https://arxiv.org/pdf/2404.17582"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)",
        "Applications (stat.AP)"
      ],
      "submission_historys": [
        {
          "date": "Thu, 04 Apr 2024 02:21:38 GMT",
          "size": "2665kb",
          "version": "v1"
        },
        {
          "date": "Wed, 25 Jun 2025 17:56:08 GMT",
          "size": "1719kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "Data Quality in Crowdsourcing and Spamming Behavior Detection",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper deals with data quality in crowdsourcing, lacking any explicit connection to creativity."
      },
      "tasks": [
        "Face Verification"
      ]
    },
    {
      "id": "2409.02244",
      "abstract": "Large language models (LLMs) are being used as ad-hoc therapists. Research suggests that LLMs outperform human counselors when generating a single, isolated empathetic response; however, their session-level behavior remains understudied. In this study, we compare the session-level behaviors of human counselors with those of an LLM prompted by a team of peer counselors to deliver single-session Cognitive Behavioral Therapy (CBT). Our three-stage, mixed-methods study involved: a) a year-long ethnography of a text-based support platform where seven counselors iteratively refined CBT prompts through self-counseling and weekly focus groups; b) the manual simulation of human counselor sessions with a CBT-prompted LLM, given the full patient dialogue and contextual notes; and c) session evaluations of both human and LLM sessions by three licensed clinical psychologists using CBT competence measures. Our results show a clear trade-off. Human counselors excel at relational strategies -- small talk, self-disclosure, and culturally situated language -- that lead to higher empathy, collaboration, and deeper user reflection. LLM counselors demonstrate higher procedural adherence to CBT techniques but struggle to sustain collaboration, misread cultural cues, and sometimes produce \"deceptive empathy,\" i.e., formulaic warmth that can inflate users' expectations of genuine human care. Taken together, our findings imply that while LLMs might outperform counselors in generating single empathetic responses, their ability to lead sessions is more limited, highlighting that therapy cannot be reduced to a standalone natural language processing (NLP) task. We call for carefully designed human-AI workflows in scalable support: LLMs can scaffold evidence-based techniques, while peers provide relational support. We conclude by mapping concrete design opportunities and ethical guardrails for such hybrid systems.",
      "authors": [
        "Zainab Iftikhar",
        "Sean Ransom",
        "Amy Xiao",
        "Nicole Nugent",
        "Jeff Huang"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2409.02244",
        "HTML": "https://arxiv.org/html/2409.02244",
        "PDF": "https://arxiv.org/pdf/2409.02244"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 03 Sep 2024 19:19:13 GMT",
          "size": "623kb",
          "version": "v1"
        },
        {
          "date": "Wed, 25 Jun 2025 02:07:35 GMT",
          "size": "207kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "Therapy as an NLP Task: Psychologists' Comparison of LLMs and Human Peers in CBT",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper is focused on comparing LLMs and human counselors in therapy, specifically CBT, with no mention of creativity."
      },
      "tasks": [
        "Language Modelling",
        "Large Language Model"
      ]
    },
    {
      "id": "2506.19519",
      "abstract": "Virtual reality (VR) can enrich neuropsychological testing, yet the ergonomic trade-offs of its input modes remain under-examined. Seventy-seven healthy volunteers-young (19-29 y) and middle-aged (35-56 y)-completed a VR Trail-Making Test with three pointing methods: eye-tracking, head-gaze, and a six-degree-of-freedom hand controller. Completion time, spatial accuracy, and error counts for the simple (Trail A) and alternating (Trail B) sequences were analysed in 3 x 2 x 2 mixed-model ANOVAs; post-trial scales captured usability (SUS), user experience (UEQ-S), and acceptability. Age dominated behaviour: younger adults were reliably faster, more precise, and less error-prone. Against this backdrop, input modality mattered. Eye-tracking yielded the best spatial accuracy and shortened Trail A time relative to manual control; head-gaze matched eye-tracking on Trail A speed and became the quickest, least error-prone option on Trail B. Controllers lagged on every metric. Subjective ratings were high across the board, with only a small usability dip in middle-aged low-gamers. Overall, gaze-based ray-casting clearly outperformed manual pointing, but optimal choice depended on task demands: eye-tracking maximised spatial precision, whereas head-gaze offered calibration-free enhanced speed and error-avoidance under heavier cognitive load. TMT-VR appears to be accurate, engaging, and ergonomically adaptable assessment, yet it requires age-specific-stratified norms.",
      "authors": [
        "Panagiotis Kourtesis",
        "Evgenia Giatzoglou",
        "Panagiotis Vorias",
        "Katerina Alkisti Gounari",
        "Eleni Orfanidou and Chrysanthi Nega"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19519",
        "HTML": "https://arxiv.org/html/2506.19519",
        "PDF": "https://arxiv.org/pdf/2506.19519"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 11:10:09 GMT",
          "size": "1325kb",
          "version": "v1"
        },
        {
          "date": "Wed, 25 Jun 2025 08:18:21 GMT",
          "size": "1322kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "Examination of Eye-Tracking, Head-Gaze, and Controller-Based Ray-casting in TMT-VR: Performance and Usability Across Adulthood",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper examines usability and performance of VR input methods and does not discuss creativity."
      }
    },
    {
      "id": "2506.11727",
      "abstract": "This paper critically audits the search endpoint of YouTube's Data API (v3), a common tool for academic research. Through systematic weekly searches over six months using eleven queries, we identify major limitations regarding completeness, representativeness, consistency, and bias. Our findings reveal substantial differences between ranking parameters like relevance and date in terms of video recall and precision, with relevance often retrieving numerous off-topic videos. We also find severe temporal decay, as the number of findable videos for a specific period dramatically decreases after just 20-60 days from the publication date, potentially hampering many different research designs. Furthermore, search results lack consistency, with identical queries yielding different video sets over time, compromising replicability. A case study on the European Parliament elections highlights how these issues impact research outcomes. While the paper offers several mitigation strategies, it concludes that the API's search function, potentially prioritizing \"freshness\" over comprehensive retrieval, is not adequate for robust academic research, especially concerning Digital Services Act requirements.",
      "authors": [
        "Bernhard Rieder",
        "Adrian Padilla and Oscar Coromina"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.11727",
        "HTML": "https://arxiv.org/html/2506.11727",
        "PDF": "https://arxiv.org/pdf/2506.11727"
      },
      "subjects": [
        "Information Retrieval (cs.IR)",
        "Human-Computer Interaction (cs.HC)",
        "Social and Information Networks (cs.SI)"
      ],
      "submission_historys": [
        {
          "date": "Fri, 13 Jun 2025 12:39:59 GMT",
          "size": "1231kb",
          "version": "v1"
        },
        {
          "date": "Wed, 25 Jun 2025 14:06:24 GMT",
          "size": "860kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "Forgetful by Design? A Critical Audit of YouTube's Search API for Academic Research",
      "relevance": {
        "keyword": "creativity",
        "level": "none",
        "reason": "The paper focuses on auditing YouTube's Search API and does not address creativity."
      },
      "tasks": []
    }
  ],
  "subjects": [
    "Computational Engineering, Finance, and Science (cs.CE)",
    "Computation and Language (cs.CL)",
    "Systems and Control (eess.SY)",
    "Cryptography and Security (cs.CR)",
    "Robotics (cs.RO)",
    "Information Retrieval (cs.IR)",
    "Artificial Intelligence (cs.AI)",
    "Applications (stat.AP)",
    "Systems and Control (cs.SY)",
    "Social and Information Networks (cs.SI)",
    "Computers and Society (cs.CY)",
    "Multiagent Systems (cs.MA)",
    "Human-Computer Interaction (cs.HC)",
    "Machine Learning (cs.LG)"
  ],
  "prompt": {
    "creativity": "\nYou are an expert in information retrieval. I will provide you with a list of research papers from arXiv, specifically in the *cs.HC* (Human-Computer Interaction) category.\n\nYour task is to analyze each paper and determine its relevance to the topic of **Creativity**.\n\nClassify each paper into one of the following relevance levels:\n\n* `\"strong\"`: The paper is explicitly focused on creativity.\n* `\"weak\"`: The paper mentions or touches on creativity, but it is not the main focus.\n* `\"none\"`: The paper is not related to creativity.\n\nReturn your results in the following JSON format:\n\n```json\n{\n  \"result\": [\n    {\n      \"id\": \"paper id\",\n      \"level\": \"strong | weak | none\",\n      \"reason\": \"Brief justification based on the paper content\"\n    },\n    {\n      \"id\": \"paper id\",\n      \"level\": \"strong | weak | none\",\n      \"reason\": \"Brief justification based on the paper content\"\n    }\n  ]\n}\n```\n\nBe concise but specific in your reasoning, referencing key terms or sections when applicable.\n"
  },
  "description": "Data source: https://arxiv.org/list/cs.HC/new"
}