{
  "data": [
    {
      "id": "2506.18919",
      "abstract": "The rapid development of social media has intensified the spread of harmful content. Harmful memes, which integrate both images and text, pose significant challenges for automated detection due to their implicit semantics and complex multimodal interactions. Although existing research has made progress in detection accuracy and interpretability, the lack of a systematic, large-scale, diverse, and highly explainable dataset continues to hinder further advancement in this field. To address this gap, we introduce MemeMind, a novel dataset featuring scientifically rigorous standards, large scale, diversity, bilingual support (Chinese and English), and detailed Chain-of-Thought (CoT) annotations. MemeMind fills critical gaps in current datasets by offering comprehensive labeling and explicit reasoning traces, thereby providing a solid foundation for enhancing harmful meme detection. In addition, we propose an innovative detection framework, MemeGuard, which effectively integrates multimodal information with reasoning process modeling, significantly improving models' ability to understand and identify harmful memes. Extensive experiments conducted on the MemeMind dataset demonstrate that MemeGuard consistently outperforms existing state-of-the-art methods in harmful meme detection tasks.",
      "authors": [
        "Hexiang Gu",
        "Qifan Yu",
        "Saihui Hou",
        "Zhiqin Fang",
        "Huijia Wu",
        "Zhaofeng He"
      ],
      "last_revised_date": "2025/06/15",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18919",
        "HTML": "https://arxiv.org/html/2506.18919",
        "PDF": "https://arxiv.org/pdf/2506.18919"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Sun, 15 Jun 2025 13:45:30 GMT",
          "size": "14753kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/15",
      "title": "MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper introduces MemeMind, a new dataset for harmful meme detection, involving dataset curation and annotation, which is directly related to LLM training data topics."
      }
    },
    {
      "id": "2506.18951",
      "abstract": "Resolution of complex SQL issues persists as a significant bottleneck in real-world database applications. Current Large Language Models (LLMs), while adept at text-to-SQL translation, have not been rigorously evaluated on the more challenging task of debugging SQL issues. To address this gap, we introduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530 PostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks (BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within new environments to facilitate rigorous evaluation. Baseline evaluations underscore the task's complexity, with the leading reasoning model O3-Mini achieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on BIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks is crucial for empowering local development while safeguarding data privacy. Therefore, we present Six-Gym (Sql-fIX-Gym), a training environment for elevating open-source model capabilities for SQL issue debugging. This environment leverages SQL-Rewind strategy, which automatically generates executable issue-solution datasets by reverse-engineering issues from verified SQLs. However, popular trajectory-based fine-tuning methods do not explore substantial supervisory signals. We further propose f-Plan Boosting, which extracts high-level debugging plans from SQL solutions, enabling teacher LLMs to produce 73.7% more successful trajectories for training. We integrate these components into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B, Bird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on BIRD-CRITIC-Multi, surpassing leading proprietary models such as Claude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing sophisticated SQL-debugging capabilities. The leaderboard and source code are available: https://bird-critic.github.io/",
      "authors": [
        "Jinyang Li",
        "Xiaolong Li",
        "Ge Qu",
        "Per Jacobsson",
        "Bowen Qin",
        "Binyuan Hui",
        "Shuzheng Si",
        "Nan Huo",
        "Xiaohan Xu",
        "Yue Zhang",
        "Ziwei Tang",
        "Yuanshuai Li",
        "Florensia Widjaja",
        "Xintong Zhu",
        "Feige Zhou",
        "Yongfeng Huang",
        "Yannis Papakonstantinou",
        "Fatma Ozcan",
        "Chenhao Ma",
        "Reynold Cheng"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18951",
        "HTML": "https://arxiv.org/html/2506.18951",
        "PDF": "https://arxiv.org/pdf/2506.18951"
      },
      "subjects": [
        "Databases (cs.DB)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 09:41:37 GMT",
          "size": "2761kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in Real-World Applications",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper discusses SQL issue debugging benchmarks, data replaying, and dataset generation (SQL-Rewind), directly addressing data processes related to LLM evaluation and enhancement."
      },
      "datasets": [
        {
          "dataset_name": "birdsql/bird-critic-1.0-open",
          "downloads": "407",
          "likes": "1",
          "link": "https://huggingface.co/datasets/birdsql/bird-critic-1.0-open"
        }
      ]
    },
    {
      "id": "2506.19054",
      "abstract": "As LLMs become widespread across diverse applications, concerns about the security and safety of LLM interactions have intensified. Numerous guardrail models and benchmarks have been developed to ensure LLM content safety. However, existing guardrail benchmarks are often built upon ad hoc risk taxonomies that lack a principled grounding in standardized safety policies, limiting their alignment with real-world operational requirements. Moreover, they tend to overlook domain-specific risks, while the same risk category can carry different implications across different domains. To bridge these gaps, we introduce PolyGuard, the first massive multi-domain safety policy-grounded guardrail dataset. PolyGuard offers: (1) broad domain coverage across eight safety-critical domains, such as finance, law, and codeGen; (2) policy-grounded risk construction based on authentic, domain-specific safety guidelines; (3) diverse interaction formats, encompassing declarative statements, questions, instructions, and multi-turn conversations; (4) advanced benign data curation via detoxification prompting to challenge over-refusal behaviors; and (5) \\textbf{attack-enhanced instances} that simulate adversarial inputs designed to bypass guardrails. Based on PolyGuard, we benchmark 19 advanced guardrail models and uncover a series of findings, such as: (1) All models achieve varied F1 scores, with many demonstrating high variance across risk categories, highlighting their limited domain coverage and insufficient handling of domain-specific safety concerns; (2) As models evolve, their coverage of safety risks broadens, but performance on common risk categories may decrease; (3) All models remain vulnerable to optimized adversarial attacks. We believe that \\dataset and the unique insights derived from our evaluations will advance the development of policy-aligned and resilient guardrail systems.",
      "authors": [
        "Mintong Kang",
        "Zhaorun Chen",
        "Chejian Xu",
        "Jiawei Zhang",
        "Chengquan Guo",
        "Minzhou Pan",
        "Ivan Revilla",
        "Yu Sun",
        "Bo Li"
      ],
      "last_revised_date": "2025/06/18",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19054",
        "HTML": "https://arxiv.org/html/2506.19054",
        "PDF": "https://arxiv.org/pdf/2506.19054"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 18 Jun 2025 01:35:33 GMT",
          "size": "3611kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/18",
      "title": "PolyGuard: Massive Multi-Domain Safety Policy-Grounded Guardrail Dataset",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper introduces PolyGuard, a dataset related to LLMs, specifically aimed at enhancing the safety and security of LLM interactions. It discusses dataset curation and evaluation in the context of LLMs, which is directly related to LLM training data."
      }
    },
    {
      "id": "2506.19073",
      "abstract": "Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is a growing concern as these systems are used in socially sensitive tasks. Nevertheless, current evaluation benchmarks present two major shortcomings: a lack of annotations that justify moral classifications, which limits transparency and interpretability; and a predominant focus on English, which constrains the assessment of moral reasoning across diverse cultural settings. In this paper, we introduce MFTCXplain, a multilingual benchmark dataset for evaluating the moral reasoning of LLMs via hate speech multi-hop explanation using Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across Portuguese, Italian, Persian, and English, annotated with binary hate speech labels, moral categories, and text span-level rationales. Empirical results highlight a misalignment between LLM outputs and human annotations in moral reasoning tasks. While LLMs perform well in hate speech detection (F1 up to 0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35). Furthermore, rationale alignment remains limited mainly in underrepresented languages. These findings show the limited capacity of current LLMs to internalize and reflect human moral reasoning.",
      "authors": [
        "Jackson Trager and Francielle Vargas and Diego Alves and Matteo Guida and Mikel K. Ngueajio and Ameeta Agrawal and Flor Plaza-del-Arco and Yalda Daryanai and Farzan Karimi-Malekabadi"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19073",
        "HTML": "https://arxiv.org/html/2506.19073",
        "PDF": "https://arxiv.org/pdf/2506.19073"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 19:44:21 GMT",
          "size": "3432kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanation",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "This paper introduces a benchmark dataset for evaluating LLMs, discussing dataset creation and annotation, which is directly related to LLM training data."
      }
    },
    {
      "id": "2506.19089",
      "abstract": "We introduce $\\texttt{StorySim}$, a programmable framework for synthetically generating stories to evaluate the theory of mind (ToM) and world modeling (WM) capabilities of large language models (LLMs). Unlike prior benchmarks that may suffer from contamination in pretraining data, $\\texttt{StorySim}$ produces novel, compositional story prompts anchored by a highly controllable $\\texttt{Storyboard}$, enabling precise manipulation of character perspectives and events. We use this framework to design first- and second-order ToM tasks alongside WM tasks that control for the ability to track and model mental states. Our experiments across a suite of state-of-the-art LLMs reveal that most models perform better on WM tasks than ToM tasks, and that models tend to perform better reasoning with humans compared to inanimate objects. Additionally, our framework enabled us to find evidence of heuristic behavior such as recency bias and an over-reliance on earlier events in the story. All code for generating data and evaluations is freely available.",
      "authors": [
        "Nathaniel Getachew",
        "Abulhair Saparov"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19089",
        "HTML": "https://arxiv.org/html/2506.19089",
        "PDF": "https://arxiv.org/pdf/2506.19089"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 20:06:53 GMT",
          "size": "9555kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper introduces $\\texttt{StorySim}$, a framework for generating synthetic stories to evaluate LLM capabilities. It directly discusses novel data generation relevant to LLM pretraining contamination."
      }
    },
    {
      "id": "2506.19140",
      "abstract": "Retrofitting large language models (LLMs) with new behaviors typically requires full finetuning or distillation-costly steps that must be repeated for every architecture. In this work, we introduce Command-V, a backpropagation-free behavior transfer method that copies an existing residual activation adapter from a donor model and pastes its effect into a recipient model. Command-V profiles layer activations on a small prompt set, derives linear converters between corresponding layers, and applies the donor intervention in the recipient's activation space. This process does not require access to the original training data and needs minimal compute. In three case studies-safety-refusal enhancement, jailbreak facilitation, and automatic chain-of-thought reasoning--Command-V matches or exceeds the performance of direct finetuning while using orders of magnitude less compute. Our code and data are accessible at https://github.com/GithuBarry/Command-V/.",
      "authors": [
        "Barry Wang",
        "Avi Schwarzschild",
        "Alexander Robey",
        "Ali Payani",
        "Charles Fleming",
        "Mingjie Sun",
        "Daphne Ippolito"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19140",
        "HTML": "https://arxiv.org/html/2506.19140",
        "PDF": "https://arxiv.org/pdf/2506.19140"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 21:21:49 GMT",
          "size": "2907kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Command-V: Pasting LLM Behaviors via Activation Profiles",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper relates to LLMs, specifically addressing behavior transfer without requiring the original training data, suggesting a focus on modifications relevant to LLM training dynamics."
      }
    },
    {
      "id": "2506.19153",
      "abstract": "The YulCode dataset presents a comprehensive collection of 348,840 Yul-based smart contract instances, comprising approximately 135,013 unique contracts. These contracts were generated through the compilation of Solidity source files that have been deployed on the Ethereum mainnet, making the dataset directly representative of real-world decentralized applications. YulCode provides a rich foundation for a variety of research and development tasks, including but not limited to machine learning applications, formal verification, optimization analysis, and software engineering tool evaluation in the context of low-level smart contract code. To the best of our knowledge at the time of writing, YulCode is the first and only publicly available dataset that focuses specifically on Yul, an intermediate language designed for the Ethereum Virtual Machine (EVM). As such, it fills a critical gap in the current ecosystem of smart contract datasets and opens new avenues for research and tooling aimed at low-level contract analysis and generation.",
      "authors": [
        "Krzysztof Fonal"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19153",
        "HTML": "https://arxiv.org/html/2506.19153",
        "PDF": "https://arxiv.org/pdf/2506.19153"
      },
      "subjects": [
        "Software Engineering (cs.SE)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 21:43:05 GMT",
          "size": "8kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Dataset of Yul Contracts to Support Solidity Compiler Research",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper presents the YulCode dataset, which is relevant to the realm of data collection and curation for specific applications, analogous to the datasets used in LLM training. The focus on dataset creation justifies a 'strong' relevance."
      }
    },
    {
      "id": "2506.19185",
      "abstract": "Traditional mental health support systems often generate responses based solely on the user's current emotion and situations, resulting in superficial interventions that fail to address deeper emotional needs. This study introduces a novel framework by integrating spiritual wisdom from the Bhagavad Gita with advanced large language model GPT-4o to enhance emotional well-being. We present the GITes (Gita Integrated Therapy for Emotional Support) dataset, which enhances the existing ExTES mental health dataset by including 10,729 spiritually guided responses generated by GPT-4o and evaluated by domain experts. We benchmark GITes against 12 state-of-the-art LLMs, including both mental health specific and general purpose models. To evaluate spiritual relevance in generated responses beyond what conventional n-gram based metrics capture, we propose a novel Spiritual Insight metric and automate assessment via an LLM as jury framework using chain-of-thought prompting. Integrating spiritual guidance into AI driven support enhances both NLP and spiritual metrics for the best performing LLM Phi3-Mini 3.2B Instruct, achieving improvements of 122.71% in ROUGE, 126.53% in METEOR, 8.15% in BERT score, 15.92% in Spiritual Insight, 18.61% in Sufficiency and 13.22% in Relevance compared to its zero-shot counterpart. While these results reflect substantial improvements across automated empathy and spirituality metrics, further validation in real world patient populations remains a necessary step. Our findings indicate a strong potential for AI systems enriched with spiritual guidance to enhance user satisfaction and perceived support outcomes. The code and dataset will be publicly available to advance further research in this emerging area.",
      "authors": [
        "Janak Kapuriya",
        "Aman Singh",
        "Jainendra Shukla",
        "Rajiv Ratn Shah"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19185",
        "HTML": "https://arxiv.org/html/2506.19185",
        "PDF": "https://arxiv.org/pdf/2506.19185"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 23:02:57 GMT",
          "size": "28714kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Spiritual-LLM : Gita Inspired Mental Health Therapy In the Era of LLMs",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper introduces a novel dataset, GITes, specifically enhanced for LLM performance, which relates directly to LLM training data in terms of creation, curation, and evaluation."
      }
    },
    {
      "id": "2506.19187",
      "abstract": "LLMs are typically trained in high-resource languages, and tasks in lower-resourced languages tend to underperform the higher-resource language counterparts for in-context learning. Despite the large body of work on prompting settings, it is still unclear how LLMs should be adapted cross-lingually specifically for in-context learning in the low-resource target languages. We perform a comprehensive study spanning five diverse target languages, three base LLMs, and seven downstream tasks spanning over 4,100 GPU training hours (9,900+ TFLOPs) across various adaptation techniques: few-shot prompting, translate-test, fine-tuning, embedding re-initialization, and instruction fine-tuning. Our results show that the few-shot prompting and translate-test settings tend to heavily outperform the gradient-based adaptation methods. To better understand this discrepancy, we design a novel metric, Valid Output Recall (VOR), and analyze model outputs to empirically attribute the degradation of these trained models to catastrophic forgetting. To the extent of our knowledge, this is the largest study done on in-context learning for low-resource languages with respect to train compute and number of adaptation techniques considered. We make all our datasets and trained models available for public use.",
      "authors": [
        "Christopher Toukmaji",
        "Jeffrey Flanigan"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19187",
        "HTML": "https://arxiv.org/html/2506.19187",
        "PDF": "https://arxiv.org/pdf/2506.19187"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 23:22:11 GMT",
          "size": "2047kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Prompt, Translate, Fine-Tune, Re-Initialize, or Instruction-Tune? Adapting LLMs for In-Context Learning in Low-Resource Languages",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper involves creating and adapting datasets for LLM training across different languages, which is directly relevant to the topic of LLM training data."
      }
    },
    {
      "id": "2506.19220",
      "abstract": "We study model personalization under user-level differential privacy (DP) in the shared representation framework. In this problem, there are $n$ users whose data is statistically heterogeneous, and their optimal parameters share an unknown embedding $U^* \\in\\mathbb{R}^{d\\times k}$ that maps the user parameters in $\\mathbb{R}^d$ to low-dimensional representations in $\\mathbb{R}^k$, where $k\\ll d$. Our goal is to privately recover the shared embedding and the local low-dimensional representations with small excess risk in the federated setting. We propose a private, efficient federated learning algorithm to learn the shared embedding based on the FedRep algorithm in [CHM+21]. Unlike [CHM+21], our algorithm satisfies differential privacy, and our results hold for the case of noisy labels. In contrast to prior work on private model personalization [JRS+21], our utility guarantees hold under a larger class of users' distributions (sub-Gaussian instead of Gaussian distributions). Additionally, in natural parameter regimes, we improve the privacy error term in [JRS+21] by a factor of $\\widetilde{O}(dk)$. Next, we consider the binary classification setting. We present an information-theoretic construction to privately learn the shared embedding and derive a margin-based accuracy guarantee that is independent of $d$. Our method utilizes the Johnson-Lindenstrauss transform to reduce the effective dimensions of the shared embedding and the users' data. This result shows that dimension-independent risk bounds are possible in this setting under a margin loss.",
      "authors": [
        "Conor Snedeker",
        "Xinyu Zhou",
        "Raef Bassily"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19220",
        "HTML": "https://arxiv.org/html/2506.19220",
        "PDF": "https://arxiv.org/pdf/2506.19220"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Cryptography and Security (cs.CR)",
        "Distributed, Parallel, and Cluster Computing (cs.DC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 00:57:17 GMT",
          "size": "231kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Private Model Personalization Revisited",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The research proposes a federated learning algorithm with differential privacy for model personalization, closely related to dataset usage in training scenarios and privacy concerns in LLM training data."
      }
    },
    {
      "id": "2506.19235",
      "abstract": "Traditional recommendation systems often grapple with \"filter bubbles\", underutilization of external knowledge, and a disconnect between model optimization and business policy iteration. To address these limitations, this paper introduces RecLLM-R1, a novel recommendation framework leveraging Large Language Models (LLMs) and drawing inspiration from the DeepSeek R1 methodology. The framework initiates by transforming user profiles, historical interactions, and multi-faceted item attributes into LLM-interpretable natural language prompts through a carefully engineered data construction process. Subsequently, a two-stage training paradigm is employed: the initial stage involves Supervised Fine-Tuning (SFT) to imbue the LLM with fundamental recommendation capabilities. The subsequent stage utilizes Group Relative Policy Optimization (GRPO), a reinforcement learning technique, augmented with a Chain-of-Thought (CoT) mechanism. This stage guides the model through multi-step reasoning and holistic decision-making via a flexibly defined reward function, aiming to concurrently optimize recommendation accuracy, diversity, and other bespoke business objectives. Empirical evaluations on a real-world user behavior dataset from a large-scale social media platform demonstrate that RecLLM-R1 significantly surpasses existing baseline methods across a spectrum of evaluation metrics, including accuracy, diversity, and novelty. It effectively mitigates the filter bubble effect and presents a promising avenue for the integrated optimization of recommendation models and policies under intricate business goals.",
      "authors": [
        "Yu Xie",
        "Xingkai Ren",
        "Ying Qi",
        "Yao Hu",
        "Lianlei Shan"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19235",
        "HTML": "https://arxiv.org/html/2506.19235",
        "PDF": "https://arxiv.org/pdf/2506.19235"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 01:39:34 GMT",
          "size": "1621kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "RecLLM-R1: A Two-Stage Training Paradigm with Reinforcement Learning and Chain-of-Thought v1",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper focuses on a recommendation framework leveraging LLMs where a critical component is the transformation of user profiles and interactions into LLM-interpretable natural language prompts, suggesting the relevance to data construction for LLM training."
      }
    },
    {
      "id": "2506.19262",
      "abstract": "With the remarkable generative capabilities of large language models (LLMs), using LLM-generated data to train downstream models has emerged as a promising approach to mitigate data scarcity in specific domains and reduce time-consuming annotations. However, recent studies have highlighted a critical issue: iterative training on self-generated data results in model collapse, where model performance degrades over time. Despite extensive research on the implications of LLM-generated data, these works often neglect the importance of data diversity, a key factor in data quality. In this work, we aim to understand the implications of the diversity of LLM-generated data on downstream model performance. Specifically, we explore how varying levels of diversity in LLM-generated data affect downstream model performance. Additionally, we investigate the performance of models trained on data that mixes different proportions of LLM-generated data, which we refer to as synthetic data. Our experimental results show that, with minimal distribution shift, moderately diverse LLM-generated data can enhance model performance in scenarios with insufficient labeled data, whereas highly diverse generated data has a negative impact. We hope our empirical findings will offer valuable guidance for future studies on LLMs as data generators.",
      "authors": [
        "Yuchang Zhu",
        "Zhonghua zhen",
        "Qunshu Lin",
        "Haotong Wei",
        "Xiaolong Sun",
        "Zixuan Yu",
        "Minghao Liu",
        "Zibin Zheng",
        "Liang Chen"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19262",
        "HTML": "https://arxiv.org/html/2506.19262",
        "PDF": "https://arxiv.org/pdf/2506.19262"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 02:44:58 GMT",
          "size": "625kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper explicitly focuses on LLM-generated data, its diversity, and implications for model fine-tuning, addressing key aspects of LLM training data quality and effectiveness."
      }
    },
    {
      "id": "2506.19290",
      "abstract": "Software engineering (SWE) has recently emerged as a crucial testbed for next-generation LLM agents, demanding inherent capabilities in two critical dimensions: sustained iterative problem-solving (e.g., >50 interaction rounds) and long-context dependency resolution (e.g., >32k tokens). However, the data curation process in SWE remains notoriously time-consuming, as it heavily relies on manual annotation for code file filtering and the setup of dedicated runtime environments to execute and validate unit tests. Consequently, most existing datasets are limited to only a few thousand GitHub-sourced instances. To this end, we propose an incremental, automated data-curation pipeline that systematically scales both the volume and diversity of SWE datasets. Our dataset comprises 10,169 real-world Python task instances from 2,531 distinct GitHub repositories, each accompanied by a task specified in natural language and a dedicated runtime-environment image for automated unit-test validation. We have carefully curated over 8,000 successfully runtime-validated training trajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE model on these trajectories, we uncover a striking data scaling phenomenon: the trained model's performance for software engineering capabilities in LLMs continues to improve as the data size increases, showing no signs of saturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on the SWE-bench Verified benchmark without using verifiers or multiple rollouts, establishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based LLMs built on the OpenHands agent framework. Furthermore, with the incorporation of test-time scaling techniques, the performance further improves to 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter models. We release the Skywork-SWE-32B model checkpoint to accelerate future research.",
      "authors": [
        "Liang Zeng",
        "Yongcong Li",
        "Yuzhen Xiao",
        "Changshi Li",
        "Chris Yuhao Liu",
        "Rui Yan",
        "Tianwen Wei",
        "Jujie He",
        "Xuchen Song",
        "Yang Liu",
        "Yahui Zhou"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19290",
        "HTML": "https://arxiv.org/html/2506.19290",
        "PDF": "https://arxiv.org/pdf/2506.19290"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 03:53:36 GMT",
          "size": "2772kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper is explicitly focused on LLM training data, proposing an automated data-curation pipeline and a dataset for software engineering, discussing data scaling phenomenon, and evaluating the dataset's impact on LLM capabilities."
      },
      "models": [
        {
          "model_path": "Skywork/Skywork-SWE-32B",
          "downloads": "178",
          "likes": "55",
          "trending_score": "55.0",
          "link": "https://huggingface.co/Skywork/Skywork-SWE-32B"
        }
      ]
    },
    {
      "id": "2506.19325",
      "abstract": "In English education tutoring, teacher feedback is essential for guiding students. Recently, AI-based tutoring systems have emerged to assist teachers; however, these systems require high-quality and large-scale teacher feedback data, which is both time-consuming and costly to generate manually. In this study, we propose FEAT, a cost-effective framework for generating teacher feedback, and have constructed three complementary datasets: (1) DIRECT-Manual (DM), where both humans and large language models (LLMs) collaboratively generate high-quality teacher feedback, albeit at a higher cost; (2) DIRECT-Generated (DG), an LLM-only generated, cost-effective dataset with lower quality;, and (3) DIRECT-Augmented (DA), primarily based on DG with a small portion of DM added to enhance quality while maintaining cost-efficiency. Experimental results showed that incorporating a small portion of DM (5-10%) into DG leads to superior performance compared to using 100% DM alone.",
      "authors": [
        "Hyein Seo",
        "Taewook Hwang",
        "Yohan Lee",
        "sangkeun Jung"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19325",
        "HTML": "https://arxiv.org/html/2506.19325",
        "PDF": "https://arxiv.org/pdf/2506.19325"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 05:32:06 GMT",
          "size": "704kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "FEAT: A Preference Feedback Dataset through a Cost-Effective Auto-Generation and Labeling Framework for English AI Tutoring",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper explicitly focuses on generating datasets for training AI tutoring systems involving LLMs. It discusses the creation and augmentation of feedback datasets, directly linked to LLM training data."
      }
    },
    {
      "id": "2506.19399",
      "abstract": "Detecting whether a given text is a member of the pre-training data of Large Language Models (LLMs) is crucial for ensuring data privacy and copyright protection. Most existing methods rely on the LLM's hidden information (e.g., model parameters or token probabilities), making them ineffective in the black-box setting, where only input and output texts are accessible. Although some methods have been proposed for the black-box setting, they rely on massive manual efforts such as designing complicated questions or instructions. To address these issues, we propose VeilProbe, the first framework for automatically detecting LLMs' pre-training texts in a black-box setting without human intervention. VeilProbe utilizes a sequence-to-sequence mapping model to infer the latent mapping feature between the input text and the corresponding output suffix generated by the LLM. Then it performs the key token perturbations to obtain more distinguishable membership features. Additionally, considering real-world scenarios where the ground-truth training text samples are limited, a prototype-based membership classifier is introduced to alleviate the overfitting issue. Extensive evaluations on three widely used datasets demonstrate that our framework is effective and superior in the black-box setting.",
      "authors": [
        "Ruihan Hu",
        "Yu-Ming Shang",
        "Jiankun Peng",
        "Wei Luo",
        "Yazhe Wang and Xi Zhang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19399",
        "HTML": "https://arxiv.org/html/2506.19399",
        "PDF": "https://arxiv.org/pdf/2506.19399"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 08:08:15 GMT",
          "size": "972kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Automated Detection of Pre-training Text in Black-box LLMs",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper directly addresses the detection of pre-training text in LLMs, which is explicitly related to LLM training data. It discusses methodologies for identifying whether text was part of an LLM's training dataset."
      }
    },
    {
      "id": "2506.19445",
      "abstract": "We introduce the largest real-world image deblurring dataset constructed from smartphone slow-motion videos. Using 240 frames captured over one second, we simulate realistic long-exposure blur by averaging frames to produce blurry images, while using the temporally centered frame as the sharp reference. Our dataset contains over 42,000 high-resolution blur-sharp image pairs, making it approximately 10 times larger than widely used datasets, with 8 times the amount of different scenes, including indoor and outdoor environments, with varying object and camera motions. We benchmark multiple state-of-the-art (SOTA) deblurring models on our dataset and observe significant performance degradation, highlighting the complexity and diversity of our benchmark. Our dataset serves as a challenging new benchmark to facilitate robust and generalizable deblurring models.",
      "authors": [
        "Mahdi Mohd Hossain Noki",
        "Syed Mumtahin Mahmud",
        "Prothito Shovon Majumder",
        "Abdul Mohaimen Al Radi",
        "Md. Haider Ali",
        "Md. Mosaddek Khan"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19445",
        "HTML": "https://arxiv.org/html/2506.19445",
        "PDF": "https://arxiv.org/pdf/2506.19445"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 09:17:29 GMT",
          "size": "2457kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Deblurring in the Wild: A Real-World Dataset from Smartphone High-Speed Videos",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper introduces a real-world dataset for image deblurring, dealing explicitly with data collection and dataset construction, which are directly relevant to LLM training data concerns."
      }
    },
    {
      "id": "2506.19472",
      "abstract": "Inspired by the biological visual system that selectively allocates attention to efficiently identify salient objects or regions, underwater salient instance segmentation (USIS) aims to jointly address the problems of where to look (saliency prediction) and what is there (instance segmentation) in underwater scenarios. However, USIS remains an underexplored challenge due to the inaccessibility and dynamic nature of underwater environments, as well as the scarcity of large-scale, high-quality annotated datasets. In this paper, we introduce USIS16K, a large-scale dataset comprising 16,151 high-resolution underwater images collected from diverse environmental settings and covering 158 categories of underwater objects. Each image is annotated with high-quality instance-level salient object masks, representing a significant advance in terms of diversity, complexity, and scalability. Furthermore, we provide benchmark evaluations on underwater object detection and USIS tasks using USIS16K. To facilitate future research in this domain, the dataset and benchmark models are publicly available.",
      "authors": [
        "Lin Hong and Xin Wang and Yihao Li and Xia Wang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19472",
        "HTML": "https://arxiv.org/html/2506.19472",
        "PDF": "https://arxiv.org/pdf/2506.19472"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 09:58:01 GMT",
          "size": "32078kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "USIS16K: High-Quality Dataset for Underwater Salient Instance Segmentation",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper introduces a new high-quality dataset for underwater instance segmentation, directly related to issues of dataset quality, representativeness, and scope. This has clear implications for LLM training data."
      }
    },
    {
      "id": "2506.19481",
      "abstract": "Refactoring is a constant activity in software development and maintenance. Scale and maintain software systems are based on code refactoring. However, this process is still labor intensive, as it requires programmers to analyze the codebases in detail to avoid introducing new defects. In this research, we put forward a large language model (LLM)-based multi-agent system to automate the refactoring process on Haskell code. The objective of this research is to evaluate the effect of LLM-based agents in performing structured and semantically accurate refactoring on Haskell code. Our proposed multi-agent system based on specialized agents with distinct roles, including code analysis, refactoring execution, verification, and debugging. To test the effectiveness and practical applicability of the multi-agent system, we conducted evaluations using different open-source Haskell codebases. The results of the experiments carried out showed that the proposed LLM-based multi-agent system could average 11.03% decreased complexity in code, an improvement of 22.46% in overall code quality, and increase performance efficiency by an average of 13.27%. Furthermore, memory allocation was optimized by up to 14.57%. These results highlight the ability of LLM-based multi-agent in managing refactoring tasks targeted toward functional programming paradigms. Our findings hint that LLM-based multi-agent systems integration into the refactoring of functional programming languages can enhance maintainability and support automated development workflows.",
      "authors": [
        "Shahbaz Siddeeq",
        "Muhammad Waseem",
        "Zeeshan Rasheed",
        "Md Mahade Hasan",
        "Jussi Rasku",
        "Mika Saari",
        "Henri Terho",
        "Kalle Makela",
        "Kai-Kristian Kemell and Pekka Abrahamsson"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19481",
        "HTML": "https://arxiv.org/html/2506.19481",
        "PDF": "https://arxiv.org/pdf/2506.19481"
      },
      "subjects": [
        "Software Engineering (cs.SE)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 10:17:34 GMT",
          "size": "2119kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "LLM-based Multi-Agent System for Intelligent Refactoring of Haskell Code",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper discusses the use of LLM-based multi-agent systems for intelligent refactoring of Haskell code, explicitly involving LLMs and their application, thus directly relevant to LLM training and methodologies."
      }
    },
    {
      "id": "2506.19483",
      "abstract": "This paper provides preliminary results on exploring the task of performing turn-level data augmentation for dialogue system based on different types of commonsense relationships, and the automatic evaluation of the generated synthetic turns. The proposed methodology takes advantage of the extended knowledge and zero-shot capabilities of pretrained Large Language Models (LLMs) to follow instructions, understand contextual information, and their commonsense reasoning capabilities. The approach draws inspiration from methodologies like Chain-of-Thought (CoT), applied more explicitly to the task of prompt-based generation for dialogue-based data augmentation conditioned on commonsense attributes, and the automatic evaluation of the generated dialogues.\n  To assess the effectiveness of the proposed approach, first we extracted 200 randomly selected partial dialogues, from 5 different well-known dialogue datasets, and generate alternative responses conditioned on different event commonsense attributes. This novel dataset allows us to measure the proficiency of LLMs in generating contextually relevant commonsense knowledge, particularly up to 12 different specific ATOMIC [10] database relations. Secondly, we propose an evaluation framework to automatically detect the quality of the generated dataset inspired by the ACCENT [26] metric, which offers a nuanced approach to assess event commonsense. However, our method does not follow ACCENT's complex eventrelation tuple extraction process. Instead, we propose an instruction-based prompt for each commonsense attribute and use state-of-the-art LLMs to automatically detect the original attributes used when creating each augmented turn in the previous step.\n  Preliminary results suggest that our approach effectively harnesses LLMs capabilities for commonsense reasoning and evaluation in dialogue systems.",
      "authors": [
        "Marcos Estecha-Garitagoitia",
        "Chen Zhang",
        "Mario Rodr\\'iguez-Cantelar",
        "Luis Fernando D'Haro"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19483",
        "HTML": "https://arxiv.org/html/2506.19483",
        "PDF": "https://arxiv.org/pdf/2506.19483"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 10:18:05 GMT",
          "size": "426kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Commonsense Generation and Evaluation for Dialogue Systems using Large Language Models",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper centers on using LLMs for dialogue systems, with an explicit focus on data augmentation and evaluation, making it directly relevant to LLM training data."
      }
    },
    {
      "id": "2506.19563",
      "abstract": "Large Language Models (LLMs) are widely used in sensitive domains, including healthcare, finance, and legal services, raising concerns about potential private information leaks during inference. Privacy extraction attacks, such as jailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the models to output sensitive information. However, these attacks cannot verify whether the extracted private information is accurate, as no public datasets exist for cross-validation, leaving a critical gap in private information detection during inference. To address this, we propose PrivacyXray, a novel framework detecting privacy breaches by analyzing LLM inner states. Our analysis reveals that LLMs exhibit higher semantic coherence and probabilistic certainty when generating correct private outputs. Based on this, PrivacyXray detects privacy breaches using four metrics: intra-layer and inter-layer semantic similarity, token-level and sentence-level probability distributions. PrivacyXray addresses critical challenges in private information detection by overcoming the lack of open-source private datasets and eliminating reliance on external data for validation. It achieves this through the synthesis of realistic private data and a detection mechanism based on the inner states of LLMs. Experiments show that PrivacyXray achieves consistent performance, with an average accuracy of 92.69% across five LLMs. Compared to state-of-the-art methods, PrivacyXray achieves significant improvements, with an average accuracy increase of 20.06%, highlighting its stability and practical utility in real-world applications.",
      "authors": [
        "Jinwen He",
        "Yiyang Lu",
        "Zijin Lin",
        "Kai Chen",
        "Yue Zhao"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19563",
        "HTML": "https://arxiv.org/html/2506.19563",
        "PDF": "https://arxiv.org/pdf/2506.19563"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 12:22:59 GMT",
          "size": "3414kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic Consistency and Probability Certainty",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "Focuses on privacy breaches in LLMs, involving privacy extraction attacks and detection of sensitive information leaks, directly relating to data privacy in LLM training datasets."
      }
    },
    {
      "id": "2506.19624",
      "abstract": "The widespread lack of broad source code verification on blockchain explorers such as Etherscan, where despite 78,047,845 smart contracts deployed on Ethereum (as of May 26, 2025), a mere 767,520 (< 1%) are open source, presents a severe impediment to blockchain security. This opacity necessitates the automated semantic analysis of on-chain smart contract bytecode, a fundamental research challenge with direct implications for identifying vulnerabilities and understanding malicious behavior. Prevailing decompilers struggle to reverse bytecode in a readable manner, often yielding convoluted code that critically hampers vulnerability analysis and thwarts efforts to dissect contract functionalities for security auditing.\n  This paper addresses this challenge by introducing a pioneering decompilation pipeline that, for the first time, successfully leverages Large Language Models (LLMs) to transform Ethereum Virtual Machine (EVM) bytecode into human-readable and semantically faithful Solidity code. Our novel methodology first employs rigorous static program analysis to convert bytecode into a structured three-address code (TAC) representation. This intermediate representation then guides a Llama-3.2-3B model, specifically fine-tuned on a comprehensive dataset of 238,446 TAC-to-Solidity function pairs, to generate high-quality Solidity. This approach uniquely recovers meaningful variable names, intricate control flow, and precise function signatures. Our extensive empirical evaluation demonstrates a significant leap beyond traditional decompilers, achieving an average semantic similarity of 0.82 with original source and markedly superior readability. The practical viability and effectiveness of our research are demonstrated through its implementation in a publicly accessible system, available at https://evmdecompiler.com.",
      "authors": [
        "Isaac David",
        "Liyi Zhou",
        "Dawn Song",
        "Arthur Gervais",
        "Kaihua Qin"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19624",
        "HTML": "https://arxiv.org/html/2506.19624",
        "PDF": "https://arxiv.org/pdf/2506.19624"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 13:42:59 GMT",
          "size": "875kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Decompiling Smart Contracts with a Large Language Model",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper discusses the use of a large language model in the decompilation of smart contracts, emphasizing training data in LLM by detailing its fine-tuning on a dataset of TAC-to-Solidity pairs."
      }
    },
    {
      "id": "2506.19697",
      "abstract": "Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, hindering efficient on-device deployment. While channel-wise operations and adaptive gradient scaling are recognized causes, practical mitigation remains challenging. We introduce Outlier-Safe Pre-Training (OSP), a practical guideline that proactively prevents outlier formation rather than relying on post-hoc mitigation. OSP combines three key innovations: (1) the Muon optimizer, eliminating privileged bases while maintaining training efficiency; (2) Single-Scale RMSNorm, preventing channel-wise amplification; and (3) a learnable embedding projection, redistributing activation magnitudes originating from embedding matrices. We validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is the first production-scale LLM trained without such outliers. Under aggressive 4-bit quantization, our OSP model achieves a 35.7 average score across 10 benchmarks (compared to 26.5 for an Adam-trained model), with only a 2% training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis (0.04) compared to extreme values (1818.56) in standard models, fundamentally altering LLM quantization behavior. Our work demonstrates that outliers are not inherent to LLMs but are consequences of training strategies, paving the way for more efficient LLM deployment. The source code and pretrained checkpoints are available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",
      "authors": [
        "Jungwoo Park",
        "Taewhoo Lee",
        "Chanwoong Yoon",
        "Hyeon Hwang",
        "Jaewoo Kang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19697",
        "HTML": "https://arxiv.org/html/2506.19697",
        "PDF": "https://arxiv.org/pdf/2506.19697"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 15:03:57 GMT",
          "size": "10136kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper directly addresses the training process of LLMs, proposing methods (Outlier-Safe Pre-Training) to enhance training and quantization, which is closely linked to LLM training data discussions."
      },
      "models": [
        {
          "model_path": "dmis-lab/OSP-1.4B-100B-Adam",
          "downloads": "0",
          "likes": "2",
          "trending_score": "2.0",
          "link": "https://huggingface.co/dmis-lab/OSP-1.4B-100B-Adam"
        },
        {
          "model_path": "dmis-lab/OSP-1.4B-100B-Muon",
          "downloads": "0",
          "likes": "2",
          "trending_score": "2.0",
          "link": "https://huggingface.co/dmis-lab/OSP-1.4B-100B-Muon"
        },
        {
          "model_path": "dmis-lab/OSP-1.4B-100B-Muon-EmbProj",
          "downloads": "0",
          "likes": "2",
          "trending_score": "2.0",
          "link": "https://huggingface.co/dmis-lab/OSP-1.4B-100B-Muon-EmbProj"
        },
        {
          "model_path": "dmis-lab/OSP-1.4B-100B-Muon-Only",
          "downloads": "0",
          "likes": "2",
          "trending_score": "2.0",
          "link": "https://huggingface.co/dmis-lab/OSP-1.4B-100B-Muon-Only"
        },
        {
          "model_path": "dmis-lab/OSP-1.4B-100B-Muon-SSNorm",
          "downloads": "0",
          "likes": "2",
          "trending_score": "2.0",
          "link": "https://huggingface.co/dmis-lab/OSP-1.4B-100B-Muon-SSNorm"
        },
        {
          "model_path": "dmis-lab/OSP-1.4B-100B-Muon-SSNorm-EmbProj",
          "downloads": "0",
          "likes": "2",
          "trending_score": "2.0",
          "link": "https://huggingface.co/dmis-lab/OSP-1.4B-100B-Muon-SSNorm-EmbProj"
        },
        {
          "model_path": "dmis-lab/OSP-1.4B-100B-Shampoo-SSNorm",
          "downloads": "0",
          "likes": "2",
          "trending_score": "2.0",
          "link": "https://huggingface.co/dmis-lab/OSP-1.4B-100B-Shampoo-SSNorm"
        },
        {
          "model_path": "dmis-lab/OSP-1.4B-100B-Shampoo-SSNorm-EmbProj",
          "downloads": "0",
          "likes": "2",
          "trending_score": "2.0",
          "link": "https://huggingface.co/dmis-lab/OSP-1.4B-100B-Shampoo-SSNorm-EmbProj"
        },
        {
          "model_path": "dmis-lab/OSP-1.4B-1T-Adam",
          "downloads": "0",
          "likes": "2",
          "trending_score": "2.0",
          "link": "https://huggingface.co/dmis-lab/OSP-1.4B-1T-Adam"
        },
        {
          "model_path": "dmis-lab/OSP-1.4B-1T-Muon-SSNorm-EmbProj",
          "downloads": "0",
          "likes": "2",
          "trending_score": "2.0",
          "link": "https://huggingface.co/dmis-lab/OSP-1.4B-1T-Muon-SSNorm-EmbProj"
        }
      ]
    },
    {
      "id": "2506.19794",
      "abstract": "Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate models across three dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities.",
      "authors": [
        "Yuqi Zhu",
        "Yi Zhong",
        "Jintian Zhang",
        "Ziheng Zhang",
        "Shuofei Qiao",
        "Yujie Luo",
        "Lun Du",
        "Da Zheng",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19794",
        "HTML": "https://arxiv.org/html/2506.19794",
        "PDF": "https://arxiv.org/pdf/2506.19794"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Information Retrieval (cs.IR)",
        "Machine Learning (cs.LG)",
        "Multiagent Systems (cs.MA)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 17:04:23 GMT",
          "size": "1401kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper explicitly discusses strategies to enhance LLM capabilities via curated seed datasets and data quality analysis, which directly pertains to LLM training data."
      }
    },
    {
      "id": "2506.19802",
      "abstract": "Despite extensive research on Machine Learning-based Network Intrusion Detection Systems (ML-NIDS), their capability to detect diverse attack variants remains uncertain. Prior studies have largely relied on homogeneous datasets, which artificially inflate performance scores and offer a false sense of security. Designing systems that can effectively detect a wide range of attack variants remains a significant challenge. The progress of ML-NIDS continues to depend heavily on human expertise, which can embed subjective judgments of system designers into the model, potentially hindering its ability to generalize across diverse attack types.\n  To address this gap, we propose KnowML, a framework for knowledge-guided machine learning that integrates attack knowledge into ML-NIDS. KnowML systematically explores the threat landscape by leveraging Large Language Models (LLMs) to perform automated analysis of attack implementations. It constructs a unified Knowledge Graph (KG) of attack strategies, on which it applies symbolic reasoning to generate KG-Augmented Input, embedding domain knowledge directly into the design process of ML-NIDS.\n  We evaluate KnowML on 28 realistic attack variants, of which 10 are newly collected for this study. Our findings reveal that baseline ML-NIDS models fail to detect several variants entirely, achieving F1 scores as low as 0 %. In contrast, our knowledge-guided approach achieves up to 99 % F1 score while maintaining a False Positive Rate below 0.1 %.",
      "authors": [
        "Xin Fan Guo",
        "Albert Merono Penuela",
        "Sergio Maffeis",
        "Fabio Pierazzi"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19802",
        "HTML": "https://arxiv.org/html/2506.19802",
        "PDF": "https://arxiv.org/pdf/2506.19802"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Information Retrieval (cs.IR)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 17:08:58 GMT",
          "size": "400kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "KnowML: Improving Generalization of ML-NIDS with Attack Knowledge Graphs",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper discusses leveraging Large Language Models to analyze attack implementations as part of its methodology, which involves constructing knowledge graphs that improve ML-NIDS. It touches on how LLMs are used in the context of training data and integration in machine learning processes."
      }
    },
    {
      "id": "2506.19848",
      "abstract": "This paper presents ScaleCap, an inference-time scalable image captioning strategy that generates comprehensive and detailed image captions. The key challenges of high-quality image captioning lie in the inherent biases of LVLMs: multimodal bias resulting in imbalanced descriptive granularity, offering detailed accounts of some elements while merely skimming over others; linguistic bias leading to hallucinated descriptions of non-existent objects. To address these issues, we propose a scalable debiased captioning strategy, which continuously enriches and calibrates the caption with increased inference budget. Specifically, we propose two novel components: heuristic question answering and contrastive sentence rating. The former generates content-specific questions based on the image and answers them to progressively inject relevant information into the caption. The latter employs sentence-level offline contrastive decoding to effectively identify and eliminate hallucinations caused by linguistic biases. With increased inference cost, more heuristic questions are raised by ScaleCap to progressively capture additional visual details, generating captions that are more accurate, balanced, and informative. Extensive modality alignment experiments demonstrate the effectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them for LVLM pretraining leads to consistent performance gains across 11 widely used benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity of generated captions with two additional tasks: replacing images with captions in VQA task, and reconstructing images from captions to assess semantic coverage. Code is available at https://github.com/Cooperx521/ScaleCap.",
      "authors": [
        "Long Xing",
        "Qidong Huang",
        "Xiaoyi Dong",
        "Pan Zhang",
        "Yuhang Zang",
        "Yuhang Cao",
        "Jinsong Li",
        "Shuangrui Ding",
        "Weiming Zhang",
        "Nenghai Yu",
        "Jiaqi Wang",
        "Feng Wu",
        "Dahua Lin"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19848",
        "HTML": "https://arxiv.org/html/2506.19848",
        "PDF": "https://arxiv.org/pdf/2506.19848"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 17:59:55 GMT",
          "size": "4857kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality Debiasing",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper discusses ScaleCap, an image captioning approach that generates captions for 450K images used in LVLM pretraining. This relates directly to LLM training data as it involves creating and using a dataset for training purposes."
      },
      "datasets": [
        {
          "dataset_name": "long-xing1/ScaleCap-450k",
          "downloads": "0",
          "likes": "3",
          "link": "https://huggingface.co/datasets/long-xing1/ScaleCap-450k"
        }
      ]
    },
    {
      "id": "2404.01799",
      "abstract": "Many existing benchmarks of large (multimodal) language models (LLMs) focus on measuring LLMs' academic proficiency, often with also an interest in comparing model performance with human test takers'. While such benchmarks have proven key to the development of LLMs, they suffer from several limitations, including questionable measurement quality (e.g., Do they measure what they are supposed to in a reliable way?), lack of quality assessment on the item level (e.g., Are some items more important or difficult than others?) and unclear human population reference (e.g., To whom can the model be compared?). In response to these challenges, we propose leveraging knowledge from psychometrics -- a field dedicated to the measurement of latent variables like academic proficiency -- into LLM benchmarking. We make four primary contributions. First, we reflect on current LLM benchmark developments and contrast them with psychometrics-based test development. Second, we introduce PATCH: a novel framework for {P}sychometrics-{A}ssis{T}ed ben{CH}marking of LLMs. PATCH addresses the aforementioned limitations. In particular, PATCH enables valid comparison between LLMs and human populations. Third, we demonstrate PATCH by measuring several LLMs' proficiency in 8th grade mathematics against 56 human populations. We show that adopting a psychometrics-based approach yields evaluation outcomes that diverge from those based on current benchmarking practices. Fourth, we release 4 high-quality datasets to support measuring and comparing LLM proficiency in grade school mathematics and science with human populations.",
      "authors": [
        "Qixiang Fang",
        "Daniel L. Oberski",
        "Dong Nguyen"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2404.01799",
        "HTML": "https://arxiv.org/html/2404.01799",
        "PDF": "https://arxiv.org/pdf/2404.01799"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 02 Apr 2024 09:58:57 GMT",
          "size": "130kb",
          "version": "v1"
        },
        {
          "date": "Thu, 25 Jul 2024 13:12:47 GMT",
          "size": "197kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 13:11:54 GMT",
          "size": "219kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "PATCH! {P}sychometrics-{A}ssis{T}ed Ben{CH}marking of Large Language Models against Human Populations: A Case Study of Proficiency in 8th Grade Mathematics",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper discusses benchmarking LLMs using psychometric techniques and introduces datasets specifically for this purpose, directly relating to LLM training data and its evaluation."
      },
      "tasks": [
        "Benchmarking"
      ],
      "repo_urls": [
        "https://github.com/fqixiang/patch_llm_benchmarking_with_psychometrics"
      ]
    },
    {
      "id": "2406.09838",
      "abstract": "Meteorological heatmaps play a vital role in deciphering extreme weather phenomena, yet their inherent complexities marked by irregular contours, unstructured patterns, and complex color variations present unique analytical hurdles for state-of-the-art Vision-Language Models (VLMs). Current state-of-the-art models like GPT-4o, Qwen-VL, and LLaVA 1.6 struggle with tasks such as precise color identification and spatial localization, resulting in inaccurate or incomplete interpretations. To address these challenges, we introduce Sparse Position and Outline Tracking (SPOT), a novel algorithm specifically designed to process irregularly shaped colored regions in visual data. SPOT identifies and localizes these regions by extracting their spatial coordinates, enabling structured representations of irregular shapes. Building on SPOT, we construct ClimateIQA, a novel meteorological visual question answering (VQA) dataset, comprising 26,280 high-resolution heatmaps and 762,120 instruction samples for wind gust, total precipitation, wind chill index and heat index analysis. ClimateIQA enhances VLM training by incorporating spatial cues, geographic metadata, and reanalysis data, improving model accuracy in interpreting and describing extreme weather features. Furthermore, we develop Climate-Zoo, a suite of fine-tuned VLMs based on SPOT-empowered ClimateIQA, which significantly outperforms existing models in meteorological heatmap tasks.",
      "authors": [
        "Jian Chen",
        "Peilin Zhou",
        "Yining Hua",
        "Dading Chong",
        "Meng Cao",
        "Yaowei Li",
        "Zixuan Yuan",
        "Bing Zhu",
        "Junwei Liang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2406.09838",
        "HTML": "https://arxiv.org/html/2406.09838",
        "PDF": "https://arxiv.org/pdf/2406.09838"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Fri, 14 Jun 2024 08:46:44 GMT",
          "size": "8338kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 03:53:09 GMT",
          "size": "2933kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "ClimateIQA: A New Dataset and Benchmark to Advance Vision-Language Models in Meteorology Anomalies Analysis",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper introduces ClimateIQA, a new dataset for enhancing vision-language models, directly focusing on dataset creation, which is relevant to LLM training data concerns like data collection and model improvement."
      },
      "datasets": [
        {
          "dataset_name": "GPS-Lab/ClimateIQA",
          "downloads": "44",
          "likes": "4",
          "link": "https://huggingface.co/datasets/GPS-Lab/ClimateIQA"
        }
      ],
      "tasks": [
        "Question Answering",
        "Visual Question Answering",
        "Visual Question Answering (VQA)"
      ],
      "repo_urls": [
        "https://github.com/AlexJJJChen/Climate-Zoo"
      ]
    },
    {
      "id": "2411.06102",
      "abstract": "With the proliferation of Large Language Models (LLMs) in Business Intelligence (BI), existing solutions face critical challenges in industrial deployments: functionality deficiencies from legacy systems failing to meet evolving LLM-era user demands, interaction limitations from single-round SQL generation paradigms inadequate for multi-round clarification, and cost for domain adaptation arising from cross-domain methods migration.\n  We present SiriusBI, a practical LLM-powered BI system addressing the challenges of industrial deployments through three key innovations: (a) An end-to-end architecture integrating multi-module coordination to overcome functionality gaps in legacy systems; (b) A multi-round dialogue with querying mechanism, consisting of semantic completion, knowledge-guided clarification, and proactive querying processes, to resolve interaction constraints in SQL generation; (c) A data-conditioned SQL generation method selection strategy that supports both an efficient one-step Fine-Tuning approach and a two-step method leveraging Semantic Intermediate Representation for low-cost cross-domain applications. Experiments on both real-world datasets and public benchmarks demonstrate the effectiveness of SiriusBI. User studies further confirm that SiriusBI enhances both productivity and user experience.\n  As an independent service on Tencent's data platform, SiriusBI is deployed across finance, advertising, and cloud sectors, serving dozens of enterprise clients. It achieves over 93% accuracy in SQL generation and reduces data analysts' query time from minutes to seconds in real-world applications.",
      "authors": [
        "Jie Jiang",
        "Haining Xie",
        "Siqishen",
        "Yu Shen",
        "Zihan Zhang",
        "Meng Lei",
        "Yifeng Zheng",
        "Yang Li",
        "Chunyou Li",
        "Danqing Huang",
        "Yinjun Wu",
        "Wentao Zhang",
        "Xiaofeng Yang",
        "Bin Cui",
        "Peng Chen"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2411.06102",
        "HTML": "https://arxiv.org/html/2411.06102",
        "PDF": "https://arxiv.org/pdf/2411.06102"
      },
      "subjects": [
        "Databases (cs.DB)"
      ],
      "submission_historys": [
        {
          "date": "Sat, 09 Nov 2024 07:32:40 GMT",
          "size": "14720kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 11:23:34 GMT",
          "size": "1584kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "SiriusBI: A Comprehensive LLM-Powered Solution for Data Analytics in Business Intelligence",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper presents SiriusBI, a system that leverages LLMs for data analytics and discusses methods such as SQL generation which may involve issues around LLM training data conditioning and adaptation."
      }
    },
    {
      "id": "2411.19832",
      "abstract": "The detection of sensitive content in large datasets is crucial for ensuring that shared and analysed data is free from harmful material. However, current moderation tools, such as external APIs, suffer from limitations in customisation, accuracy across diverse sensitive categories, and privacy concerns. Additionally, existing datasets and open-source models focus predominantly on toxic language, leaving gaps in detecting other sensitive categories such as substance abuse or self-harm. In this paper, we put forward a unified dataset tailored for social media content moderation across six sensitive categories: conflictual language, profanity, sexually explicit material, drug-related content, self-harm, and spam. By collecting and annotating data with consistent retrieval strategies and guidelines, we address the shortcomings of previous focalised research. Our analysis demonstrates that fine-tuning large language models (LLMs) on this novel dataset yields significant improvements in detection performance compared to open off-the-shelf models such as LLaMA, and even proprietary OpenAI models, which underperform by 10-15% overall. This limitation is even more pronounced on popular moderation APIs, which cannot be easily tailored to specific sensitive content categories, among others.",
      "authors": [
        "Dimosthenis Antypas",
        "Indira Sen",
        "Carla Perez-Almendros",
        "Jose Camacho-Collados",
        "Francesco Barbieri"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2411.19832",
        "HTML": "https://arxiv.org/html/2411.19832",
        "PDF": "https://arxiv.org/pdf/2411.19832"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Fri, 29 Nov 2024 16:44:02 GMT",
          "size": "9217kb",
          "version": "v1"
        },
        {
          "date": "Fri, 06 Dec 2024 13:41:53 GMT",
          "size": "9218kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 16:31:28 GMT",
          "size": "8613kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Sensitive Content Classification in Social Media: A Holistic Resource and Evaluation",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper discusses creating a dataset for social media content moderation and fine-tuning LLMs with it, focusing directly on LLM training data and its enhancement through dataset development and analysis."
      },
      "models": [
        {
          "model_path": "cardiffnlp/twitter-roberta-large-sensitive-binary",
          "downloads": "1768",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/cardiffnlp/twitter-roberta-large-sensitive-binary"
        },
        {
          "model_path": "cardiffnlp/twitter-roberta-large-sensitive-multilabel",
          "downloads": "1833",
          "likes": "3",
          "trending_score": "0.0",
          "link": "https://huggingface.co/cardiffnlp/twitter-roberta-large-sensitive-multilabel"
        },
        {
          "model_path": "cardiffnlp/twitter-roberta-base-sensitive-multilabel",
          "downloads": "1609",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/cardiffnlp/twitter-roberta-base-sensitive-multilabel"
        },
        {
          "model_path": "cardiffnlp/twitter-roberta-base-sensitive-binary",
          "downloads": "25",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/cardiffnlp/twitter-roberta-base-sensitive-binary"
        }
      ],
      "datasets": [
        {
          "dataset_name": "cardiffnlp/x_sensitive",
          "downloads": "77",
          "likes": "5",
          "link": "https://huggingface.co/datasets/cardiffnlp/x_sensitive"
        }
      ],
      "tasks": []
    },
    {
      "id": "2412.05153",
      "abstract": "Access to large-scale high-quality healthcare databases is key to accelerate medical research and make insightful discoveries about diseases. However, access to such data is often limited by patient privacy concerns, data sharing restrictions and high costs. To overcome these limitations, synthetic patient data has emerged as an alternative. However, synthetic data generation (SDG) methods typically rely on machine learning (ML) models trained on original data, leading back to the data scarcity problem. We propose an approach to generate synthetic tabular patient data that does not require access to the original data, but only a description of the desired database. We leverage prior medical knowledge and in-context learning capabilities of large language models (LLMs) to generate realistic patient data, even in a low-resource setting. We quantitatively evaluate our approach against state-of-the-art SDG models, using fidelity, privacy, and utility metrics. Our results show that while LLMs may not match the performance of state-of-the-art models trained on the original data, they effectively generate realistic patient data with well-preserved clinical correlations. An ablation study highlights key elements of our prompt contributing to high-quality synthetic patient data generation. This approach, which is easy to use and does not require original data or advanced ML skills, is particularly valuable for quickly generating custom-designed patient data, supporting project implementation and providing educational resources.",
      "authors": [
        "Margaux Tornqvist",
        "Jean-Daniel Zucker",
        "Tristan Fauvel",
        "Nicolas Lambert",
        "Mathilde Berthelot",
        "Antoine Movschin"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2412.05153",
        "HTML": "https://arxiv.org/html/2412.05153",
        "PDF": "https://arxiv.org/pdf/2412.05153"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Fri, 06 Dec 2024 16:10:40 GMT",
          "size": "1047kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 13:24:58 GMT",
          "size": "777kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "A text-to-tabular approach to generate synthetic patient data using LLMs",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper explicitly discusses generating synthetic data using LLMs, focusing on the methodology of data generation. It evaluates data quality through fidelity, privacy, and utility metrics, which are central to LLM training data considerations."
      },
      "tasks": [
        "In-Context Learning",
        "Synthetic Data Generation"
      ],
      "repo_urls": [
        "https://github.com/quinten-health-os/synth-data-gen-from-text"
      ]
    },
    {
      "id": "2501.10326",
      "abstract": "Large language models (LLMs) have significantly impacted human society, influencing various domains. Among them, academia is not simply a domain affected by LLMs, but it is also the pivotal force in the development of LLMs. In academic publication, this phenomenon is represented during the incorporation of LLMs into the peer review mechanism for reviewing manuscripts. LLMs hold transformative potential for the full-scale implementation of automated scholarly paper review (ASPR), but they also pose new issues and challenges that need to be addressed. In this survey paper, we aim to provide a holistic view of ASPR in the era of LLMs. We begin with a survey to find out which LLMs are used to conduct ASPR. Then, we review what ASPR-related technological bottlenecks have been solved with the incorporation of LLM technology. After that, we move on to explore new methods, new datasets, new source code, and new online systems that come with LLMs for ASPR. Furthermore, we summarize the performance and issues of LLMs in ASPR, and investigate the attitudes and reactions of publishers and academia to ASPR. Lastly, we discuss the challenges and future directions associated with the development of LLMs for ASPR. This survey serves as an inspirational reference for the researchers and can promote the progress of ASPR for its actual implementation.",
      "authors": [
        "Zhenzhen Zhuang",
        "Jiandong Chen",
        "Hongfeng Xu",
        "Yuwen Jiang",
        "Jialiang Lin"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2501.10326",
        "HTML": "https://arxiv.org/html/2501.10326",
        "PDF": "https://arxiv.org/pdf/2501.10326"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Digital Libraries (cs.DL)"
      ],
      "submission_historys": [
        {
          "date": "Fri, 17 Jan 2025 17:56:58 GMT",
          "size": "60kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 16:45:26 GMT",
          "size": "2088kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Large language models for automated scholarly paper review: A survey",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper explicitly surveys the use of LLMs in automating scholarly paper review, discussing datasets and methods related to LLMs, which directly pertains to the topic of LLM training data."
      },
      "tasks": [
        "Survey"
      ]
    },
    {
      "id": "2502.02514",
      "abstract": "Image AutoRegressive generation has emerged as a new powerful paradigm with image autoregressive models (IARs) matching state-of-the-art diffusion models (DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for a higher generation speed. However, the privacy risks associated with IARs remain unexplored, raising concerns regarding their responsible deployment. To address this gap, we conduct a comprehensive privacy analysis of IARs, comparing their privacy risks to the ones of DMs as reference points. Concretely, we develop a novel membership inference attack (MIA) that achieves a remarkably high success rate in detecting training images (with a True Positive Rate at False Positive Rate = 1% of 86.38% vs. 6.38% for DMs with comparable attacks). We leverage our novel MIA to provide dataset inference (DI) for IARs, and show that it requires as few as 6 samples to detect dataset membership (compared to 200 for DI in DMs), confirming a higher information leakage in IARs. Finally, we are able to extract hundreds of training data points from an IAR (e.g., 698 from VAR-d30). Our results suggest a fundamental privacy-utility trade-off: while IARs excel in image generation quality and speed, they are empirically significantly more vulnerable to privacy attacks compared to DMs that achieve similar performance. We release the code at https://github.com/sprintml/privacy_attacks_against_iars for reproducibility.",
      "authors": [
        "Antoni Kowalczuk",
        "Jan Dubi\\'nski",
        "Franziska Boenisch",
        "Adam Dziedzic"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2502.02514",
        "HTML": "https://arxiv.org/html/2502.02514",
        "PDF": "https://arxiv.org/pdf/2502.02514"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 04 Feb 2025 17:33:08 GMT",
          "size": "16046kb",
          "version": "v1"
        },
        {
          "date": "Tue, 08 Apr 2025 17:28:09 GMT",
          "size": "18305kb",
          "version": "v2"
        },
        {
          "date": "Wed, 09 Apr 2025 08:33:54 GMT",
          "size": "18305kb",
          "version": "v3"
        },
        {
          "date": "Tue, 24 Jun 2025 10:19:57 GMT",
          "size": "14247kb",
          "version": "v4"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Privacy Attacks on Image AutoRegressive Models",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper conducts a privacy analysis of image autoregressive models, including membership inference attacks and dataset inference, closely related to privacy and inference risks associated with training data."
      },
      "tasks": [
        "Inference Attack",
        "Membership Inference Attack"
      ],
      "repo_urls": [
        "https://github.com/sprintml/privacy_attacks_against_iars"
      ]
    },
    {
      "id": "2502.12743",
      "abstract": "Distinguishing between human- and LLM-generated texts is crucial given the risks associated with misuse of LLMs. This paper investigates detection and explanation capabilities of current LLMs across two settings: binary (human vs. LLM-generated) and ternary classification (including an ``undecided'' class). We evaluate 6 close- and open-source LLMs of varying sizes and find that self-detection (LLMs identifying their own outputs) consistently outperforms cross-detection (identifying outputs from other LLMs), though both remain suboptimal. Introducing a ternary classification framework improves both detection accuracy and explanation quality across all models. Through comprehensive quantitative and qualitative analyses using our human-annotated dataset, we identify key explanation failures, primarily reliance on inaccurate features, hallucinations, and flawed reasoning. Our findings underscore the limitations of current LLMs in self-detection and self-explanation, highlighting the need for further research to address overfitting and enhance generalizability.",
      "authors": [
        "Jiazhou Ji",
        "Jie Guo",
        "Weidong Qiu",
        "Zheng Huang",
        "Yang Xu",
        "Xinru Lu",
        "Xiaoyu Jiang",
        "Ruizhe Li",
        "Shujun Li"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2502.12743",
        "HTML": "https://arxiv.org/html/2502.12743",
        "PDF": "https://arxiv.org/pdf/2502.12743"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 18 Feb 2025 11:00:28 GMT",
          "size": "7703kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 16:03:17 GMT",
          "size": "52kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "\"I know myself better, but not really greatly\": How Well Can LLMs Detect and Explain LLM-Generated Texts?",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper directly investigates LLM capabilities in detecting and explaining LLM-generated texts, relating to the quality and representativeness of LLM outputs, which are tied to training data discussions."
      }
    },
    {
      "id": "2503.07584",
      "abstract": "In this work we study various Retrieval Augmented Regeneration (RAG) approaches to gain an understanding of the strengths and weaknesses of each approach in a question-answering analysis. To gain this understanding we use a case-study subset of the Global Database of Events, Language, and Tone (GDELT) dataset as well as a corpus of raw text scraped from the online news articles. To retrieve information from the text corpus we implement a traditional vector store RAG as well as state-of-the-art large language model (LLM) based approaches for automatically constructing KGs and retrieving the relevant subgraphs. In addition to these corpus approaches, we develop a novel ontology-based framework for constructing knowledge graphs (KGs) from GDELT directly which leverages the underlying schema of GDELT to create structured representations of global events. For retrieving relevant information from the ontology-based KGs we implement both direct graph queries and state-of-the-art graph retrieval approaches. We compare the performance of each method in a question-answering task. We find that while our ontology-based KGs are valuable for question-answering, automated extraction of the relevant subgraphs is challenging. Conversely, LLM-generated KGs, while capturing event summaries, often lack consistency and interpretability. Our findings suggest benefits of a synergistic approach between ontology and LLM-based KG construction, with proposed avenues toward that end.",
      "authors": [
        "Audun Myers",
        "Max Vargas",
        "Sinan G. Aksoy",
        "Cliff Joslyn",
        "Benjamin Wilson",
        "Lee Burke",
        "Tom Grimes"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.07584",
        "HTML": "https://arxiv.org/html/2503.07584",
        "PDF": "https://arxiv.org/pdf/2503.07584"
      },
      "subjects": [
        "Information Retrieval (cs.IR)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 10 Mar 2025 17:48:10 GMT",
          "size": "13367kb",
          "version": "v1"
        },
        {
          "date": "Mon, 23 Jun 2025 17:39:34 GMT",
          "size": "2932kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 10:10:56 GMT",
          "size": "2932kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Talking to GDELT Through Knowledge Graphs",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper explores retrieval-augmented regeneration approaches, using both LLM and corpus data, involving the Global Database of Events, Language, and Tone (GDELT) dataset. It directly deals with methodologies that involve LLM training data."
      },
      "tasks": [
        "Articles",
        "Knowledge Graphs",
        "Large Language Model",
        "Question Answering",
        "RAG",
        "Retrieval"
      ]
    },
    {
      "id": "2504.04942",
      "abstract": "Automatically conjecturing useful, interesting and novel lemmas would greatly improve automated reasoning tools and lower the bar for formalizing mathematics in proof assistants. It is however a very challenging task for both neural and symbolic approaches. We present the first steps towards a practical neuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language Models (LLMs) and symbolic methods, and evaluate it on proof libraries for the Isabelle proof assistant. We train an LLM to generate lemma templates that describe the shape of a lemma, and use symbolic methods to fill in the details. We compare Lemmanaid against an LLM trained to generate complete lemma statements as well as previous fully symbolic conjecturing methods. Lemmanaid outperforms both neural and symbolic methods on test sets from Isabelle's HOL library and from its Archive of Formal Proofs, discovering between 29-39.5% of the gold standard human written lemmas. This is 8-15% more lemmas than the neural-only method. By leveraging the best of both symbolic and neural methods we can generate useful lemmas for a wide range of input domains, facilitating computer-assisted theory development and formalization.",
      "authors": [
        "Yousef Alhessi",
        "S\\'olr\\'un Halla Einarsd\\'ottir",
        "George Granberry",
        "Emily First",
        "Moa Johansson",
        "Sorin Lerner",
        "Nicholas Smallbone"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2504.04942",
        "HTML": "https://arxiv.org/html/2504.04942",
        "PDF": "https://arxiv.org/pdf/2504.04942"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Logic in Computer Science (cs.LO)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 07 Apr 2025 11:30:36 GMT",
          "size": "123kb",
          "version": "v1"
        },
        {
          "date": "Mon, 23 Jun 2025 12:21:10 GMT",
          "size": "139kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 14:21:33 GMT",
          "size": "139kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper explicitly involves training an LLM for lemma conjecturing, mentioning the training data, training process, and evaluation datasets, which are core topics related to LLM training data."
      },
      "tasks": [
        "LEMMA"
      ]
    },
    {
      "id": "2504.13816",
      "abstract": "While understanding the knowledge boundaries of LLMs is crucial to prevent hallucination, research on the knowledge boundaries of LLMs has predominantly focused on English. In this work, we present the first study to analyze how LLMs recognize knowledge boundaries across different languages by probing their internal representations when processing known and unknown questions in multiple languages. Our empirical studies reveal three key findings: 1) LLMs' perceptions of knowledge boundaries are encoded in the middle to middle-upper layers across different languages. 2) Language differences in knowledge boundary perception follow a linear structure, which motivates our proposal of a training-free alignment method that effectively transfers knowledge boundary perception ability across languages, thereby helping reduce hallucination risk in low-resource languages; 3) Fine-tuning on bilingual question pair translation further enhances LLMs' recognition of knowledge boundaries across languages. Given the absence of standard testbeds for cross-lingual knowledge boundary analysis, we construct a multilingual evaluation suite comprising three representative types of knowledge boundary data. Our code and datasets are publicly available at https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries.",
      "authors": [
        "Chenghao Xiao",
        "Hou Pong Chan",
        "Hao Zhang",
        "Mahani Aljunied",
        "Lidong Bing",
        "Noura Al Moubayed",
        "Yu Rong"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2504.13816",
        "HTML": "https://arxiv.org/html/2504.13816",
        "PDF": "https://arxiv.org/pdf/2504.13816"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Fri, 18 Apr 2025 17:44:12 GMT",
          "size": "30155kb",
          "version": "v1"
        },
        {
          "date": "Fri, 06 Jun 2025 04:24:25 GMT",
          "size": "4768kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 06:24:15 GMT",
          "size": "4760kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper analyzes LLMs' knowledge boundaries across languages, utilizing datasets for evaluation. It directly involves the study of data as related to LLMs, crossing into dataset construction and evaluation specifically for LLM applications."
      },
      "datasets": [
        {
          "dataset_name": "SeaLLMs/TrueFalse-Statements-multilingual",
          "downloads": "115",
          "likes": "1",
          "link": "https://huggingface.co/datasets/SeaLLMs/TrueFalse-Statements-multilingual"
        }
      ],
      "tasks": [
        "Hallucination"
      ],
      "repo_urls": [
        "https://github.com/damo-nlp-sg/llm-multilingual-knowledge-boundaries"
      ]
    },
    {
      "id": "2505.16078",
      "abstract": "With the emergence of ChatGPT, Transformer models have significantly advanced text classification and related tasks. Decoder-only models such as Llama exhibit strong performance and flexibility, yet they suffer from inefficiency on inference due to token-by-token generation, and their effectiveness in text classification tasks heavily depends on prompt quality. Moreover, their substantial GPU resource requirements often limit widespread adoption. Thus, the question of whether smaller language models are capable of effectively handling text classification tasks emerges as a topic of significant interest. However, the selection of appropriate models and methodologies remains largely underexplored. In this paper, we conduct a comprehensive evaluation of prompt engineering and supervised fine-tuning methods for transformer-based text classification. Specifically, we focus on practical industrial scenarios, including email classification, legal document categorization, and the classification of extremely long academic texts. We examine the strengths and limitations of smaller models, with particular attention to both their performance and their efficiency in Video Random-Access Memory (VRAM) utilization, thereby providing valuable insights for the local deployment and application of compact models in industrial settings.",
      "authors": [
        "Lujun Li",
        "Lama Sleem",
        "Niccolo' Gentile",
        "Geoffrey Nichil",
        "Radu State"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.16078",
        "HTML": "https://arxiv.org/html/2505.16078",
        "PDF": "https://arxiv.org/pdf/2505.16078"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 21 May 2025 23:39:24 GMT",
          "size": "157kb",
          "version": "v1"
        },
        {
          "date": "Fri, 23 May 2025 07:49:10 GMT",
          "size": "157kb",
          "version": "v2"
        },
        {
          "date": "Mon, 23 Jun 2025 20:09:36 GMT",
          "size": "157kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Small Language Models in the Real World: Insights from Industrial Text Classification",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper evaluates text classification using smaller language models and discusses methodologies like prompt engineering, which involves understanding and selecting training data for model efficiency."
      },
      "tasks": [
        "Classification",
        "Decoder",
        "Prompt Engineering",
        "text-classification",
        "Text Classification"
      ]
    },
    {
      "id": "2506.06877",
      "abstract": "Outcome-rewarded Large Language Models (LLMs) have demonstrated remarkable success in mathematical problem-solving. However, this success often masks a critical issue: models frequently achieve correct answers through fundamentally unsound reasoning processes, a phenomenon indicative of reward hacking. We introduce MathOlympiadEval, a new dataset with fine-grained annotations, which reveals a significant gap between LLMs' answer correctness and their low process correctness. Existing automated methods like LLM-as-a-judge struggle to reliably detect these reasoning flaws. To address this, we propose ParaStepVerifier, a novel methodology for meticulous, step-by-step verification of mathematical solutions. ParaStepVerifier identifies incorrect reasoning steps. Empirical results demonstrate that ParaStepVerifier substantially improves the accuracy of identifying flawed solutions compared to baselines, especially for complex, multi-step problems. This offers a more robust path towards evaluating and training LLMs with genuine mathematical reasoning.",
      "authors": [
        "Jiaxing Guo",
        "Wenjie Yang",
        "Shengzhong Zhang",
        "Tongshan Xu",
        "Lun Du",
        "Da Zheng",
        "Zengfeng Huang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.06877",
        "HTML": "https://arxiv.org/html/2506.06877",
        "PDF": "https://arxiv.org/pdf/2506.06877"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Sat, 07 Jun 2025 17:54:56 GMT",
          "size": "822kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 13:55:38 GMT",
          "size": "822kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper introduces a new dataset (MathOlympiadEval) specifically for evaluating LLMs' reasoning, which directly pertains to training data used in improving LLMs' performance."
      }
    },
    {
      "id": "2506.11903",
      "abstract": "Advances in transformer-based language models have highlighted the benefits of language-specific pre-training on high-quality corpora. In this context, German NLP stands to gain from updated architectures and modern datasets tailored to the linguistic characteristics of the German language. GeistBERT seeks to improve German language processing by incrementally training on a diverse corpus and optimizing model performance across various NLP tasks. It was pre-trained using fairseq with standard hyperparameters, initialized from GottBERT weights, and trained on a large-scale German corpus using Whole Word Masking (WWM). Based on the pre-trained model, we derived extended-input variants using Nystr\\\"omformer and Longformer architectures with support for sequences up to 8k tokens. While these long-context models were not evaluated on dedicated long-context benchmarks, they are included in our release. We assessed all models on NER (CoNLL 2003, GermEval 2014) and text classification (GermEval 2018 fine/coarse, 10kGNAD) using $F_1$ score and accuracy. The GeistBERT models achieved strong performance, leading all tasks among the base models and setting a new state-of-the-art (SOTA). Notably, the base models outperformed larger models in several tasks. To support the German NLP research community, we are releasing GeistBERT under the MIT license.",
      "authors": [
        "Raphael Scheible-Schmitt and Johann Frei"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.11903",
        "HTML": "https://arxiv.org/html/2506.11903",
        "PDF": "https://arxiv.org/pdf/2506.11903"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Fri, 13 Jun 2025 15:53:17 GMT",
          "size": "131kb",
          "version": "v1"
        },
        {
          "date": "Wed, 18 Jun 2025 23:06:09 GMT",
          "size": "131kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 12:31:06 GMT",
          "size": "131kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "GeistBERT: Breathing Life into German NLP",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper is explicitly focused on training data as it deals with creating and optimizing a German corpus for training a language model, specifically mentioning preprocessing and dataset use in training."
      },
      "models": [
        {
          "model_path": "GeistBERT/GeistBERT_base",
          "downloads": "730",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/GeistBERT/GeistBERT_base"
        },
        {
          "model_path": "GeistBERT/GeistBERT_base_longformer",
          "downloads": "873",
          "likes": "1",
          "trending_score": "0.0",
          "link": "https://huggingface.co/GeistBERT/GeistBERT_base_longformer"
        },
        {
          "model_path": "GeistBERT/GeistBERT_base_nystromformer",
          "downloads": "24",
          "likes": "1",
          "trending_score": "0.0",
          "link": "https://huggingface.co/GeistBERT/GeistBERT_base_nystromformer"
        }
      ],
      "tasks": [
        "8k",
        "NER",
        "text-classification",
        "Text Classification"
      ]
    },
    {
      "id": "2506.14293",
      "abstract": "We present Sleeping-DISCO 9M, a large-scale pre-training dataset for music and song. To the best of our knowledge, there are no open-source high-quality dataset representing popular and well-known songs for generative music modeling tasks such as text-music, music-captioning, singing-voice synthesis, melody reconstruction and cross-model retrieval. Past contributions focused on isolated and constrained factors whose core perspective was to create synthetic or re-recorded music corpus (e.g. GTSinger, M4Singer) and arbitrarily large-scale audio datasets (e.g. DISCO-10M and LAIONDISCO-12M) had been another focus for the community. Unfortunately, adoption of these datasets has been below substantial in the generative music community as these datasets fail to reflect real-world music and its flavour. Our dataset changes this narrative and provides a dataset that is constructed using actual popular music and world-renowned artists.",
      "authors": [
        "Tawsif Ahmed",
        "Andrej Radonjic",
        "Gollam Rabby"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.14293",
        "HTML": "https://arxiv.org/html/2506.14293",
        "PDF": "https://arxiv.org/pdf/2506.14293"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Machine Learning (cs.LG)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 17 Jun 2025 08:08:08 GMT",
          "size": "380kb",
          "version": "v1"
        },
        {
          "date": "Mon, 23 Jun 2025 18:39:59 GMT",
          "size": "380kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "SLEEPING-DISCO 9M: A large-scale pre-training dataset for generative music modeling",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper presents a large-scale pre-training dataset for generative music modeling, explicitly focusing on dataset creation and its application, which is highly relevant to LLM training data."
      },
      "datasets": [
        {
          "dataset_name": "sleeping-ai/Sleeping-DISCO-9M",
          "downloads": "707",
          "likes": "7",
          "link": "https://huggingface.co/datasets/sleeping-ai/Sleeping-DISCO-9M"
        }
      ],
      "tasks": [
        "Music Captioning",
        "Music Modeling",
        "Singing Voice Synthesis"
      ]
    },
    {
      "id": "2506.18269",
      "abstract": "This study introduces Co-Persona, a methodological framework bridging large-scale social media analysis with authentic user understanding through systematic integration of Large Language Models and expert validation. Through a case study of B.Co, a Chinese manufacturer, we investigated Co-Persona application in bedside lamp development. Our methodology analyzed over 38 million posts from Xiao Hongshu, employing multi-stage data processing combining advanced NLP with expert validation. Analysis revealed five user personas derived from bedtime behaviors: Health Aficionados, Night Owls, Interior Decorators, Child-care Workers, and Workaholics-each showing unique pre-sleep activities and product preferences. Findings demonstrate Co-Persona enhances manufacturers' ability to process large datasets while maintaining user understanding. The methodology provides structured approaches for targeted marketing and product strategies. Research contributes to theoretical understanding of data-driven persona development and practical applications in consumer-driven innovation. Code and data available at https://github.com/INFPa/LLMwithPersona.",
      "authors": [
        "Min Yin and Haoyu Liu and Boyi Lian and Chunlei Chai"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18269",
        "HTML": "https://arxiv.org/html/2506.18269",
        "PDF": "https://arxiv.org/pdf/2506.18269"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 03:54:30 GMT",
          "size": "2629kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 09:12:31 GMT",
          "size": "1173kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Co-persona: Leveraging LLMs and Expert Collaboration to Understand User Personas through Social Media Data Analysis",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "The paper discusses using large-scale social media data analysis and LLMs for understanding user personas, directly involving the methodology of handling large datasets relevant to LLM training."
      }
    },
    {
      "id": "2412.02065",
      "abstract": "Unequal access to costly datasets essential for empirical research has long hindered researchers from disadvantaged institutions, limiting their ability to contribute to their fields and advance their careers. Recent breakthroughs in Large Language Models (LLMs) have the potential to democratize data access by automating data collection from unstructured sources. We develop and evaluate a novel methodology using GPT-4o-mini within a Retrieval-Augmented Generation (RAG) framework to collect data from corporate disclosures. Our approach achieves human-level accuracy in collecting CEO pay ratios from approximately 10,000 proxy statements and Critical Audit Matters (CAMs) from more than 12,000 10-K filings, with LLM processing times of 9 and 40 minutes respectively, each at a cost under $10. This stands in stark contrast to the hundreds of hours needed for manual collection or the thousands of dollars required for commercial database subscriptions. To foster a more inclusive research community by empowering researchers with limited resources to explore new avenues of inquiry, we share our methodology and the resulting datasets.",
      "authors": [
        "Julian Junyan Wang",
        "Victor Xiaoqi Wang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2412.02065",
        "HTML": "https://arxiv.org/html/2412.02065",
        "PDF": "https://arxiv.org/pdf/2412.02065"
      },
      "subjects": [
        "General Finance (q-fin.GN)",
        "Artificial Intelligence (cs.AI)",
        "Computational Engineering, Finance, and Science (cs.CE)",
        "Machine Learning (cs.LG)",
        "General Economics (econ.GN)",
        "Economics (q-fin.EC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 03 Dec 2024 00:59:56 GMT",
          "size": "1670kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 02:52:00 GMT",
          "size": "1301kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Leveraging Large Language Models to Democratize Access to Costly Datasets for Academic Research",
      "relevance": {
        "keyword": "creativity",
        "level": "strong",
        "reason": "This paper targets the democratization of access to costly datasets using LLMs to automate data collection, directly engaging with the topic of LLM training data and data methodologies."
      },
      "tasks": [
        "RAG",
        "Retrieval-augmented Generation"
      ]
    },
    {
      "id": "2506.18923",
      "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in aiding developers with tasks like code comprehension, generation, and translation. Supporting multilingual programming -- i.e., coding tasks across multiple programming languages -- typically requires either (1) finetuning a single LLM across all programming languages, which is cost-efficient but sacrifices language-specific specialization and performance, or (2) finetuning separate LLMs for each programming language, which allows for specialization but is computationally expensive and storage-intensive due to the duplication of parameters. This paper introduces MoLE (Mix-of-Language-Experts), a novel architecture that balances efficiency and specialization for multilingual programming. MoLE is composed of a base model, a shared LoRA (low-rank adaptation) module, and a collection of language-specific LoRA modules. These modules are jointly optimized during the finetuning process, enabling effective knowledge sharing and specialization across programming languages. During inference, MoLE automatically routes to the language-specific LoRA module corresponding to the programming language of the code token being generated. Our experiments demonstrate that MoLE achieves greater parameter efficiency compared to training separate language-specific LoRAs, while outperforming a single shared LLM finetuned for all programming languages in terms of accuracy.",
      "authors": [
        "Yifan Zong",
        "Yuntian Deng",
        "Pengyu Nie"
      ],
      "last_revised_date": "2025/06/18",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18923",
        "HTML": "https://arxiv.org/html/2506.18923",
        "PDF": "https://arxiv.org/pdf/2506.18923"
      },
      "subjects": [
        "Programming Languages (cs.PL)",
        "Computation and Language (cs.CL)",
        "Software Engineering (cs.SE)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 18 Jun 2025 06:20:51 GMT",
          "size": "719kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/18",
      "title": "Mix-of-Language-Experts Architecture for Multilingual Programming",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses a model architecture for multilingual programming but indirectly touches on LLMs through topics like model finetuning and parameter efficiency."
      }
    },
    {
      "id": "2506.18931",
      "abstract": "Fine-tuning Large Language Models (LLMs) with Low-Rank Adaptation (LoRA) enhances adaptability while reducing computational costs. However, fine-tuning can compromise safety alignment, even with benign data, increasing susceptibility to harmful outputs. Existing safety alignment methods struggle to capture complex parameter shifts, leading to suboptimal safety-utility trade-offs. To address this issue, we propose Safe Pruning LoRA (SPLoRA), a novel pruning-based approach that selectively removes LoRA layers that weaken safety alignment, improving safety while preserving performance. At its core, we introduce Empirical-DIEM (E-DIEM), a dimension-insensitive similarity metric that effectively detects safety misalignment in LoRA-adapted models. We conduct extensive experiments on LLMs fine-tuned with mixed of benign and malicious data, and purely benign datasets, evaluating SPLoRA across utility, safety, and reliability metrics. Results demonstrate that SPLoRA outperforms state-of-the-art safety alignment techniques, significantly reducing safety risks while maintaining or improving model performance and reliability. Additionally, SPLoRA reduces inference overhead, making it a scalable and efficient solution for deploying safer and more reliable LLMs. The code is available at https://github.com/AoShuang92/SPLoRA.",
      "authors": [
        "Shuang Ao",
        "Yi Dong",
        "Jinwei Hu",
        "Sarvapali Ramchurn"
      ],
      "last_revised_date": "2025/06/21",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18931",
        "HTML": "https://arxiv.org/html/2506.18931",
        "PDF": "https://arxiv.org/pdf/2506.18931"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Sat, 21 Jun 2025 14:59:54 GMT",
          "size": "172kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Safe Pruning LoRA: Robust Distance-Guided Pruning for Safety Alignment in Adaptation of LLMs",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While this paper focuses on fine-tuning LLMs and improving safety, it does mention fine-tuning with different datasets, which indirectly relates to LLM training data."
      }
    },
    {
      "id": "2506.18959",
      "abstract": "Information retrieval is a cornerstone of modern knowledge acquisition, enabling billions of queries each day across diverse domains. However, traditional keyword-based search engines are increasingly inadequate for handling complex, multi-step information needs. Our position is that Large Language Models (LLMs), endowed with reasoning and agentic capabilities, are ushering in a new paradigm termed Agentic Deep Research. These systems transcend conventional information search techniques by tightly integrating autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. We trace the evolution from static web search to interactive, agent-based systems that plan, explore, and learn. We also introduce a test-time scaling law to formalize the impact of computational depth on reasoning and search. Supported by benchmark results and the rise of open-source implementations, we demonstrate that Agentic Deep Research not only significantly outperforms existing approaches, but is also poised to become the dominant paradigm for future information seeking. All the related resources, including industry products, research papers, benchmark datasets, and open-source implementations, are collected for the community in https://github.com/DavidZWZ/Awesome-Deep-Research.",
      "authors": [
        "Weizhi Zhang",
        "Yangning Li",
        "Yuanchen Bei",
        "Junyu Luo",
        "Guancheng Wan",
        "Liangwei Yang",
        "Chenxuan Xie",
        "Yuyao Yang",
        "Wei-Chieh Huang",
        "Chunyu Miao",
        "Henry Peng Zou",
        "Xiao Luo",
        "Yusheng Zhao",
        "Yankai Chen",
        "Chunkit Chan",
        "Peilin Zhou",
        "Xinyang Zhang",
        "Chenwei Zhang",
        "Jingbo Shang",
        "Ming Zhang",
        "Yangqiu Song",
        "Irwin King",
        "Philip S. Yu"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18959",
        "HTML": "https://arxiv.org/html/2506.18959",
        "PDF": "https://arxiv.org/pdf/2506.18959"
      },
      "subjects": [
        "Information Retrieval (cs.IR)",
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 17:27:19 GMT",
          "size": "2851kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "Although the paper mentions 'benchmark datasets' and improvements in search paradigms involving LLMs, its main focus is on agentic deep research and not on LLM training data."
      }
    },
    {
      "id": "2506.18962",
      "abstract": "Decoding human brain activity from electroencephalography (EEG) signals is a central challenge at the intersection of neuroscience and artificial intelligence, enabling diverse applications in mental state assessment, clinical monitoring, and human-machine interaction. Recent efforts have extensively explored EEG-based brain foundation models for generalized brain decoding, employing large-scale training on multiple datasets. However, most of these attempts struggle with generalizability and fail to achieve satisfactory performance without task-specific tuning due to pronounced inherent heterogeneity among decoding tasks. To address these challenges, we present UniMind, a general-purpose EEG foundation model for unified multi-task brain decoding by uniquely unleashing the power of large language models to comprehend complex neural patterns. UniMind offers several advantages. First, we design a Neuro-Language Connector to bridge the modality gap between neural signals and large language models, distilling and transforming the spatiotemporal neural patterns of EEG data into representations understandable by language models. Second, a Task-aware Query Selection module is proposed to inject task-awareness into the cross-modal alignment by dynamically generating task-adaptive query tokens, enabling learning of task-relevant neural patterns across diverse tasks. Extensive experiments across ten datasets demonstrate that UniMind substantially outperforms state-of-the-art multi-task decoding models, with an average gain of 12 percent, while also offering valuable neuroscientific insights into neural functional correlations across tasks. The code will be made publicly available.",
      "authors": [
        "Weiheng Lu",
        "Chunfeng Song",
        "Jiamin Wu",
        "Pengyu Zhu",
        "Yuchen Zhou",
        "Weijian Mai",
        "Qihao Zheng",
        "Wanli Ouyang"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18962",
        "HTML": "https://arxiv.org/html/2506.18962",
        "PDF": "https://arxiv.org/pdf/2506.18962"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 17:58:17 GMT",
          "size": "5359kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "UniMind: Unleashing the Power of LLMs for Unified Multi-Task Brain Decoding",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper involves using LLMs for EEG signal interpretation but focuses on decoding and cross-modal alignment rather than LLM training data directly."
      }
    },
    {
      "id": "2506.18998",
      "abstract": "When artificial intelligence mistakes memorization for intelligence, it creates a dangerous mirage of reasoning. Existing studies treat memorization and self-knowledge deficits in LLMs as separate issues and do not recognize an intertwining link that degrades the trustworthiness of LLM responses. In our study, we utilize a novel framework to ascertain if LLMs genuinely learn reasoning patterns from training data or merely memorize them to assume competence across problems of similar complexity focused on STEM domains. Our analysis shows a noteworthy problem in generalization: LLMs draw confidence from memorized solutions to infer a higher self-knowledge about their reasoning ability, which manifests as an over 45% inconsistency in feasibility assessments when faced with self-validated, logically coherent task perturbations. This effect is most pronounced in science and medicine domains, which tend to have maximal standardized jargon and problems, further confirming our approach. Significant wavering within the self-knowledge of LLMs also shows flaws in current architectures and training patterns, highlighting the need for techniques that ensure a balanced, consistent stance on models' perceptions of their own knowledge for maximum AI explainability and trustworthiness. Our code and results are available publicly at https://github.com/knowledge-verse-ai/LLM-Memorization_SK_Eval-.",
      "authors": [
        "Sahil Kale",
        "Vijaykant Nadadur"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18998",
        "HTML": "https://arxiv.org/html/2506.18998",
        "PDF": "https://arxiv.org/pdf/2506.18998"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 18:01:16 GMT",
          "size": "4297kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper explores the memorization issue in LLMs related to training data but does not focus on the data itself. It looks at how LLM architectures interpret reasoning patterns learned from training data."
      }
    },
    {
      "id": "2506.19004",
      "abstract": "Modern tokenizers employ deterministic algorithms to map text into a single \"canonical\" token sequence, yet the same string can be encoded as many non-canonical tokenizations using the tokenizer vocabulary. In this work, we investigate the robustness of LMs to text encoded with non-canonical tokenizations entirely unseen during training. Surprisingly, when evaluated across 20 benchmarks, we find that instruction-tuned models retain up to 93.4% of their original performance when given a randomly sampled tokenization, and 90.8% with character-level tokenization. We see that overall stronger models tend to be more robust, and robustness diminishes as the tokenization departs farther from the canonical form. Motivated by these results, we then identify settings where non-canonical tokenization schemes can *improve* performance, finding that character-level segmentation improves string manipulation and code understanding tasks by up to +14%, and right-aligned digit grouping enhances large-number arithmetic by +33%. Finally, we investigate the source of this robustness, finding that it arises in the instruction-tuning phase. We show that while both base and post-trained models grasp the semantics of non-canonical tokenizations (perceiving them as containing misspellings), base models try to mimic the imagined mistakes and degenerate into nonsensical output, while post-trained models are committed to fluent responses. Overall, our findings suggest that models are less tied to their tokenizer than previously believed, and demonstrate the promise of intervening on tokenization at inference time to boost performance.",
      "authors": [
        "Brian Siyuan Zheng",
        "Alisa Liu",
        "Orevaoghene Ahia",
        "Jonathan Hayase",
        "Yejin Choi",
        "Noah A. Smith"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19004",
        "HTML": "https://arxiv.org/html/2506.19004",
        "PDF": "https://arxiv.org/pdf/2506.19004"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 18:02:26 GMT",
          "size": "8210kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "It examines model robustness with different tokenizations and partially explores data encoding's impact on models, but it does not focus on the dataset or training data directly used in LLMs."
      }
    },
    {
      "id": "2506.19014",
      "abstract": "Advancements in audio deepfake technology offers benefits like AI assistants, better accessibility for speech impairments, and enhanced entertainment. However, it also poses significant risks to security, privacy, and trust in digital communications. Detecting and mitigating these threats requires comprehensive datasets. Existing datasets lack diverse ethnic accents, making them inadequate for many real-world scenarios. Consequently, models trained on these datasets struggle to detect audio deepfakes in diverse linguistic and cultural contexts such as in South-Asian countries. Ironically, there is a stark lack of South-Asian speaker samples in the existing datasets despite constituting a quarter of the worlds population. This work introduces the IndieFake Dataset (IFD), featuring 27.17 hours of bonafide and deepfake audio from 50 English speaking Indian speakers. IFD offers balanced data distribution and includes speaker-level characterization, absent in datasets like ASVspoof21 (DF). We evaluated various baselines on IFD against existing ASVspoof21 (DF) and In-The-Wild (ITW) datasets. IFD outperforms ASVspoof21 (DF) and proves to be more challenging compared to benchmark ITW dataset. The dataset will be publicly available upon acceptance.",
      "authors": [
        "Abhay Kumar",
        "Kunal Verma",
        "Omkar More"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19014",
        "HTML": "https://arxiv.org/html/2506.19014",
        "PDF": "https://arxiv.org/pdf/2506.19014"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Artificial Intelligence (cs.AI)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 18:10:06 GMT",
          "size": "1739kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "IndieFake Dataset: A Benchmark Dataset for Audio Deepfake Detection",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper introduces a new dataset for audio deepfake detection, which is indirectly related to LLM training data since it involves dataset creation and quality considerations."
      }
    },
    {
      "id": "2506.19028",
      "abstract": "Large Language Models (LLMs) often generate responses with inherent biases, undermining their reliability in real-world applications. Existing evaluation methods often overlook biases in long-form responses and the intrinsic variability of LLM outputs. To address these challenges, we propose FiSCo(Fine-grained Semantic Computation), a novel statistical framework to evaluate group-level fairness in LLMs by detecting subtle semantic differences in long-form responses across demographic groups. Unlike prior work focusing on sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis by operating at the claim level, leveraging entailment checks to assess the consistency of meaning across responses. We decompose model outputs into semantically distinct claims and apply statistical hypothesis testing to compare inter- and intra-group similarities, enabling robust detection of subtle biases. We formalize a new group counterfactual fairness definition and validate FiSCo on both synthetic and human-annotated datasets spanning gender, race, and age. Experiments show that FiSco more reliably identifies nuanced biases while reducing the impact of stochastic LLM variability, outperforming various evaluation metrics.",
      "authors": [
        "Weijie Xu",
        "Yiwen Wang",
        "Chi Xue",
        "Xiangkun Hu",
        "Xi Fang",
        "Guimin Dong",
        "Chandan K. Reddy"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19028",
        "HTML": "https://arxiv.org/html/2506.19028",
        "PDF": "https://arxiv.org/pdf/2506.19028"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 18:31:22 GMT",
          "size": "3359kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper addresses bias in LLM outputs using a statistical framework which indirectly relates to the implications of LLM training data on bias, but there is no direct focus on the data itself."
      }
    },
    {
      "id": "2506.19030",
      "abstract": "The rapid evolution of LLMs threatens to overwhelm existing wireless infrastructure, necessitating architectural innovations for burgeoning mobile LLM services. This paper introduces WiLLM, the first open-source wireless system specifically designed for these services. First, we establish a new paradigm by deploying LLMs in core networks (CNs) with abundant GPUs. This enables distributed inference services, strategically positioning LLM inference at the convergence of backbone bandwidth and the cellular network's edge. Second, we propose an innovative \"Tree-Branch-Fruit\" extension to the conventional network slicing architecture. This specialized design allows telecom operators to monetize LLM services through slice subscriptions while maintaining infrastructure ownership. Finally, to realize this vision, WiLLM addresses critical limitations in current solutions with several novel capabilities. It features enhanced slice orchestration through a dual-layer slicing architecture, enabling coordinated multi-UE-multi-slice scheduling for finer-grained resource allocation. To ensure universal compatibility, an application-layer tunneling mechanism allows legacy devices without native slicing to access LLM slice services without hardware upgrades. Furthermore, its dual-mode scheduling and cross-layer APIs support flexible deployment from CNs to servers. Built on OpenAirInterface, WiLLM extends this established framework, lowering the adoption barrier for researchers. We also release the first LLM wireless communication dataset with 1,649,996 records and synchronized 58-dimensional metrics, alongside two benchmarks. A case study with smart glasses demonstrates practical viability for resource-constrained devices. WiLLM aims to foster an open platform for cross-layer optimization and AI-telecom convergence. The code, datasets, and hardware details are available at https://openwillm.github.io.",
      "authors": [
        "Boyi Liu",
        "Yongguang Lu",
        "Jianguo Zhao",
        "Qiang Yang",
        "Wen Wu",
        "Lin Chen",
        "Jagmohan Chauhan",
        "Jun Zhang"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/publicdomain/zero/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19030",
        "HTML": "https://arxiv.org/html/2506.19030",
        "PDF": "https://arxiv.org/pdf/2506.19030"
      },
      "subjects": [
        "Networking and Internet Architecture (cs.NI)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 18:37:55 GMT",
          "size": "7774kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "WiLLM: An Open Wireless LLM Communication System",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "WiLLM paper discusses LLM services and infrastructure but only indirectly references LLM datasets with a focus on wireless communication systems, not specifically on training data."
      }
    },
    {
      "id": "2506.19058",
      "abstract": "Matching job titles is a highly relevant task in the computational job market domain, as it improves e.g., automatic candidate matching, career path prediction, and job market analysis. Furthermore, aligning job titles to job skills can be considered an extension to this task, with similar relevance for the same downstream tasks. In this report, we outline NLPnorth's submission to TalentCLEF 2025, which includes both of these tasks: Multilingual Job Title Matching, and Job Title-Based Skill Prediction. For both tasks we compare (fine-tuned) classification-based, (fine-tuned) contrastive-based, and prompting methods. We observe that for Task A, our prompting approach performs best with an average of 0.492 mean average precision (MAP) on test data, averaged over English, Spanish, and German. For Task B, we obtain an MAP of 0.290 on test data with our fine-tuned classification-based approach. Additionally, we made use of extra data by pulling all the language-specific titles and corresponding \\emph{descriptions} from ESCO for each job and skill. Overall, we find that the largest multilingual language models perform best for both tasks. Per the provisional results and only counting the unique teams, the ranking on Task A is 5$^{\\text{th}}$/20 and for Task B 3$^{\\text{rd}}$/14.",
      "authors": [
        "Mike Zhang and Rob van der Goot"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19058",
        "HTML": "https://arxiv.org/html/2506.19058",
        "PDF": "https://arxiv.org/pdf/2506.19058"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 19:18:25 GMT",
          "size": "37kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "NLPnorth @ TalentCLEF 2025: Comparing Discriminative, Contrastive, and Prompt-Based Methods for Job Title and Skill Matching",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the primary focus is on job title and skill matching, the paper mentions multilingual language models and data usage, which could indirectly relate to discussions of data in LLM contexts, though it's not the core topic."
      }
    },
    {
      "id": "2506.19082",
      "abstract": "Synthetic data generation creates data based on real-world data using generative models. In health applications, generating high-quality data while maintaining fairness for sensitive attributes is essential for equitable outcomes. Existing GAN-based and LLM-based methods focus on counterfactual fairness and are primarily applied in finance and legal domains. Causal fairness provides a more comprehensive evaluation framework by preserving causal structure, but current synthetic data generation methods do not address it in health settings. To fill this gap, we develop the first LLM-augmented synthetic data generation method to enhance causal fairness using real-world tabular health data. Our generated data deviates by less than 10% from real data on causal fairness metrics. When trained on causally fair predictors, synthetic data reduces bias on the sensitive attribute by 70% compared to real data. This work improves access to fair synthetic data, supporting equitable health research and healthcare delivery.",
      "authors": [
        "Nitish Nagesh",
        "Ziyu Wang",
        "Amir M. Rahmani"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19082",
        "HTML": "https://arxiv.org/html/2506.19082",
        "PDF": "https://arxiv.org/pdf/2506.19082"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 19:59:26 GMT",
          "size": "183kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "FairCauseSyn: Towards Causally Fair LLM-Augmented Synthetic Data Generation",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses synthetic data generation and fairness, which could indirectly relate to LLM training data if the methods are relevant for LLM applications, albeit not the main focus."
      }
    },
    {
      "id": "2506.19107",
      "abstract": "With the proliferation of large language model (LLM) applications since 2022, their use in education has sparked both excitement and concern. Recent studies consistently highlight students' (mis)use of LLMs can hinder learning outcomes. This work aims to teach students how to effectively prompt LLMs to improve their learning. We first proposed pedagogical prompting, a theoretically-grounded new concept to elicit learning-oriented responses from LLMs. To move from concept design to a proof-of-concept learning intervention in real educational settings, we selected early undergraduate CS education (CS1/CS2) as the example context. We began with a formative survey study with instructors (N=36) teaching early-stage undergraduate-level CS courses to inform the instructional design based on classroom needs. Based on their insights, we designed and developed a learning intervention through an interactive system with scenario-based instruction to train pedagogical prompting skills. Finally, we evaluated its instructional effectiveness through a user study with CS novice students (N=22) using pre/post-tests. Through mixed methods analyses, our results indicate significant improvements in learners' LLM-based pedagogical help-seeking skills, along with positive attitudes toward the system and increased willingness to use pedagogical prompts in the future. Our contributions include (1) a theoretical framework of pedagogical prompting; (2) empirical insights into current instructor attitudes toward pedagogical prompting; and (3) a learning intervention design with an interactive learning tool and scenario-based instruction leading to promising results on teaching LLM-based help-seeking. Our approach is scalable for broader implementation in classrooms and has the potential to be integrated into tools like ChatGPT as an on-boarding experience to encourage learning-oriented use of generative AI.",
      "authors": [
        "Ruiwei Xiao",
        "Xinying Hou",
        "Runlong Ye",
        "Majeed Kazemitabaar",
        "Nicholas Diana",
        "Michael Liut",
        "John Stamper"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19107",
        "HTML": "https://arxiv.org/html/2506.19107",
        "PDF": "https://arxiv.org/pdf/2506.19107"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 20:39:17 GMT",
          "size": "17055kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Improving Student-AI Interaction Through Pedagogical Prompting: An Example in Computer Science Education",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper addresses educational use of LLMs and pedagogical prompting. It does touch on LLM interaction and instructional design, which indirectly relate to LLM data use but not specifically on training data."
      }
    },
    {
      "id": "2506.19113",
      "abstract": "The discourse around toxicity and LLMs in NLP largely revolves around detection tasks. This work shifts the focus to evaluating LLMs' reasoning about toxicity -- from their explanations that justify a stance -- to enhance their trustworthiness in downstream tasks. Despite extensive research on explainability, it is not straightforward to adopt existing methods to evaluate free-form toxicity explanation due to their over-reliance on input text perturbations, among other challenges. To account for these, we propose a novel, theoretically-grounded multi-dimensional criterion, Human-Aligned Faithfulness (HAF), that measures the extent to which LLMs' free-form toxicity explanations align with those of a rational human under ideal conditions. We develop six metrics, based on uncertainty quantification, to comprehensively evaluate \\haf of LLMs' toxicity explanations with no human involvement, and highlight how \"non-ideal\" the explanations are. We conduct several experiments on three Llama models (of size up to 70B) and an 8B Ministral model on five diverse toxicity datasets. Our results show that while LLMs generate plausible explanations to simple prompts, their reasoning about toxicity breaks down when prompted about the nuanced relations between the complete set of reasons, the individual reasons, and their toxicity stances, resulting in inconsistent and nonsensical responses. We open-source our code and LLM-generated explanations at https://github.com/uofthcdslab/HAF.",
      "authors": [
        "Ramaravind K. Mothilal and Joanna Roy and Syed Ishtiaque Ahmed and Shion Guha"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19113",
        "HTML": "https://arxiv.org/html/2506.19113",
        "PDF": "https://arxiv.org/pdf/2506.19113"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 20:41:45 GMT",
          "size": "360kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Human-Aligned Faithfulness in Toxicity Explanations of LLMs",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper evaluates LLMs' reasoning about toxicity, which may be indirectly related to how the data used could impact toxicity detection, but does not focus on training data itself."
      }
    },
    {
      "id": "2506.19121",
      "abstract": "In robot imitation learning, policy performance is tightly coupled with the quality and composition of the demonstration data. Yet, developing a precise understanding of how individual demonstrations contribute to downstream outcomes - such as closed-loop task success or failure - remains a persistent challenge. We propose CUPID, a robot data curation method based on a novel influence function-theoretic formulation for imitation learning policies. Given a set of evaluation rollouts, CUPID estimates the influence of each training demonstration on the policy's expected return. This enables ranking and selection of demonstrations according to their impact on the policy's closed-loop performance. We use CUPID to curate data by 1) filtering out training demonstrations that harm policy performance and 2) subselecting newly collected trajectories that will most improve the policy. Extensive simulated and hardware experiments show that our approach consistently identifies which data drives test-time performance. For example, training with less than 33% of curated data can yield state-of-the-art diffusion policies on the simulated RoboMimic benchmark, with similar gains observed in hardware. Furthermore, hardware experiments show that our method can identify robust strategies under distribution shift, isolate spurious correlations, and even enhance the post-training of generalist robot policies. Additional materials are made available at: https://cupid-curation.github.io.",
      "authors": [
        "Christopher Agia",
        "Rohan Sinha",
        "Jingyun Yang",
        "Rika Antonova",
        "Marco Pavone",
        "Haruki Nishimura",
        "Masha Itkina",
        "Jeannette Bohg"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19121",
        "HTML": "https://arxiv.org/html/2506.19121",
        "PDF": "https://arxiv.org/pdf/2506.19121"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 20:49:34 GMT",
          "size": "11496kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "CUPID: Curating Data your Robot Loves with Influence Functions",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The CUPID paper involves data curation for robot imitation learning and discusses filtering and selecting training demonstrations. While not directly about LLMs, it relates indirectly to training data curation and quality, which are relevant topics in LLM data preparation."
      }
    },
    {
      "id": "2506.19143",
      "abstract": "Reasoning large language models have recently achieved state-of-the-art performance in many fields. However, their long-form chain-of-thought reasoning creates interpretability challenges as each generated token depends on all previous ones, making the computation harder to decompose. We argue that analyzing reasoning traces at the sentence level is a promising approach to understanding reasoning processes. We present three complementary attribution methods: (1) a black-box method measuring each sentence's counterfactual importance by comparing final answers across 100 rollouts conditioned on the model generating that sentence or one with a different meaning; (2) a white-box method of aggregating attention patterns between pairs of sentences, which identified ``broadcasting'' sentences that receive disproportionate attention from all future sentences via ``receiver'' attention heads; (3) a causal attribution method measuring logical connections between sentences by suppressing attention toward one sentence and measuring the effect on each future sentence's tokens. Each method provides evidence for the existence of thought anchors, reasoning steps that have outsized importance and that disproportionately influence the subsequent reasoning process. These thought anchors are typically planning or backtracking sentences. We provide an open-source tool (www.thought-anchors.com) for visualizing the outputs of our methods, and present a case study showing converging patterns across methods that map how a model performs multi-step reasoning. The consistency across methods demonstrates the potential of sentence-level analysis for a deeper understanding of reasoning models.",
      "authors": [
        "Paul C. Bogdan",
        "Uzay Macar",
        "Neel Nanda",
        "Arthur Conmy"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19143",
        "HTML": "https://arxiv.org/html/2506.19143",
        "PDF": "https://arxiv.org/pdf/2506.19143"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 21:28:45 GMT",
          "size": "20532kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Thought Anchors: Which LLM Reasoning Steps Matter?",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses reasoning steps in large language models, which can relate to LLMs' performance, but it does not directly address LLM training data."
      },
      "datasets": [
        {
          "dataset_name": "uzaymacar/math-rollouts",
          "downloads": "0",
          "likes": "0",
          "link": "https://huggingface.co/datasets/uzaymacar/math-rollouts"
        }
      ]
    },
    {
      "id": "2506.19159",
      "abstract": "A joint speech and text optimization method is proposed for hybrid transducer and attention-based encoder decoder (TAED) modeling to leverage large amounts of text corpus and enhance ASR accuracy. The joint TAED (J-TAED) is trained with both speech and text input modalities together, while it only takes speech data as input during inference. The trained model can unify the internal representations from different modalities, and be further extended to text-based domain adaptation. It can effectively alleviate data scarcity for mismatch domain tasks since no speech data is required. Our experiments show J-TAED successfully integrates speech and linguistic information into one model, and reduce the WER by 5.8 ~12.8% on the Librispeech dataset. The model is also evaluated on two out-of-domain datasets: one is finance and another is named entity focused. The text-based domain adaptation brings 15.3% and 17.8% WER reduction on those two datasets respectively.",
      "authors": [
        "Yun Tang",
        "Eesung Kim",
        "Vijendra Raj Apsingekar"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19159",
        "HTML": "https://arxiv.org/html/2506.19159",
        "PDF": "https://arxiv.org/pdf/2506.19159"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Sound (cs.SD)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 21:51:39 GMT",
          "size": "495kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Enhanced Hybrid Transducer and Attention Encoder Decoder with Text Data",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses leveraging large text corpora for speech and text optimization. While related to data usage for improving model performance, its focus is more on speech recognition than LLM training data."
      }
    },
    {
      "id": "2506.19160",
      "abstract": "Traditional control system design, reliant on expert knowledge and precise models, struggles with complex, nonlinear, or uncertain dynamics. This paper introduces AgenticControl, a novel multi-agent framework that automates controller design using coordinated Large Language Model (LLM) agents. Through structured JSON communication, these agents handle tasks including controller selection, scenario design, parameter optimization, performance evaluation, and decision-making. Through an actor-critic optimization approach, the system iteratively improves performance while progressing through scenarios of increasing complexity to ensure robustness under nominal conditions, measurement noise, actuator disturbances, and parametric uncertainties. Key innovations include structured multi-agent collaboration, robust optimization mechanisms, and real-time adaptability via in-context learning. Validated across four diverse control systems, namely, DC Motor Position control, Ball and Beam, Inverted Pendulum, and Double Inverted Pendulum, the framework achieves competitive performance against classical methods. Its Full State Feedback solution closely matches Linear Quadratic Regulator (LQR) results, while the designed PID controller significantly outperforming MATLAB's PIDTuner, reducing PID tracking error by 55% through adaptive parameter exploration. A comparative study of five LLM models reveals distinct optimization profiles, with DeepSeek achieving the fastest convergence. This work demonstrates the potential of LLM-driven control design, paving the way for advanced techniques like model predictive control and reinforcement learning.",
      "authors": [
        "Mohammad Narimani",
        "Seyyed Ali Emami"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19160",
        "HTML": "https://arxiv.org/html/2506.19160",
        "PDF": "https://arxiv.org/pdf/2506.19160"
      },
      "subjects": [
        "Systems and Control (eess.SY)",
        "Systems and Control (cs.SY)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 21:53:05 GMT",
          "size": "868kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "AgenticControl: An Automated Control Design Framework Using Large Language Models",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper notes the use of LLMs in automated control design, hinting at applications of LLMs but not directly discussing training data collection or curation. Thus, it bears weak relevance."
      }
    },
    {
      "id": "2506.19171",
      "abstract": "Large language models (LLMs) often struggle with mathematical problems that require exact computation or multi-step algebraic reasoning. Tool-integrated reasoning (TIR) offers a promising solution by leveraging external tools such as code interpreters to ensure correctness, but it introduces inference-time dependencies that hinder scalability and deployment. In this work, we propose a new paradigm for distilling tool knowledge into LLMs purely through natural language. We first construct a Solver Agent that solves math problems by interleaving planning, symbolic tool calls, and reflective reasoning. Then, using a back-translation pipeline powered by multiple LLM-based agents, we convert interleaved TIR traces into natural language reasoning traces. A Translator Agent generates explanations for individual tool calls, while a Rephrase Agent merges them into a fluent and globally coherent narrative. Empirically, we show that fine-tuning a small open-source model on these synthesized traces enables it to internalize both tool knowledge and structured reasoning patterns, yielding gains on competition-level math benchmarks without requiring tool access at inference.",
      "authors": [
        "Xingyue Huang",
        "Xianglong Hu",
        "Zifeng Ding",
        "Yuan He",
        "Rishabh",
        "Waleed Alzarooni",
        "Ziyu Ye",
        "Wendong Fan",
        "Bailan He",
        "Haige Bo",
        "Changran Hu",
        "Guohao Li"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19171",
        "HTML": "https://arxiv.org/html/2506.19171",
        "PDF": "https://arxiv.org/pdf/2506.19171"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 22:10:38 GMT",
          "size": "391kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Distilling Tool Knowledge into Language Models via Back-Translated Traces",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses fine-tuning a language model using synthesized traces, which indirectly relates to training data methodology. However, the primary focus is tool knowledge distillation, not dataset creation or evaluation."
      }
    },
    {
      "id": "2506.19204",
      "abstract": "We introduce OpenWildlife (OW), an open-vocabulary wildlife detector designed for multi-species identification in diverse aerial imagery. While existing automated methods perform well in specific settings, they often struggle to generalize across different species and environments due to limited taxonomic coverage and rigid model architectures. In contrast, OW leverages language-aware embeddings and a novel adaptation of the Grounding-DINO framework, enabling it to identify species specified through natural language inputs across both terrestrial and marine environments. Trained on 15 datasets, OW outperforms most existing methods, achieving up to \\textbf{0.981} mAP50 with fine-tuning and \\textbf{0.597} mAP50 on seven datasets featuring novel species. Additionally, we introduce an efficient search algorithm that combines k-nearest neighbors and breadth-first search to prioritize areas where social species are likely to be found. This approach captures over \\textbf{95\\%} of species while exploring only \\textbf{33\\%} of the available images. To support reproducibility, we publicly release our source code and dataset splits, establishing OW as a flexible, cost-effective solution for global biodiversity assessments.",
      "authors": [
        "Muhammed Patel",
        "Javier Noa Turnes",
        "Jayden Hsiao",
        "Linlin Xu",
        "David Clausi"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19204",
        "HTML": "https://arxiv.org/html/2506.19204",
        "PDF": "https://arxiv.org/pdf/2506.19204"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 00:10:19 GMT",
          "size": "6125kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "OpenWildlife: Open-Vocabulary Multi-Species Wildlife Detector for Geographically-Diverse Aerial Imagery",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "This paper discusses training an open-vocabulary wildlife detector using multiple datasets, implying dataset usage and training considerations, but it's not primarily focused on LLM-specific training data."
      }
    },
    {
      "id": "2506.19208",
      "abstract": "Ancient scripts, e.g., Egyptian hieroglyphs, Oracle Bone Inscriptions, and Ancient Greek inscriptions, serve as vital carriers of human civilization, embedding invaluable historical and cultural information. Automating ancient script image recognition has gained importance, enabling large-scale interpretation and advancing research in archaeology and digital humanities. With the rise of deep learning, this field has progressed rapidly, with numerous script-specific datasets and models proposed. While these scripts vary widely, spanning phonographic systems with limited glyphs to logographic systems with thousands of complex symbols, they share common challenges and methodological overlaps. Moreover, ancient scripts face unique challenges, including imbalanced data distribution and image degradation, which have driven the development of various dedicated methods. This survey provides a comprehensive review of ancient script image recognition methods. We begin by categorizing existing studies based on script types and analyzing respective recognition methods, highlighting both their differences and shared strategies. We then focus on challenges unique to ancient scripts, systematically examining their impact and reviewing recent solutions, including few-shot learning and noise-robust techniques. Finally, we summarize current limitations and outline promising future directions. Our goal is to offer a structured, forward-looking perspective to support ongoing advancements in the recognition, interpretation, and decipherment of ancient scripts.",
      "authors": [
        "Xiaolei Diao",
        "Rite Bo",
        "Yanling Xiao",
        "Lida Shi",
        "Zhihan Zhou",
        "Hao Xu",
        "Chuntao Li",
        "Xiongfeng Tang",
        "Massimo Poesio",
        "C\\'edric M. John",
        "Daqian Shi"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19208",
        "HTML": "https://arxiv.org/html/2506.19208",
        "PDF": "https://arxiv.org/pdf/2506.19208"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 00:34:55 GMT",
          "size": "45161kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Ancient Script Image Recognition and Processing: A Review",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper reviews ancient script image recognition methods using deep learning with relevant mentions of datasets and imbalanced data issues, which indirectly relates to data aspects albeit not specifically for LLMs."
      }
    },
    {
      "id": "2506.19209",
      "abstract": "Multi-agent techniques such as role playing or multi-turn debates have been shown to be effective in improving the performance of large language models (LLMs) in downstream tasks. Despite their differences in workflows, existing LLM-based multi-agent systems mostly use natural language for agent communication. While this is appealing for its simplicity and interpretability, it also introduces inevitable information loss as one model must down sample its continuous state vectors to concrete tokens before transferring them to the other model. Such losses are particularly significant when the information to transfer is not simple facts, but reasoning logics or abstractive thoughts. To tackle this problem, we propose a new communication protocol that transfers both natural language tokens and token-wise state transition trajectory from one agent to another. Particularly, compared to the actual state value, we find that the sequence of state changes in LLMs after generating each token can better reflect the information hidden behind the inference process, so we propose a State Delta Encoding (SDE) method to represent state transition trajectories. The experimental results show that multi-agent systems with SDE achieve SOTA performance compared to other communication protocols, particularly in tasks that involve complex reasoning. This shows the potential of communication augmentation for LLM-based multi-agent systems.",
      "authors": [
        "Yichen Tang",
        "Weihang Su",
        "Yujia Zhou",
        "Yiqun Liu",
        "Min Zhang",
        "Shaoping Ma",
        "Qingyao Ai"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19209",
        "HTML": "https://arxiv.org/html/2506.19209",
        "PDF": "https://arxiv.org/pdf/2506.19209"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 00:38:25 GMT",
          "size": "276kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Augmenting Multi-Agent Communication with State Delta Trajectory",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses enhancing communication in LLM-based multi-agent systems, indirectly touching on how language models function and may benefit from improved data communication without focusing specifically on training data."
      }
    },
    {
      "id": "2506.19212",
      "abstract": "Dexterous robotic hands are essential for performing complex manipulation tasks, yet remain difficult to train due to the challenges of demonstration collection and high-dimensional control. While reinforcement learning (RL) can alleviate the data bottleneck by generating experience in simulation, it typically relies on carefully designed, task-specific reward functions, which hinder scalability and generalization. Thus, contemporary works in dexterous manipulation have often bootstrapped from reference trajectories. These trajectories specify target hand poses that guide the exploration of RL policies and object poses that enable dense, task-agnostic rewards. However, sourcing suitable trajectories - particularly for dexterous hands - remains a significant challenge. Yet, the precise details in explicit reference trajectories are often unnecessary, as RL ultimately refines the motion. Our key insight is that modern vision-language models (VLMs) already encode the commonsense spatial and semantic knowledge needed to specify tasks and guide exploration effectively. Given a task description (e.g., \"open the cabinet\") and a visual scene, our method uses an off-the-shelf VLM to first identify task-relevant keypoints (e.g., handles, buttons) and then synthesize 3D trajectories for hand motion and object motion. Subsequently, we train a low-level residual RL policy in simulation to track these coarse trajectories or \"scaffolds\" with high fidelity. Across a number of simulated tasks involving articulated objects and semantic understanding, we demonstrate that our method is able to learn robust dexterous manipulation policies. Moreover, we showcase that our method transfers to real-world robotic hands without any human demonstrations or handcrafted rewards.",
      "authors": [
        "Vincent de Bakker and Joey Hejna and Tyler Ga Wei Lum and Onur Celik and Aleksandar Taranovic and Denis Blessing and Gerhard Neumann and Jeannette Bohg and Dorsa Sadigh"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19212",
        "HTML": "https://arxiv.org/html/2506.19212",
        "PDF": "https://arxiv.org/pdf/2506.19212"
      },
      "subjects": [
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 00:43:00 GMT",
          "size": "1824kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Scaffolding Dexterous Manipulation with Vision-Language Models",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The discussion on using vision-language models suggests some indirect relevance to training data, particularly in leveraging VLMs for task guidance, but training data itself is not the focus."
      }
    },
    {
      "id": "2506.19217",
      "abstract": "Computed Tomography (CT) plays a crucial role in clinical diagnosis, but the growing demand for CT examinations has raised concerns about diagnostic errors. While Multimodal Large Language Models (MLLMs) demonstrate promising comprehension of medical knowledge, their tendency to produce inaccurate information highlights the need for rigorous validation. However, existing medical visual question answering (VQA) benchmarks primarily focus on simple visual recognition tasks, lacking clinical relevance and failing to assess expert-level knowledge. We introduce MedErr-CT, a novel benchmark for evaluating medical MLLMs' ability to identify and correct errors in CT reports through a VQA framework. The benchmark includes six error categories - four vision-centric errors (Omission, Insertion, Direction, Size) and two lexical error types (Unit, Typo) - and is organized into three task levels: classification, detection, and correction. Using this benchmark, we quantitatively assess the performance of state-of-the-art 3D medical MLLMs, revealing substantial variation in their capabilities across different error types. Our benchmark contributes to the development of more reliable and clinically applicable MLLMs, ultimately helping reduce diagnostic errors and improve accuracy in clinical practice. The code and datasets are available at https://github.com/babbu3682/MedErr-CT.",
      "authors": [
        "Sunggu Kyung",
        "Hyungbin Park",
        "Jinyoung Seo",
        "Jimin Sung",
        "Jihyun Kim",
        "Dongyeong Kim",
        "Wooyoung Jo",
        "Yoojin Nam",
        "Sangah Park",
        "Taehee Kwon",
        "Sang Min Lee",
        "Namkug Kim"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19217",
        "HTML": "https://arxiv.org/html/2506.19217",
        "PDF": "https://arxiv.org/pdf/2506.19217"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 00:51:03 GMT",
          "size": "1380kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "MedErr-CT: A Visual Question Answering Benchmark for Identifying and Correcting Errors in CT Reports",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "Although the paper introduces a benchmark relevant to MLLMs, it touches on aspects of data assessment indirectly related to training data, such as error correction, but does not focus on dataset creation or quality."
      }
    },
    {
      "id": "2506.19225",
      "abstract": "Multi-modal large language models (MLLMs) models have made significant progress in video understanding over the past few years. However, processing long video inputs remains a major challenge due to high memory and computational costs. This makes it difficult for current models to achieve both strong performance and high efficiency in long video understanding. To address this challenge, we propose Video-XL-2, a novel MLLM that delivers superior cost-effectiveness for long-video understanding based on task-aware KV sparsification. The proposed framework operates with two key steps: chunk-based pre-filling and bi-level key-value decoding. Chunk-based pre-filling divides the visual token sequence into chunks, applying full attention within each chunk and sparse attention across chunks. This significantly reduces computational and memory overhead. During decoding, bi-level key-value decoding selectively reloads either dense or sparse key-values for each chunk based on its relevance to the task. This approach further improves memory efficiency and enhances the model's ability to capture fine-grained information. Video-XL-2 achieves state-of-the-art performance on various long video understanding benchmarks, outperforming existing open-source lightweight models. It also demonstrates exceptional efficiency, capable of processing over 10,000 frames on a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few seconds.",
      "authors": [
        "Minghao Qin",
        "Xiangrui Liu",
        "Zhengyang Liang",
        "Yan Shu",
        "Huaying Yuan",
        "Juenjie Zhou",
        "Shitao Xiao",
        "Bo Zhao",
        "Zheng Liu"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19225",
        "HTML": "https://arxiv.org/html/2506.19225",
        "PDF": "https://arxiv.org/pdf/2506.19225"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 01:19:56 GMT",
          "size": "26300kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV Sparsification",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses multi-modal large language models and efficient processing techniques for long video inputs but does not focus on LLM training data directly. It may indirectly relate as it deals with video data processing, which can be a part of LLMs' training datasets."
      }
    },
    {
      "id": "2506.19248",
      "abstract": "A common paradigm to improve the performance of large language models is optimizing for a reward model. Reward models assign a numerical score to LLM outputs indicating, for example, which response would likely be preferred by a user or is most aligned with safety goals. However, reward models are never perfect. They inevitably function as proxies for complex desiderata such as correctness, helpfulness, and safety. By overoptimizing for a misspecified reward, we can subvert intended alignment goals and reduce overall performance -- a phenomenon commonly referred to as reward hacking. In this work, we characterize reward hacking in inference-time alignment and demonstrate when and how we can mitigate it by hedging on the proxy reward. We study this phenomenon under Best-of-$n$ (BoN) and Soft-Best-of-$n$ (SBoN), and we introduce Best-of-Poisson (BoP) that provides an efficient, near-exact approximation of the optimal reward-KL divergence policy at inference time. We show that the characteristic pattern of hacking as observed in practice (where the true reward first increases before declining) is an inevitable property of a broad class of inference-time mechanisms, including BoN and BoP. To counter this effect, hedging offers a tactical choice to avoid placing undue confidence in high but potentially misleading proxy reward signals. We introduce HedgeTune, an efficient algorithm to find the optimal inference-time parameter and avoid reward hacking. We demonstrate through experiments that hedging mitigates reward hacking and achieves superior distortion-reward tradeoffs with minimal computational overhead.",
      "authors": [
        "Hadi Khalaf",
        "Claudio Mayrink Verdun",
        "Alex Oesterling",
        "Himabindu Lakkaraju",
        "Flavio du Pin Calmon"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19248",
        "HTML": "https://arxiv.org/html/2506.19248",
        "PDF": "https://arxiv.org/pdf/2506.19248"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 02:05:25 GMT",
          "size": "298kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Inference-Time Reward Hacking in Large Language Models",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "This paper touches on how reward models interface with LLMs and discusses the optimization of such models. While it does not focus directly on training data, it involves data-related discussions in terms of model performance and alignment."
      }
    },
    {
      "id": "2506.19257",
      "abstract": "Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning tasks through enhanced chain-of-thought capabilities. However, this advancement also introduces novel safety risks, as these models become increasingly vulnerable to harmful multimodal prompts that can trigger unethical or unsafe behaviors. Existing safety alignment approaches, primarily designed for unimodal language models, fall short in addressing the complex and nuanced threats posed by multimodal inputs. Moreover, current safety datasets lack the fine-grained, policy-grounded reasoning required to robustly align reasoning-capable VLMs. In this work, we introduce {MSR-Align}, a high-quality Multimodal Safety Reasoning dataset tailored to bridge this gap. MSR-Align supports fine-grained, deliberative reasoning over standardized safety policies across both vision and text modalities. Our data generation pipeline emphasizes multimodal diversity, policy-grounded reasoning, and rigorous quality filtering using strong multimodal judges. Extensive experiments demonstrate that fine-tuning VLMs on MSR-Align substantially improves robustness against both textual and vision-language jailbreak attacks, while preserving or enhancing general reasoning performance. MSR-Align provides a scalable and effective foundation for advancing the safety alignment of reasoning-capable VLMs. Our dataset is made publicly available at https://huggingface.co/datasets/Leigest/MSR-Align.",
      "authors": [
        "Yinan Xia",
        "Yilei Jiang",
        "Yingshui Tan",
        "Xiaoyong Zhu",
        "Xiangyu Yue",
        "Bo Zheng"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19257",
        "HTML": "https://arxiv.org/html/2506.19257",
        "PDF": "https://arxiv.org/pdf/2506.19257"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 02:37:59 GMT",
          "size": "2536kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "MSR-Align: Policy-Grounded Multimodal Alignment for Safety-Aware Reasoning in Vision-Language Models",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses safety alignment in vision-language models (VLMs) and introduces a dataset related to safety reasoning. While it touches on data curation for VLMs, the focus is not on LLM training data specifically."
      }
    },
    {
      "id": "2506.19258",
      "abstract": "Natural Language Processing (NLP) offers new avenues for personality assessment by leveraging rich, open-ended text, moving beyond traditional questionnaires. In this study, we address the challenge of modeling long narrative interview where each exceeds 2000 tokens so as to predict Five-Factor Model (FFM) personality traits. We propose a two-step approach: first, we extract contextual embeddings using sliding-window fine-tuning of pretrained language models; then, we apply Recurrent Neural Networks (RNNs) with attention mechanisms to integrate long-range dependencies and enhance interpretability. This hybrid method effectively bridges the strengths of pretrained transformers and sequence modeling to handle long-context data. Through ablation studies and comparisons with state-of-the-art long-context models such as LLaMA and Longformer, we demonstrate improvements in prediction accuracy, efficiency, and interpretability. Our results highlight the potential of combining language-based features with long-context modeling to advance personality assessment from life narratives.",
      "authors": [
        "Rasiq Hussain",
        "Jerry Ma",
        "Rithik Khandelwal",
        "Joshua Oltmanns",
        "Mehak Gupta"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19258",
        "HTML": "https://arxiv.org/html/2506.19258",
        "PDF": "https://arxiv.org/pdf/2506.19258"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 02:39:06 GMT",
          "size": "2012kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Personality Prediction from Life Stories using Language Models",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the paper uses language models for personality prediction from life stories, it focuses on modeling techniques rather than training data aspects. There is no explicit emphasis on LLM training data."
      }
    },
    {
      "id": "2506.19261",
      "abstract": "While the efficacy of deep learning models heavily relies on data, gathering and annotating data for specific tasks, particularly when addressing novel or sensitive subjects lacking relevant datasets, poses significant time and resource challenges. In response to this, we propose a novel Automated Image Recognition (AIR) framework that harnesses the power of generative AI. AIR empowers end-users to synthesize high-quality, pre-annotated datasets, eliminating the necessity for manual labeling. It also automatically trains deep learning models on the generated datasets with robust image recognition performance. Our framework includes two main data synthesis processes, AIR-Gen and AIR-Aug. The AIR-Gen enables end-users to seamlessly generate datasets tailored to their specifications. To improve image quality, we introduce a novel automated prompt engineering module that leverages the capabilities of large language models. We also introduce a distribution adjustment algorithm to eliminate duplicates and outliers, enhancing the robustness and reliability of generated datasets. On the other hand, the AIR-Aug enhances a given dataset, thereby improving the performance of deep classifier models. AIR-Aug is particularly beneficial when users have limited data for specific tasks. Through comprehensive experiments, we demonstrated the efficacy of our generated data in training deep learning models and showcased the system's potential to provide image recognition models for a wide range of objects. We also conducted a user study that achieved an impressive score of 4.4 out of 5.0, underscoring the AI community's positive perception of AIR.",
      "authors": [
        "Quang-Binh Nguyen and Trong-Vu Hoang and Ngoc-Do Tran and Tam V. Nguyen and Minh-Triet Tran and Trung-Nghia Le"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19261",
        "HTML": "https://arxiv.org/html/2506.19261",
        "PDF": "https://arxiv.org/pdf/2506.19261"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 02:42:34 GMT",
          "size": "2832kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Automated Image Recognition Framework",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses generating and annotating datasets, which could relate to LLMs insofar as data curation and synthesis are concerned, but does not specifically focus on LLM training data."
      }
    },
    {
      "id": "2506.19268",
      "abstract": "We present HARPT, a large-scale annotated corpus of mobile health app store reviews aimed at advancing research in user privacy and trust. The dataset comprises over 480,000 user reviews labeled into seven categories that capture critical aspects of trust in applications, trust in providers and privacy concerns. Creating HARPT required addressing multiple complexities, such as defining a nuanced label schema, isolating relevant content from large volumes of noisy data, and designing an annotation strategy that balanced scalability with accuracy. This strategy integrated rule-based filtering, iterative manual labeling with review, targeted data augmentation, and weak supervision using transformer-based classifiers to accelerate coverage. In parallel, a carefully curated subset of 7,000 reviews was manually annotated to support model development and evaluation. We benchmark a broad range of classification models, demonstrating that strong performance is achievable and providing a baseline for future research. HARPT is released as a public resource to support work in health informatics, cybersecurity, and natural language processing.",
      "authors": [
        "Timoteo Kelly",
        "Abdulkadir Korkmaz",
        "Samuel Mallet",
        "Connor Souders",
        "Sadra Aliakbarpour",
        "Praveen Rao"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19268",
        "HTML": "https://arxiv.org/html/2506.19268",
        "PDF": "https://arxiv.org/pdf/2506.19268"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Cryptography and Security (cs.CR)",
        "Emerging Technologies (cs.ET)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 02:59:14 GMT",
          "size": "213kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "HARPT: A Corpus for Analyzing Consumers' Trust and Privacy Concerns in Mobile Health Apps",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses the creation of a large-scale annotated corpus related to privacy and trust concerns in mobile health apps. It involves dataset curation and annotation, which are tangentially related to LLM training data."
      }
    },
    {
      "id": "2506.19279",
      "abstract": "The rising demand for mental health care has fueled interest in AI-driven counseling systems. While large language models (LLMs) offer significant potential, current approaches face challenges, including limited understanding of clients' psychological states and counseling stages, reliance on high-quality training data, and privacy concerns associated with commercial deployment. To address these issues, we propose EmoStage, a framework that enhances empathetic response generation by leveraging the inference capabilities of open-source LLMs without additional training data. Our framework introduces perspective-taking to infer clients' psychological states and support needs, enabling the generation of emotionally resonant responses. In addition, phase recognition is incorporated to ensure alignment with the counseling process and to prevent contextually inappropriate or inopportune responses. Experiments conducted in both Japanese and Chinese counseling settings demonstrate that EmoStage improves the quality of responses generated by base models and performs competitively with data-driven methods.",
      "authors": [
        "Zhiyang Qi",
        "Keiko Takamizo",
        "Mariko Ukiyo",
        "Michimasa Inaba"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19279",
        "HTML": "https://arxiv.org/html/2506.19279",
        "PDF": "https://arxiv.org/pdf/2506.19279"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 03:18:37 GMT",
          "size": "1686kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "EmoStage: A Framework for Accurate Empathetic Response Generation via Perspective-Taking and Phase Recognition",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The abstract mentions the reliance on high-quality training data and privacy concerns, indirectly linking it to LLM training data discussions, but the primary focus is on empathetic response generation, not data."
      }
    },
    {
      "id": "2506.19287",
      "abstract": "Symbolic execution is a widely used technique for test generation, offering systematic exploration of program paths through constraint solving. However, it is fundamentally constrained by the capability to model the target code including library functions in terms of symbolic constraint and the capability of underlying constraint solvers. As a result, many paths involving complex features remain unanalyzed or insufficiently modeled. Recent advances in large language models (LLMs) have shown promise in generating diverse and valid test inputs. Yet, LLMs lack mechanisms for systematically enumerating program paths and often fail to cover subtle corner cases. We observe that directly prompting an LLM with the full program leads to missed coverage of interesting paths. In this paper, we present PALM, a test generation system that combines symbolic path enumeration with LLM-assisted test generation. PALM statically enumerates possible paths through AST-level analysis and transforms each into an executable variant with embedded assertions that specify the target path. This avoids the need to translate path constraints into SMT formulae, by instead constructing program variants that LLM can interpret. Importantly, PALM is the first to provide an interactive frontend that visualizes path coverage alongside generated tests, assembling tests based on the specific paths they exercise. A user study with 12 participants demonstrates that PALM's frontend helps users better understand path coverage and identify which paths are actually exercised by PALM-generated tests, through verification and visualization of their path profiles.",
      "authors": [
        "Yaoxuan Wu",
        "Xiaojie Zhou",
        "Ahmad Humayun",
        "Muhammad Ali Gulzar",
        "Miryung Kim"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19287",
        "HTML": "https://arxiv.org/html/2506.19287",
        "PDF": "https://arxiv.org/pdf/2506.19287"
      },
      "subjects": [
        "Software Engineering (cs.SE)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 03:46:16 GMT",
          "size": "639kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Generating and Understanding Tests via Path-Aware Symbolic Execution with LLMs",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The discussion on LLM-assisted test generation indirectly relates to LLM capabilities, but the focus is not on LLM training data."
      }
    },
    {
      "id": "2506.19288",
      "abstract": "Automated waterway environment perception is crucial for enabling unmanned surface vessels (USVs) to understand their surroundings and make informed decisions. Most existing waterway perception models primarily focus on instance-level object perception paradigms (e.g., detection, segmentation). However, due to the complexity of waterway environments, current perception datasets and models fail to achieve global semantic understanding of waterways, limiting large-scale monitoring and structured log generation. With the advancement of vision-language models (VLMs), we leverage image captioning to introduce WaterCaption, the first captioning dataset specifically designed for waterway environments. WaterCaption focuses on fine-grained, multi-region long-text descriptions, providing a new research direction for visual geo-understanding and spatial scene cognition. Exactly, it includes 20.2k image-text pair data with 1.8 million vocabulary size. Additionally, we propose Da Yu, an edge-deployable multi-modal large language model for USVs, where we propose a novel vision-to-language projector called Nano Transformer Adaptor (NTA). NTA effectively balances computational efficiency with the capacity for both global and fine-grained local modeling of visual features, thereby significantly enhancing the model's ability to generate long-form textual outputs. Da Yu achieves an optimal balance between performance and efficiency, surpassing state-of-the-art models on WaterCaption and several other captioning benchmarks.",
      "authors": [
        "Runwei Guan",
        "Ningwei Ouyang",
        "Tianhao Xu",
        "Shaofeng Liang",
        "Wei Dai",
        "Yafeng Sun",
        "Shang Gao",
        "Songning Lai",
        "Shanliang Yao",
        "Xuming Hu",
        "Ryan Wen Liu",
        "Yutao Yue",
        "Hui Xiong"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19288",
        "HTML": "https://arxiv.org/html/2506.19288",
        "PDF": "https://arxiv.org/pdf/2506.19288"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 03:48:48 GMT",
          "size": "4816kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Da Yu: Towards USV-Based Image Captioning for Waterway Surveillance and Scene Understanding",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the paper introduces a dataset for image captioning, the primary focus is on visual geo-understanding rather than on LLM training data."
      }
    },
    {
      "id": "2506.19300",
      "abstract": "Open-Vocabulary Camouflaged Object Segmentation (OVCOS) seeks to segment and classify camouflaged objects from arbitrary categories, presenting unique challenges due to visual ambiguity and unseen categories.Recent approaches typically adopt a two-stage paradigm: first segmenting objects, then classifying the segmented regions using Vision Language Models (VLMs).However, these methods (1) suffer from a domain gap caused by the mismatch between VLMs' full-image training and cropped-region inference, and (2) depend on generic segmentation models optimized for well-delineated objects, making them less effective for camouflaged objects.Without explicit guidance, generic segmentation models often overlook subtle boundaries, leading to imprecise segmentation.In this paper,we introduce a novel VLM-guided cascaded framework to address these issues in OVCOS.For segmentation, we leverage the Segment Anything Model (SAM), guided by the VLM.Our framework uses VLM-derived features as explicit prompts to SAM, effectively directing attention to camouflaged regions and significantly improving localization accuracy.For classification, we avoid the domain gap introduced by hard cropping.Instead, we treat the segmentation output as a soft spatial prior via the alpha channel, which retains the full image context while providing precise spatial guidance, leading to more accurate and context-aware classification of camouflaged objects.The same VLM is shared across both segmentation and classification to ensure efficiency and semantic consistency.Extensive experiments on both OVCOS and conventional camouflaged object segmentation benchmarks demonstrate the clear superiority of our method, highlighting the effectiveness of leveraging rich VLM semantics for both segmentation and classification of camouflaged objects.",
      "authors": [
        "Kai Zhao",
        "Wubang Yuan",
        "Zheng Wang",
        "Guanyi Li",
        "Xiaoqiang Zhu",
        "Deng-ping Fan",
        "Dan Zeng"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19300",
        "HTML": "https://arxiv.org/html/2506.19300",
        "PDF": "https://arxiv.org/pdf/2506.19300"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 04:16:41 GMT",
          "size": "5079kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Open-Vocabulary Camouflaged Object Segmentation with Cascaded Vision Language Models",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses open-vocabulary segmentation and classification using vision language models, which indirectly touches on topics relevant to LLMs, but not specifically on training data for LLMs."
      }
    },
    {
      "id": "2506.19320",
      "abstract": "Traditional fundus image analysis models focus on single-modal tasks, ignoring fundus modality complementarity, which limits their versatility. Recently, retinal foundation models have emerged, but most still remain modality-specific. Integrating multiple fundus imaging modalities into a single foundation model is valuable. However, in dynamic environments, data from different modalities often arrive incrementally, necessitating continual pre-training. To address this, we propose RetCoP, the first continual vision-language pre-training framework in the fundus domain, which incrementally integrates image and text features from different imaging modalities into a single unified foundation model. To mitigate catastrophic forgetting in continual pre-training, we introduce a rehearsal strategy utilizing representative image-text pairs and an off-diagonal information distillation approach. The former allows the model to revisit knowledge from previous stages, while the latter explicitly preserves the alignment between image and text representations. Experiments show that RetCoP outperforms all the compared methods, achieving the best generalization and lowest forgetting rate. The code can be found at https://github.com/Yuang-Yao/RetCoP.",
      "authors": [
        "Yuang Yao",
        "Ruiqi Wu",
        "Yi Zhou",
        "and Tao Zhou"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19320",
        "HTML": "https://arxiv.org/html/2506.19320",
        "PDF": "https://arxiv.org/pdf/2506.19320"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 05:18:31 GMT",
          "size": "147kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Continual Retinal Vision-Language Pre-training upon Incremental Imaging Modalities",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses a vision-language pre-training framework in the fundus domain, incorporating rehearsal strategies to mitigate forgetting. While it mentions language data integration, its primary focus isn't on LLM training data."
      }
    },
    {
      "id": "2506.19329",
      "abstract": "Modern diagnostic workflows are increasingly multimodal, integrating diverse data sources such as medical images, structured records, and physiological time series. Among these, electrocardiograms (ECGs) and chest X-rays (CXRs) are two of the most widely used modalities for cardiac assessment. While CXRs provide rich diagnostic information, ECGs are more accessible and can support scalable early warning systems. In this work, we propose CroMoTEX, a novel contrastive learning-based framework that leverages chest X-rays during training to learn clinically informative ECG representations for multiple cardiac-related pathologies: cardiomegaly, pleural effusion, and edema. Our method aligns ECG and CXR representations using a novel supervised cross-modal contrastive objective with adaptive hard negative weighting, enabling robust and task-relevant feature learning. At test time, CroMoTEX relies solely on ECG input, allowing scalable deployment in real-world settings where CXRs may be unavailable. Evaluated on the large-scale MIMIC-IV-ECG and MIMIC-CXR datasets, CroMoTEX outperforms baselines across all three pathologies, achieving up to 78.31 AUROC on edema. Our code is available at github.com/vineetpmoorty/cromotex.",
      "authors": [
        "Vineet Punyamoorty",
        "Aditya Malusare",
        "Vaneet Aggarwal"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19329",
        "HTML": "https://arxiv.org/html/2506.19329",
        "PDF": "https://arxiv.org/pdf/2506.19329"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 05:47:26 GMT",
          "size": "1173kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Contrastive Cross-Modal Learning for Infusing Chest X-ray Knowledge into ECGs",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper involves cross-modal learning using contrastive datasets like MIMIC-IV-ECG and MIMIC-CXR, touching on data use for model training but does not focus on LLM training data."
      }
    },
    {
      "id": "2506.19331",
      "abstract": "This paper aims to achieve the segmentation of any 3D part in a scene based on natural language descriptions, extending beyond traditional object-level 3D scene understanding and addressing both data and methodological challenges. Due to the expensive acquisition and annotation burden, existing datasets and methods are predominantly limited to object-level comprehension. To overcome the limitations of data and annotation availability, we introduce the 3D-PU dataset, the first large-scale 3D dataset with dense part annotations, created through an innovative and cost-effective method for constructing synthetic 3D scenes with fine-grained part-level annotations, paving the way for advanced 3D-part scene understanding. On the methodological side, we propose OpenPart3D, a 3D-input-only framework to effectively tackle the challenges of part-level segmentation. Extensive experiments demonstrate the superiority of our approach in open-vocabulary 3D scene understanding tasks at the part level, with strong generalization capabilities across various 3D scene datasets.",
      "authors": [
        "Hongyu Wu",
        "Pengwan Yang",
        "Yuki M. Asano and Cees G. M. Snoek"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19331",
        "HTML": "https://arxiv.org/html/2506.19331",
        "PDF": "https://arxiv.org/pdf/2506.19331"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 05:51:22 GMT",
          "size": "13717kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Segment Any 3D-Part in a Scene from a Sentence",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The introduction of a 3D-PU dataset is indirectly related to data creation and annotation, which could be a distant aspect of data considerations in LLM training."
      }
    },
    {
      "id": "2506.19335",
      "abstract": "We tackle a new task of training neural network models that can assess subjective impressions conveyed through speech and assign scores accordingly, inspired by the work on automatic speech quality assessment (SQA). Speech impressions are often described using phrases like `cute voice.' We define such phrases as subjective voice descriptors (SVDs). Focusing on the difference in usage scenarios between the proposed task and automatic SQA, we design a framework capable of accommodating SVDs personalized to each individual, such as `my favorite voice.' In this work, we compiled a dataset containing speech labels derived from both abosolute category ratings (ACR) and comparison category ratings (CCR).\n  As an evaluation metric for assessment performance, we introduce ppref, the accuracy of the predicted score ordering of two samples on CCR test samples. Alongside the conventional model and learning methods based on ACR data, we also investigated RankNet learning using CCR data. We experimentally find that the ppref is moderate even with very limited training data. We also discover the CCR training is superior to the ACR training. These results support the idea that assessment models based on personalized SVDs, which typically must be trained on limited data, can be effectively learned from CCR data.",
      "authors": [
        "Yuto Kondo",
        "Hirokazu Kameoka",
        "Kou Tanaka",
        "Takuhiro Kaneko and Noboru Harada"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19335",
        "HTML": "https://arxiv.org/html/2506.19335",
        "PDF": "https://arxiv.org/pdf/2506.19335"
      },
      "subjects": [
        "Sound (cs.SD)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 06:00:05 GMT",
          "size": "324kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Learning to assess subjective impressions from speech",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "Touches on limited training data for neural network models, which could marginally relate to LLMs, but the main focus is on subjective speech impression assessment."
      }
    },
    {
      "id": "2506.19341",
      "abstract": "This study provides a detailed analysis of current advancements in dynamic object tracking (DOT) and trajectory prediction (TP) methodologies, including their applications and challenges. It covers various approaches, such as feature-based, segmentation-based, estimation-based, and learning-based methods, evaluating their effectiveness, deployment, and limitations in real-world scenarios. The study highlights the significant impact of these technologies in automotive and autonomous vehicles, surveillance and security, healthcare, and industrial automation, contributing to safety and efficiency. Despite the progress, challenges such as improved generalization, computational efficiency, reduced data dependency, and ethical considerations still exist. The study suggests future research directions to address these challenges, emphasizing the importance of multimodal data integration, semantic information fusion, and developing context-aware systems, along with ethical and privacy-preserving frameworks.",
      "authors": [
        "Zhongping Dong",
        "Liming Chen",
        "Mohand Tahar Kechadi"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19341",
        "HTML": "https://arxiv.org/html/2506.19341",
        "PDF": "https://arxiv.org/pdf/2506.19341"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 06:10:01 GMT",
          "size": "6816kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Trajectory Prediction in Dynamic Object Tracking: A Critical Study",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "Mentions data dependency and ethical considerations in trajectory prediction, vaguely linking to LLM data issues, but the focus is not on training data."
      }
    },
    {
      "id": "2506.19352",
      "abstract": "Ensuring persona fidelity in large language models (LLMs) is essential for maintaining coherent and engaging human-AI interactions. However, LLMs often exhibit Out-of-Character (OOC) behavior, where generated responses deviate from an assigned persona, leading to inconsistencies that affect model reliability. Existing evaluation methods typically assign single scores to entire responses, struggling to capture subtle persona misalignment, particularly in long-form text generation. To address this limitation, we propose an atomic-level evaluation framework that quantifies persona fidelity at a finer granularity. Our three key metrics measure the degree of persona alignment and consistency within and across generations. Our approach enables a more precise and realistic assessment of persona fidelity by identifying subtle deviations that real users would encounter. Through our experiments, we demonstrate that our framework effectively detects persona inconsistencies that prior methods overlook. By analyzing persona fidelity across diverse tasks and personality types, we reveal how task structure and persona desirability influence model adaptability, highlighting challenges in maintaining consistent persona expression.",
      "authors": [
        "Jisu Shin",
        "Juhyun Oh",
        "Eunsu Kim",
        "Hoyun Song",
        "Alice Oh"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19352",
        "HTML": "https://arxiv.org/html/2506.19352",
        "PDF": "https://arxiv.org/pdf/2506.19352"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 06:33:10 GMT",
          "size": "469kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The evaluation of persona fidelity in LLMs indirectly concerns training data quality since maintaining persona fidelity may involve ensuring appropriate training samples. However, it is not the main focus of the paper."
      }
    },
    {
      "id": "2506.19360",
      "abstract": "Advances in generative models have transformed the field of synthetic image generation for privacy-preserving data synthesis (PPDS). However, the field lacks a comprehensive survey and comparison of synthetic image generation methods across diverse settings. In particular, when we generate synthetic images for the purpose of training a classifier, there is a pipeline of generation-sampling-classification which takes private training as input and outputs the final classifier of interest. In this survey, we systematically categorize existing image synthesis methods, privacy attacks, and mitigations along this generation-sampling-classification pipeline. To empirically compare diverse synthesis approaches, we provide a benchmark with representative generative methods and use model-agnostic membership inference attacks (MIAs) as a measure of privacy risk. Through this study, we seek to answer critical questions in PPDS: Can synthetic data effectively replace real data? Which release strategy balances utility and privacy? Do mitigations improve the utility-privacy tradeoff? Which generative models perform best across different scenarios? With a systematic evaluation of diverse methods, our study provides actionable insights into the utility-privacy tradeoffs of synthetic data generation methods and guides the decision on optimal data releasing strategies for real-world applications.",
      "authors": [
        "Yunsung Chung",
        "Yunbei Zhang",
        "Nassir Marrouche",
        "Jihun Hamm"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19360",
        "HTML": "https://arxiv.org/html/2506.19360",
        "PDF": "https://arxiv.org/pdf/2506.19360"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 06:41:34 GMT",
          "size": "3840kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "SoK: Can Synthetic Images Replace Real Data? A Survey of Utility and Privacy of Synthetic Image Generation",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper surveys synthetic image generation methods, focusing on privacy and utility. It indirectly touches on data generation and privacy relevant to training data but is not focused on LLM training data."
      }
    },
    {
      "id": "2506.19364",
      "abstract": "The integration of Generative AI (GenAI) into education has raised concerns about over-reliance and superficial learning, particularly in writing tasks in higher education. This study explores whether a theory-driven learning analytics dashboard (LAD) can enhance human-AI collaboration in the academic writing task by improving writing knowledge gains, fostering self-regulated learning (SRL) skills and building different human-AI dialogue characteristics. Grounded in Zimmerman's SRL framework, the LAD provided real-time feedback on learners' goal-setting, writing processes and reflection, while monitoring the quality of learner-AI interactions. A quasi-experiment was conducted involving 52 postgraduate students divided into an experimental group (EG) using the LAD to a control group (CG) without it in a human-AI collaborative writing task. Pre- and post- knowledge tests, questionnaires measuring SRL and cognitive load, and students' dialogue data with GenAI were collected and analyzed. Results showed that the EG achieved significantly higher writing knowledge gains and improved SRL skills, particularly in self-efficacy and cognitive strategies. However, the EG also reported increased test anxiety and cognitive load, possibly due to heightened metacognitive awareness. Epistemic Network Analysis revealed that the EG engaged in more reflective, evaluative interactions with GenAI, while the CG focused on more transactional and information-seeking exchanges. These findings contribute to the growing body of literature on the educational use of GenAI and highlight the importance of designing interventions that complement GenAI tools, ensuring that technology enhances rather than undermines the learning process.",
      "authors": [
        "Angxuan Chen",
        "Jingjing Lian",
        "Xinran Kuang",
        "Jiyou Jia"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19364",
        "HTML": "https://arxiv.org/html/2506.19364",
        "PDF": "https://arxiv.org/pdf/2506.19364"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 06:51:13 GMT",
          "size": "1085kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Can theory-driven learning analytics dashboard enhance human-AI collaboration in writing learning? Insights from an empirical experiment",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the study involves GenAI in educational settings, it indirectly relates to LLM training data through its discussion on human-AI interactions and educational use. However, LLM training data is not the focal point of the research."
      }
    },
    {
      "id": "2506.19368",
      "abstract": "Data trading is one of the key focuses of Web 3.0. However, all the current methods that rely on blockchain-based smart contracts for data exchange cannot support large-scale data trading while ensuring data security, which falls short of fulfilling the spirit of Web 3.0. Even worse, there is currently a lack of discussion on the essential properties that large-scale data trading should satisfy. In this work, we are the first to formalize the property requirements for enabling data trading in Web 3.0. Based on these requirements, we are the first to propose Yotta, a complete batch data trading scheme for blockchain, which features a data trading design that leverages our innovative cryptographic workflow with IPFS and zk-SNARK. Our simulation results demonstrate that Yotta outperforms baseline approaches up to 130 times and exhibits excellent scalability to satisfy all the properties.",
      "authors": [
        "Xiang Liu",
        "Zhanpeng Guo",
        "Liangxi Liu",
        "Mengyao Zheng",
        "Yiming Qiu",
        "Linshan Jiang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19368",
        "HTML": "https://arxiv.org/html/2506.19368",
        "PDF": "https://arxiv.org/pdf/2506.19368"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 06:57:25 GMT",
          "size": "198kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Yotta: A Large-Scale Trustless Data Trading Scheme for Blockchain System",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper addresses data trading schemes, which can indirectly relate to data used in training language models, especially in contexts where LLMs might require acquiring data through such means. However, it is focused more on blockchain technology and cryptographic workflows than on specifics of LLM training data."
      }
    },
    {
      "id": "2506.19382",
      "abstract": "There is growing interest in leveraging mechanistic interpretability and controllability to better understand and influence the internal dynamics of large language models (LLMs). However, current methods face fundamental challenges in reliably localizing and manipulating feature representations. Sparse Autoencoders (SAEs) have recently emerged as a promising direction for feature extraction at scale, yet they, too, are limited by incomplete feature isolation and unreliable monosemanticity. To systematically quantify these limitations, we introduce Feature Monosemanticity Score (FMS), a novel metric to quantify feature monosemanticity in latent representation. Building on these insights, we propose Guided Sparse Autoencoders (G-SAE), a method that conditions latent representations on labeled concepts during training. We demonstrate that reliable localization and disentanglement of target concepts within the latent space improve interpretability, detection of behavior, and control. Specifically, our evaluations on toxicity detection, writing style identification, and privacy attribute recognition show that G-SAE not only enhances monosemanticity but also enables more effective and fine-grained steering with less quality degradation. Our findings provide actionable guidelines for measuring and advancing mechanistic interpretability and control of LLMs.",
      "authors": [
        "Ruben H\\\"arle",
        "Felix Friedrich",
        "Manuel Brack",
        "Stephan W\\\"aldchen",
        "Bj\\\"orn Deiseroth",
        "Patrick Schramowski",
        "Kristian Kersting"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19382",
        "HTML": "https://arxiv.org/html/2506.19382",
        "PDF": "https://arxiv.org/pdf/2506.19382"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 07:18:20 GMT",
          "size": "1030kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Measuring and Guiding Monosemanticity",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses mechanistic interpretability and controllability of LLMs using Sparse Autoencoders, but it does not focus on LLM training data. The connection to data is indirect, relating to the manipulation and representation of features within LLMs."
      }
    },
    {
      "id": "2506.19389",
      "abstract": "We investigate how the ability to recognize textual content within images emerges during the training of Vision-Language Models (VLMs). Our analysis reveals a critical phenomenon: the ability to read textual information in a given image \\textbf{(text readability)} emerges abruptly after substantial training iterations, in contrast to semantic content understanding which develops gradually from the early stages of training. This delayed emergence may reflect how contrastive learning tends to initially prioritize general semantic understanding, with text-specific symbolic processing developing later. Interestingly, the ability to match images with rendered text develops even slower, indicating a deeper need for semantic integration. These findings highlight the need for tailored training strategies to accelerate robust text comprehension in VLMs, laying the groundwork for future research on optimizing multimodal learning.",
      "authors": [
        "Jaeyoo Park",
        "Sanghyuk Chun",
        "Wonjae Kim",
        "Sangdoo Yun",
        "Bohyung Han"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19389",
        "HTML": "https://arxiv.org/html/2506.19389",
        "PDF": "https://arxiv.org/pdf/2506.19389"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 07:35:32 GMT",
          "size": "303kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Emergence of Text Readability in Vision Language Models",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper explores text readability in Vision-Language Models, which indirectly pertains to the training of models with textual data, but does not primarily focus on LLM training data."
      }
    },
    {
      "id": "2506.19398",
      "abstract": "This paper introduces ClearerVoice-Studio, an open-source, AI-powered speech processing toolkit designed to bridge cutting-edge research and practical application. Unlike broad platforms like SpeechBrain and ESPnet, ClearerVoice-Studio focuses on interconnected speech tasks of speech enhancement, separation, super-resolution, and multimodal target speaker extraction. A key advantage is its state-of-the-art pretrained models, including FRCRN with 3 million uses and MossFormer with 2.5 million uses, optimized for real-world scenarios. It also offers model optimization tools, multi-format audio support, the SpeechScore evaluation toolkit, and user-friendly interfaces, catering to researchers, developers, and end-users. Its rapid adoption attracting 3000 GitHub stars and 239 forks highlights its academic and industrial impact. This paper details ClearerVoice-Studio's capabilities, architectures, training strategies, benchmarks, community impact, and future plan. Source code is available at https://github.com/modelscope/ClearerVoice-Studio.",
      "authors": [
        "Shengkui Zhao",
        "Zexu Pan",
        "Bin Ma"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19398",
        "HTML": "https://arxiv.org/html/2506.19398",
        "PDF": "https://arxiv.org/pdf/2506.19398"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 08:01:33 GMT",
          "size": "50kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "ClearerVoice-Studio: Bridging Advanced Speech Processing Research and Practical Deployment",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While primarily about a speech processing toolkit, the mention of 'training strategies' for its models could touch on data usage. However, it doesn't focus on LLM training data specifically."
      }
    },
    {
      "id": "2506.19410",
      "abstract": "This paper introduces a novel approach, Unsupervised Dataset Dictionary Learning (U-DaDiL), for totally unsupervised robust clustering applied to sitting posture identification. Traditional methods often lack adaptability to diverse datasets and suffer from domain shift issues. U-DaDiL addresses these challenges by aligning distributions from different datasets using Wasserstein barycenter based representation. Experimental evaluations on the Office31 dataset demonstrate significant improvements in cluster alignment accuracy. This work also presents a promising step for addressing domain shift and robust clustering for unsupervised sitting posture identification",
      "authors": [
        "Anas Hattay",
        "Mayara Ayat",
        "and Fred Ngole Mboula"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19410",
        "HTML": "https://arxiv.org/html/2506.19410",
        "PDF": "https://arxiv.org/pdf/2506.19410"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 08:28:29 GMT",
          "size": "334kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Unsupervised Dataset Dictionary Learning for domain shift robust clustering: application to sitting posture identification",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses dataset dictionary learning for clustering, which has an indirect relation to LLM training data as it addresses domain adaptation and the organization of data (datasets). However, its primary focus is on clustering methodologies rather than LLM training data specifics."
      }
    },
    {
      "id": "2506.19416",
      "abstract": "Existing micro aerial vehicle (MAV) detection methods mainly rely on the target's appearance features in RGB images, whose diversity makes it difficult to achieve generalized MAV detection. We notice that different types of MAVs share the same distinctive features in event streams due to their high-speed rotating propellers, which are hard to see in RGB images. This paper studies how to detect different types of MAVs from an event camera by fully exploiting the features of propellers in the original event stream. The proposed method consists of three modules to extract the salient and spatio-temporal features of the propellers while filtering out noise from background objects and camera motion. Since there are no existing event-based MAV datasets, we introduce a novel MAV dataset for the community. This is the first event-based MAV dataset comprising multiple scenarios and different types of MAVs. Without training, our method significantly outperforms state-of-the-art methods and can deal with challenging scenarios, achieving a precision rate of 83.0\\% (+30.3\\%) and a recall rate of 81.5\\% (+36.4\\%) on the proposed testing dataset. The dataset and code are available at: https://github.com/WindyLab/EvDetMAV.",
      "authors": [
        "Yin Zhang",
        "Zian Ning",
        "Xiaoyu Zhang",
        "Shiliang Guo",
        "Peidong Liu",
        "Shiyu Zhao"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19416",
        "HTML": "https://arxiv.org/html/2506.19416",
        "PDF": "https://arxiv.org/pdf/2506.19416"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 08:35:15 GMT",
          "size": "6140kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "EvDetMAV: Generalized MAV Detection from Moving Event Cameras",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper introduces an event-based dataset for MAV detection, which slightly relates to dataset creation. However, its primary focus is on MAV detection techniques using event cameras, and it doesn't significantly contribute to discussions on LLM training data."
      }
    },
    {
      "id": "2506.19418",
      "abstract": "Incorporating explicit reasoning rules within the latent space of language models (LMs) offers a promising pathway to enhance generalisation, interpretability, and controllability. While current Transformer-based language models have shown strong performance on Natural Language Inference (NLI) tasks, they often rely on memorisation rather than rule-based inference. This work investigates how reasoning rules can be explicitly embedded and memorised within the LMs through Language Variational Autoencoders (VAEs). We propose a complete pipeline for learning reasoning rules within Transformer-based language VAEs. This pipeline encompasses three rule-based reasoning tasks, a supporting theoretical framework, and a practical end-to-end architecture. The experiment illustrates the following findings: Disentangled reasoning: Under explicit signal supervision, reasoning rules - viewed as functional mappings - can be disentangled within the encoder's parametric space. This separation results in distinct clustering of rules in the output feature space. Prior knowledge injection: injecting reasoning information into the Query enables the model to more effectively retrieve the stored value Value from memory based on Key. This approach offers a simple method for integrating prior knowledge into decoder-only language models. Performance bottleneck: In mathematical reasoning tasks using Qwen2.5(0.5B), increasing sample count doesn't improve performance beyond a point. Moreover, ffn layers are better than attention layers at preserving the separation of reasoning rules in the model's parameters.",
      "authors": [
        "Yingji Zhang",
        "Marco Valentino",
        "Danilo S. Carvalho",
        "Andr\\'e Freitas"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19418",
        "HTML": "https://arxiv.org/html/2506.19418",
        "PDF": "https://arxiv.org/pdf/2506.19418"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 08:38:03 GMT",
          "size": "41865kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Learning to Disentangle Latent Reasoning Rules with Language VAEs: A Systematic Study",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper involves language models and discusses disentangling latent reasoning rules with language VAEs. It indirectly relates to LLM training data through the discussion of prior knowledge injection into models, but does not focus on dataset creation or data quality for LLMs."
      }
    },
    {
      "id": "2506.19430",
      "abstract": "The automated analysis of human behaviour provides many opportunities for the creation of interactive systems and the post-experiment investigations for user studies. Commodity depth cameras offer reasonable body tracking accuracy at a low price point, without the need for users to wear or hold any extra equipment. The resulting systems typically perform body tracking through a dedicated machine learning model, but they can be enhanced with additional AI components providing extra capabilities. This leads to opportunities but also challenges, for example regarding the orchestration of such AI components and the engineering of the resulting tracking pipeline. In this paper, we discuss these elements, based on our experience with the creation of a remote collaboration system across distant wall-sized displays, that we built using existing and readily available building blocks, including AI-based recognition models.",
      "authors": [
        "Adrien Coppens and Val\\'erie Maquil"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19430",
        "HTML": "https://arxiv.org/html/2506.19430",
        "PDF": "https://arxiv.org/pdf/2506.19430"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 08:55:06 GMT",
          "size": "1387kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Integrating AIs With Body Tracking Technology for Human Behaviour Analysis: Challenges and Opportunities",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses the integration of AI components with body tracking technology, which may touch on data from body tracking models, but there is no direct focus on LLM training data or datasets."
      }
    },
    {
      "id": "2506.19433",
      "abstract": "Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce \\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system that can augment any VLN backbone. Mem4Nav fuses a sparse octree for fine-grained voxel indexing with a semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via a reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13 pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav.",
      "authors": [
        "Lixuan He",
        "Haoyu Dong",
        "Zhenxing Chen",
        "Yangcheng Yu",
        "Jie Feng",
        "Yong Li"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19433",
        "HTML": "https://arxiv.org/html/2506.19433",
        "PDF": "https://arxiv.org/pdf/2506.19433"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 09:00:43 GMT",
          "size": "33275kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the paper focuses on Vision-and-Language Navigation, it discusses architectures that might work with large language models but does not specifically address LLM training data aspects."
      }
    },
    {
      "id": "2506.19441",
      "abstract": "Evaluation of Text to Speech (TTS) systems is challenging and resource-intensive. Subjective metrics such as Mean Opinion Score (MOS) are not easily comparable between works. Objective metrics are frequently used, but rarely validated against subjective ones. Both kinds of metrics are challenged by recent TTS systems capable of producing synthetic speech indistinguishable from real speech. In this work, we introduce Text to Speech Distribution Score 2 (TTSDS2), a more robust and improved version of TTSDS. Across a range of domains and languages, it is the only one out of 16 compared metrics to correlate with a Spearman correlation above 0.50 for every domain and subjective score evaluated. We also release a range of resources for evaluating synthetic speech close to real speech: A dataset with over 11,000 subjective opinion score ratings; a pipeline for continually recreating a multilingual test dataset to avoid data leakage; and a continually updated benchmark for TTS in 14 languages.",
      "authors": [
        "Christoph Minixhofer",
        "Ondrej Klejch",
        "Peter Bell"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19441",
        "HTML": "https://arxiv.org/html/2506.19441",
        "PDF": "https://arxiv.org/pdf/2506.19441"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Computation and Language (cs.CL)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 09:12:02 GMT",
          "size": "779kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "TTSDS2: Resources and Benchmark for Evaluating Human-Quality Text to Speech Systems",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper involves datasets for evaluating Text to Speech systems, which can touch on data collection and curation. However, LLM training data is not the main focus."
      }
    },
    {
      "id": "2506.19453",
      "abstract": "Software supply chain vulnerabilities arise when attackers exploit weaknesses by injecting vulnerable code into widely used packages or libraries within software repositories. While most existing approaches focus on identifying vulnerable packages or libraries, they often overlook the specific functions responsible for these vulnerabilities. Pinpointing vulnerable functions within packages or libraries is critical, as it can significantly reduce the risks associated with using open-source software. Identifying vulnerable patches is challenging because developers often submit code changes that are unrelated to vulnerability fixes. To address this issue, this paper introduces FuncVul, an innovative code chunk-based model for function-level vulnerability detection in C/C++ and Python, designed to identify multiple vulnerabilities within a function by focusing on smaller, critical code segments. To assess the model's effectiveness, we construct six code and generic code chunk based datasets using two approaches: (1) integrating patch information with large language models to label vulnerable samples and (2) leveraging large language models alone to detect vulnerabilities in function-level code. To design FuncVul vulnerability model, we utilise GraphCodeBERT fine tune model that captures both the syntactic and semantic aspects of code. Experimental results show that FuncVul outperforms existing state-of-the-art models, achieving an average accuracy of 87-92% and an F1 score of 86-92% across all datasets. Furthermore, we have demonstrated that our code-chunk-based FuncVul model improves 53.9% accuracy and 42.0% F1-score than the full function-based vulnerability prediction. The FuncVul code and datasets are publicly available on GitHub at https://github.com/sajalhalder/FuncVul.",
      "authors": [
        "Sajal Halder",
        "Muhammad Ejaz Ahmed",
        "Seyit Camtepe"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19453",
        "HTML": "https://arxiv.org/html/2506.19453",
        "PDF": "https://arxiv.org/pdf/2506.19453"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 09:30:40 GMT",
          "size": "916kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "FuncVul: An Effective Function Level Vulnerability Detection Model using LLM and Code Chunk",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper mentions the use of large language models for function-level vulnerability detection, but does not focus on LLM training data itself, rather it uses LLMs as part of their methodology."
      }
    },
    {
      "id": "2506.19465",
      "abstract": "Modern deep learning models in computer vision require large datasets of real images, which are difficult to curate and pose privacy and legal concerns, limiting their commercial use. Recent works suggest synthetic data as an alternative, yet models trained with it often underperform. This paper proposes a two-step approach to bridge this gap. First, we propose an improved neural fractal formulation through which we introduce a new class of synthetic data. Second, we propose reverse stylization, a technique that transfers visual features from a small, license-free set of real images onto synthetic datasets, enhancing their effectiveness. We analyze the domain gap between our synthetic datasets and real images using Kernel Inception Distance (KID) and show that our method achieves a significantly lower distributional gap compared to existing synthetic datasets. Furthermore, our experiments across different tasks demonstrate the practical impact of this reduced gap. We show that pretraining the EDM2 diffusion model on our synthetic dataset leads to an 11% reduction in FID during image generation, compared to models trained on existing synthetic datasets, and a 20% decrease in autoencoder reconstruction error, indicating improved performance in data representation. Furthermore, a ViT-S model trained for classification on this synthetic data achieves over a 10% improvement in ImageNet-100 accuracy. Our work opens up exciting possibilities for training practical models when sufficiently large real training sets are not available.",
      "authors": [
        "Farnood Salehi",
        "Vandit Sharma",
        "Amirhossein Askari Farsangi",
        "Tun\\c{c} Ozan Ayd{\\i}n"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19465",
        "HTML": "https://arxiv.org/html/2506.19465",
        "PDF": "https://arxiv.org/pdf/2506.19465"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 09:47:31 GMT",
          "size": "27481kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Stylized Structural Patterns for Improved Neural Network Pre-training",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "This paper discusses synthetic data for neural network pre-training and mentions privacy and legal concerns about data, which indirectly relates to data issues in LLM training. But it does not directly focus on LLM training data."
      }
    },
    {
      "id": "2506.19467",
      "abstract": "Human annotation variation (i.e., annotation disagreements) is common in NLP and often reflects important information such as task subjectivity and sample ambiguity. While Large Language Models (LLMs) are increasingly used for automatic annotation to reduce human effort, their evaluation often focuses on predicting the majority-voted \"ground truth\" labels. It is still unclear, however, whether these models also capture informative human annotation variation. Our work addresses this gap by extensively evaluating LLMs' ability to predict annotation disagreements without access to repeated human labels. Our results show that LLMs struggle with modeling disagreements, which can be overlooked by majority label-based evaluations. Notably, while RLVR-style (Reinforcement learning with verifiable rewards) reasoning generally boosts LLM performance, it degrades performance in disagreement prediction. Our findings highlight the critical need for evaluating and improving LLM annotators in disagreement modeling. Code and data at https://github.com/EdisonNi-hku/Disagreement_Prediction.",
      "authors": [
        "Jingwei Ni",
        "Yu Fan",
        "Vil\\'em Zouhar",
        "Donya Rooein",
        "Alexander Hoyle",
        "Mrinmaya Sachan",
        "Markus Leippold",
        "Dirk Hovy",
        "Elliott Ash"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19467",
        "HTML": "https://arxiv.org/html/2506.19467",
        "PDF": "https://arxiv.org/pdf/2506.19467"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 09:49:26 GMT",
          "size": "9537kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Can Large Language Models Capture Human Annotator Disagreements?",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper evaluates LLMs on capturing human annotation disagreements, indirectly touching on LLM data annotation processes without explicit focus on training datasets."
      }
    },
    {
      "id": "2506.19468",
      "abstract": "Multilingual large language models (LLMs) are advancing rapidly, with new models frequently claiming support for an increasing number of languages. However, existing evaluation datasets are limited and lack cross-lingual alignment, leaving assessments of multilingual capabilities fragmented in both language and skill coverage. To address this, we introduce MuBench, a benchmark covering 61 languages and evaluating a broad range of capabilities. We evaluate several state-of-the-art multilingual LLMs and find notable gaps between claimed and actual language coverage, particularly a persistent performance disparity between English and low-resource languages. Leveraging MuBench's alignment, we propose Multilingual Consistency (MLC) as a complementary metric to accuracy for analyzing performance bottlenecks and guiding model improvement. Finally, we pretrain a suite of 1.2B-parameter models on English and Chinese with 500B tokens, varying language ratios and parallel data proportions to investigate cross-lingual transfer dynamics.",
      "authors": [
        "Wenhan Han and Yifan Zhang and Zhixun Chen and Binbin Liu and Haobin Lin and Bingni Zhang and Taifeng Wang and Mykola Pechenizkiy and Meng Fang and Yin Zheng"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19468",
        "HTML": "https://arxiv.org/html/2506.19468",
        "PDF": "https://arxiv.org/pdf/2506.19468"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 09:53:00 GMT",
          "size": "1097kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "MuBench assesses multilingual capabilities, potentially involving LLM training in different languages, but it primarily focuses on evaluation rather than the training data itself."
      }
    },
    {
      "id": "2506.19469",
      "abstract": "In recent years, significant progress has been made in the field of surgical scene understanding, particularly in the task of Visual Question Localized-Answering in robotic surgery (Surgical-VQLA). However, existing Surgical-VQLA models lack deep reasoning capabilities and interpretability in surgical scenes, which limits their reliability and potential for development in clinical applications. To address this issue, inspired by the development of Reasoning Multimodal Large Language Models (MLLMs), we first build the Surgery-R1-54k dataset, including paired data for Visual-QA, Grounding-QA, and Chain-of-Thought (CoT). Then, we propose the first Reasoning MLLM for Surgical-VQLA (Surgery-R1). In our Surgery-R1, we design a two-stage fine-tuning mechanism to enable the basic MLLM with complex reasoning abilities by utilizing supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). Furthermore, for an efficient and high-quality rule-based reward system in our RFT, we design a Multimodal Coherence reward mechanism to mitigate positional illusions that may arise in surgical scenarios. Experiment results demonstrate that Surgery-R1 outperforms other existing state-of-the-art (SOTA) models in the Surgical-VQLA task and widely-used MLLMs, while also validating its reasoning capabilities and the effectiveness of our approach. The code and dataset will be organized in https://github.com/FiFi-HAO467/Surgery-R1.",
      "authors": [
        "Pengfei Hao",
        "Shuaibo Li",
        "Hongqiu Wang",
        "Zhizhuo Kou",
        "Junhang Zhang",
        "Guang Yang",
        "Lei Zhu"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19469",
        "HTML": "https://arxiv.org/html/2506.19469",
        "PDF": "https://arxiv.org/pdf/2506.19469"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 09:53:10 GMT",
          "size": "3474kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Surgery-R1: Advancing Surgical-VQLA with Reasoning Multimodal Large Language Model via Reinforcement Learning",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses a multimodal large language model for surgical visual question answering, which involves the creation of a dataset. While this indirectly relates to LLM training data, the focus is on the surgical domain and fine-tuning, not on general LLM data issues."
      }
    },
    {
      "id": "2506.19484",
      "abstract": "Large Language Models (LLMs) are rapidly transforming education by enabling rich conversational learning experiences. This article provides a comprehensive review of how LLM-based conversational agents are being used in higher education, with extensions to secondary and lifelong learning contexts. We synthesize existing literature on LLMs in education and theories of conversational and dialogic pedagogy - including Vygotsky's sociocultural learning (scaffolding and the Zone of Proximal Development), the Socratic method, and Laurillard's conversational framework - and examine how prompting strategies and retrieval-augmented generation (RAG) can align LLM behaviors with these pedagogical theories, and how it can support personalized, adaptive learning. We map educational theories to LLM capabilities, highlighting where LLM-driven dialogue supports established learning principles and where it challenges or falls short of traditional pedagogical assumptions. Notable gaps in applying prior theories to LLMs are identified, such as the models tendency to provide direct answers instead of fostering co-construction of knowledge, and the need to account for the constant availability and broad but non-human expertise of LLM tutors. In response, we propose practical strategies to better align LLM interactions with sound pedagogy - for example, designing prompts that encourage Socratic questioning, scaffolded guidance, and student reflection, as well as integrating retrieval mechanisms to ensure accuracy and contextual relevance. Our aim is to bridge the gap between educational theory and the emerging practice of AI-driven conversational learning, offering insights and tools for making LLM-based dialogues more educationally productive and theory-aligned.",
      "authors": [
        "Russell Beale"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19484",
        "HTML": "https://arxiv.org/html/2506.19484",
        "PDF": "https://arxiv.org/pdf/2506.19484"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 10:19:09 GMT",
          "size": "159kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While it discusses the application of LLMs in education and dialogues, the focus is on pedagogical alignment rather than on LLM training data, placing it in a weak relevance category."
      }
    },
    {
      "id": "2506.19486",
      "abstract": "Machine Unlearning (MU) technology facilitates the removal of the influence of specific data instances from trained models on request. Despite rapid advancements in MU technology, its vulnerabilities are still underexplored, posing potential risks of privacy breaches through leaks of ostensibly unlearned information. Current limited research on MU attacks requires access to original models containing privacy data, which violates the critical privacy-preserving objective of MU. To address this gap, we initiate an innovative study on recalling the forgotten class memberships from unlearned models (ULMs) without requiring access to the original one. Specifically, we implement a Membership Recall Attack (MRA) framework with a teacher-student knowledge distillation architecture, where ULMs serve as noisy labelers to transfer knowledge to student models. Then, it is translated into a Learning with Noisy Labels (LNL) problem for inferring the correct labels of the forgetting instances. Extensive experiments on state-of-the-art MU methods with multiple real datasets demonstrate that the proposed MRA strategy exhibits high efficacy in recovering class memberships of unlearned instances. As a result, our study and evaluation have established a benchmark for future research on MU vulnerabilities.",
      "authors": [
        "Zhihao Sui",
        "Liang Hu",
        "Jian Cao",
        "Dora D. Liu",
        "Usman Naseem",
        "Zhongyuan Lai",
        "Qi Zhang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19486",
        "HTML": "https://arxiv.org/html/2506.19486",
        "PDF": "https://arxiv.org/pdf/2506.19486"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 10:21:10 GMT",
          "size": "579kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Recalling The Forgotten Class Memberships: Unlearned Models Can Be Noisy Labelers to Leak Privacy",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper addresses machine unlearning and membership recall attacks, which concern data privacy and potentially the handling of data. This has indirect relevance to LLM training data particularly with respect to privacy and data removal techniques."
      }
    },
    {
      "id": "2506.19492",
      "abstract": "Large Reasoning Models (LRMs) have achieved remarkable performance on complex tasks by engaging in extended reasoning before producing final answers, yet this strength introduces the risk of overthinking, where excessive token generation occurs even for simple tasks. While recent work in efficient reasoning seeks to reduce reasoning length while preserving accuracy, it remains unclear whether such optimization is truly a free lunch. Drawing on the intuition that compressing reasoning may reduce the robustness of model responses and lead models to omit key reasoning steps, we investigate whether efficient reasoning strategies introduce behavioral inconsistencies. To systematically assess this, we introduce $ICBENCH$, a benchmark designed to measure inconsistency in LRMs across three dimensions: inconsistency across task settings (ITS), inconsistency between training objectives and learned behavior (TR-LB), and inconsistency between internal reasoning and self-explanations (IR-SE). Applying $ICBENCH$ to a range of open-source LRMs, we find that while larger models generally exhibit greater consistency than smaller ones, they all display widespread \"scheming\" behaviors, including self-disagreement, post-hoc rationalization, and the withholding of reasoning cues. Crucially, our results demonstrate that efficient reasoning strategies such as No-Thinking and Simple Token-Budget consistently increase all three defined types of inconsistency. These findings suggest that although efficient reasoning enhances token-level efficiency, further investigation is imperative to ascertain whether it concurrently introduces the risk of models evading effective supervision.",
      "authors": [
        "Shu Yang",
        "Junchao Wu",
        "Xuansheng Wu",
        "Derek Wong",
        "Ninhao Liu",
        "Di Wang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19492",
        "HTML": "https://arxiv.org/html/2506.19492",
        "PDF": "https://arxiv.org/pdf/2506.19492"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 10:25:28 GMT",
          "size": "217kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Is Long-to-Short a Free Lunch? Investigating Inconsistency and Reasoning Efficiency in LRMs",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses efficient reasoning in Large Reasoning Models (LRMs), which indirectly relates to LLMs but not specifically to training data. It examines model behavior and reasoning, which are related to the performance of LLMs but not to their training data specifically."
      }
    },
    {
      "id": "2506.19496",
      "abstract": "Large deep learning models have achieved significant success in various tasks. However, the performance of a model can significantly degrade if it is needed to train on datasets with noisy labels with misleading or ambiguous information. To date, there are limited investigations on how to restore performance when model degradation has been incurred by noisy label data. Inspired by the ``forgetting mechanism'' in neuroscience, which enables accelerating the relearning of correct knowledge by unlearning the wrong knowledge, we propose a robust model restoration and refinement (MRR) framework COLUR, namely Confidence-Oriented Learning, Unlearning and Relearning. Specifically, we implement COLUR with an efficient co-training architecture to unlearn the influence of label noise, and then refine model confidence on each label for relearning. Extensive experiments are conducted on four real datasets and all evaluation results show that COLUR consistently outperforms other SOTA methods after MRR.",
      "authors": [
        "Zhihao Sui",
        "Liang Hu",
        "Jian Cao",
        "Usman Naseem",
        "Zhongyuan Lai",
        "Qi Zhang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19496",
        "HTML": "https://arxiv.org/html/2506.19496",
        "PDF": "https://arxiv.org/pdf/2506.19496"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 10:33:48 GMT",
          "size": "3003kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "COLUR: Confidence-Oriented Learning, Unlearning and Relearning with Noisy-Label Data for Model Restoration and Refinement",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper addresses issues of noisy-label data in training models. While it discusses data quality, which is relevant to LLMs, its primary focus is on the algorithm for model restoration and not on the data used in LLM training."
      }
    },
    {
      "id": "2506.19498",
      "abstract": "Building a general robotic manipulation system capable of performing a wide variety of tasks in real-world settings is a challenging task. Vision-Language Models (VLMs) have demonstrated remarkable potential in robotic manipulation tasks, primarily due to the extensive world knowledge they gain from large-scale datasets. In this process, Spatial Representations (such as points representing object positions or vectors representing object orientations) act as a bridge between VLMs and real-world scene, effectively grounding the reasoning abilities of VLMs and applying them to specific task scenarios. However, existing VLM-based robotic approaches often adopt a fixed spatial representation extraction scheme for various tasks, resulting in insufficient representational capability or excessive extraction time. In this work, we introduce T-Rex, a Task-Adaptive Framework for Spatial Representation Extraction, which dynamically selects the most appropriate spatial representation extraction scheme for each entity based on specific task requirements. Our key insight is that task complexity determines the types and granularity of spatial representations, and Stronger representational capabilities are typically associated with Higher overall system operation costs. Through comprehensive experiments in real-world robotic environments, we show that our approach delivers significant advantages in spatial understanding, efficiency, and stability without additional training.",
      "authors": [
        "Yiteng Chen",
        "Wenbo Li",
        "Shiyi Wang",
        "Huiping Zhuang and Qingyao Wu"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19498",
        "HTML": "https://arxiv.org/html/2506.19498",
        "PDF": "https://arxiv.org/pdf/2506.19498"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 10:36:15 GMT",
          "size": "25657kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses Vision-Language Models (VLMs) trained on large-scale datasets, indirectly touching on LLM training data through the mention of dataset size and diversity, but it focuses more on spatial representation extraction for robotic tasks."
      }
    },
    {
      "id": "2506.19502",
      "abstract": "Accessibility remains a critical concern in today's society, as many technologies are not developed to support the full range of user needs. Existing multi-agent systems (MAS) often cannot provide comprehensive assistance for users in need due to the lack of customization stemming from closed-source designs. Consequently, individuals with disabilities frequently encounter significant barriers when attempting to interact with digital environments. We introduce MATE, a multimodal accessibility MAS, which performs the modality conversions based on the user's needs. The system is useful for assisting people with disabilities by ensuring that data will be converted to an understandable format. For instance, if the user cannot see well and receives an image, the system converts this image to its audio description. MATE can be applied to a wide range of domains, industries, and areas, such as healthcare, and can become a useful assistant for various groups of users. The system supports multiple types of models, ranging from LLM API calling to using custom machine learning (ML) classifiers. This flexibility ensures that the system can be adapted to various needs and is compatible with a wide variety of hardware. Since the system is expected to run locally, it ensures the privacy and security of sensitive information. In addition, the framework can be effectively integrated with institutional technologies (e.g., digital healthcare service) for real-time user assistance. Furthermore, we introduce ModCon-Task-Identifier, a model that is capable of extracting the precise modality conversion task from the user input. Numerous experiments show that ModCon-Task-Identifier consistently outperforms other LLMs and statistical models on our custom data. Our code and data are publicly available at https://github.com/AlgazinovAleksandr/Multi-Agent-MATE.",
      "authors": [
        "Aleksandr Algazinov",
        "Matt Laing",
        "and Paul Laban"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19502",
        "HTML": "https://arxiv.org/html/2506.19502",
        "PDF": "https://arxiv.org/pdf/2506.19502"
      },
      "subjects": [
        "Multiagent Systems (cs.MA)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 10:40:23 GMT",
          "size": "1313kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility Applications",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper mentions LLMs and custom data for user task modeling in accessibility applications, indicating a connection to how data is used, but does not focus on the training data specifics for LLMs."
      }
    },
    {
      "id": "2506.19512",
      "abstract": "This paper presents the approach of our team called heiDS for the ArchEHR-QA 2025 shared task. A pipeline using a retrieval augmented generation (RAG) framework is designed to generate answers that are attributed to clinical evidence from the electronic health records (EHRs) of patients in response to patient-specific questions. We explored various components of a RAG framework, focusing on ranked list truncation (RLT) retrieval strategies and attribution approaches. Instead of using a fixed top-k RLT retrieval strategy, we employ a query-dependent-k retrieval strategy, including the existing surprise and autocut methods and two new methods proposed in this work, autocut* and elbow. The experimental results show the benefits of our strategy in producing factual and relevant answers when compared to a fixed-$k$.",
      "authors": [
        "Ashish Chouhan",
        "Michael Gertz"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19512",
        "HTML": "https://arxiv.org/html/2506.19512",
        "PDF": "https://arxiv.org/pdf/2506.19512"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 11:03:01 GMT",
          "size": "9415kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "heiDS at ArchEHR-QA 2025: From Fixed-k to Query-dependent-k for Retrieval Augmented Generation",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While it involves retrieval augmented generation potentially relevant to how data may be processed or used in systems, the focus is not on training data itself."
      }
    },
    {
      "id": "2506.19527",
      "abstract": "While Large Language Models (LLMs) possess significant capabilities in open-world agent tasks, they also face challenges in rapidly adapting to new, specialized tasks due to their reliance on static pre-trained knowledge. Traditional methods such as fine-tuning are often costly, data-intensive, and may lead to \"catastrophic forgetting.\" Therefore, we present KnowMap, a novel approach that dynamically constructs a knowledge base from environmental and experiential data. KnowMap fine-tunes a small knowledge-embedding model to equip a larger LLM with valuable task-specific knowledge. Our experiments on the ScienceWorld benchmark demonstrate 17.71% improvement for the performance of gpt-4-turbo model. KnowMap not only provides an efficient and effective means for LLM task-adapting, but also highlights how integrating environmental and experiential knowledge can enhance LLMs' reasoning capabilities.",
      "authors": [
        "Kelin Fu",
        "Kaigui Bian"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19527",
        "HTML": "https://arxiv.org/html/2506.19527",
        "PDF": "https://arxiv.org/pdf/2506.19527"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 11:30:38 GMT",
          "size": "3008kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "KnowMap: Efficient Knowledge-Driven Task Adaptation for LLMs",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper presents a method for adapting LLMs with knowledge-driven task adaptation, which indirectly involves data but lacks focus on LLM training data processes such as collection or curation."
      }
    },
    {
      "id": "2506.19548",
      "abstract": "Early detection of disease outbreaks is crucial to ensure timely intervention by the health authorities. Due to the challenges associated with traditional indicator-based surveillance, monitoring informal sources such as online media has become increasingly popular. However, owing to the number of online articles getting published everyday, manual screening of the articles is impractical. To address this, we propose Health Sentinel. It is a multi-stage information extraction pipeline that uses a combination of ML and non-ML methods to extract events-structured information concerning disease outbreaks or other unusual health events-from online articles. The extracted events are made available to the Media Scanning and Verification Cell (MSVC) at the National Centre for Disease Control (NCDC), Delhi for analysis, interpretation and further dissemination to local agencies for timely intervention. From April 2022 till date, Health Sentinel has processed over 300 million news articles and identified over 95,000 unique health events across India of which over 3,500 events were shortlisted by the public health experts at NCDC as potential outbreaks.",
      "authors": [
        "Devesh Pant",
        "Rishi Raj Grandhe",
        "Vipin Samaria",
        "Mukul Paul",
        "Sudhir Kumar",
        "Saransh Khanna",
        "Jatin Agrawal",
        "Jushaan Singh Kalra",
        "Akhil VSSG",
        "Satish V Khalikar",
        "Vipin Garg",
        "Himanshu Chauhan",
        "Pranay Verma",
        "Neha Khandelwal",
        "Soma S Dhavala",
        "Minesh Mathew"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19548",
        "HTML": "https://arxiv.org/html/2506.19548",
        "PDF": "https://arxiv.org/pdf/2506.19548"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Information Retrieval (cs.IR)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 11:54:37 GMT",
          "size": "1847kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Health Sentinel: An AI Pipeline For Real-time Disease Outbreak Detection",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "Although the paper is mainly about real-time disease outbreak detection from online media, it indirectly touches on data collection and processing, which could relate to methodologies for collecting or filtering training data for LLMs."
      }
    },
    {
      "id": "2506.19549",
      "abstract": "Prior work on input-token importance in auto-regressive transformers has relied on Softmax-normalized attention weights, which obscure the richer structure of pre-Softmax query-key logits. We introduce RCStat, a statistical framework that harnesses raw attention logits via Relative Contextualization (RC), a random variable measuring contextual alignment between token segments, and derive an efficient upper bound for RC. We demonstrate two applications: (i) Key-Value compression, where RC-based thresholds drive adaptive key-value eviction for substantial cache reduction with minimal quality loss; and (ii) Attribution, where RC yields higher-fidelity token-, sentence-, and chunk-level explanations than post-Softmax methods. Across question answering, summarization, and attribution benchmarks, RCStat achieves significant empirical gains, delivering state-of-the-art compression and attribution performance without any model retraining.",
      "authors": [
        "Debabrata Mahapatra",
        "Shubham Agarwal",
        "Apoorv Saxena",
        "Subrata Mitra"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19549",
        "HTML": "https://arxiv.org/html/2506.19549",
        "PDF": "https://arxiv.org/pdf/2506.19549"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 11:55:43 GMT",
          "size": "6516kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "RCStat: A Statistical Framework for using Relative Contextualization in Transformers",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper introduces a statistical framework for transformers, involving token importance and attribution. It's related to improving transformer models but does not focus on LLM training data management directly, hence categorized as weakly relevant."
      }
    },
    {
      "id": "2506.19552",
      "abstract": "With access to large-scale, unlabeled medical datasets, researchers are confronted with two questions: Should they attempt to pretrain a custom foundation model on this medical data, or use transfer-learning from an existing generalist model? And, if a custom model is pretrained, are novel methods required? In this paper we explore these questions by conducting a case-study, in which we train a foundation model on a large regional fetal ultrasound dataset of 2M images. By selecting the well-established DINOv2 method for pretraining, we achieve state-of-the-art results on three fetal ultrasound datasets, covering data from different countries, classification, segmentation, and few-shot tasks. We compare against a series of models pretrained on natural images, ultrasound images, and supervised baselines. Our results demonstrate two key insights: (i) Pretraining on custom data is worth it, even if smaller models are trained on less data, as scaling in natural image pretraining does not translate to ultrasound performance. (ii) Well-tuned methods from computer vision are making it feasible to train custom foundation models for a given medical domain, requiring no hyperparameter tuning and little methodological adaptation. Given these findings, we argue that a bias towards methodological innovation should be avoided when developing domain specific foundation models under common computational resource constraints.",
      "authors": [
        "Jakob Ambsdorf",
        "Asbj{\\o}rn Munk",
        "Sebastian Llambias",
        "Anders Nymark Christensen",
        "Kamil Mikolaj",
        "Randall Balestriero",
        "Martin Tolsgaard",
        "Aasa Feragen",
        "Mads Nielsen"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19552",
        "HTML": "https://arxiv.org/html/2506.19552",
        "PDF": "https://arxiv.org/pdf/2506.19552"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 12:00:13 GMT",
          "size": "2513kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "General Methods Make Great Domain-specific Foundation Models: A Case-study on Fetal Ultrasound",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "Although the paper discusses training foundation models on custom data, it is specific to image datasets in medical domains and does not address aspects of LLM training data directly."
      }
    },
    {
      "id": "2506.19585",
      "abstract": "From optical sensors to microwave radars, leveraging the complementary strengths of remote sensing (RS) sensors is crucial for achieving dense spatio-temporal monitoring of our planet. In contrast, recent deep learning models, whether task-specific or foundational, are often specific to single sensors or to fixed combinations: adapting such models to different sensory inputs requires both architectural changes and re-training, limiting scalability and generalization across multiple RS sensors. On the contrary, a single model able to modulate its feature representations to accept diverse sensors as input would pave the way to agile and flexible multi-sensor RS data processing. To address this, we introduce SMARTIES, a generic and versatile foundation model lifting sensor-specific/dependent efforts and enabling scalability and generalization to diverse RS sensors: SMARTIES projects data from heterogeneous sensors into a shared spectrum-aware space, enabling the use of arbitrary combinations of bands both for training and inference. To obtain sensor-agnostic representations, we train a single, unified transformer model reconstructing masked multi-sensor data with cross-sensor token mixup. On both single- and multi-modal tasks across diverse sensors, SMARTIES outperforms previous models that rely on sensor-specific pretraining. Our code and pretrained models are available at https://gsumbul.github.io/SMARTIES.",
      "authors": [
        "Gencer Sumbul",
        "Chang Xu",
        "Emanuele Dalsasso",
        "Devis Tuia"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19585",
        "HTML": "https://arxiv.org/html/2506.19585",
        "PDF": "https://arxiv.org/pdf/2506.19585"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 12:51:39 GMT",
          "size": "2400kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "SMARTIES: Spectrum-Aware Multi-Sensor Auto-Encoder for Remote Sensing Images",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the focus is on generalizing models for remote sensing images, the paper discusses training models that can handle diverse sensor data, which is tangentially related to aspects of dataset diversity and generalization, a concern in LLM training datasets."
      },
      "models": [
        {
          "model_path": "gsumbul/SMARTIES-v1-ViT-B",
          "downloads": "0",
          "likes": "1",
          "trending_score": "1.0",
          "link": "https://huggingface.co/gsumbul/SMARTIES-v1-ViT-B"
        },
        {
          "model_path": "gsumbul/SMARTIES-v1-ViT-L",
          "downloads": "0",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/gsumbul/SMARTIES-v1-ViT-L"
        },
        {
          "model_path": "gsumbul/SMARTIES-v1-finetuned-models",
          "downloads": "0",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/gsumbul/SMARTIES-v1-finetuned-models"
        }
      ]
    },
    {
      "id": "2506.19592",
      "abstract": "We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a multi-agent framework that integrates Large Language Models (LLMs) with symbolic planning to solve complex tasks without the need for manually defined environment models. TAPAS employs specialized LLM-based agents that collaboratively generate and adapt domain models, initial states, and goal specifications as needed using structured tool-calling mechanisms. Through this tool-based interaction, downstream agents can request modifications from upstream agents, enabling adaptation to novel attributes and constraints without manual domain redefinition. A ReAct (Reason+Act)-style execution agent, coupled with natural language plan translation, bridges the gap between dynamically generated plans and real-world robot capabilities. TAPAS demonstrates strong performance in benchmark planning domains and in the VirtualHome simulated real-world environment.",
      "authors": [
        "Harisankar Babu",
        "Philipp Schillinger",
        "Tamim Asfour"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19592",
        "HTML": "https://arxiv.org/html/2506.19592",
        "PDF": "https://arxiv.org/pdf/2506.19592"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 13:02:06 GMT",
          "size": "1670kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to Task Planning",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper describes a multi-agent framework that involves LLMs for task planning, which indirectly relates to using LLMs but does not focus on training data specifically."
      }
    },
    {
      "id": "2506.19599",
      "abstract": "In the era of large-scale artificial intelligence, Large Language Models (LLMs) have made significant strides in natural language processing. However, they often lack transparency and generate unreliable outputs, raising concerns about their interpretability. To address this, the Chain of Thought (CoT) prompting method structures reasoning into step-by-step deductions. Yet, not all reasoning chains are valid, and errors can lead to unreliable conclusions. We propose ECCoT, an End-to-End Cognitive Chain of Thought Validation Framework, to evaluate and refine reasoning chains in LLMs. ECCoT integrates the Markov Random Field-Embedded Topic Model (MRF-ETM) for topic-aware CoT generation and Causal Sentence-BERT (CSBert) for causal reasoning alignment. By filtering ineffective chains using structured ordering statistics, ECCoT improves interpretability, reduces biases, and enhances the trustworthiness of LLM-based decision-making. Key contributions include the introduction of ECCoT, MRF-ETM for topic-driven CoT generation, and CSBert for causal reasoning enhancement. Code is released at: https://github.com/erwinmsmith/ECCoT.git.",
      "authors": [
        "Zhenke Duan",
        "Jiqun Pan",
        "Jiani Tu",
        "Xiaoyi Wang",
        "Yanqing Wang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19599",
        "HTML": "https://arxiv.org/html/2506.19599",
        "PDF": "https://arxiv.org/pdf/2506.19599"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 13:09:53 GMT",
          "size": "3647kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "ECCoT: A Framework for Enhancing Effective Cognition via Chain of Thought in Large Language Model",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper introduces ECCoT, which focuses on improving the interpretability and reasoning of LLMs but does not directly address training data specifics. It is relevant insofar as it discusses the refinement of reasoning within LLMs."
      }
    },
    {
      "id": "2506.19603",
      "abstract": "Automatic detection of online hate speech serves as a crucial step in the detoxification of the online discourse. Moreover, accurate classification can promote a better understanding of the proliferation of hate as a social phenomenon. While most prior work focus on the detection of hateful utterances, we argue that focusing on the user level is as important, albeit challenging. In this paper we consider a multimodal aggregative approach for the detection of hate-mongers, taking into account the potentially hateful texts, user activity, and the user network. Evaluating our method on three unique datasets X (Twitter), Gab, and Parler we show that processing a user's texts in her social context significantly improves the detection of hate mongers, compared to previously used text and graph-based methods. We offer comprehensive set of results obtained in different experimental settings as well as qualitative analysis of illustrative cases. Our method can be used to improve the classification of coded messages, dog-whistling, and racial gas-lighting, as well as to inform intervention measures. Moreover, we demonstrate that our multimodal approach performs well across very different content platforms and over large datasets and networks.",
      "authors": [
        "Tom Marzea",
        "Abraham Israeli",
        "Oren Tsur"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19603",
        "HTML": "https://arxiv.org/html/2506.19603",
        "PDF": "https://arxiv.org/pdf/2506.19603"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Social and Information Networks (cs.SI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 13:16:21 GMT",
          "size": "280kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Social Hatred: Efficient Multimodal Detection of Hatemongers",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the main focus is on hate speech detection, the use of datasets from social media platforms like Twitter touches on data collection in terms of user-generated content, which can be related to LLM training data indirectly."
      }
    },
    {
      "id": "2506.19607",
      "abstract": "While large language models (LLMs) have shown remarkable capabilities to generate coherent text, they suffer from the issue of hallucinations -- factually inaccurate statements. Among numerous approaches to tackle hallucinations, especially promising are the self-correcting methods. They leverage the multi-turn nature of LLMs to iteratively generate verification questions inquiring additional evidence, answer them with internal or external knowledge, and use that to refine the original response with the new corrections. These methods have been explored for encyclopedic generation, but less so for domains like news summarization. In this work, we investigate two state-of-the-art self-correcting systems by applying them to correct hallucinated summaries using evidence from three search engines. We analyze the results and provide insights into systems' performance, revealing interesting practical findings on the benefits of search engine snippets and few-shot prompts, as well as high alignment of G-Eval and human evaluation.",
      "authors": [
        "Juraj Vladika",
        "Ihsan Soydemir",
        "Florian Matthes"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19607",
        "HTML": "https://arxiv.org/html/2506.19607",
        "PDF": "https://arxiv.org/pdf/2506.19607"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 13:20:31 GMT",
          "size": "206kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Correcting Hallucinations in News Summaries: Exploration of Self-Correcting LLM Methods with External Knowledge",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses methods to correct hallucinations in news summaries using LLMs, which indirectly ties into how LLMs process and refine information, relevant to training data quality and processing."
      }
    },
    {
      "id": "2506.19610",
      "abstract": "Recent advances in multimodal techniques have led to significant progress in Medical Visual Question Answering (Med-VQA). However, most existing models focus on global image features rather than localizing disease-specific regions crucial for diagnosis. Additionally, current research tends to emphasize answer accuracy at the expense of the reasoning pathway, yet both are crucial for clinical decision-making. To address these challenges, we propose From Vision to Text Chain-of-Thought (V2T-CoT), a novel approach that automates the localization of preference areas within biomedical images and incorporates this localization into region-level pixel attention as knowledge for Vision CoT. By fine-tuning the vision language model on constructed R-Med 39K dataset, V2T-CoT provides definitive medical reasoning paths. V2T-CoT integrates visual grounding with textual rationale generation to establish precise and explainable diagnostic results. Experimental results across four Med-VQA benchmarks demonstrate state-of-the-art performance, achieving substantial improvements in both performance and interpretability.",
      "authors": [
        "Yuan Wang",
        "Jiaxiang Liu",
        "Shujian Gao",
        "Bin Feng",
        "Zhihang Tang",
        "Xiaotang Gai",
        "Jian Wu",
        "Zuozhu Liu"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19610",
        "HTML": "https://arxiv.org/html/2506.19610",
        "PDF": "https://arxiv.org/pdf/2506.19610"
      },
      "subjects": [
        "Computational Engineering, Finance, and Science (cs.CE)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 13:23:25 GMT",
          "size": "1397kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "V2T-CoT: From Vision to Text Chain-of-Thought for Medical Reasoning and Diagnosis",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses fine-tuning a vision-language model on a constructed dataset (R-Med 39K) for medical diagnosis, indirectly touching on data curation but not focusing on LLM training data."
      }
    },
    {
      "id": "2506.19639",
      "abstract": "When humans and robotic agents coexist in an environment, scene understanding becomes crucial for the agents to carry out various downstream tasks like navigation and planning. Hence, an agent must be capable of localizing and identifying actions performed by the human. Current research lacks reliable datasets for performing scene understanding within indoor environments where humans are also a part of the scene. Scene Graphs enable us to generate a structured representation of a scene or an image to perform visual scene understanding. To tackle this, we present HOIverse a synthetic dataset at the intersection of scene graph and human-object interaction, consisting of accurate and dense relationship ground truths between humans and surrounding objects along with corresponding RGB images, segmentation masks, depth images and human keypoints. We compute parametric relations between various pairs of objects and human-object pairs, resulting in an accurate and unambiguous relation definitions. In addition, we benchmark our dataset on state-of-the-art scene graph generation models to predict parametric relations and human-object interactions. Through this dataset, we aim to accelerate research in the field of scene understanding involving people.",
      "authors": [
        "Mrunmai Vivek Phatak",
        "Julian Lorenz",
        "Nico H\\\"ormann",
        "J\\\"org H\\\"ahner",
        "Rainer Lienhart"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19639",
        "HTML": "https://arxiv.org/html/2506.19639",
        "PDF": "https://arxiv.org/pdf/2506.19639"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 14:00:31 GMT",
          "size": "22662kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "HOIverse: A Synthetic Scene Graph Dataset With Human Object Interactions",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses a synthetic dataset for scene understanding involving human-object interactions, indirectly related through dataset creation but not focused on LLM training data."
      }
    },
    {
      "id": "2506.19643",
      "abstract": "Offline reinforcement learning (RL) recently gains growing interests from RL researchers. However, the performance of offline RL suffers from the out-of-distribution problem, which can be corrected by feedback in online RL. Previous offline RL research focuses on restricting the offline algorithm in in-distribution even in-sample action sampling. In contrast, fewer work pays attention to the influence of the batch data. In this paper, we first build a bridge over the batch data and the performance of offline RL algorithms theoretically, from the perspective of model-based offline RL optimization. We draw a conclusion that, with mild assumptions, the distance between the state-action pair distribution generated by the behavioural policy and the distribution generated by the optimal policy, accounts for the performance gap between the policy learned by model-based offline RL and the optimal policy. Secondly, we reveal that in task-agnostic settings, a series of policies trained by unsupervised RL can minimize the worst-case regret in the performance gap. Inspired by the theoretical conclusions, UDG (Unsupervised Data Generation) is devised to generate data and select proper data for offline training under tasks-agnostic settings. Empirical results demonstrate that UDG can outperform supervised data generation on solving unknown tasks.",
      "authors": [
        "Shuncheng He",
        "Hongchang Zhang",
        "Jianzhun Shao",
        "Yuhang Jiang",
        "Xiangyang Ji"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19643",
        "HTML": "https://arxiv.org/html/2506.19643",
        "PDF": "https://arxiv.org/pdf/2506.19643"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 14:08:36 GMT",
          "size": "5256kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Unsupervised Data Generation for Offline Reinforcement Learning: A Perspective from Model",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While focusing on offline reinforcement learning, it touches on data generation and selection, which can relate indirectly to data handling methodologies relevant to LLM training."
      }
    },
    {
      "id": "2506.19644",
      "abstract": "Diversity in image generation is essential to ensure fair representations and support creativity in ideation. Hence, many text-to-image models have implemented diversification mechanisms. Yet, after a few iterations of generation, a lack of diversity becomes apparent, because each user has their own diversity goals (e.g., different colors, brands of cars), and there are diverse attributions to be specified. To support user-driven diversity control, we propose Varif.ai that employs text-to-image and Large Language Models to iteratively i) (re)generate a set of images, ii) verify if user-specified attributes have sufficient coverage, and iii) vary existing or new attributes. Through an elicitation study, we uncovered user needs for diversity in image generation. A pilot validation showed that Varif.ai made achieving diverse image sets easier. In a controlled evaluation with 20 participants, Varif.ai proved more effective than baseline methods across various scenarios. Thus, this supports user control of diversity in image generation for creative ideation and scalable image generation.",
      "authors": [
        "M. Michelessa",
        "J. Ng",
        "C. Hurter",
        "B. Y. Lim"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19644",
        "HTML": "https://arxiv.org/html/2506.19644",
        "PDF": "https://arxiv.org/pdf/2506.19644"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 14:08:48 GMT",
          "size": "9439kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Varif.ai to Vary and Verify User-Driven Diversity in Scalable Image Generation",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper explores diversity in image generation using LLMs and text-to-image models, indirectly touching on data generation aspects but not specifically on LLM training data."
      }
    },
    {
      "id": "2506.19656",
      "abstract": "Large-scale Earth system datasets, from high-resolution remote sensing imagery to spatiotemporal climate model outputs, exhibit characteristics analogous to those of standard videos. Their inherent spatial, temporal, and spectral redundancies can thus be readily exploited by established video compression techniques. Here, we present xarrayvideo, a Python library for compressing multichannel spatiotemporal datasets by encoding them as videos. Our approach achieves compression ratios of up to 250x while maintaining high fidelity by leveraging standard, well-optimized video codecs through ffmpeg. We demonstrate the library's effectiveness on four real-world multichannel spatiotemporal datasets: DynamicEarthNet (very high resolution Planet images), DeepExtremeCubes (high resolution Sentinel-2 images), ERA5 (weather reanalysis data), and the SimpleS2 dataset (high resolution multichannel Sentinel-2 images), achieving Peak Signal-to-Noise Ratios (PSNRs) of 55.86, 40.60, 46.58, and 43.23 dB at 0.1 bits per pixel per band (bpppb) and 65.91, 54.28, 62.90, and 55.04 dB at 1 bpppb. We are redistributing two of these datasets, DeepExtremeCubes (2.3 Tb) and DynamicEarthNet (525 Gb), in the machine-learning-ready and cloud-ready TACO format through HuggingFace at significantly reduced sizes (270 Gb and 8.5 Gb, respectively) without compromising quality (PSNR 55.77-56.65 and 60.15). No performance loss is observed when the compressed versions of these datasets are used in their respective deep learning-based downstream tasks (next step reflectance prediction and landcover segmentation). In conclusion, xarrayvideo presents an efficient solution for handling the rapidly growing size of Earth observation datasets, making advanced compression techniques accessible and practical to the Earth science community. The library is available for use at https://github.com/IPL-UV/xarrayvideo",
      "authors": [
        "Oscar J. Pellicer-Valero",
        "Cesar Aybar",
        "Gustau Camps Valls"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19656",
        "HTML": "https://arxiv.org/html/2506.19656",
        "PDF": "https://arxiv.org/pdf/2506.19656"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Digital Libraries (cs.DL)",
        "Image and Video Processing (eess.IV)",
        "Geophysics (physics.geo-ph)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 14:20:05 GMT",
          "size": "2043kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Video Compression for Spatiotemporal Earth System Data",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "This paper discusses compression techniques for Earth system datasets, which are shared in a machine-learning-ready format. While it does not focus explicitly on LLM training data, it touches on data preparation aspects relevant to machine learning."
      }
    },
    {
      "id": "2506.19665",
      "abstract": "Generating reports for computed tomography (CT) images is a challenging task, while similar to existing studies for medical image report generation, yet has its unique characteristics, such as spatial encoding of multiple images, alignment between image volume and texts, etc. Existing solutions typically use general 2D or 3D image processing techniques to extract features from a CT volume, where they firstly compress the volume and then divide the compressed CT slices into patches for visual encoding. These approaches do not explicitly account for the transformations among CT slices, nor do they effectively integrate multi-level image features, particularly those containing specific organ lesions, to instruct CT report generation (CTRG). In considering the strong correlation among consecutive slices in CT scans, in this paper, we propose a large language model (LLM) based CTRG method with recurrent visual feature extraction and stereo attentions for hierarchical feature modeling. Specifically, we use a vision Transformer to recurrently process each slice in a CT volume, and employ a set of attentions over the encoded slices from different perspectives to selectively obtain important visual information and align them with textual features, so as to better instruct an LLM for CTRG. Experiment results and further analysis on the benchmark M3D-Cap dataset show that our method outperforms strong baseline models and achieves state-of-the-art results, demonstrating its validity and effectiveness.",
      "authors": [
        "Yuanhe Tian",
        "Lei Mao",
        "Yan Song"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19665",
        "HTML": "https://arxiv.org/html/2506.19665",
        "PDF": "https://arxiv.org/pdf/2506.19665"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 14:29:06 GMT",
          "size": "2276kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Recurrent Visual Feature Extraction and Stereo Attentions for CT Report Generation",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "This paper covers CT report generation using LLMs, with potential implications on how LLM training data is applied in practice. However, the primary focus is not on LLM training data itself."
      }
    },
    {
      "id": "2506.19680",
      "abstract": "Controlling the patterns a model learns is essential to preventing reliance on irrelevant or misleading features. Such reliance on irrelevant features, often called shortcut features, has been observed across domains, including medical imaging and natural language processing, where it may lead to real-world harms. A common mitigation strategy leverages annotations (provided by humans or machines) indicating which features are relevant or irrelevant. These annotations are compared to model explanations, typically in the form of feature salience, and used to guide the loss function during training. Unfortunately, recent works have demonstrated that feature salience methods are unreliable and therefore offer a poor signal to optimize. In this work, we propose a simplified objective that simultaneously optimizes for explanation robustness and mitigation of shortcut learning. Unlike prior objectives with similar aims, we demonstrate theoretically why our approach ought to be more effective. Across a comprehensive series of experiments, we show that our approach consistently reduces test-time misclassifications by 20% compared to state-of-the-art methods. We also extend prior experimental settings to include natural language processing tasks. Additionally, we conduct novel ablations that yield practical insights, including the relative importance of annotation quality over quantity. Code for our method and experiments is available at: https://github.com/Mihneaghitu/ModelGuidanceViaRobustFeatureAttribution.",
      "authors": [
        "Mihnea Ghitu",
        "Matthew Wicker",
        "Vihari Piratla"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19680",
        "HTML": "https://arxiv.org/html/2506.19680",
        "PDF": "https://arxiv.org/pdf/2506.19680"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 14:47:15 GMT",
          "size": "6667kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Model Guidance via Robust Feature Attribution",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses model guidance through feature attribution with potential relevance to data annotation quality, which could be tangentially related to LLM training data quality."
      }
    },
    {
      "id": "2506.19683",
      "abstract": "Understanding medical ultrasound imaging remains a long-standing challenge due to significant visual variability caused by differences in imaging and acquisition parameters. Recent advancements in large language models (LLMs) have been used to automatically generate terminology-rich summaries orientated to clinicians with sufficient physiological knowledge. Nevertheless, the increasing demand for improved ultrasound interpretability and basic scanning guidance among non-expert users, e.g., in point-of-care settings, has not yet been explored. In this study, we first introduce the scene graph (SG) for ultrasound images to explain image content to ordinary and provide guidance for ultrasound scanning. The ultrasound SG is first computed using a transformer-based one-stage method, eliminating the need for explicit object detection. To generate a graspable image explanation for ordinary, the user query is then used to further refine the abstract SG representation through LLMs. Additionally, the predicted SG is explored for its potential in guiding ultrasound scanning toward missing anatomies within the current imaging view, assisting ordinary users in achieving more standardized and complete anatomical exploration. The effectiveness of this SG-based image explanation and scanning guidance has been validated on images from the left and right neck regions, including the carotid and thyroid, across five volunteers. The results demonstrate the potential of the method to maximally democratize ultrasound by enhancing its interpretability and usability for ordinaries.",
      "authors": [
        "Xuesong Li",
        "Dianye Huang",
        "Yameng Zhang",
        "Nassir Navab and Zhongliang Jiang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19683",
        "HTML": "https://arxiv.org/html/2506.19683",
        "PDF": "https://arxiv.org/pdf/2506.19683"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)",
        "Image and Video Processing (eess.IV)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 14:49:40 GMT",
          "size": "464kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Semantic Scene Graph for Ultrasound Image Explanation and Scanning Guidance",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper deals with explanations in medical image analysis using large language models (LLMs), which may indirectly relate to LLM training data, especially in terms of how information is processed and potentially annotated. However, the focus is not on the data used for training LLMs but on applying LLMs for ultrasound interpretability."
      }
    },
    {
      "id": "2506.19693",
      "abstract": "Growing concerns over data privacy underscore the need for deep learning methods capable of processing sensitive information without compromising confidentiality. Among privacy-enhancing technologies, Homomorphic Encryption (HE) stands out by providing post-quantum cryptographic security and end-to-end data protection, safeguarding data even during computation. While Deep Neural Networks (DNNs) have gained attention in HE settings, their use has largely been restricted to encrypted inference. Prior research on encrypted training has primarily focused on logistic regression or has relied on multi-party computation to enable model fine-tuning. This stems from the substantial computational overhead and algorithmic complexity involved in DNNs training under HE. In this paper, we present ReBoot, the first framework to enable fully encrypted and non-interactive training of DNNs. Built upon the CKKS scheme, ReBoot introduces a novel HE-compliant neural network architecture based on local error signals, specifically designed to minimize multiplicative depth and reduce noise accumulation. ReBoot employs a tailored packing strategy that leverages real-number arithmetic via SIMD operations, significantly lowering both computational and memory overhead. Furthermore, by integrating approximate bootstrapping, ReBoot learning algorithm supports effective training of arbitrarily deep multi-layer perceptrons, making it well-suited for machine learning as-a-service. ReBoot is evaluated on both image recognition and tabular benchmarks, achieving accuracy comparable to 32-bit floating-point plaintext training while enabling fully encrypted training. It improves test accuracy by up to +3.27% over encrypted logistic regression, and up to +6.83% over existing encrypted DNN frameworks, while reducing training latency by up to 8.83x. ReBoot is made available to the scientific community as a public repository.",
      "authors": [
        "Alberto Pirillo and Luca Colombo"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19693",
        "HTML": "https://arxiv.org/html/2506.19693",
        "PDF": "https://arxiv.org/pdf/2506.19693"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 15:00:14 GMT",
          "size": "211kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "ReBoot: Encrypted Training of Deep Neural Networks with CKKS Bootstrapping",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the paper discusses encrypted training methods, it touches on data privacy which could indirectly relate to LLM training data concerns in secure data handling but is not the main focus."
      }
    },
    {
      "id": "2506.19702",
      "abstract": "Medical document analysis plays a crucial role in extracting essential clinical insights from unstructured healthcare records, supporting critical tasks such as differential diagnosis. Determining the most probable condition among overlapping symptoms requires precise evaluation and deep medical expertise. While recent advancements in large language models (LLMs) have significantly enhanced performance in medical document analysis, privacy concerns related to sensitive patient data limit the use of online LLMs services in clinical settings. To address these challenges, we propose a trustworthy medical document analysis platform that fine-tunes a LLaMA-v3 using low-rank adaptation, specifically optimized for differential diagnosis tasks. Our approach utilizes DDXPlus, the largest benchmark dataset for differential diagnosis, and demonstrates superior performance in pathology prediction and variable-length differential diagnosis compared to existing methods. The developed web-based platform allows users to submit their own unstructured medical documents and receive accurate, explainable diagnostic results. By incorporating advanced explainability techniques, the system ensures transparent and reliable predictions, fostering user trust and confidence. Extensive evaluations confirm that the proposed method surpasses current state-of-the-art models in predictive accuracy while offering practical utility in clinical settings. This work addresses the urgent need for reliable, explainable, and privacy-preserving artificial intelligence solutions, representing a significant advancement in intelligent medical document analysis for real-world healthcare applications. The code can be found at \\href{https://github.com/leitro/Differential-Diagnosis-LoRA}{https://github.com/leitro/Differential-Diagnosis-LoRA}.",
      "authors": [
        "Lei Kang",
        "Xuanshuo Fu",
        "Oriol Ramos Terrades",
        "Javier Vazquez-Corral",
        "Ernest Valveny",
        "Dimosthenis Karatzas"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19702",
        "HTML": "https://arxiv.org/html/2506.19702",
        "PDF": "https://arxiv.org/pdf/2506.19702"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 15:12:42 GMT",
          "size": "1673kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "LLM-Driven Medical Document Analysis: Enhancing Trustworthy Pathology and Differential Diagnosis",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper involves fine-tuning a large language model (LLaMA-v3) for medical document analysis, addressing privacy in LLM usage. It indirectly touches on data aspects but focuses more on application than on training data specifics."
      }
    },
    {
      "id": "2506.19708",
      "abstract": "Despite their impressive performance, generative image models trained on large-scale datasets frequently fail to produce images with seemingly simple concepts -- e.g., human hands or objects appearing in groups of four -- that are reasonably expected to appear in the training data. These failure modes have largely been documented anecdotally, leaving open the question of whether they reflect idiosyncratic anomalies or more structural limitations of these models. To address this, we introduce a systematic approach for identifying and characterizing \"conceptual blindspots\" -- concepts present in the training data but absent or misrepresented in a model's generations. Our method leverages sparse autoencoders (SAEs) to extract interpretable concept embeddings, enabling a quantitative comparison of concept prevalence between real and generated images. We train an archetypal SAE (RA-SAE) on DINOv2 features with 32,000 concepts -- the largest such SAE to date -- enabling fine-grained analysis of conceptual disparities. Applied to four popular generative models (Stable Diffusion 1.5/2.1, PixArt, and Kandinsky), our approach reveals specific suppressed blindspots (e.g., bird feeders, DVD discs, and whitespaces on documents) and exaggerated blindspots (e.g., wood background texture and palm trees). At the individual datapoint level, we further isolate memorization artifacts -- instances where models reproduce highly specific visual templates seen during training. Overall, we propose a theoretically grounded framework for systematically identifying conceptual blindspots in generative models by assessing their conceptual fidelity with respect to the underlying data-generating process.",
      "authors": [
        "Matyas Bohacek",
        "Thomas Fel",
        "Maneesh Agrawala",
        "and Ekdeep Singh Lubana"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19708",
        "HTML": "https://arxiv.org/html/2506.19708",
        "PDF": "https://arxiv.org/pdf/2506.19708"
      },
      "subjects": [
        "Graphics (cs.GR)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 15:15:15 GMT",
          "size": "34705kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Uncovering Conceptual Blindspots in Generative Image Models Using Sparse Autoencoders",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "This paper analyzes conceptual representation discrepancies in generative models, which indirectly relates to LLM training data topics by assessing model fidelity and dataset representation."
      }
    },
    {
      "id": "2506.19724",
      "abstract": "Recent progress in autonomous code generation has fueled excitement around AI agents capable of accelerating scientific discovery by running experiments. However, there is currently no benchmark that evaluates whether such agents can implement scientific ideas when given varied amounts of code as a starting point, interpolating between reproduction (running code) and from-scratch replication (fully re-implementing and running code). We introduce AutoExperiment, a benchmark that evaluates AI agents' ability to implement and run machine learning experiments based on natural language descriptions in research papers. In each task, agents are given a research paper, a codebase with key functions masked out, and a command to run the experiment. The goal is to generate the missing code, execute the experiment in a sandboxed environment, and reproduce the results. AutoExperiment scales in difficulty by varying the number of missing functions $n$, ranging from partial reproduction to full replication. We evaluate state-of-the-art agents and find that performance degrades rapidly as $n$ increases. Agents that can dynamically interact with the environment (e.g. to debug their code) can outperform agents in fixed \"agentless\" harnesses, and there exists a significant gap between single-shot and multi-trial success rates (Pass@1 vs. Pass@5), motivating verifier approaches to our benchmark. Our findings highlight critical challenges in long-horizon code generation, context retrieval, and autonomous experiment execution, establishing AutoExperiment as a new benchmark for evaluating progress in AI-driven scientific experimentation. Our data and code are open-sourced at https://github.com/j1mk1m/AutoExperiment .",
      "authors": [
        "Gyeongwon James Kim",
        "Alex Wilf",
        "Louis-Philippe Morency",
        "Daniel Fried"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19724",
        "HTML": "https://arxiv.org/html/2506.19724",
        "PDF": "https://arxiv.org/pdf/2506.19724"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 15:39:20 GMT",
          "size": "1227kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "From Reproduction to Replication: Evaluating Research Agents with Progressive Code Masking",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "Discusses autonomous code generation and benchmarks for AI agents, indirectly related to LLM training via data use in AI model evaluation, but not specifically focused on LLM training data."
      }
    },
    {
      "id": "2506.19732",
      "abstract": "Neural networks now generate text, images, and speech with billions of parameters, producing a need to know how each neural unit contributes to these high-dimensional outputs. Existing explainable-AI methods, such as SHAP, attribute importance to inputs, but cannot quantify the contributions of neural units across thousands of output pixels, tokens, or logits. Here we close that gap with Multiperturbation Shapley-value Analysis (MSA), a model-agnostic game-theoretic framework. By systematically lesioning combinations of units, MSA yields Shapley Modes, unit-wise contribution maps that share the exact dimensionality of the model's output. We apply MSA across scales, from multi-layer perceptrons to the 56-billion-parameter Mixtral-8x7B and Generative Adversarial Networks (GAN). The approach demonstrates how regularisation concentrates computation in a few hubs, exposes language-specific experts inside the LLM, and reveals an inverted pixel-generation hierarchy in GANs. Together, these results showcase MSA as a powerful approach for interpreting, editing, and compressing deep neural networks.",
      "authors": [
        "Shrey Dixit",
        "Kayson Fakhar",
        "Fatemeh Hadaeghi",
        "Patrick Mineault",
        "Konrad P. Kording",
        "Claus C. Hilgetag"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19732",
        "HTML": "https://arxiv.org/html/2506.19732",
        "PDF": "https://arxiv.org/pdf/2506.19732"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 15:50:35 GMT",
          "size": "3954kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Who Does What in Deep Learning? Multidimensional Game-Theoretic Attribution of Function of Neural Units",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "Touches on understanding the function of neural units within large models like LLMs, relevant indirectly to LLM training but not focused on training data itself."
      }
    },
    {
      "id": "2506.19734",
      "abstract": "Modern deep learning architectures excel at optimization, but only after the data has entered the network. The true bottleneck lies in preparing the right input: minimal, salient, and structured in a way that reflects the essential patterns of the data. We propose DRIFT (Data Reduction via Informative Feature Transformation), a novel preprocessing technique inspired by vibrational analysis in physical systems, to identify and extract the most resonant modes of input data prior to training. Unlike traditional models that attempt to learn amidst both signal and noise, DRIFT mimics physics perception by emphasizing informative features while discarding irrelevant elements. The result is a more compact and interpretable representation that enhances training stability and generalization performance. In DRIFT, images are projected onto a low-dimensional basis formed by spatial vibration mode shapes of plates, offering a physically grounded feature set. This enables neural networks to operate with drastically fewer input dimensions (~ 50 features on MNIST and less than 100 on CIFAR100) while achieving competitive classification accuracy. Extensive experiments across MNIST and CIFAR100 demonstrate DRIFT's superiority over standard pixel-based models and PCA in terms of training stability, resistance to overfitting, and generalization robustness. Notably, DRIFT displays minimal sensitivity to changes in batch size, network architecture, and image resolution, further establishing it as a resilient and efficient data representation strategy. This work shifts the focus from architecture engineering to input curation and underscores the power of physics-driven data transformations in advancing deep learning performance.",
      "authors": [
        "Ben Keslaki"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19734",
        "HTML": "https://arxiv.org/html/2506.19734",
        "PDF": "https://arxiv.org/pdf/2506.19734"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 15:53:18 GMT",
          "size": "1678kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "DRIFT: Data Reduction via Informative Feature Transformation- Generalization Begins Before Deep Learning starts",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper addresses data preprocessing and dimensional reduction, which indirectly relates to how data is prepared for training, but it does not focus specifically on LLM training data or its curation for language models."
      }
    },
    {
      "id": "2506.19747",
      "abstract": "Fisheye cameras offer robots the ability to capture human movements across a wider field of view (FOV) than standard pinhole cameras, making them particularly useful for applications in human-robot interaction and automotive contexts. However, accurately detecting human poses in fisheye images is challenging due to the curved distortions inherent to fisheye optics. While various methods for undistorting fisheye images have been proposed, their effectiveness and limitations for poses that cover a wide FOV has not been systematically evaluated in the context of absolute human pose estimation from monocular fisheye images. To address this gap, we evaluate the impact of pinhole, equidistant and double sphere camera models, as well as cylindrical projection methods, on 3D human pose estimation accuracy. We find that in close-up scenarios, pinhole projection is inadequate, and the optimal projection method varies with the FOV covered by the human pose. The usage of advanced fisheye models like the double sphere model significantly enhances 3D human pose estimation accuracy. We propose a heuristic for selecting the appropriate projection model based on the detection bounding box to enhance prediction quality. Additionally, we introduce and evaluate on our novel dataset FISHnCHIPS, which features 3D human skeleton annotations in fisheye images, including images from unconventional angles, such as extreme close-ups, ground-mounted cameras, and wide-FOV poses, available at: https://www.vision.rwth-aachen.de/fishnchips",
      "authors": [
        "Stephanie K\\\"as",
        "Sven Peter",
        "Henrik Thillmann",
        "Anton Burenko",
        "David Benjamin Adrian",
        "Dennis Mack",
        "Timm Linder",
        "Bastian Leibe"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19747",
        "HTML": "https://arxiv.org/html/2506.19747",
        "PDF": "https://arxiv.org/pdf/2506.19747"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 16:05:36 GMT",
          "size": "4014kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Systematic Comparison of Projection Methods for Monocular 3D Human Pose Estimation on Fisheye Images",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper introduces a dataset (FISHnCHIPS) for evaluating human pose estimation in fisheye images, which is indirectly relevant as it involves dataset creation for training models, though not specifically focused on LLMs."
      }
    },
    {
      "id": "2506.19751",
      "abstract": "Simulation-driven development of intelligent machines benefits from artificial terrains with controllable, well-defined characteristics. However, most existing tools for terrain generation focus on artist-driven workflows and visual realism, with limited support for parameterization, reproducibility, or scripting. We present a modular, Python-based library for procedural terrain generation that enables users to construct complex, parameterized terrains by chaining together simple modules. The system supports both structured and noise-based terrain elements, and integrates with Blender for rendering and object placement. The framework is designed to support applications such as generating synthetic terrains for training machine learning models or producing ground truth for perception tasks. By using a minimal but extensible set of modules, the system achieves high flexibility while remaining easy to configure and expand. We demonstrate that this enables fine-grained control over features such as slope, roughness, and the number of rocks, as well as extension to additional measures. This makes it well suited for workflows that demand reproducibility, variation, and integration with automated pipelines.",
      "authors": [
        "Erik Wallin"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19751",
        "HTML": "https://arxiv.org/html/2506.19751",
        "PDF": "https://arxiv.org/pdf/2506.19751"
      },
      "subjects": [
        "Computational Engineering, Finance, and Science (cs.CE)",
        "Mathematical Software (cs.MS)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 16:06:55 GMT",
          "size": "2848kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "A modular and extensible library for parameterized terrain generation",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "This paper discusses a library for terrain generation, mentioning applications in training machine learning models, which touches on using data for model training but does not relate directly to LLM-specific training data."
      }
    },
    {
      "id": "2506.19753",
      "abstract": "The Arabic language is among the most popular languages in the world with a huge variety of dialects spoken in 22 countries. In this study, we address the problem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets. RNN models, Transformer models, and large language models (LLMs) via prompt engineering are created and tested. Among these, MARBERTv2 performed best with 65% accuracy and 64% F1-score. Through the use of state-of-the-art preprocessing techniques and the latest NLP models, this paper identifies the most significant linguistic issues in Arabic dialect identification. The results corroborate applications like personalized chatbots that respond in users' dialects, social media monitoring, and greater accessibility for Arabic communities.",
      "authors": [
        "Omar A.Essameldin",
        "Ali O.Elbeih",
        "Wael H.Gomaa",
        "Wael F.Elsersy"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19753",
        "HTML": "https://arxiv.org/html/2506.19753",
        "PDF": "https://arxiv.org/pdf/2506.19753"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 16:06:58 GMT",
          "size": "362kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Arabic Dialect Classification using RNNs, Transformers, and Large Language Models: A Comparative Analysis",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper involves LLMs in context to dialect classification which indirectly relates to training data, but its main focus is on model comparison for dialect classification and not on training data itself."
      }
    },
    {
      "id": "2506.19767",
      "abstract": "Large language models (LLMs) have achieved remarkable progress in reasoning tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) remains a fundamental challenge. Through comprehensive analysis of token distributions, learning dynamics, and integration mechanisms from entropy-based perspectives, we reveal key differences between these paradigms: SFT induces coarse-grained global changes to LLM policy distributions, while RL performs fine-grained selective optimizations, with entropy serving as a critical indicator of training effectiveness. Building on these observations, we propose Supervised Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach simultaneously applies SFT and RL to directly optimize the LLM using demonstrations and self-exploration rollouts rather than through two-stage sequential methods. Extensive experiments show that SRFT achieves 59.1% average accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning benchmarks and 10.9% on three out-of-distribution benchmarks.",
      "authors": [
        "Yuqian Fu",
        "Tinghong Chen",
        "Jiajun Chai",
        "Xihuai Wang",
        "Songjun Tu",
        "Guojun Yin",
        "Wei Lin",
        "Qichao Zhang",
        "Yuanheng Zhu",
        "Dongbin Zhao"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19767",
        "HTML": "https://arxiv.org/html/2506.19767",
        "PDF": "https://arxiv.org/pdf/2506.19767"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 16:31:37 GMT",
          "size": "2594kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the paper explores fine-tuning methods for LLMs, it primarily discusses the training methods (SFT and RL) rather than focusing on the LLM training data itself."
      },
      "models": [
        {
          "model_path": "Yuqian-Fu/SRFT",
          "downloads": "0",
          "likes": "2",
          "trending_score": "2.0",
          "link": "https://huggingface.co/Yuqian-Fu/SRFT"
        }
      ]
    },
    {
      "id": "2506.19773",
      "abstract": "A KG represents a network of entities and illustrates relationships between them. KGs are used for various applications, including semantic search and discovery, reasoning, decision-making, natural language processing, machine learning, and recommendation systems. Triple (subject-relation-object) extraction from text is the fundamental building block of KG construction and has been widely studied, for example, in early benchmarks such as ACE 2002 to more recent ones, such as WebNLG 2020, REBEL and SynthIE. While the use of LLMs is explored for KG construction, handcrafting reasonable task-specific prompts for LLMs is a labour-intensive exercise and can be brittle due to subtle changes in the LLM models employed. Recent work in NLP tasks (e.g. autonomy generation) uses automatic prompt optimization/engineering to address this challenge by generating optimal or near-optimal task-specific prompts given input-output examples.\n  This empirical study explores the application of automatic prompt optimization for the triple extraction task using experimental benchmarking. We evaluate different settings by changing (a) the prompting strategy, (b) the LLM being used for prompt optimization and task execution, (c) the number of canonical relations in the schema (schema complexity), (d) the length and diversity of input text, (e) the metric used to drive the prompt optimization, and (f) the dataset being used for training and testing. We evaluate three different automatic prompt optimizers, namely, DSPy, APE, and TextGrad and use two different triple extraction datasets, SynthIE and REBEL. Through rigorous empirical evaluation, our main contribution highlights that automatic prompt optimization techniques can generate reasonable prompts similar to humans for triple extraction. In turn, these optimized prompts achieve improved results, particularly with increasing schema complexity and text size.",
      "authors": [
        "Nandana Mihindukulasooriya",
        "Niharika S. D'Souza",
        "Faisal Chowdhury",
        "Horst Samulowitz"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19773",
        "HTML": "https://arxiv.org/html/2506.19773",
        "PDF": "https://arxiv.org/pdf/2506.19773"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 16:38:49 GMT",
          "size": "538kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Automatic Prompt Optimization for Knowledge Graph Construction: Insights from an Empirical Study",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper focuses on prompt optimization for knowledge graph construction, which involves LLMs. It touches indirectly on using datasets for training/testing LLM tasks but does not focus on LLM training data itself."
      }
    },
    {
      "id": "2506.19783",
      "abstract": "Query rewriting is pivotal for enhancing dense retrieval, yet current methods demand large-scale supervised data or suffer from inefficient reinforcement learning (RL) exploration. In this work, we first establish that guiding Large Language Models (LLMs) with a concise set of expert-crafted strategies, such as semantic expansion and entity disambiguation, substantially improves retrieval effectiveness on challenging benchmarks, including HotpotQA, FEVER, NFCorpus, and SciFact. Building on this insight, we introduce the Strategy-Adaptive Generation Engine (SAGE), which operationalizes these strategies in an RL framework. SAGE introduces two novel reward shaping mechanisms-Strategic Credit Shaping (SCS) and Contrastive Reward Shaping (CRS)-to deliver more informative learning signals. This strategy-guided approach not only achieves new state-of-the-art NDCG@10 results, but also uncovers a compelling emergent behavior: the agent learns to select optimal strategies, reduces unnecessary exploration, and generates concise rewrites, lowering inference cost without sacrificing performance. Our findings demonstrate that strategy-guided RL, enhanced with nuanced reward shaping, offers a scalable, efficient, and more interpretable paradigm for developing the next generation of robust information retrieval systems.",
      "authors": [
        "Teng Wang",
        "Hailei Gong",
        "Changwang Zhang",
        "Jun Wang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19783",
        "HTML": "https://arxiv.org/html/2506.19783",
        "PDF": "https://arxiv.org/pdf/2506.19783"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 16:50:51 GMT",
          "size": "3685kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "SAGE: Strategy-Adaptive Generation Engine for Query Rewriting",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper addresses query rewriting and mentions guiding LLMs with strategies for better performance which indirectly relates to leveraging data strategies but does not specifically focus on LLM training data."
      }
    },
    {
      "id": "2506.19806",
      "abstract": "This position paper argues that large language model (LLM)-based social simulations should establish clear boundaries to meaningfully contribute to social science research. While LLMs offer promising capabilities for modeling human-like agents compared to traditional agent-based modeling, they face fundamental limitations that constrain their reliability for social pattern discovery. The core issue lies in LLMs' tendency towards an ``average persona'' that lacks sufficient behavioral heterogeneity, a critical requirement for simulating complex social dynamics. We examine three key boundary problems: alignment (simulated behaviors matching real-world patterns), consistency (maintaining coherent agent behavior over time), and robustness (reproducibility under varying conditions). We propose heuristic boundaries for determining when LLM-based simulations can reliably advance social science understanding. We believe that these simulations are more valuable when focusing on (1) collective patterns rather than individual trajectories, (2) agent behaviors aligning with real population averages despite limited variance, and (3) proper validation methods available for testing simulation robustness. We provide a practical checklist to guide researchers in determining the appropriate scope and claims for LLM-based social simulations.",
      "authors": [
        "Zengqing Wu",
        "Run Peng",
        "Takayuki Ito",
        "Chuan Xiao"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19806",
        "HTML": "https://arxiv.org/html/2506.19806",
        "PDF": "https://arxiv.org/pdf/2506.19806"
      },
      "subjects": [
        "Computers and Society (cs.CY)",
        "Computation and Language (cs.CL)",
        "Multiagent Systems (cs.MA)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 17:14:47 GMT",
          "size": "167kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "LLM-Based Social Simulations Require a Boundary",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper provides a critical perspective on LLMs in social simulations, discussing limitations due to lack of diversity in data representations, which indirectly relates to the broader discourse of training data, but it is not the main focus."
      }
    },
    {
      "id": "2506.19807",
      "abstract": "Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at https://github.com/zjunlp/KnowRL.",
      "authors": [
        "Baochang Ren",
        "Shuofei Qiao",
        "Wenhao Yu",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19807",
        "HTML": "https://arxiv.org/html/2506.19807",
        "PDF": "https://arxiv.org/pdf/2506.19807"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)",
        "Multiagent Systems (cs.MA)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 17:17:17 GMT",
          "size": "8503kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses enhancing LLMs' reasoning capabilities and factual accuracy, indirectly touching on how factuality in data affects LLM performance. However, it doesn't focus on data used in LLM training."
      },
      "models": [
        {
          "model_path": "zjunlp/KnowRL-DeepSeek-R1-Distill-Qwen-7B",
          "downloads": "2",
          "likes": "1",
          "trending_score": "1.0",
          "link": "https://huggingface.co/zjunlp/KnowRL-DeepSeek-R1-Distill-Qwen-7B"
        },
        {
          "model_path": "zjunlp/KnowRL-Skywork-OR1-7B-Preview",
          "downloads": "1",
          "likes": "1",
          "trending_score": "1.0",
          "link": "https://huggingface.co/zjunlp/KnowRL-Skywork-OR1-7B-Preview"
        }
      ],
      "datasets": [
        {
          "dataset_name": "zjunlp/KnowRL-Train-Data",
          "downloads": "1",
          "likes": "1",
          "link": "https://huggingface.co/datasets/zjunlp/KnowRL-Train-Data"
        }
      ]
    },
    {
      "id": "2506.19813",
      "abstract": "Art curatorship has always been mostly the subjective work of human experts, who, with extensive knowledge of many and diverse artworks, select a few of those to present in communal spaces, spaces that evolved into what we now call art galleries. There are no hard and fast set of rules on how to select these artworks, given a theme which either is presented to the art curator or constructed by her/him. Here we present a series of artificial models -- a total of four related models -- based on machine learning techniques (a subset of artificial intelligence) that attempt to learn from existing exhibitions which have been curated by human experts, in order to be able to do similar curatorship work. We focus exclusively on the last 25 years of past exhibitions at the Metropolitan Museum of Art in New York, due to the quality of the data available and the physical and time limitations of our research. Our four artificial intelligence models achieve a reasonable ability at imitating these various curators responsible for all those exhibitions, with various degrees of precision and curatorial coherence. In particular, we can conclude two key insights: first, that there is sufficient information in these exhibitions to construct an artificial intelligence model that replicates past exhibitions with an accuracy well above random choices; second, that using feature engineering and carefully designing the architecture of modest size models can make them as good as those using the so-called large language models such as GPT in a brute force approach. We also believe, based on small attempts to use the models in out-of-sample experiments, that given more much more data, it should be possible for these kinds of artificial intelligence agents to be closer and closer to the aesthetic and curatorial judgment of human art curators.",
      "authors": [
        "Eurico Covas"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19813",
        "HTML": "https://arxiv.org/html/2506.19813",
        "PDF": "https://arxiv.org/pdf/2506.19813"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 17:25:03 GMT",
          "size": "7175kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Curating art exhibitions using machine learning",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper involves using machine learning to curate art exhibitions, discussing the use of AI models similar to large language models like GPT, indirectly touching on LLM and data used for training AI models."
      }
    },
    {
      "id": "2506.19823",
      "abstract": "Understanding how language models generalize behaviors from their training to a broader deployment distribution is an important problem in AI safety. Betley et al. discovered that fine-tuning GPT-4o on intentionally insecure code causes \"emergent misalignment,\" where models give stereotypically malicious responses to unrelated prompts. We extend this work, demonstrating emergent misalignment across diverse conditions, including reinforcement learning on reasoning models, fine-tuning on various synthetic datasets, and in models without safety training. To investigate the mechanisms behind this generalized misalignment, we apply a \"model diffing\" approach using sparse autoencoders to compare internal model representations before and after fine-tuning. This approach reveals several \"misaligned persona\" features in activation space, including a toxic persona feature which most strongly controls emergent misalignment and can be used to predict whether a model will exhibit such behavior. Additionally, we investigate mitigation strategies, discovering that fine-tuning an emergently misaligned model on just a few hundred benign samples efficiently restores alignment.",
      "authors": [
        "Miles Wang",
        "Tom Dupr\\'e la Tour",
        "Olivia Watkins",
        "Alex Makelov",
        "Ryan A. Chi",
        "Samuel Miserendino",
        "Johannes Heidecke",
        "Tejal Patwardhan",
        "Dan Mossing"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19823",
        "HTML": "https://arxiv.org/html/2506.19823",
        "PDF": "https://arxiv.org/pdf/2506.19823"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 17:38:21 GMT",
          "size": "9047kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Persona Features Control Emergent Misalignment",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses the concept of 'emergent misalignment' in LLMs and touches on the impact of fine-tuning with synthetic datasets, which indirectly relates to training data, but is not the main focus."
      }
    },
    {
      "id": "2506.19831",
      "abstract": "The spread of cyber hatred has led to communal violence, fueling aggression and conflicts between various religious, ethnic, and social groups, posing a significant threat to social harmony. Despite its critical importance, the classification of communal violent text remains an underexplored area in existing research. This study aims to enhance the accuracy of detecting text that incites communal violence, focusing specifically on Bengali textual data sourced from social media platforms. We introduce a fine-tuned BanglaBERT model tailored for this task, achieving a macro F1 score of 0.60. To address the issue of data imbalance, our dataset was expanded by adding 1,794 instances, which facilitated the development and evaluation of a fine-tuned ensemble model. This ensemble model demonstrated an improved performance, achieving a macro F1 score of 0.63, thus highlighting its effectiveness in this domain. In addition to quantitative performance metrics, qualitative analysis revealed instances where the models struggled with context understanding, leading to occasional misclassifications, even when predictions were made with high confidence. Through analyzing the cosine similarity between words, we identified certain limitations in the pre-trained BanglaBERT models, particularly in their ability to distinguish between closely related communal and non-communal terms. To further interpret the model's decisions, we applied LIME, which helped to uncover specific areas where the model struggled in understanding context, contributing to errors in classification. These findings highlight the promise of NLP and interpretability tools in reducing online communal violence. Our work contributes to the growing body of research in communal violence detection and offers a foundation for future studies aiming to refine these techniques for better accuracy and societal impact.",
      "authors": [
        "Abdullah Khondoker",
        "Enam Ahmed Taufik",
        "Md. Iftekhar Islam Tashik",
        "S M Ishtiak Mahmud",
        "Farig Sadeque"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19831",
        "HTML": "https://arxiv.org/html/2506.19831",
        "PDF": "https://arxiv.org/pdf/2506.19831"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 17:48:49 GMT",
          "size": "1164kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "How Effectively Can BERT Models Interpret Context and Detect Bengali Communal Violent Text?",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper involves the use of BERT models for text classification related to social media content, dealing with data imbalance and annotation, indirectly touching on data preparation aspects relevant to LLM training."
      }
    },
    {
      "id": "2506.19836",
      "abstract": "Differential privacy (DP) has become the standard for private data analysis. Certain machine learning applications only require privacy protection for specific protected attributes. Using naive variants of differential privacy in such use cases can result in unnecessary degradation of utility. In this work, we refine the definition of DP to create a more general and flexible framework that we call feature differential privacy (FDP). Our definition is simulation-based and allows for both addition/removal and replacement variants of privacy, and can handle arbitrary and adaptive separation of protected and non-protected features. We prove the properties of FDP, such as adaptive composition, and demonstrate its implications for limiting attribute inference attacks. We also propose a modification of the standard DP-SGD algorithm that satisfies FDP while leveraging desirable properties such as amplification via sub-sampling. We apply our framework to various machine learning tasks and show that it can significantly improve the utility of DP-trained models when public features are available. For example, we train diffusion models on the AFHQ dataset of animal faces and observe a drastic improvement in FID compared to DP, from 286.7 to 101.9 at $\\epsilon=8$, assuming that the blurred version of a training image is available as a public feature. Overall, our work provides a new approach to private data analysis that can help reduce the utility cost of DP while still providing strong privacy guarantees.",
      "authors": [
        "Saeed Mahloujifar",
        "Chuan Guo",
        "G. Edward Suh",
        "Kamalika Chaudhuri"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19836",
        "HTML": "https://arxiv.org/html/2506.19836",
        "PDF": "https://arxiv.org/pdf/2506.19836"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 17:53:28 GMT",
          "size": "2541kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Machine Learning with Privacy for Protected Attributes",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses differential privacy related to machine learning tasks, focusing on a new framework for feature differential privacy, which is indirectly related to LLM training data in terms of protecting data privacy. However, it does not specifically focus on LLM training data collection or datasets."
      }
    },
    {
      "id": "2506.19851",
      "abstract": "We present AnimaX, a feed-forward 3D animation framework that bridges the motion priors of video diffusion models with the controllable structure of skeleton-based animation. Traditional motion synthesis methods are either restricted to fixed skeletal topologies or require costly optimization in high-dimensional deformation spaces. In contrast, AnimaX effectively transfers video-based motion knowledge to the 3D domain, supporting diverse articulated meshes with arbitrary skeletons. Our method represents 3D motion as multi-view, multi-frame 2D pose maps, and enables joint video-pose diffusion conditioned on template renderings and a textual motion prompt. We introduce shared positional encodings and modality-aware embeddings to ensure spatial-temporal alignment between video and pose sequences, effectively transferring video priors to motion generation task. The resulting multi-view pose sequences are triangulated into 3D joint positions and converted into mesh animation via inverse kinematics. Trained on a newly curated dataset of 160,000 rigged sequences, AnimaX achieves state-of-the-art results on VBench in generalization, motion fidelity, and efficiency, offering a scalable solution for category-agnostic 3D animation. Project page: \\href{https://anima-x.github.io/}{https://anima-x.github.io/}.",
      "authors": [
        "Zehuan Huang",
        "Haoran Feng",
        "Yangtian Sun",
        "Yuanchen Guo",
        "Yanpei Cao",
        "Lu Sheng"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19851",
        "HTML": "https://arxiv.org/html/2506.19851",
        "PDF": "https://arxiv.org/pdf/2506.19851"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 17:59:58 GMT",
          "size": "6996kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper involves training on a newly curated dataset of 160,000 sequences for 3D animation but mainly focuses on animation rather than LLM training data aspects."
      }
    },
    {
      "id": "2506.18915",
      "abstract": "Depression is a common mental illness across current human society. Traditional depression assessment relying on inventories and interviews with psychologists frequently suffer from subjective diagnosis results, slow and expensive diagnosis process as well as lack of human resources. Since there is a solid evidence that depression is reflected by various human internal brain activities and external expressive behaviours, early traditional machine learning (ML) and advanced deep learning (DL) models have been widely explored for human behaviour-based automatic depression assessment (ADA) since 2012. However, recent ADA surveys typically only focus on a limited number of human behaviour modalities. Despite being used as a theoretical basis for developing ADA approaches, existing ADA surveys lack a comprehensive review and summary of multi-modal depression-related human behaviours. To bridge this gap, this paper specifically summarises depression-related human behaviours across a range of modalities (e.g. the human brain, verbal language and non-verbal audio/facial/body behaviours). We focus on conducting an up-to-date and comprehensive survey of ML-based ADA approaches for learning depression cues from these behaviours as well as discussing and comparing their distinctive features and limitations. In addition, we also review existing ADA competitions and datasets, identify and discuss the main challenges and opportunities to provide further research directions for future ADA researchers.",
      "authors": [
        "Siyang Song",
        "Yupeng Huo",
        "Shiqing Tang",
        "Jiaee Cheong",
        "Rui Gao",
        "Michel Valstar",
        "Hatice Gunes"
      ],
      "last_revised_date": "2025/06/09",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18915",
        "HTML": "https://arxiv.org/html/2506.18915",
        "PDF": "https://arxiv.org/pdf/2506.18915"
      },
      "subjects": [
        "Neurons and Cognition (q-bio.NC)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 09 Jun 2025 14:40:16 GMT",
          "size": "4559kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/09",
      "title": "Automatic Depression Assessment using Machine Learning: A Comprehensive Survey",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper is a survey on machine learning models for depression assessment, mentioning datasets in the context of automatic depression assessment but not explicitly focused on LLM training data."
      }
    },
    {
      "id": "2506.19051",
      "abstract": "Adversarial robustness of neural networks is an increasingly important area of research, combining studies on computer vision models, large language models (LLMs), and others. With the release of JPEG AI -- the first standard for end-to-end neural image compression (NIC) methods -- the question of evaluating NIC robustness has become critically significant. However, previous research has been limited to a narrow range of codecs and attacks. To address this, we present \\textbf{NIC-RobustBench}, the first open-source framework to evaluate NIC robustness and adversarial defenses' efficiency, in addition to comparing Rate-Distortion (RD) performance. The framework includes the largest number of codecs among all known NIC libraries and is easily scalable. The paper demonstrates a comprehensive overview of the NIC-RobustBench framework and employs it to analyze NIC robustness. Our code is available online at https://github.com/msu-video-group/NIC-RobustBench.",
      "authors": [
        "Georgii Bychkov",
        "Khaled Abud",
        "Egor Kovalev",
        "Alexander Gushchin",
        "Dmitriy Vatolin",
        "Anastasia Antsiferova"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19051",
        "HTML": "https://arxiv.org/html/2506.19051",
        "PDF": "https://arxiv.org/pdf/2506.19051"
      },
      "subjects": [
        "Image and Video Processing (eess.IV)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Multimedia (cs.MM)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 19:11:15 GMT",
          "size": "1629kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "NIC-RobustBench: A Comprehensive Open-Source Toolkit for Neural Image Compression and Robustness Analysis",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper mentions LLMs in the context of adversarial robustness but focuses on neural image compression and defenses, not particularly on LLM training data."
      }
    },
    {
      "id": "2506.19106",
      "abstract": "Hematoxylin and Eosin (H&E) has been the gold standard in tissue analysis for decades, however, tissue specimens stained in different laboratories vary, often significantly, in appearance. This variation poses a challenge for both pathologists' and AI-based downstream analysis. Minimizing stain variation computationally is an active area of research. To further investigate this problem, we collected a unique multi-center tissue image dataset, wherein tissue samples from colon, kidney, and skin tissue blocks were distributed to 66 different labs for routine H&E staining. To isolate staining variation, other factors affecting the tissue appearance were kept constant. Further, we used this tissue image dataset to compare the performance of eight different stain normalization methods, including four traditional methods, namely, histogram matching, Macenko, Vahadane, and Reinhard normalization, and two deep learning-based methods namely CycleGAN and Pixp2pix, both with two variants each. We used both quantitative and qualitative evaluation to assess the performance of these methods. The dataset's inter-laboratory staining variation could also guide strategies to improve model generalizability through varied training data",
      "authors": [
        "Umair Khan",
        "Jouni H\\\"ark\\\"onen",
        "Marjukka Friman",
        "Leena Latonen",
        "Teijo Kuopio and Pekka Ruusuvuori"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19106",
        "HTML": "https://arxiv.org/html/2506.19106",
        "PDF": "https://arxiv.org/pdf/2506.19106"
      },
      "subjects": [
        "Image and Video Processing (eess.IV)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Tissues and Organs (q-bio.TO)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 20:37:40 GMT",
          "size": "7370kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Staining normalization in histopathology: Method benchmarking using multicenter dataset",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the main focus is on benchmark comparison of stain normalization methods, the process of collecting a multi-center dataset might provide indirect insights relevant to dataset construction and variability, which is weakly related to LLM training data concerns."
      }
    },
    {
      "id": "2506.19275",
      "abstract": "Efficiently embedding high-dimensional datasets onto noisy and low-qubit quantum systems is a significant barrier to practical Quantum Machine Learning (QML). Approaches such as quantum autoencoders can be constrained by current hardware capabilities and may exhibit vulnerabilities to reconstruction attacks due to their invertibility. We propose Quantum Principal Geodesic Analysis (qPGA), a novel, non-invertible method for dimensionality reduction and qubit-efficient encoding. Executed classically, qPGA leverages Riemannian geometry to project data onto the unit Hilbert sphere, generating outputs inherently suitable for quantum amplitude encoding. This technique preserves the neighborhood structure of high-dimensional datasets within a compact latent space, significantly reducing qubit requirements for amplitude encoding. We derive theoretical bounds quantifying qubit requirements for effective encoding onto noisy systems. Empirical results on MNIST, Fashion-MNIST, and CIFAR-10 show that qPGA preserves local structure more effectively than both quantum and hybrid autoencoders. Additionally, we demonstrate that qPGA enhances resistance to reconstruction attacks due to its non-invertible nature. In downstream QML classification tasks, qPGA can achieve over 99% accuracy and F1-score on MNIST and Fashion-MNIST, outperforming quantum-dependent baselines. Initial tests on real hardware and noisy simulators confirm its potential for noise-resilient performance, offering a scalable solution for advancing QML applications.",
      "authors": [
        "Hevish Cowlessur",
        "Tansu Alpcan",
        "Chandra Thapa",
        "Seyit Camtepe",
        "Neel Kanth Kundu"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19275",
        "HTML": "https://arxiv.org/html/2506.19275",
        "PDF": "https://arxiv.org/pdf/2506.19275"
      },
      "subjects": [
        "Quantum Physics (quant-ph)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 03:09:16 GMT",
          "size": "1885kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "A Qubit-Efficient Hybrid Quantum Encoding Mechanism for Quantum Machine Learning",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses Quantum Machine Learning, focusing on dimensionality reduction and encoding high-dimensional datasets. Indirectly related to LLM training data in terms of handling and processing large datasets but not the focus."
      }
    },
    {
      "id": "2506.19451",
      "abstract": "Tokens are fundamental processing units of generative AI (GenAI) and large language models (LLMs), and token communication (TC) is essential for enabling remote AI-generate content (AIGC) and wireless LLM applications. Unlike traditional bits, each of which is independently treated, the semantics of each token depends on its surrounding context tokens. This inter-token dependency makes TC vulnerable to outage channels, where the loss of a single token can significantly distort the original message semantics. Motivated by this, this paper focuses on optimizing token packetization to maximize the average token similarity (ATS) between the original and received token messages under outage channels. Due to inter-token dependency, this token grouping problem is combinatorial, with complexity growing exponentially with message length. To address this, we propose a novel framework of semantic packet aggregation with lookahead search (SemPA-Look), built on two core ideas. First, it introduces the residual semantic score (RSS) as a token-level surrogate for the message-level ATS, allowing robust semantic preservation even when a certain token packet is lost. Second, instead of full search, SemPA-Look applies a lookahead search-inspired algorithm that samples intra-packet token candidates without replacement (fixed depth), conditioned on inter-packet token candidates sampled with replacement (fixed width), thereby achieving linear complexity. Experiments on a remote AIGC task with the MS-COCO dataset (text captioned images) demonstrate that SemPA-Look achieves high ATS and LPIPS scores comparable to exhaustive search, while reducing computational complexity by up to 40$\\times$. Compared to other linear-complexity algorithms such as the genetic algorithm (GA), SemPA-Look achieves 10$\\times$ lower complexity, demonstrating its practicality for remote AIGC and other TC applications.",
      "authors": [
        "Seunghun Lee",
        "Jihong Park",
        "Jinho Choi",
        "Hyuncheol Park"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19451",
        "HTML": "https://arxiv.org/html/2506.19451",
        "PDF": "https://arxiv.org/pdf/2506.19451"
      },
      "subjects": [
        "Signal Processing (eess.SP)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 09:25:44 GMT",
          "size": "6237kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Low-Complexity Semantic Packet Aggregation for Token Communication via Lookahead Search",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "This paper discusses token communication, a concept closely tied to LLMs, but it is more about optimizing communication over channels rather than directly addressing LLM training data."
      }
    },
    {
      "id": "2506.19797",
      "abstract": "Purpose: Accurate segmentation of both the pituitary gland and adenomas from magnetic resonance imaging (MRI) is essential for diagnosis and treatment of pituitary adenomas. This systematic review evaluates automatic segmentation methods for improving the accuracy and efficiency of MRI-based segmentation of pituitary adenomas and the gland itself. Methods: We reviewed 34 studies that employed automatic and semi-automatic segmentation methods. We extracted and synthesized data on segmentation techniques and performance metrics (such as Dice overlap scores). Results: The majority of reviewed studies utilized deep learning approaches, with U-Net-based models being the most prevalent. Automatic methods yielded Dice scores of 0.19--89.00\\% for pituitary gland and 4.60--96.41\\% for adenoma segmentation. Semi-automatic methods reported 80.00--92.10\\% for pituitary gland and 75.90--88.36\\% for adenoma segmentation. Conclusion: Most studies did not report important metrics such as MR field strength, age and adenoma size. Automated segmentation techniques such as U-Net-based models show promise, especially for adenoma segmentation, but further improvements are needed to achieve consistently good performance in small structures like the normal pituitary gland. Continued innovation and larger, diverse datasets are likely critical to enhancing clinical applicability.",
      "authors": [
        "Mubaraq Yakubu",
        "Navodini Wijethilake",
        "Jonathan Shapey",
        "Andrew King",
        "Alexander Hammers"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19797",
        "HTML": "https://arxiv.org/html/2506.19797",
        "PDF": "https://arxiv.org/pdf/2506.19797"
      },
      "subjects": [
        "Image and Video Processing (eess.IV)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 17:05:01 GMT",
          "size": "548kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Systematic Review of Pituitary Gland and Pituitary Adenoma Automatic Segmentation Techniques in Magnetic Resonance Imaging",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the paper discusses automatic segmentation in MRI using datasets, it does not directly focus on LLM training data, though there is an indirect relation through data quality and diversity discussion."
      }
    },
    {
      "id": "2308.16075",
      "abstract": "Neural Machine Translation (NMT) has made remarkable progress using large-scale textual data, but the potential of incorporating multimodal inputs, especially visual information, remains underexplored in high-resource settings. While prior research has focused on using multimodal data in low-resource scenarios, this study examines how image features impact translation when added to a large-scale, pre-trained unimodal NMT system. Surprisingly, the study finds that images might be redundant in this context. Additionally, the research introduces synthetic noise to assess whether images help the model handle textual noise. Multimodal models slightly outperform text-only models in noisy settings, even when random images are used. The study's experiments translate from English to Hindi, Bengali, and Malayalam, significantly outperforming state-of-the-art benchmarks. Interestingly, the effect of visual context varies with the level of source text noise: no visual context works best for non-noisy translations, cropped image features are optimal for low noise, and full image features perform better in high-noise scenarios. This sheds light on the role of visual context, especially in noisy settings, and opens up a new research direction for Noisy Neural Machine Translation in multimodal setups. The research emphasizes the importance of combining visual and textual information to improve translation across various environments. Our code is publicly available at https://github.com/babangain/indicMMT.",
      "authors": [
        "Baban Gain",
        "Dibyanayan Bandyopadhyay",
        "Samrat Mukherjee",
        "Chandranath Adak",
        "Asif Ekbal"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2308.16075",
        "HTML": "https://arxiv.org/html/2308.16075",
        "PDF": "https://arxiv.org/pdf/2308.16075"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 30 Aug 2023 14:52:14 GMT",
          "size": "10036kb",
          "version": "v1"
        },
        {
          "date": "Mon, 23 Jun 2025 19:07:19 GMT",
          "size": "2359kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the paper focuses on Neural Machine Translation incorporating visual data, it touches on multimodal data integration, which is tangentially related to LLM training data but not directly focused on it."
      },
      "tasks": [
        "Machine Translation",
        "NMT",
        "Translation"
      ],
      "repo_urls": [
        "https://github.com/babangain/indicmmt"
      ]
    },
    {
      "id": "2310.08785",
      "abstract": "Text-guided image editing faces significant challenges when considering training and inference flexibility. Much literature collects large amounts of annotated image-text pairs to train text-conditioned generative models from scratch, which is expensive and not efficient. After that, some approaches that leverage pre-trained vision-language models have been proposed to avoid data collection, but they are limited by either per text-prompt optimization or inference-time hyper-parameters tuning. To address these issues, we investigate and identify a specific space, referred to as CLIP DeltaSpace, where the CLIP visual feature difference of two images is semantically aligned with the CLIP textual feature difference of their corresponding text descriptions. Based on DeltaSpace, we propose a novel framework called DeltaEdit, which maps the CLIP visual feature differences to the latent space directions of a generative model during the training phase, and predicts the latent space directions from the CLIP textual feature differences during the inference phase. And this design endows DeltaEdit with two advantages: (1) text-free training; (2) generalization to various text prompts for zero-shot inference. Extensive experiments validate the effectiveness and versatility of DeltaEdit with different generative models, including both the GAN model and the diffusion model, in achieving flexible text-guided image editing. Code is available at https://github.com/Yueming6568/DeltaEdit.",
      "authors": [
        "Yueming Lyu",
        "Kang Zhao",
        "Bo Peng",
        "Huafeng Chen",
        "Yue Jiang",
        "Yingya Zhang",
        "Jing Dong",
        "Caifeng Shan"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2310.08785",
        "HTML": "https://arxiv.org/html/2310.08785",
        "PDF": "https://arxiv.org/pdf/2310.08785"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Thu, 12 Oct 2023 15:43:12 GMT",
          "size": "19782kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 06:09:37 GMT",
          "size": "20200kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided Image Editing",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper addresses text-guided image editing and leverages pre-trained vision-language models. While it indirectly relates to data in terms of avoiding extensive data collection for retraining models, it doesn't focus on LLM training data explicitly."
      },
      "tasks": [
        "text-guided-image-editing"
      ],
      "repo_urls": [
        "https://github.com/yueming6568/deltaedit"
      ]
    },
    {
      "id": "2401.08405",
      "abstract": "In an era of AI's growing capabilities and influences, recent advancements are reshaping HCI and CSCW's view of AI. Playful interactions emerged as an important way for users to make sense of the ever-changing AI technologies, yet remained underexamined. We target this gap by investigating playful interactions exhibited by users of a popular AI technology, ChatGPT. Through a thematic analysis of 372 user-generated posts on the ChatGPT subreddit, we found that more than half (54\\%) of user discourse revolved around playful interactions. The analysis further allowed us to construct a preliminary framework to describe these interactions, categorizing them into six types: reflecting, jesting, imitating, challenging, tricking, and contriving; each included sub-categories. This study contributes to HCI and CSCW by identifying the diverse ways users engage in playful interactions with AI. It examines how these interactions can help users understand AI's agency, shape human-AI relationships, and provide insights for designing AI systems.",
      "authors": [
        "Mohammad Ronagh Nikghalb",
        "Jinghui Cheng"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2401.08405",
        "HTML": "https://arxiv.org/html/2401.08405",
        "PDF": "https://arxiv.org/pdf/2401.08405"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 16 Jan 2024 14:44:13 GMT",
          "size": "35kb",
          "version": "v1"
        },
        {
          "date": "Mon, 22 Jul 2024 16:44:14 GMT",
          "size": "56kb",
          "version": "v2"
        },
        {
          "date": "Tue, 15 Oct 2024 02:57:10 GMT",
          "size": "54kb",
          "version": "v3"
        },
        {
          "date": "Tue, 24 Jun 2025 16:45:18 GMT",
          "size": "98kb",
          "version": "v4"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Interrogating AI: Characterizing Emergent Playful Interactions with ChatGPT",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper analyzes user interactions with ChatGPT, indirectly touching on user-generated content and interaction data, but does not focus on LLM training data itself."
      },
      "tasks": []
    },
    {
      "id": "2403.01471",
      "abstract": "We propose a method to generate statistically representative synthetic data from a given dataset. The main goal of our method is for the created data set to mimic the inter--feature correlations present in the original data, while also offering a tunable parameter to influence the privacy level. In particular, our method constructs a statistical map by using the empirical conditional distributions between the features of the original dataset. Part of the tunability is achieved by limiting the depths of conditional distributions that are being used. We describe in detail our algorithms used both in the construction of a statistical map and how to use this map to generate synthetic observations. This approach is tested in three different ways: with a hand calculated example; a manufactured dataset; and a real world energy-related dataset of consumption/production of households in Madeira Island. We evaluate the method by comparing the datasets using the Pearson correlation matrix with different levels of resolution and depths of correlation. These two considerations are being viewed as tunable parameters influencing the resulting datasets fidelity and privacy. The proposed methodology is general in the sense that it does not rely on the used test dataset. We expect it to be applicable in a much broader context than indicated here.",
      "authors": [
        "Nicklas J\\\"averg{\\aa}rd",
        "Rainey Lyons",
        "Adrian Muntean and Jonas Forsman"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2403.01471",
        "HTML": "https://arxiv.org/html/2403.01471",
        "PDF": "https://arxiv.org/pdf/2403.01471"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Probability (math.PR)",
        "Data Analysis, Statistics and Probability (physics.data-an)"
      ],
      "submission_historys": [
        {
          "date": "Sun, 03 Mar 2024 10:35:46 GMT",
          "size": "493kb",
          "version": "v1"
        },
        {
          "date": "Mon, 11 Nov 2024 12:01:06 GMT",
          "size": "1138kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 10:32:44 GMT",
          "size": "479kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Tunable correlation retention: A statistical method for generating synthetic data",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper involves generating synthetic data and preserving statistical properties, relevant to LLM training data in the context of data privacy and dataset representativeness."
      },
      "tasks": []
    },
    {
      "id": "2403.12029",
      "abstract": "Object detectors often perform poorly on data that differs from their training set. Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks. We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real-world data, and (4) A new method, ALDI++, that achieves state-of-the-art results by a large margin. ALDI++ outperforms the previous state-of-the-art by +3.5 AP50 on Cityscapes to Foggy Cityscapes, +5.7 AP50 on Sim10k to Cityscapes (where ours is the only method to outperform a fair baseline), and +0.6 AP50 on CFC Kenai to Channel. ALDI and ALDI++ are architecture-agnostic, setting a new state-of-the-art for YOLO and DETR-based DAOD as well without additional hyperparameter tuning. Our framework, dataset, and state-of-the-art method offer a critical reset for DAOD and provide a strong foundation for future research. Code and data are available: https://github.com/justinkay/aldi and https://github.com/visipedia/caltech-fish-counting.",
      "authors": [
        "Justin Kay",
        "Timm Haucke",
        "Suzanne Stathatos",
        "Siqi Deng",
        "Erik Young",
        "Pietro Perona",
        "Sara Beery",
        "Grant Van Horn"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2403.12029",
        "HTML": "https://arxiv.org/html/2403.12029",
        "PDF": "https://arxiv.org/pdf/2403.12029"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 18 Mar 2024 17:58:02 GMT",
          "size": "36026kb",
          "version": "v1"
        },
        {
          "date": "Sun, 25 Aug 2024 14:05:18 GMT",
          "size": "36337kb",
          "version": "v2"
        },
        {
          "date": "Mon, 17 Mar 2025 20:18:16 GMT",
          "size": "22367kb",
          "version": "v3"
        },
        {
          "date": "Mon, 23 Jun 2025 23:18:32 GMT",
          "size": "18106kb",
          "version": "v4"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Align and Distill: Unifying and Improving Domain Adaptive Object Detection",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper mentions dataset creation for domain adaptive object detection. Although it touches on dataset aspects, it is focused on visual data and domain adaptation rather than LLMs or NLP datasets."
      },
      "tasks": [
        "Benchmarking",
        "object-detection",
        "Object Detection",
        "Unsupervised Domain Adaptation"
      ],
      "repo_urls": [
        "https://github.com/estrellaxyu/differential-alignment-for-daod",
        "https://github.com/justinkay/aldi"
      ]
    },
    {
      "id": "2404.08844",
      "abstract": "The deep learning models has significantly advanced dexterous manipulation techniques for multi-fingered hand grasping. However, the contact information-guided grasping in cluttered environments remains largely underexplored. To address this gap, we have developed a method for generating multi-fingered hand grasp samples in cluttered settings through contact semantic map. We introduce a contact semantic conditional variational autoencoder network (CoSe-CVAE) for creating comprehensive contact semantic map from object point cloud. We utilize grasp detection method to estimate hand grasp poses from the contact semantic map. Finally, an unified grasp evaluation model PointNetGPD++ is designed to assess grasp quality and collision probability, substantially improving the reliability of identifying optimal grasps in cluttered scenarios. Our grasp generation method has demonstrated remarkable success, outperforming state-of-the-art methods by at least 4.65% with 81.0% average grasping success rate in real-world single-object environment and 75.3% grasping success rate in cluttered scenes. We also proposed the multi-modal multi-fingered grasping dataset generation method. Our multi-fingered hand grasping dataset outperforms previous datasets in scene diversity, modality diversity. The dataset, code and supplementary materials can be found at https://sites.google.com/view/contact-dexnet.",
      "authors": [
        "Lei Zhang",
        "Kaixin Bai",
        "Guowen Huang",
        "Zhenshan Bing",
        "Zhaopeng Chen",
        "Alois Knoll and Jianwei Zhang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2404.08844",
        "HTML": "https://arxiv.org/html/2404.08844",
        "PDF": "https://arxiv.org/pdf/2404.08844"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Fri, 12 Apr 2024 23:11:36 GMT",
          "size": "5768kb",
          "version": "v1"
        },
        {
          "date": "Sun, 22 Sep 2024 11:46:14 GMT",
          "size": "5641kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 09:49:27 GMT",
          "size": "8398kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "ContactDexNet: Multi-fingered Robotic Hand Grasping in Cluttered Environments through Hand-object Contact Semantic Mapping",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper involves generating data (multi-fingered grasping datasets) which could be loosely related to training data, but it is not centered around LLM training data."
      },
      "tasks": [
        "Dataset Generation",
        "Diversity",
        "Grasp Generation"
      ]
    },
    {
      "id": "2405.09922",
      "abstract": "Large-scale ''foundation models'' have gained traction as a way to leverage the vast amounts of unlabeled remote sensing data collected every day. However, due to the multiplicity of Earth Observation satellites, these models should learn ''sensor agnostic'' representations, that generalize across sensor characteristics with minimal fine-tuning. This is complicated by data availability, as low-resolution imagery, such as Sentinel-2 and Landsat-8 data, are available in large amounts, while very high-resolution aerial or satellite data is less common. To tackle these challenges, we introduce cross-sensor self-supervised training and alignment for remote sensing (X-STARS). We design a self-supervised training loss, the Multi-Sensor Alignment Dense loss (MSAD), to align representations across sensors, even with vastly different resolutions. Our X-STARS can be applied to train models from scratch, or to adapt large models pretrained on e.g low-resolution EO data to new high-resolution sensors, in a continual pretraining framework. We collect and release MSC-France, a new multi-sensor dataset, on which we train our X-STARS models, then evaluated on seven downstream classification and segmentation tasks. We demonstrate that X-STARS outperform s the state-of-the-art by a significant margin with less data across various conditions of data availability and resolutions.",
      "authors": [
        "Valerio Marsocci (CEDRIC - VERTIGO",
        "Cnam)",
        "Nicolas Audebert (CEDRIC - VERTIGO",
        "Cnam",
        "LaSTIG",
        "IGN)"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2405.09922",
        "HTML": "https://arxiv.org/html/2405.09922",
        "PDF": "https://arxiv.org/pdf/2405.09922"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Thu, 16 May 2024 09:25:45 GMT",
          "size": "2758kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 12:52:02 GMT",
          "size": "7075kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Cross-sensor self-supervised training and alignment for remote sensing",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the focus is not on LLM training data, the paper discusses remote sensing data and challenges of data heterogeneity, which is tangentially related to data issues in LLMs."
      },
      "tasks": [
        "Continual Pretraining",
        "Earth Observation"
      ]
    },
    {
      "id": "2405.10213",
      "abstract": "Political debates on social media sometimes flare up. From that moment on, users engage much more with one another; their communication is also more emotional and polarised. While it has been difficult to grasp such moments with computational methods, we suggest that trigger points are a useful concept to understand and ultimately model such behaviour. Established in qualitative focus group interviews to understand political polarisation (Mau, Lux, and Westheuser 2023), trigger points represent moments when individuals feel that their understanding of what is fair, normal, or appropriate in society is questioned. In the original studies, individuals show strong and negative emotional responses when certain triggering words or topics are mentioned. Our paper finds that these trigger points also exist in online debates. We examine online deliberations on Reddit between 2020 and 2022 and collect >100 million comments from subreddits related to a set of words identified as trigger points in UK politics. Analysing the comments, we find that trigger words increase user engagement and animosity, i.e., more negativity, hate speech, and controversial comments. Introducing trigger points to computational studies of online communication, our findings are relevant to researchers interested in affective computing, online deliberation, and how citizens debate politics and society in light of affective polarisation.",
      "authors": [
        "Dimosthenis Antypas",
        "Christian Arnold",
        "Jose Camacho-Collados",
        "Nedjma Ousidhoum",
        "Carla Perez Almendros"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2405.10213",
        "HTML": "https://arxiv.org/html/2405.10213",
        "PDF": "https://arxiv.org/pdf/2405.10213"
      },
      "subjects": [
        "Social and Information Networks (cs.SI)",
        "Computation and Language (cs.CL)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "Thu, 16 May 2024 16:02:42 GMT",
          "size": "8129kb",
          "version": "v1"
        },
        {
          "date": "Tue, 15 Oct 2024 13:37:03 GMT",
          "size": "8215kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 16:59:23 GMT",
          "size": "7415kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Words as Trigger Points in Social Media Discussions: A Large-Scale Case Study about UK Politics on Reddit",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The study explores social media discussions and trigger words which could be indirectly related to datasets used for training language models, especially regarding data annotation and the representation of user-generated content."
      },
      "tasks": [
        "Abusive Language"
      ]
    },
    {
      "id": "2406.05410",
      "abstract": "Formulas are the language of communication between humans and nature. The discovery of formulas to describe natural laws from observational data is the purpose of scientific research. It is also an important research topic in artificial intelligence, which is called a symbolic regression problem. Most of the existing symbolic regression methods generate expressions directly from observed data. Although in some methods, we can inject some prior knowledge into the model by adding constraints or introducing some special character hints. However, these methods can only introduce a limited amount of prior knowledge specified in advance. Not to mention understanding natural language instructions. In this article, based on the powerful knowledge reserve and language understanding ability of multi-modal large language models, we present ChatSR, which acts like a knowledgeable human scientist, and we can tell it any prior knowledge through natural language to guide it in formula generation. By testing on 13 datasets, ChatSR not only shows state-of-the-art performance on traditional symbolic regression tasks. More notably, ChatSR can well understand the prior knowledge contained in natural language prompts and improve the quality of generated expressions. In addition, it is exciting that ChatSR has a good zero-shot capability to understand prior knowledge that is not present in the training data.",
      "authors": [
        "Yanjie Li",
        "Lina Yu",
        "Weijun Li",
        "Min Wu",
        "Jingyi Liu",
        "Wenqiang Li",
        "Shu Wei",
        "Yusong Deng"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2406.05410",
        "HTML": "https://arxiv.org/html/2406.05410",
        "PDF": "https://arxiv.org/pdf/2406.05410"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Sat, 08 Jun 2024 09:17:54 GMT",
          "size": "767kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 12:22:55 GMT",
          "size": "1170kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "ChatSR: Multimodal Large Language Models for Scientific Formula Discovery",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses ChatSR, a multimodal model that understands natural language knowledge to guide formula generation. It mentions 'prior knowledge' and 'training data' but focuses on symbolic regression and formula discovery, not directly on LLM training data."
      },
      "tasks": [
        "regression",
        "Symbolic Regression"
      ]
    },
    {
      "id": "2406.08809",
      "abstract": "Deep learning models for music have advanced drastically in recent years, but how good are machine learning models at capturing emotion, and what challenges are researchers facing? In this paper, we provide a comprehensive overview of the available music-emotion datasets and discuss evaluation standards as well as competitions in the field. We also offer a brief overview of various types of music emotion prediction models that have been built over the years, providing insights into the diverse approaches within the field. Through this examination, we highlight the challenges that persist in accurately capturing emotion in music, including issues related to dataset quality, annotation consistency, and model generalization. Additionally, we explore the impact of different modalities, such as audio, MIDI, and physiological signals, on the effectiveness of emotion prediction models. Through this examination, we identify persistent challenges in music emotion recognition (MER), including issues related to dataset quality, the ambiguity in emotion labels, and the difficulties of cross-dataset generalization. We argue that future advancements in MER require standardized benchmarks, larger and more diverse datasets, and improved model interpretability. Recognizing the dynamic nature of this field, we have complemented our findings with an accompanying GitHub repository. This repository contains a comprehensive list of music emotion datasets and recent predictive models.",
      "authors": [
        "Jaeyong Kang",
        "Dorien Herremans"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2406.08809",
        "HTML": "https://arxiv.org/html/2406.08809",
        "PDF": "https://arxiv.org/pdf/2406.08809"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Artificial Intelligence (cs.AI)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "date": "Thu, 13 Jun 2024 05:00:27 GMT",
          "size": "151kb",
          "version": "v1"
        },
        {
          "date": "Tue, 22 Oct 2024 12:18:27 GMT",
          "size": "3351kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 05:19:36 GMT",
          "size": "3447kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Are We There Yet? A Brief Survey of Music Emotion Prediction Datasets, Models and Outstanding Challenges",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the paper delves into datasets for music emotion prediction, touching upon dataset quality and challenges, its primary focus is not on LLM training data but rather on datasets specific to the music domain."
      },
      "tasks": [],
      "repo_urls": [
        "https://github.com/amaai-lab/awesome-mer"
      ]
    },
    {
      "id": "2406.18259",
      "abstract": "As LLMs rapidly advance, increasing concerns arise regarding risks about actual authorship of texts we see online and in real world. The task of distinguishing LLM-authored texts is complicated by the nuanced and overlapping behaviors of both machines and humans. In this paper, we challenge the current practice of considering LLM-generated text detection a binary classification task of differentiating human from AI. Instead, we introduce a novel ternary text classification scheme, adding an \"undecided\" category for texts that could be attributed to either source, and we show that this new category is crucial to understand how to make the detection result more explainable to lay users. This research shifts the paradigm from merely classifying to explaining machine-generated texts, emphasizing need for detectors to provide clear and understandable explanations to users. Our study involves creating four new datasets comprised of texts from various LLMs and human authors. Based on new datasets, we performed binary classification tests to ascertain the most effective SOTA detection methods and identified SOTA LLMs capable of producing harder-to-detect texts. We constructed a new dataset of texts generated by two top-performing LLMs and human authors, and asked three human annotators to produce ternary labels with explanation notes. This dataset was used to investigate how three top-performing SOTA detectors behave in new ternary classification context. Our results highlight why \"undecided\" category is much needed from the viewpoint of explainability. Additionally, we conducted an analysis of explainability of the three best-performing detectors and the explanation notes of the human annotators, revealing insights about the complexity of explainable detection of machine-generated texts. Finally, we propose guidelines for developing future detection systems with improved explanatory power.",
      "authors": [
        "Jiazhou Ji",
        "Ruizhe Li",
        "Shujun Li",
        "Jie Guo",
        "Weidong Qiu",
        "Zheng Huang",
        "Chiyu Chen",
        "Xiaoyu Jiang",
        "Xinru Lu"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2406.18259",
        "HTML": "https://arxiv.org/html/2406.18259",
        "PDF": "https://arxiv.org/pdf/2406.18259"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 26 Jun 2024 11:11:47 GMT",
          "size": "8007kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 15:45:05 GMT",
          "size": "53kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Detecting Machine-Generated Texts: Not Just \"AI vs Humans\" and Explainability is Complicated",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses detecting machine-generated texts and utilizes datasets for this purpose. It indirectly relates to LLMs concerning distinguishing outputs but doesn't focus on the training data itself."
      },
      "tasks": [
        "Binary Classification",
        "LLM-generated Text Detection",
        "text-classification",
        "Text Classification",
        "Text Detection"
      ]
    },
    {
      "id": "2407.02157",
      "abstract": "Dynamic Facial Expression Recognition (DFER) is crucial for understanding human behavior. However, current methods exhibit limited performance mainly due to the scarcity of high-quality data, the insufficient utilization of facial dynamics, and the ambiguity of expression semantics, etc. To this end, we propose a novel framework, named Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs (FineCLIPER), incorporating the following novel designs: 1) To better distinguish between similar facial expressions, we extend the class labels to textual descriptions from both positive and negative aspects, and obtain supervision by calculating the cross-modal similarity based on the CLIP model; 2) Our FineCLIPER adopts a hierarchical manner to effectively mine useful cues from DFE videos. Specifically, besides directly embedding video frames as input (low semantic level), we propose to extract the face segmentation masks and landmarks based on each frame (middle semantic level) and utilize the Multi-modal Large Language Model (MLLM) to further generate detailed descriptions of facial changes across frames with designed prompts (high semantic level). Additionally, we also adopt Parameter-Efficient Fine-Tuning (PEFT) to enable efficient adaptation of large pre-trained models (i.e., CLIP) for this task. Our FineCLIPER achieves SOTA performance on the DFEW, FERV39k, and MAFW datasets in both supervised and zero-shot settings with few tunable parameters. Project Page: https://haroldchen19.github.io/FineCLIPER-Page/",
      "authors": [
        "Haodong Chen",
        "Haojian Huang",
        "Junhao Dong",
        "Mingzhe Zheng",
        "Dian Shao"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2407.02157",
        "HTML": "https://arxiv.org/html/2407.02157",
        "PDF": "https://arxiv.org/pdf/2407.02157"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 02 Jul 2024 10:55:43 GMT",
          "size": "3607kb",
          "version": "v1"
        },
        {
          "date": "Tue, 23 Jul 2024 10:08:52 GMT",
          "size": "4017kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 07:42:09 GMT",
          "size": "3328kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "This paper is primarily about dynamic facial expression recognition using a framework that includes large language models like CLIP. It indirectly relates to LLMs by using them for understanding semantics in facial expressions, but does not focus on LLM training data specifics."
      },
      "tasks": [
        "Dynamic Facial Expression Recognition",
        "Facial Expression Recognition",
        "Language Modelling",
        "Large Language Model",
        "parameter-efficient fine-tuning"
      ]
    },
    {
      "id": "2407.06331",
      "abstract": "Translating technical terms into lexically similar, low-resource Indian languages remains a challenge due to limited parallel data and the complexity of linguistic structures. We propose a novel use-case of Sanskrit-based segments for linguistically informed translation of such terms, leveraging subword-level similarity and morphological alignment across related languages. Our approach uses character-level segmentation to identify meaningful subword units, facilitating more accurate and context-aware translation. To enable this, we utilize a Character-level Transformer model for Sanskrit Word Segmentation (CharSS), which addresses the complexities of sandhi and morpho-phonemic changes during segmentation. We observe consistent improvements in two experimental settings for technical term translation using Sanskrit-derived segments, averaging 8.46 and 6.79 chrF++ scores, respectively. Further, we conduct a post hoc human evaluation to verify the quality assessment of the translated technical terms using automated metrics. This work has important implications for the education field, especially in creating accessible, high-quality learning materials in Indian languages. By supporting the accurate and linguistically rooted translation of technical content, our approach facilitates inclusivity and aids in bridging the resource gap for learners in low-resource language communities.",
      "authors": [
        "Karthika N J",
        "Krishnakant Bhatt",
        "Ganesh Ramakrishnan",
        "and Preethi Jyothi"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2407.06331",
        "HTML": "https://arxiv.org/html/2407.06331",
        "PDF": "https://arxiv.org/pdf/2407.06331"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 08 Jul 2024 18:50:13 GMT",
          "size": "8461kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 10:06:32 GMT",
          "size": "8526kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "LEVOS: Leveraging Vocabulary Overlap with Sanskrit to Generate Technical Lexicons in Indian Languages",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses translation challenges using Sanskrit for low-resource languages. It indirectly relates to data preparation and representation but does not focus on LLM training data specifically."
      },
      "tasks": []
    },
    {
      "id": "2407.16804",
      "abstract": "Multimodal machine learning (MML) is rapidly reshaping the way mental-health disorders are detected, characterized, and longitudinally monitored. Whereas early studies relied on isolated data streams -- such as speech, text, or wearable signals -- recent research has converged on architectures that integrate heterogeneous modalities to capture the rich, complex signatures of psychiatric conditions. This survey provides the first comprehensive, clinically grounded synthesis of MML for mental health. We (i) catalog 26 public datasets spanning audio, visual, physiological signals, and text modalities; (ii) systematically compare transformer, graph, and hybrid-based fusion strategies across 28 models, highlighting trends in representation learning and cross-modal alignment. Beyond summarizing current capabilities, we interrogate open challenges: data governance and privacy, demographic and intersectional fairness, evaluation explainability, and the complexity of mental health disorders in multimodal settings. By bridging methodological innovation with psychiatric utility, this survey aims to orient both ML researchers and mental-health practitioners toward the next generation of trustworthy, multimodal decision-support systems.",
      "authors": [
        "Zahraa Al Sahili",
        "Ioannis Patras",
        "Matthew Purver"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2407.16804",
        "HTML": "https://arxiv.org/html/2407.16804",
        "PDF": "https://arxiv.org/pdf/2407.16804"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Computers and Society (cs.CY)",
        "Emerging Technologies (cs.ET)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 23 Jul 2024 19:07:56 GMT",
          "size": "496kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 13:40:09 GMT",
          "size": "92kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Multimodal Machine Learning in Mental Health: A Survey of Data, Algorithms, and Challenges",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses multiple datasets spanning various modalities for mental health research, touching on aspects of data governance and privacy, which are tangentially related to the concerns around LLM training data but not the core focus."
      },
      "tasks": [
        "Survey"
      ]
    },
    {
      "id": "2408.01933",
      "abstract": "This paper introduces REACT, a benchmark designed to rigorously evaluate the reasoning capabilities of large language models (LLMs) within accountable, high-stakes decision-making tasks in medical and legal domains. Unlike traditional benchmarks primarily focused on prediction accuracy, REACT emphasizes transparent and interpretable reasoning, requiring models to align their logic closely with expert-derived procedures. To assess whether LLM reasoning aligns closely with human experts, we annotated 511 clinical cases from the medical domain and 86 legal cases from the legal domain, each enriched with detailed expert-extracted rationales and evidence supporting each step of the reasoning process. These annotations were guided by carefully constructed reasoning graphs, which explicitly encode domain-specific inference structures and decision criteria derived by domain experts. These reasoning graphs serve not only as standards for expert annotation but also as structured guidelines enabling models to reason transparently and step-by-step. To address the scalability challenges of manual annotation, we further developed a semi-automatic annotation pipeline leveraging expert-defined reasoning graph templates to efficiently generate new graphs, exploring the potential to extend our approach into additional critical domains. Experimental results demonstrate that reasoning graphs substantially enhance the interpretability and accuracy of LLM reasoning compared to traditional baselines, although significant gaps remain relative to expert-level reasoning performance.",
      "authors": [
        "Junhao Chen",
        "Bowen Wang",
        "Jiuyang Chang",
        "and Yuta Nakashima"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2408.01933",
        "HTML": "https://arxiv.org/html/2408.01933",
        "PDF": "https://arxiv.org/pdf/2408.01933"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Sun, 04 Aug 2024 05:15:02 GMT",
          "size": "798kb",
          "version": "v1"
        },
        {
          "date": "Tue, 06 Aug 2024 04:28:01 GMT",
          "size": "798kb",
          "version": "v2"
        },
        {
          "date": "Fri, 10 Jan 2025 04:09:43 GMT",
          "size": "866kb",
          "version": "v3"
        },
        {
          "date": "Mon, 13 Jan 2025 07:13:56 GMT",
          "size": "866kb",
          "version": "v4"
        },
        {
          "date": "Tue, 24 Jun 2025 03:31:03 GMT",
          "size": "4748kb",
          "version": "v5"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Evaluating Transparent Reasoning in Large Language Models for Accountable Critical Tasks",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper focuses on evaluating reasoning in LLMs, which involves annotated datasets. While it is centered around benchmark creation and annotation, which is related to data curation, it does not specifically focus on LLM training data."
      },
      "tasks": [
        "Diagnostic",
        "Question Answering"
      ],
      "repo_urls": [
        "https://github.com/wbw520/direct"
      ]
    },
    {
      "id": "2410.05563",
      "abstract": "Being prompted to engage in reasoning has emerged as a core technique for using large language models (LLMs), deploying additional inference-time compute to improve task performance. However, as LLMs increase in both size and adoption, inference costs are correspondingly becoming increasingly burdensome. How, then, might we optimize reasoning's cost-performance tradeoff? This work introduces a novel approach based on computational models of metareasoning used in cognitive science, training LLMs to selectively use intermediate reasoning steps only when necessary. We first develop a reward function that incorporates the Value of Computation by penalizing unnecessary reasoning, then use this reward function with Expert Iteration to train the LLM. Compared to few-shot chain-of-thought prompting and STaR, our method significantly reduces inference costs (20-37\\% fewer tokens generated across three models) while maintaining task performance across diverse datasets.",
      "authors": [
        "C. Nicol\\`o De Sabbata",
        "Theodore R. Sumers",
        "Badr AlKhamissi",
        "Antoine Bosselut",
        "Thomas L. Griffiths"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2410.05563",
        "HTML": "https://arxiv.org/html/2410.05563",
        "PDF": "https://arxiv.org/pdf/2410.05563"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 07 Oct 2024 23:48:52 GMT",
          "size": "1957kb",
          "version": "v1"
        },
        {
          "date": "Sat, 21 Dec 2024 11:08:42 GMT",
          "size": "2198kb",
          "version": "v2"
        },
        {
          "date": "Mon, 23 Jun 2025 18:59:37 GMT",
          "size": "2921kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Rational Metareasoning for Large Language Models",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "Although the paper is about reasoning in LLMs, it does not explicitly discuss LLM training data. It focuses on optimizing reasoning steps during inference, which can indirectly involve dataset considerations for training LLMs' reasoning capabilities."
      },
      "tasks": []
    },
    {
      "id": "2410.17933",
      "abstract": "One of the biggest challenges of building artificial intelligence (AI) model in the healthcare area is the data sharing. Since healthcare data is private, sensitive, and heterogeneous, collecting sufficient data for modelling is exhausting, costly, and sometimes impossible. In this paper, we propose a framework for global healthcare modelling using datasets from multi-continents (Europe, North America, and Asia) without sharing the local datasets, and choose glucose management as a study model to verify its effectiveness. Technically, blockchain-enabled federated learning is implemented with adaptation to meet the privacy and safety requirements of healthcare data, meanwhile, it rewards honest participation and penalizes malicious activities using its on-chain incentive mechanism. Experimental results show that the proposed framework is effective, efficient, and privacy-preserving. Its prediction accuracy consistently outperforms models trained on limited personal data and achieves comparable or even slightly better results than centralized training in certain scenarios, all while preserving data privacy. This work paves the way for international collaborations on healthcare projects, where additional data is crucial for reducing bias and providing benefits to humanity.",
      "authors": [
        "Rui Sun",
        "Zhipeng Wang",
        "Hengrui Zhang",
        "Ming Jiang",
        "Yizhe Wen",
        "Jiahao Sun",
        "Xinyu Qu",
        "Kezhi Li"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2410.17933",
        "HTML": "https://arxiv.org/html/2410.17933",
        "PDF": "https://arxiv.org/pdf/2410.17933"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 23 Oct 2024 14:55:53 GMT",
          "size": "2295kb",
          "version": "v1"
        },
        {
          "date": "Fri, 30 May 2025 19:44:28 GMT",
          "size": "1045kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 08:32:03 GMT",
          "size": "1043kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Multi-Continental Healthcare Modelling Using Blockchain-Enabled Federated Learning",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper focuses on blockchain-enabled federated learning for healthcare data, which indirectly relates to aspects like data privacy and sharing that could pertain to LLM training data."
      },
      "tasks": [
        "Federated Learning"
      ]
    },
    {
      "id": "2410.18469",
      "abstract": "Recent research has shown that Large Language Models (LLMs) are vulnerable to automated jailbreak attacks, where adversarial suffixes crafted by algorithms appended to harmful queries bypass safety alignment and trigger unintended responses. Current methods for generating these suffixes are computationally expensive and have low Attack Success Rates (ASR), especially against well-aligned models like Llama2 and Llama3. To overcome these limitations, we introduce ADV-LLM, an iterative self-tuning process that crafts adversarial LLMs with enhanced jailbreak ability. Our framework significantly reduces the computational cost of generating adversarial suffixes while achieving nearly 100\\% ASR on various open-source LLMs. Moreover, it exhibits strong attack transferability to closed-source models, achieving 99\\% ASR on GPT-3.5 and 49\\% ASR on GPT-4, despite being optimized solely on Llama3. Beyond improving jailbreak ability, ADV-LLM provides valuable insights for future safety alignment research through its ability to generate large datasets for studying LLM safety.",
      "authors": [
        "Chung-En Sun",
        "Xiaodong Liu",
        "Weiwei Yang",
        "Tsui-Wei Weng",
        "Hao Cheng",
        "Aidan San",
        "Michel Galley",
        "Jianfeng Gao"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2410.18469",
        "HTML": "https://arxiv.org/html/2410.18469",
        "PDF": "https://arxiv.org/pdf/2410.18469"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Thu, 24 Oct 2024 06:36:12 GMT",
          "size": "3984kb",
          "version": "v1"
        },
        {
          "date": "Fri, 25 Oct 2024 23:05:59 GMT",
          "size": "3964kb",
          "version": "v2"
        },
        {
          "date": "Tue, 11 Mar 2025 23:26:25 GMT",
          "size": "7817kb",
          "version": "v3"
        },
        {
          "date": "Mon, 23 Jun 2025 20:12:31 GMT",
          "size": "4126kb",
          "version": "v4"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "ADVLLM: Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While primarily addressing safety and security vulnerabilities in LLMs, the creation of adversarial datasets for testing could relate to dataset applications in LLM training contexts."
      },
      "models": [
        {
          "model_path": "cesun/advllm_vicuna",
          "downloads": "35",
          "likes": "1",
          "trending_score": "1.0",
          "link": "https://huggingface.co/cesun/advllm_vicuna"
        },
        {
          "model_path": "cesun/advllm_guanaco",
          "downloads": "19",
          "likes": "1",
          "trending_score": "1.0",
          "link": "https://huggingface.co/cesun/advllm_guanaco"
        },
        {
          "model_path": "cesun/advllm_mistral",
          "downloads": "20",
          "likes": "1",
          "trending_score": "1.0",
          "link": "https://huggingface.co/cesun/advllm_mistral"
        },
        {
          "model_path": "cesun/advllm_llama2",
          "downloads": "27",
          "likes": "1",
          "trending_score": "1.0",
          "link": "https://huggingface.co/cesun/advllm_llama2"
        },
        {
          "model_path": "cesun/advllm_llama3",
          "downloads": "31",
          "likes": "1",
          "trending_score": "1.0",
          "link": "https://huggingface.co/cesun/advllm_llama3"
        },
        {
          "model_path": "cesun/advllm_phi3",
          "downloads": "25",
          "likes": "1",
          "trending_score": "1.0",
          "link": "https://huggingface.co/cesun/advllm_phi3"
        }
      ],
      "tasks": [
        "Safety Alignment"
      ],
      "repo_urls": [
        "https://github.com/sunchungen/adv-llm"
      ]
    },
    {
      "id": "2410.23478",
      "abstract": "Recent years in NLP have seen the continued development of domain-specific information extraction tools for scientific documents, alongside the release of increasingly multimodal pretrained transformer models. While the opportunity for scientists outside of NLP to evaluate and apply such systems to their own domains has never been clearer, these models are difficult to compare: they accept different input formats, are often black-box and give little insight into processing failures, and rarely handle PDF documents, the most common format of scientific publication. In this work, we present Collage, a tool designed for rapid prototyping, visualization, and evaluation of different information extraction models on scientific PDFs. Collage allows the use and evaluation of any HuggingFace token classifier, several LLMs, and multiple other task-specific models out of the box, and provides extensible software interfaces to accelerate experimentation with new models. Further, we enable both developers and users of NLP-based tools to inspect, debug, and better understand modeling pipelines by providing granular views of intermediate states of processing. We demonstrate our system in the context of information extraction to assist with literature review in materials science.",
      "authors": [
        "Sireesh Gururaja",
        "Yueheng Zhang",
        "Guannan Tang",
        "Tianhao Zhang",
        "Kevin Murphy",
        "Yu-Tsen Yi",
        "Junwon Seo",
        "Anthony Rollett",
        "Emma Strubell"
      ],
      "last_revised_date": "2025/06/22",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2410.23478",
        "HTML": "https://arxiv.org/html/2410.23478",
        "PDF": "https://arxiv.org/pdf/2410.23478"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 30 Oct 2024 22:00:34 GMT",
          "size": "9693kb",
          "version": "v1"
        },
        {
          "date": "Sun, 22 Jun 2025 18:50:21 GMT",
          "size": "17414kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Collage: Decomposable Rapid Prototyping for Information Extraction on Scientific PDFs",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper introduces Collage, a tool for information extraction on scientific PDFs using LLMs and other models. It touches on model use in NLP but does not specifically focus on LLM training data."
      },
      "tasks": [],
      "repo_urls": [
        "https://github.com/gsireesh/ht-max"
      ]
    },
    {
      "id": "2411.10227",
      "abstract": "There are different ways of measuring diversity in complex systems. In particular, in language, lexical diversity is characterized in terms of the type-token ratio and the word entropy. We here investigate both diversity metrics in six massive linguistic datasets in English, Spanish, and Turkish, consisting of books, news articles, and tweets. These gigaword corpora correspond to languages with distinct morphological features and differ in registers and genres, thus constituting a varied testbed for a quantitative approach to lexical diversity. We unveil an empirical functional relation between entropy and type-token ratio of texts of a given corpus and language, which is a consequence of the statistical laws observed in natural language. Further, in the limit of large text lengths we find an analytical expression for this relation relying on both Zipf and Heaps laws that agrees with our empirical findings.",
      "authors": [
        "Pablo Rosillo-Rodes",
        "Maxi San Miguel and David Sanchez"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2411.10227",
        "HTML": "https://arxiv.org/html/2411.10227",
        "PDF": "https://arxiv.org/pdf/2411.10227"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Information Retrieval (cs.IR)",
        "Physics and Society (physics.soc-ph)"
      ],
      "submission_historys": [
        {
          "date": "Fri, 15 Nov 2024 14:40:59 GMT",
          "size": "4045kb",
          "version": "v1"
        },
        {
          "date": "Wed, 26 Feb 2025 22:21:27 GMT",
          "size": "10920kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 17:18:03 GMT",
          "size": "6447kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Entropy and type-token ratio in gigaword corpora",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper examines gigaword corpora and metrics like type-token ratio and entropy. While it involves datasets, it does not focus on LLM training data specifically."
      },
      "tasks": [
        "Articles",
        "Diversity",
        "Relation"
      ]
    },
    {
      "id": "2501.15276",
      "abstract": "Artificial intelligence is reshaping creative domains, yet its co-creative processes, especially in group settings with novice users, remain under explored. To bridge this gap, we conducted a case study in a college-level course where nine undergraduate students were tasked with creating three original music tracks using AI tools over 10 weeks. The study spanned the entire creative journey from ideation to releasing these songs on Spotify. Participants leveraged AI for music and lyric production, cover art, and distribution. Our findings highlight how AI transforms creative workflows: accelerating ideation but compressing the traditional preparation stage, and requiring novices to navigate a challenging idea selection and validation phase. We also identified a new \"collaging and refinement\" stage, where participants creatively combined diverse AI-generated outputs into cohesive works. Furthermore, AI influenced group social dynamics and role division among human creators. Based on these insights, we propose the Human-AI Co-Creation Stage Model and the Human-AI Agency Model, offering new perspectives on collaborative co-creation with AI.",
      "authors": [
        "Yue Fu",
        "Michele Newman",
        "Lewis Going",
        "Qiuzi Feng",
        "Jin Ha Lee"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2501.15276",
        "HTML": "https://arxiv.org/html/2501.15276",
        "PDF": "https://arxiv.org/pdf/2501.15276"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Sat, 25 Jan 2025 17:00:17 GMT",
          "size": "10798kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 09:33:17 GMT",
          "size": "4953kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Exploring the Collaborative Co-Creation Process with AI: A Case Study in Novice Music Production",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper explores AI in creative processes, including using AI tools for generating music, lyrics, and art. Though indirectly related to LLM training data through AI content generation, it is not the main focus."
      },
      "tasks": [
        "Navigate"
      ]
    },
    {
      "id": "2501.16490",
      "abstract": "Smart grids are crucial for meeting rising energy demands driven by global population growth and urbanization. By integrating renewable energy sources, they enhance efficiency, reliability, and sustainability. However, ensuring their availability and security requires advanced operational control and safety measures. Although artificial intelligence and machine learning can help assess grid stability, challenges such as data scarcity and cybersecurity threats, particularly adversarial attacks, remain. Data scarcity is a major issue, as obtaining real-world instances of grid instability requires significant expertise, resources, and time. Yet, these instances are critical for testing new research advancements and security mitigations. This paper introduces a novel framework for detecting instability in smart grids using only stable data. It employs a Generative Adversarial Network (GAN) where the generator is designed not to produce near-realistic data but instead to generate Out-Of-Distribution (OOD) samples with respect to the stable class. These OOD samples represent unstable behavior, anomalies, or disturbances that deviate from the stable data distribution. By training exclusively on stable data and exposing the discriminator to OOD samples, our framework learns a robust decision boundary to distinguish stable conditions from any unstable behavior, without requiring unstable data during training. Furthermore, we incorporate an adversarial training layer to enhance resilience against attacks. Evaluated on a real-world dataset, our solution achieves up to 98.1\\% accuracy in predicting grid stability and 98.9\\% in detecting adversarial attacks. Implemented on a single-board computer, it enables real-time decision-making with an average response time of under 7ms.",
      "authors": [
        "Emad Efatinasab",
        "Alessandro Brighente",
        "Denis Donadel",
        "Mauro Conti",
        "Mirco Rampazzo"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2501.16490",
        "HTML": "https://arxiv.org/html/2501.16490",
        "PDF": "https://arxiv.org/pdf/2501.16490"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 27 Jan 2025 20:48:25 GMT",
          "size": "4946kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 11:10:26 GMT",
          "size": "955kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Towards Robust Stability Prediction in Smart Grids: GAN-based Approach under Data Constraints and Adversarial Challenges",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper addresses data scarcity in smart grids using GANs to generate training samples, relevant only in the context of generating synthetic data but not specifically about training data for LLMs."
      },
      "tasks": [
        "Generative Adversarial Network"
      ]
    },
    {
      "id": "2502.04260",
      "abstract": "Machine Unlearning allows participants to remove their data from a trained machine learning model in order to preserve their privacy, and security. However, the machine unlearning literature for generative models is rather limited. The literature for image-to-image generative model (I2I model) considers minimizing the distance between Gaussian noise and the output of I2I model for forget samples as machine unlearning. However, we argue that the machine learning model performs fairly well on unseen data i.e., a retrained model will be able to catch generic patterns in the data and hence will not generate an output which is equivalent to Gaussian noise. In this paper, we consider that the model after unlearning should treat forget samples as out-of-distribution (OOD) data, i.e., the unlearned model should no longer recognize or encode the specific patterns found in the forget samples. To achieve this, we propose a framework which decouples the model parameters with gradient ascent, ensuring that forget samples are OOD for unlearned model with theoretical guarantee. We also provide $(\\epsilon, \\delta)$-unlearning guarantee for model updates with gradient ascent. The unlearned model is further fine-tuned on the remaining samples to maintain its performance. We also propose an attack model to ensure that the unlearned model has effectively removed the influence of forget samples. Extensive empirical evaluation on two large-scale datasets, ImageNet-1K and Places365 highlights the superiority of our approach. To show comparable performance with retrained model, we also show the comparison of a simple AutoEncoder on various baselines on CIFAR-10 dataset.",
      "authors": [
        "Ayush K. Varshney",
        "Vicen\\c{c} Torra"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2502.04260",
        "HTML": "https://arxiv.org/html/2502.04260",
        "PDF": "https://arxiv.org/pdf/2502.04260"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Thu, 06 Feb 2025 17:46:49 GMT",
          "size": "11232kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 12:47:20 GMT",
          "size": "4090kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Realistic Image-to-Image Machine Unlearning via Decoupling and Knowledge Retention",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper deals with machine unlearning, which involves removing data from models to preserve privacy. It indirectly relates to data privacy and retention aspects relevant to LLM training data."
      },
      "tasks": [
        "Machine Unlearning"
      ]
    },
    {
      "id": "2502.17036",
      "abstract": "Language model (LM) re-rankers are used to refine retrieval results for retrieval-augmented generation (RAG). They are more expensive than lexical matching methods like BM25 but assumed to better process semantic information and the relations between the query and the retrieved answers. To understand whether LM re-rankers always live up to this assumption, we evaluate 6 different LM re-rankers on the NQ, LitQA2 and DRUID datasets. Our results show that LM re-rankers struggle to outperform a simple BM25 baseline on DRUID. Leveraging a novel separation metric based on BM25 scores, we explain and identify re-ranker errors stemming from lexical dissimilarities. We also investigate different methods to improve LM re-ranker performance and find these methods mainly useful for NQ. Taken together, our work identifies and explains weaknesses of LM re-rankers and points to the need for more adversarial and realistic datasets for their evaluation.",
      "authors": [
        "Lovisa Hagstr\\\"om",
        "Ercong Nie",
        "Ruben Halifa",
        "Helmut Schmid",
        "Richard Johansson",
        "Alexander Junge"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2502.17036",
        "HTML": "https://arxiv.org/html/2502.17036",
        "PDF": "https://arxiv.org/pdf/2502.17036"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 24 Feb 2025 10:37:13 GMT",
          "size": "363kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 14:03:01 GMT",
          "size": "267kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Language Model Re-rankers are Fooled by Lexical Similarities",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the paper evaluates language model re-rankers, it does not explicitly focus on training data for large language models. It is tangentially related as it touches upon LM performance and evaluation."
      },
      "tasks": [
        "Language Modeling",
        "Language Modelling",
        "RAG",
        "Retrieval",
        "Retrieval-augmented Generation"
      ]
    },
    {
      "id": "2503.05830",
      "abstract": "This article unpacks the design choices behind longstanding and newly proposed computational frameworks aimed at finding common grounds across collective preferences and examines their potential future impacts, both technically and normatively. It begins by situating AI-assisted preference elicitation within the historical role of opinion polls, emphasizing that preferences are shaped by the decision-making context and are seldom objectively captured. With that caveat in mind, we explore AI-based democratic innovations as discovery tools for fostering reasonable representations of a collective will, sense-making, and agreement-seeking. At the same time, we caution against dangerously misguided uses, such as enabling binding decisions, fostering gradual disempowerment or post-rationalizing political outcomes.",
      "authors": [
        "Manon Revel and Th\\'eophile P\\'enigaud"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.05830",
        "HTML": "https://arxiv.org/html/2503.05830",
        "PDF": "https://arxiv.org/pdf/2503.05830"
      },
      "subjects": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Thu, 06 Mar 2025 00:06:22 GMT",
          "size": "940kb",
          "version": "v1"
        },
        {
          "date": "Mon, 23 Jun 2025 21:23:49 GMT",
          "size": "1016kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "AI-Enhanced Deliberative Democracy and the Future of the Collective Will",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper examines AI in deliberative democracy and preference elicitation, discussing reasonable representations of collective will. While it touches on data representation, it does not focus on LLM training data explicitly."
      },
      "tasks": [
        "Decision Making"
      ]
    },
    {
      "id": "2503.09730",
      "abstract": "The most promising recent methods for AI reasoning require applying variants of reinforcement learning (RL) either on rolled out trajectories from the LLMs, even for the step-wise rewards, or large quantities of human-annotated trajectory data. The reliance on the rolled-out trajectory renders the compute cost and time prohibitively high. In particular, the correctness of a reasoning trajectory can typically only be judged at its completion, leading to sparse rewards in RL or requiring expensive synthetic data generation in expert iteration-like methods. In this work, we focus on the Automatic Theorem Proving (ATP) task and propose a novel verifier-in-the-loop design, which, unlike existing approaches that leverage feedback on the entire reasoning trajectory, employs an automated verifier to give intermediate feedback at each step of the reasoning process. Using Lean as the verifier, we empirically show that the step-by-step local verification produces a global improvement in the model's reasoning accuracy and efficiency.",
      "authors": [
        "Sara Rajaee",
        "Kumar Pratik",
        "Gabriele Cesa",
        "Arash Behboodi"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.09730",
        "HTML": "https://arxiv.org/html/2503.09730",
        "PDF": "https://arxiv.org/pdf/2503.09730"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)",
        "Logic in Computer Science (cs.LO)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 12 Mar 2025 18:20:47 GMT",
          "size": "1259kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 15:42:55 GMT",
          "size": "801kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem Proving",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper deals with AI reasoning in theorem proving and proposes a verifier-in-the-loop design. While related to reinforcement learning and potentially data for training AI models, it does not focus on LLM training data specifically."
      },
      "tasks": [
        "Automated Theorem Proving",
        "Reinforcement Learning (RL)",
        "Synthetic Data Generation"
      ]
    },
    {
      "id": "2503.16553",
      "abstract": "Large Language Models (LLMs) are widely applied to domain-specific tasks due to their massive general knowledge and remarkable inference capacities. Current studies on LLMs have shown immense potential in applying LLMs to model individual mobility prediction problems. However, most LLM-based mobility prediction models only train on specific datasets or use single well-designed prompts, leading to difficulty in adapting to different cities and users with diverse contexts. To fill these gaps, this paper proposes a unified fine-tuning framework to train a foundational open source LLM-based mobility prediction model. We conducted extensive experiments on six real-world mobility datasets to validate the proposed model. The results showed that the proposed model achieved the best performance in prediction accuracy and transferability over state-of-the-art models based on deep learning and LLMs.",
      "authors": [
        "Zhenlin Qin",
        "Leizhen Wang",
        "Francisco Camara Pereira",
        "Zhenliang Ma"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.16553",
        "HTML": "https://arxiv.org/html/2503.16553",
        "PDF": "https://arxiv.org/pdf/2503.16553"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 19 Mar 2025 15:08:37 GMT",
          "size": "332kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 16:54:22 GMT",
          "size": "523kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "A Foundational individual Mobility Prediction Model based on Open-Source Large Language Models",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses training an LLM-based model for mobility prediction using datasets, touching on data adaptation and transferability \u2014 indirectly related to LLM training data concerns."
      }
    },
    {
      "id": "2503.18813",
      "abstract": "Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an untrusted environment. However, LLM agents are vulnerable to prompt injection attacks when handling untrusted data. In this paper we propose CaMeL, a robust defense that creates a protective system layer around the LLM, securing it even when underlying models are susceptible to attacks. To operate, CaMeL explicitly extracts the control and data flows from the (trusted) query; therefore, the untrusted data retrieved by the LLM can never impact the program flow. To further improve security, CaMeL uses a notion of a capability to prevent the exfiltration of private data over unauthorized data flows by enforcing security policies when tools are called. We demonstrate effectiveness of CaMeL by solving $77\\%$ of tasks with provable security (compared to $84\\%$ with an undefended system) in AgentDojo. We release CaMeL at https://github.com/google-research/camel-prompt-injection.",
      "authors": [
        "Edoardo Debenedetti",
        "Ilia Shumailov",
        "Tianqi Fan",
        "Jamie Hayes",
        "Nicholas Carlini",
        "Daniel Fabian",
        "Christoph Kern",
        "Chongyang Shi",
        "Andreas Terzis",
        "Florian Tram\\`er"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.18813",
        "HTML": "https://arxiv.org/html/2503.18813",
        "PDF": "https://arxiv.org/pdf/2503.18813"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 24 Mar 2025 15:54:10 GMT",
          "size": "2408kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 08:05:33 GMT",
          "size": "2522kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Defeating Prompt Injections by Design",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper addresses prompt injection attacks and data security in LLM interactions, which indirectly relates to the data aspect, but does not focus on the training data of LLMs."
      },
      "tasks": []
    },
    {
      "id": "2504.08697",
      "abstract": "Span annotation is the task of localizing and classifying text spans according to custom guidelines. Annotated spans can be used to analyze and evaluate high-quality texts for which single-score metrics fail to provide actionable feedback. Until recently, span annotation was limited to human annotators or fine-tuned models. In this study, we show that large language models (LLMs) can serve as flexible and cost-effective span annotation backbones. To demonstrate their utility, we compare LLMs to skilled human annotators on three diverse span annotation tasks: evaluating data-to-text generation, identifying translation errors, and detecting propaganda techniques. We demonstrate that LLMs achieve inter-annotator agreement (IAA) comparable to human annotators at a fraction of a cost per output annotation. We also manually analyze model outputs, finding that LLMs make errors at a similar rate to human annotators. We release the dataset of more than 40k model and human annotations for further research.",
      "authors": [
        "Zden\\v{e}k Kasner",
        "Vil\\'em Zouhar",
        "Patr\\'icia Schmidtov\\'a",
        "Ivan Kart\\'a\\v{c}",
        "Krist\\'yna Onderkov\\'a",
        "Ond\\v{r}ej Pl\\'atek",
        "Dimitra Gkatzia",
        "Saad Mahamood",
        "Ond\\v{r}ej Du\\v{s}ek",
        "Simone Balloccu"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2504.08697",
        "HTML": "https://arxiv.org/html/2504.08697",
        "PDF": "https://arxiv.org/pdf/2504.08697"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Fri, 11 Apr 2025 17:04:51 GMT",
          "size": "741kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 13:11:18 GMT",
          "size": "730kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Large Language Models as Span Annotators",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The study involves using LLMs for span annotation tasks, which indirectly touches on topics like annotations and dataset usage but is not focused on LLM training data specifically."
      },
      "tasks": [
        "Data-to-Text Generation",
        "Machine Translation",
        "Propaganda detection",
        "Text Generation",
        "valid"
      ]
    },
    {
      "id": "2504.10044",
      "abstract": "Anime video generation faces significant challenges due to the scarcity of anime data and unusual motion patterns, leading to issues such as motion distortion and flickering artifacts, which result in misalignment with human preferences. Existing reward models, designed primarily for real-world videos, fail to capture the unique appearance and consistency requirements of anime. In this work, we propose a pipeline to enhance anime video generation by leveraging human feedback for better alignment. Specifically, we construct the first multi-dimensional reward dataset for anime videos, comprising 30k human-annotated samples that incorporating human preferences for both visual appearance and visual consistency. Based on this, we develop AnimeReward, a powerful reward model that employs specialized vision-language models for different evaluation dimensions to guide preference alignment. Furthermore, we introduce Gap-Aware Preference Optimization (GAPO), a novel training method that explicitly incorporates preference gaps into the optimization process, enhancing alignment performance and efficiency. Extensive experiment results show that AnimeReward outperforms existing reward models, and the inclusion of GAPO leads to superior alignment in both quantitative benchmarks and human evaluations, demonstrating the effectiveness of our pipeline in enhancing anime video quality. Our code and dataset are publicly available at https://github.com/bilibili/Index-anisora.",
      "authors": [
        "Bingwen Zhu",
        "Yudong Jiang",
        "Baohan Xu",
        "Siqian Yang",
        "Mingyu Yin",
        "Yidi Wu",
        "Huyang Sun",
        "Zuxuan Wu"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2504.10044",
        "HTML": "https://arxiv.org/html/2504.10044",
        "PDF": "https://arxiv.org/pdf/2504.10044"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 14 Apr 2025 09:49:34 GMT",
          "size": "5310kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 16:54:57 GMT",
          "size": "5309kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Aligning Anime Video Generation with Human Feedback",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses constructing a reward dataset for anime video generation, involving human-annotated samples. Although it's not directly about LLM training data, it touches on dataset creation and human feedback, which are indirectly related to LLM data curation methods."
      },
      "models": [
        {
          "model_path": "IndexTeam/Index-anisora",
          "downloads": "50",
          "likes": "160",
          "trending_score": "0.0",
          "link": "https://huggingface.co/IndexTeam/Index-anisora"
        }
      ],
      "tasks": [
        "Video Generation"
      ],
      "repo_urls": [
        "https://github.com/bilibili/index-anisora"
      ]
    },
    {
      "id": "2504.16828",
      "abstract": "Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at https://github.com/mukhal/thinkprm.",
      "authors": [
        "Muhammad Khalifa",
        "Rishabh Agarwal",
        "Lajanugen Logeswaran",
        "Jaekyeom Kim",
        "Hao Peng",
        "Moontae Lee",
        "Honglak Lee",
        "Lu Wang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2504.16828",
        "HTML": "https://arxiv.org/html/2504.16828",
        "PDF": "https://arxiv.org/pdf/2504.16828"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 23 Apr 2025 15:44:54 GMT",
          "size": "250kb",
          "version": "v1"
        },
        {
          "date": "Sun, 18 May 2025 01:23:04 GMT",
          "size": "240kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 03:05:02 GMT",
          "size": "235kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Process Reward Models That Think",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses building data-efficient process reward models using fewer process labels, which indirectly relates to LLMs as it involves data efficiency and model training, but it doesn't focus on LLM training data specifically."
      },
      "models": [
        {
          "model_path": "launch/ThinkPRM-1.5B",
          "downloads": "1670",
          "likes": "3",
          "trending_score": "1.0",
          "link": "https://huggingface.co/launch/ThinkPRM-1.5B"
        },
        {
          "model_path": "launch/ThinkPRM-14B",
          "downloads": "80",
          "likes": "3",
          "trending_score": "0.0",
          "link": "https://huggingface.co/launch/ThinkPRM-14B"
        },
        {
          "model_path": "launch/ThinkPRM-1.5B",
          "downloads": "1670",
          "likes": "3",
          "trending_score": "0.0",
          "link": "https://huggingface.co/launch/ThinkPRM-1.5B"
        },
        {
          "model_path": "launch/ThinkPRM-7B",
          "downloads": "53",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/launch/ThinkPRM-7B"
        }
      ],
      "datasets": [
        {
          "dataset_name": "launch/thinkprm-1K-verification-cots",
          "downloads": "86",
          "likes": "5",
          "link": "https://huggingface.co/datasets/launch/thinkprm-1K-verification-cots"
        }
      ],
      "tasks": [
        "Math"
      ],
      "repo_urls": [
        "https://github.com/mukhal/thinkprm"
      ]
    },
    {
      "id": "2505.04535",
      "abstract": "Federated Learning (FL) enables the utilization of vast, previously inaccessible data sources. At the same time, pre-trained Language Models (LMs) have taken the world by storm and for good reason. They exhibit remarkable emergent abilities and are readily adapted to downstream tasks. This opens one of the most exciting frontiers in FL: fine-tuning LMs. Yet, a persistent challenge in FL is the frequent, rigid communication of parameters -- a problem magnified by the sheer size of these contemporary models. The FedOpt family of algorithms has become the go-to approach for FL, relying on fixed but arbitrary intervals for model exchanges. Recently, the FDA algorithm prescribed a dynamic approach by monitoring the training progress. However, it introduced a hard-to-calibrate parameter and imposed a rigid synchronization scheme. In this work, we address these limitations by proposing the FDA-Opt family of algorithms -- a unified generalization of both FDA and FedOpt. Our experimental evaluation focuses on fine-tuning LMs on downstream NLP tasks and demonstrates that FDA-Opt outperforms FedOpt even when it is configured with hyper-parameters specifically optimized for the latter. In other words, we show that FDA-Opt is a practical, drop-in replacement for FedOpt in modern FL libraries and systems: it requires no additional configuration and delivers superior performance out of the box.",
      "authors": [
        "Michail Theologitis",
        "Vasilis Samoladas",
        "Antonios Deligiannakis"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.04535",
        "HTML": "https://arxiv.org/html/2505.04535",
        "PDF": "https://arxiv.org/pdf/2505.04535"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Distributed, Parallel, and Cluster Computing (cs.DC)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 07 May 2025 16:13:21 GMT",
          "size": "27069kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 16:20:46 GMT",
          "size": "13348kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "FDA-Opt: Communication-Efficient Federated Fine-Tuning of Language Models",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses fine-tuning language models in a federated learning setting. Although it focuses on federated learning algorithms rather than LLM training data itself, it mildly relates to the practical aspects of adapting LMs which involves data considerations."
      },
      "tasks": [
        "Federated Learning"
      ],
      "repo_urls": [
        "https://github.com/miketheologitis/FDA-Opt"
      ]
    },
    {
      "id": "2505.04799",
      "abstract": "Multi-agent collaboration systems (MACS), powered by large language models (LLMs), solve complex problems efficiently by leveraging each agent's specialization and communication between agents. However, the inherent exchange of information between agents and their interaction with external environments, such as LLM, tools, and users, inevitably introduces significant risks of sensitive data leakage, including vulnerabilities to attacks such as eavesdropping and prompt injection. Existing MACS lack fine-grained data protection controls, making it challenging to manage sensitive information securely. In this paper, we take the first step to mitigate the MACS's data leakage threat through a privacy-enhanced MACS development paradigm, Maris. Maris enables rigorous message flow control within MACS by embedding reference monitors into key multi-agent conversation components. We implemented Maris as an integral part of widely-adopted open-source multi-agent development frameworks, AutoGen and LangChain. To evaluate its effectiveness, we develop a Privacy Assessment Framework that emulates MACS under different threat scenarios. Our evaluation shows that Maris effectively mitigated sensitive data leakage threats across three different task suites while maintaining a high task success rate.",
      "authors": [
        "Jian Cui",
        "Zichuan Li",
        "Luyi Xing",
        "Xiaojing Liao"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.04799",
        "HTML": "https://arxiv.org/html/2505.04799",
        "PDF": "https://arxiv.org/pdf/2505.04799"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 07 May 2025 20:54:43 GMT",
          "size": "437kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 05:20:37 GMT",
          "size": "368kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Safeguard-by-Development: A Privacy-Enhanced Development Paradigm for Multi-Agent Collaboration Systems",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper addresses privacy in multi-agent collaboration systems involving LLMs, which indirectly relates to LLM training data by discussing data protection and sensitive data leakage but does not focus on the collection or curation of training data itself."
      }
    },
    {
      "id": "2505.08638",
      "abstract": "The increasing adoption of agentic workflows across diverse domains brings a critical need to scalably and systematically evaluate the complex traces these systems generate. Current evaluation methods depend on manual, domain-specific human analysis of lengthy workflow traces - an approach that does not scale with the growing complexity and volume of agentic outputs. Error analysis in these settings is further complicated by the interplay of external tool outputs and language model reasoning, making it more challenging than traditional software debugging. In this work, we (1) articulate the need for robust and dynamic evaluation methods for agentic workflow traces, (2) introduce a formal taxonomy of error types encountered in agentic systems, and (3) present a set of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and grounded in established agentic benchmarks. To ensure ecological validity, we curate traces from both single and multi-agent systems, focusing on real-world applications such as software engineering and open-world information retrieval. Our evaluations reveal that modern long context LLMs perform poorly at trace debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our dataset and code are made publicly available to support and accelerate future research in scalable evaluation for agentic workflows.",
      "authors": [
        "Darshan Deshpande",
        "Varun Gangal",
        "Hersh Mehta",
        "Jitin Krishnan",
        "Anand Kannappan",
        "Rebecca Qian"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.08638",
        "HTML": "https://arxiv.org/html/2505.08638",
        "PDF": "https://arxiv.org/pdf/2505.08638"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 13 May 2025 14:55:31 GMT",
          "size": "2885kb",
          "version": "v1"
        },
        {
          "date": "Mon, 19 May 2025 15:15:46 GMT",
          "size": "5609kb",
          "version": "v2"
        },
        {
          "date": "Mon, 23 Jun 2025 21:06:11 GMT",
          "size": "2276kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "TRAIL: Trace Reasoning and Agentic Issue Localization",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "Discusses evaluation and error analysis in agentic systems, indirectly connecting to LLMs through workflow trace evaluation. However, it doesn't directly address training data concerns."
      },
      "datasets": [
        {
          "dataset_name": "PatronusAI/TRAIL",
          "downloads": "92",
          "likes": "12",
          "link": "https://huggingface.co/datasets/PatronusAI/TRAIL"
        }
      ],
      "tasks": [
        "Information Retrieval"
      ]
    },
    {
      "id": "2505.11462",
      "abstract": "Medical reasoning in large language models (LLMs) aims to emulate clinicians' diagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and PubMedQA often mix reasoning with factual recall. We address this by separating 11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using a PubMedBERT classifier that reaches 81 percent accuracy, comparable to human performance. Our analysis shows that only 32.8 percent of questions require complex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1) and general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent gaps between knowledge and reasoning performance. For example, HuatuoGPT-o1 scores 56.9 on knowledge but only 44.8 on reasoning. In adversarial tests where models are misled with incorrect initial reasoning, biomedical models degrade sharply, while larger or RL-trained general models show more robustness. To address this, we train BioMed-R1 using fine-tuning and reinforcement learning on reasoning-heavy examples. It achieves the strongest performance among similarly sized models. Further gains may come from incorporating clinical case reports and training with adversarial and backtracking scenarios.",
      "authors": [
        "Rahul Thapa",
        "Qingyang Wu",
        "Kevin Wu",
        "Harrison Zhang",
        "Angela Zhang",
        "Eric Wu",
        "Haotian Ye",
        "Suhana Bedi",
        "Nevin Aresh",
        "Joseph Boen",
        "Shriya Reddy",
        "Ben Athiwaratkun",
        "Shuaiwen Leon Song",
        "James Zou"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.11462",
        "HTML": "https://arxiv.org/html/2505.11462",
        "PDF": "https://arxiv.org/pdf/2505.11462"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Fri, 16 May 2025 17:16:27 GMT",
          "size": "352kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 03:27:30 GMT",
          "size": "258kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Disentangling Reasoning and Knowledge in Medical Large Language Models",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses training and evaluating biomedical models, which involves datasets containing reasoning-heavy examples. Although LLM training data is indirectly related due to the focus on fine-tuning and using existing benchmarks, the main focus is not on LLM training data itself."
      },
      "models": [
        {
          "model_path": "zou-lab/BioMed-R1-32B",
          "downloads": "0",
          "likes": "1",
          "trending_score": "1.0",
          "link": "https://huggingface.co/zou-lab/BioMed-R1-32B"
        },
        {
          "model_path": "zou-lab/BioMedBERT-Knowledge-vs-Reasoning",
          "downloads": "16",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/zou-lab/BioMedBERT-Knowledge-vs-Reasoning"
        },
        {
          "model_path": "zou-lab/BioMed-R1-8B",
          "downloads": "0",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/zou-lab/BioMed-R1-8B"
        }
      ],
      "datasets": [
        {
          "dataset_name": "zou-lab/BioMed-R1-Eval",
          "downloads": "120",
          "likes": "0",
          "link": "https://huggingface.co/datasets/zou-lab/BioMed-R1-Eval"
        },
        {
          "dataset_name": "zou-lab/BioMed-R1-Train",
          "downloads": "109",
          "likes": "3",
          "link": "https://huggingface.co/datasets/zou-lab/BioMed-R1-Train"
        }
      ],
      "tasks": [
        "Diagnostic",
        "MedQA"
      ]
    },
    {
      "id": "2505.15223",
      "abstract": "International aid is a critical mechanism for promoting economic growth and well-being in developing nations, supporting progress toward the Sustainable Development Goals (SDGs). However, tracking aid contributions remains challenging due to labor-intensive data management, incomplete records, and the heterogeneous nature of aid data. Recognizing the urgency of this challenge, we partnered with government agencies to develop an AI model that complements manual classification and mitigates human bias in subjective interpretation. By integrating SDG-specific semantics and leveraging prior knowledge from language models, our approach enhances classification accuracy and accommodates the diversity of aid projects. When applied to a comprehensive dataset spanning multiple years, our model can reveal hidden trends in the temporal evolution of international development cooperation. Expert interviews further suggest how these insights can empower policymakers with data-driven decision-making tools, ultimately improving aid effectiveness and supporting progress toward SDGs.",
      "authors": [
        "Sungwon Park",
        "Dongjoon Lee",
        "Kyeongjin Ahn",
        "Yubin Choi",
        "Junho Lee",
        "Meeyoung Cha",
        "Kyung Ryul Park"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.15223",
        "HTML": "https://arxiv.org/html/2505.15223",
        "PDF": "https://arxiv.org/pdf/2505.15223"
      },
      "subjects": [
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 21 May 2025 07:50:40 GMT",
          "size": "915kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 12:59:52 GMT",
          "size": "915kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Classifying and Tracking International Aid Contribution Towards SDGs",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses data management and classification for international aid, which is indirectly related to data curation and classification relevant to LLM training but is not the main focus."
      }
    },
    {
      "id": "2505.23254",
      "abstract": "Owing to the huge success of generative artificial intelligence (AI), large language models (LLMs) have emerged as a core subclass, underpinning applications such as question answering, text generation, and code completion. While fine-tuning these models on domain-specific data can yield significant performance gains, it also poses daunting computational challenges, especially for researchers and small organizations with limited hardware resources. Although SSD offloading (i.e., ZeRO-Infinity) has emerged as a viable strategy to overcome the GPU memory barrier via leveraging both system memory (i.e., CPU DRAM) and storage space (i.e., solid-state devices, SSDs), its design primarily targets model-centric performance issues. As a result, key system-level issues, including system memory fragmentation, inefficient pinned buffer allocation, peak CPU usage spikes, and file system overhead, remain unaddressed, stifling scalability and inflating costs. Such an observation motivates this paper to introduce MemAscend, a framework that systematically tackles the underexplored system memory bottlenecks in SSD-offloaded LLM training, with a focus on resource-constrained environments. By streamlining pinned-memory allocation, eradicating fragmentation, and mitigating peak overhead, MemAscend reclaims a substantial system memory budget, enabling larger models, longer context windows, and higher batch sizes without exceeding modest hardware limits. Across diverse LLM benchmarks, MemAscend reduces peak system-memory consumption by an average of 55.7% compared with standard SSD offloading techniques, lowering the hardware barrier for fine-tuning and unlocking new possibilities for cost-effective large-scale training on limited-resource machines.",
      "authors": [
        "Yong-Cheng Liaw and Shuo-Han Chen"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.23254",
        "HTML": "https://arxiv.org/html/2505.23254",
        "PDF": "https://arxiv.org/pdf/2505.23254"
      },
      "subjects": [
        "Distributed, Parallel, and Cluster Computing (cs.DC)"
      ],
      "submission_historys": [
        {
          "date": "Thu, 29 May 2025 09:00:35 GMT",
          "size": "2814kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 12:14:06 GMT",
          "size": "2996kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses LLM fine-tuning and memory optimization but does not focus on the data used in LLM training. It is indirectly related to LLM training data as it involves model performance improvements using domain-specific data."
      }
    },
    {
      "id": "2505.23971",
      "abstract": "The right batch size is important when training language models at scale: a large batch size is necessary for fast training, but a batch size that is too large will harm token efficiency. To navigate this tradeoff, McCandlish et al. (2018) suggest that a critical batch size (CBS), below which training will not substantially degrade loss, can be estimated based on the gradient noise scale during training. While their method has been adopted in practice, e.g., when training GPT-3, strong assumptions are required to justify gradient noise as a proxy for the CBS, which makes it unclear whether their approach should be trusted in practice, limiting its applicability. In this paper, we introduce a simple, empirical approach to directly measure the CBS and show how the CBS evolves over training. Applying our approach to the OLMo models, we find that CBS is near 0 at initialization, increases rapidly at first, and then plateaus as training progresses. Furthermore, we find that this trend holds across different model sizes (1B and 7B), suggesting CBS from small training runs can inform larger-scale training runs. Our findings about how the CBS changes over training motivate batch size warmup as a natural way to reliably train language models at large batch size: start the batch size small and increase it as the CBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to slightly better loss than the original training run with 43% fewer gradient steps. This shows how our framework can be applied to reliably train language models at larger batch sizes, increasing data parallelism without compromising performance.",
      "authors": [
        "William Merrill and Shane Arora and Dirk Groeneveld and Hannaneh Hajishirzi"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.23971",
        "HTML": "https://arxiv.org/html/2505.23971",
        "PDF": "https://arxiv.org/pdf/2505.23971"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Thu, 29 May 2025 19:53:39 GMT",
          "size": "180kb",
          "version": "v1"
        },
        {
          "date": "Mon, 23 Jun 2025 18:58:20 GMT",
          "size": "181kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the paper explores batch size and training efficiency for language models, it indirectly touches on training procedures but is not focused on the data itself used in training."
      },
      "tasks": [
        "Language Modeling",
        "Language Modelling",
        "Navigate"
      ]
    },
    {
      "id": "2506.01056",
      "abstract": "True intelligence requires active capability acquisition, yet current LLM agents inject pre-defined tool schemas into prompts, reducing models to passive selectors and falling short of robust general-purpose agency. We introduce MCP-Zero, an active agent framework that restores tool discovery autonomy to LLMs themselves. Instead of overwhelming models with all available tools, MCP-Zero enables agents to actively identify capability gaps, and request specific tools on-demand, transforming them from large-scale retrievers into genuine autonomous agents. The framework operates through three core mechanisms: (1) Active Tool Request, where models autonomously generate structured requests specifying their exact tool requirements; (2) Hierarchical Semantic Routing, a two-stage algorithm that matches requests to relevant servers and tools through improved semantic alignment; (3) Iterative Capability Extension, enabling agents to progressively build cross-domain toolchains while maintaining minimal context footprint. We construct MCP-tools, a comprehensive dataset of 308 MCP servers and 2,797 tools from the official Model-Context-Protocol repository. Experiments demonstrate that MCP-Zero preserves agent autonomy while achieving substantial efficiency gains: (i) accurate tool selection from nearly 3k candidates across 248.1k tokens; (ii) 98\\% reduction in token consumption on APIBank while maintaining high accuracy; and (iii) consistent multi-turn performance that scales with tool ecosystem growth. This work establishes active tool discovery as a fundamental design pattern for scalable autonomous agent systems.",
      "authors": [
        "Xiang Fei",
        "Xiawu Zheng",
        "Hao Feng"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.01056",
        "HTML": "https://arxiv.org/html/2506.01056",
        "PDF": "https://arxiv.org/pdf/2506.01056"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Software Engineering (cs.SE)"
      ],
      "submission_historys": [
        {
          "date": "Sun, 01 Jun 2025 15:48:53 GMT",
          "size": "619kb",
          "version": "v1"
        },
        {
          "date": "Wed, 04 Jun 2025 06:37:09 GMT",
          "size": "619kb",
          "version": "v2"
        },
        {
          "date": "Mon, 23 Jun 2025 08:33:05 GMT",
          "size": "623kb",
          "version": "v3"
        },
        {
          "date": "Tue, 24 Jun 2025 06:27:29 GMT",
          "size": "623kb",
          "version": "v4"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "MCP-Zero: Active Tool Discovery for Autonomous LLM Agents",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper proposes an active agent framework for LLMs, touching on concepts like data efficiency and tool discovery. While it indirectly relates to LLM outputs, it does not focus on LLM training data or datasets."
      },
      "tasks": [
        "Retrieval",
        "Semantic Similarity",
        "Semantic Textual Similarity",
        "Task 2"
      ]
    },
    {
      "id": "2506.06609",
      "abstract": "In this work, we demonstrate that affine mappings between residual streams of language models is a cheap way to effectively transfer represented features between models. We apply this technique to transfer the weights of Sparse Autoencoders (SAEs) between models of different sizes to compare their representations. We find that small and large models learn similar representation spaces, which motivates training expensive components like SAEs on a smaller model and transferring to a larger model at a FLOPs savings. In particular, using a small-to-large transferred SAE as initialization can lead to 50% cheaper training runs when training SAEs on larger models. Next, we show that transferred probes and steering vectors can effectively recover ground truth performance. Finally, we dive deeper into feature-level transferability, finding that semantic and structural features transfer noticeably differently while specific classes of functional features have their roles faithfully mapped. Overall, our findings illustrate similarities and differences in the linear representation spaces of small and large models and demonstrate a method for improving the training efficiency of SAEs.",
      "authors": [
        "Alan Chen",
        "Jack Merullo",
        "Alessandro Stolfo",
        "Ellie Pavlick"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.06609",
        "HTML": "https://arxiv.org/html/2506.06609",
        "PDF": "https://arxiv.org/pdf/2506.06609"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Sat, 07 Jun 2025 01:03:25 GMT",
          "size": "2900kb",
          "version": "v1"
        },
        {
          "date": "Mon, 23 Jun 2025 23:21:57 GMT",
          "size": "2896kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Transferring Features Across Language Models With Model Stitching",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses transferring features across language models to improve training efficiency, indirectly relating to data involved in training LLMs, but does not focus on the nature or curation of training data itself."
      }
    },
    {
      "id": "2506.09160",
      "abstract": "As AI chatbots become increasingly integrated in education, students are turning to these systems for guidance, feedback, and information. However, the anthropomorphic characteristics of these chatbots create ambiguity regarding whether students develop trust toward them as they would a human peer or instructor, based in interpersonal trust, or as they would any other piece of technology, based in technology trust. This ambiguity presents theoretical challenges, as interpersonal trust models may inappropriately ascribe human intentionality and morality to AI, while technology trust models were developed for non-social technologies, leaving their applicability to anthropomorphic systems unclear. To address this gap, we investigate how human-like and system-like trusting beliefs comparatively influence students' perceived enjoyment, trusting intention, behavioral intention to use, and perceived usefulness of an AI chatbot - factors associated with students' engagement and learning outcomes. Through partial least squares structural equation modeling, we found that human-like and system-like trust significantly influenced student perceptions, with varied effects. Human-like trust more strongly predicted trusting intention, while system-like trust better predicted behavioral intention and perceived usefulness. Both had similar effects on perceived enjoyment. Given the partial explanatory power of each type of trust, we propose that students develop a distinct form of trust with AI chatbots (human-AI trust) that differs from human-human and human-technology models of trust. Our findings highlight the need for new theoretical frameworks specific to human-AI trust and offer practical insights for fostering appropriately calibrated trust, which is critical for the effective adoption and pedagogical impact of AI in education.",
      "authors": [
        "Griffin Pitts",
        "Sanaz Motamedi"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.09160",
        "HTML": "https://arxiv.org/html/2506.09160",
        "PDF": "https://arxiv.org/pdf/2506.09160"
      },
      "subjects": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)",
        "Emerging Technologies (cs.ET)",
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 10 Jun 2025 18:15:40 GMT",
          "size": "1011kb",
          "version": "v1"
        },
        {
          "date": "Thu, 12 Jun 2025 07:06:57 GMT",
          "size": "1013kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 05:15:49 GMT",
          "size": "1032kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Understanding Human-AI Trust in Education",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses trust in AI chatbots, which could be tangentially related to the use of such systems in collecting data or their interaction environments potentially affecting training datasets, but the main focus is not on LLM training data."
      },
      "tasks": [
        "Chatbot"
      ]
    },
    {
      "id": "2506.10412",
      "abstract": "Time series data in real-world applications such as healthcare, climate modeling, and finance are often irregular, multimodal, and messy, with varying sampling rates, asynchronous modalities, and pervasive missingness. However, existing benchmarks typically assume clean, regularly sampled, unimodal data, creating a significant gap between research and real-world deployment. We introduce Time-IMM, a dataset specifically designed to capture cause-driven irregularity in multimodal multivariate time series. Time-IMM represents nine distinct types of time series irregularity, categorized into trigger-based, constraint-based, and artifact-based mechanisms. Complementing the dataset, we introduce IMM-TSF, a benchmark library for forecasting on irregular multimodal time series, enabling asynchronous integration and realistic evaluation. IMM-TSF includes specialized fusion modules, including a timestamp-to-text fusion module and a multimodality fusion module, which support both recency-aware averaging and attention-based integration strategies. Empirical results demonstrate that explicitly modeling multimodality on irregular time series data leads to substantial gains in forecasting performance. Time-IMM and IMM-TSF provide a foundation for advancing time series analysis under real-world conditions. The dataset is publicly available at https://www.kaggle.com/datasets/blacksnail789521/time-imm/data, and the benchmark library can be accessed at https://anonymous.4open.science/r/IMMTSF_NeurIPS2025.",
      "authors": [
        "Ching Chang",
        "Jeehyun Hwang",
        "Yidan Shi",
        "Haixin Wang",
        "Wen-Chih Peng",
        "Tien-Fu Chen",
        "Wei Wang"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.10412",
        "HTML": "https://arxiv.org/html/2506.10412",
        "PDF": "https://arxiv.org/pdf/2506.10412"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Thu, 12 Jun 2025 07:07:22 GMT",
          "size": "1499kb",
          "version": "v1"
        },
        {
          "date": "Mon, 23 Jun 2025 21:10:15 GMT",
          "size": "1499kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper introduces a dataset (Time-IMM) for time series analysis, which is indirectly related to LLM concepts through data collection and dataset creation, but does not focus on LLM-specific training data."
      },
      "tasks": [
        "Irregular Time Series",
        "Time Series",
        "Time Series Analysis"
      ]
    },
    {
      "id": "2506.11555",
      "abstract": "The integration of external knowledge through Retrieval-Augmented Generation (RAG) has become foundational in enhancing large language models (LLMs) for knowledge-intensive tasks. However, existing RAG paradigms often overlook the cognitive step of applying knowledge, leaving a gap between retrieved facts and task-specific reasoning. In this work, we introduce RAG+, a principled and modular extension that explicitly incorporates application-aware reasoning into the RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and aligned application examples, created either manually or automatically, and retrieves both jointly during inference. This design enables LLMs not only to access relevant information but also to apply it within structured, goal-oriented reasoning processes. Experiments across mathematical, legal, and medical domains, conducted on multiple models, demonstrate that RAG+ consistently outperforms standard RAG variants, achieving average improvements of 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval with actionable application, RAG+ advances a more cognitively grounded framework for knowledge integration, representing a step toward more interpretable and capable LLMs.",
      "authors": [
        "Yu Wang",
        "Shiwan Zhao",
        "Zhihu Wang",
        "Yubo Zhang",
        "Xicheng Zhang",
        "Zhengfan Wang",
        "Heyuan Huang",
        "Ming Fan",
        "Ting Liu"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.11555",
        "HTML": "https://arxiv.org/html/2506.11555",
        "PDF": "https://arxiv.org/pdf/2506.11555"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Fri, 13 Jun 2025 08:06:49 GMT",
          "size": "7675kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 05:50:06 GMT",
          "size": "7675kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses Retrieval-Augmented Generation (RAG+), which involves using corpora to enhance LLMs. It involves dual corpus construction, which touches on data aspects, but LLM training data is not the primary focus as the emphasis is on reasoning enhancement."
      }
    },
    {
      "id": "2506.11558",
      "abstract": "Large Language Models (LLMs) have recently been extended to the video domain, enabling sophisticated video-language understanding. However, existing Video LLMs often exhibit limitations in fine-grained temporal reasoning, restricting their ability to precisely attribute responses to specific video moments, especially under constrained supervision. We introduce DaMO, a data-efficient Video LLM explicitly designed for accurate temporal reasoning and multimodal understanding. At its core, the proposed Temporal-aware Fuseformer employs a hierarchical dual-stream architecture that progressively captures temporal dynamics within each modality and effectively fuses complementary visual and audio information. To further enhance computational efficiency, DaMO integrates a global residual that reduces spatial redundancy while preserving essential semantic details. We train DaMO via a structured four-stage progressive training paradigm, incrementally equipping the model with multimodal alignment, semantic grounding, and temporal reasoning capabilities. This work also contributes multiple datasets augmented from existing ones with GPT-generated temporally grounded QA pairs for tasks requiring temporal supervision. Comprehensive experiments on temporal grounding and video QA benchmarks demonstrate that DaMO consistently surpasses prior methods, particularly in tasks demanding precise temporal alignment and reasoning. Our work establishes a promising direction for data-efficient video-language modeling.",
      "authors": [
        "Bo-Cheng Chiu",
        "Jen-Jee Chen",
        "Yu-Chee Tseng and Feng-Chi Chen"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.11558",
        "HTML": "https://arxiv.org/html/2506.11558",
        "PDF": "https://arxiv.org/pdf/2506.11558"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Fri, 13 Jun 2025 08:13:05 GMT",
          "size": "3014kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 11:59:30 GMT",
          "size": "0kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper introduces DaMO, focusing on multimodal understanding and temporal reasoning. Although it mentions datasets, the main focus is on model training for video LLMs rather than on specifics of LLM training data."
      }
    },
    {
      "id": "2506.13040",
      "abstract": "We present MAMMA, a markerless motion-capture pipeline that accurately recovers SMPL-X parameters from multi-view video of two-person interaction sequences. Traditional motion-capture systems rely on physical markers. Although they offer high accuracy, their requirements of specialized hardware, manual marker placement, and extensive post-processing make them costly and time-consuming. Recent learning-based methods attempt to overcome these limitations, but most are designed for single-person capture, rely on sparse keypoints, or struggle with occlusions and physical interactions. In this work, we introduce a method that predicts dense 2D surface landmarks conditioned on segmentation masks, enabling person-specific correspondence estimation even under heavy occlusion. We employ a novel architecture that exploits learnable queries for each landmark. We demonstrate that our approach can handle complex person--person interaction and offers greater accuracy than existing methods. To train our network, we construct a large, synthetic multi-view dataset combining human motions from diverse sources, including extreme poses, hand motions, and close interactions. Our dataset yields high-variability synthetic sequences with rich body contact and occlusion, and includes SMPL-X ground-truth annotations with dense 2D landmarks. The result is a system capable of capturing human motion without the need for markers. Our approach offers competitive reconstruction quality compared to commercial marker-based motion-capture solutions, without the extensive manual cleanup. Finally, we address the absence of common benchmarks for dense-landmark prediction and markerless motion capture by introducing two evaluation settings built from real multi-view sequences. We will release our dataset, benchmark, method, training code, and pre-trained model weights for research purposes.",
      "authors": [
        "Hanz Cuevas-Velasquez",
        "Anastasios Yiannakidis",
        "Soyong Shin",
        "Giorgio Becherini",
        "Markus H\\\"oschle",
        "Joachim Tesch",
        "Taylor Obersat",
        "Tsvetelina Alexiadis",
        "Michael J. Black"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.13040",
        "HTML": "https://arxiv.org/html/2506.13040",
        "PDF": "https://arxiv.org/pdf/2506.13040"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 16 Jun 2025 02:04:51 GMT",
          "size": "33864kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 15:25:06 GMT",
          "size": "33864kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "MAMMA: Markerless & Automatic Multi-Person Motion Action Capture",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses a synthetic multi-view dataset used to train a motion capture network, touching on concepts related to dataset construction, which is indirectly related to LLM training data."
      },
      "tasks": [
        "Markerless Motion Capture"
      ]
    },
    {
      "id": "2506.14667",
      "abstract": "In order to address the scalability challenge within Neural Architecture Search (NAS), we speed up NAS training via dynamic hard example mining within a curriculum learning framework. By utilizing an autoencoder that enforces an image similarity embedding in latent space, we construct an efficient kd-tree structure to order images by furthest neighbour dissimilarity in a low-dimensional embedding. From a given query image from our subsample dataset, we can identify the most dissimilar image within the global dataset in logarithmic time. Via curriculum learning, we then dynamically re-formulate an unbiased subsample dataset for NAS optimisation, upon which the current NAS solution architecture performs poorly. We show that our DDS-NAS framework speeds up gradient-based NAS strategies by up to 27x without loss in performance. By maximising the contribution of each image sample during training, we reduce the duration of a NAS training cycle and the number of iterations required for convergence.",
      "authors": [
        "Matt Poyser and Toby P. Breckon"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.14667",
        "HTML": "https://arxiv.org/html/2506.14667",
        "PDF": "https://arxiv.org/pdf/2506.14667"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 17 Jun 2025 15:58:10 GMT",
          "size": "870kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 01:31:33 GMT",
          "size": "870kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "DDS-NAS: Dynamic Data Selection within Neural Architecture Search via On-line Hard Example Mining applied to Image Classification",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper mentions dataset curation strategies (dynamic data selection) within the context of Neural Architecture Search, which indirectly relates to LLM training data concerns regarding efficient data use and selection."
      },
      "tasks": [
        "image-classification",
        "Image Classification",
        "Neural Architecture Search"
      ]
    },
    {
      "id": "2506.15201",
      "abstract": "The improved semantic understanding of vision-language pretrained (VLP) models has made it increasingly difficult to protect publicly posted images from being exploited by search engines and other similar tools. In this context, this paper seeks to protect users' privacy by implementing defenses at the image compression stage to prevent exploitation. Specifically, we propose a flexible coding method, termed Privacy-Shielded Image Compression (PSIC), that can produce bitstreams with multiple decoding options. By default, the bitstream is decoded to preserve satisfactory perceptual quality while preventing interpretation by VLP models. Our method also retains the original image compression functionality. With a customizable input condition, the proposed scheme can reconstruct the image that preserves its full semantic information. A Conditional Latent Trigger Generation (CLTG) module is proposed to produce bias information based on customizable conditions to guide the decoding process into different reconstructed versions, and an Uncertainty-Aware Encryption-Oriented (UAEO) optimization function is designed to leverage the soft labels inferred from the target VLP model's uncertainty on the training data. This paper further incorporates an adaptive multi-objective optimization strategy to obtain improved encrypting performance and perceptual quality simultaneously within a unified training process. The proposed scheme is plug-and-play and can be seamlessly integrated into most existing Learned Image Compression (LIC) models. Extensive experiments across multiple downstream tasks have demonstrated the effectiveness of our design.",
      "authors": [
        "Xuelin Shen",
        "Jiayin Xu",
        "Kangsheng Yin",
        "Wenhan Yang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.15201",
        "HTML": "https://arxiv.org/html/2506.15201",
        "PDF": "https://arxiv.org/pdf/2506.15201"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 18 Jun 2025 07:29:40 GMT",
          "size": "4673kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 01:52:22 GMT",
          "size": "4673kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Privacy-Shielded Image Compression: Defending Against Exploitation from Vision-Language Pretrained Models",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper proposes a method to protect images against exploitation by vision-language models, touching on privacy concerns indirectly related to training data. However, it does not focus specifically on LLM training data."
      },
      "tasks": [
        "Image Compression"
      ],
      "repo_urls": [
        "https://github.com/jiayinxu5499/psic"
      ]
    },
    {
      "id": "2506.15525",
      "abstract": "As generative AI (GenAI) emerges as a transformative force, clear understanding of high school students' perspectives is essential for GenAI's meaningful integration in high school environments. In this work, we draw insights from a participatory design workshop where we engaged 17 high school students -- a group rarely involved in prior research in this area -- through the design of novel GenAI tools and school policies addressing their key concerns. Students identified challenges and developed solutions outlining their ideal features in GenAI tools, appropriate school use, and regulations. These centered around the problem spaces of combating bias & misinformation, tackling crime & plagiarism, preventing over-reliance on AI, and handling false accusations of academic dishonesty. Building on our participants' underrepresented perspectives, we propose new guidelines targeted at educational technology designers for development of GenAI technologies in high schools. We also argue for further incorporation of student voices in development of AI policies in their schools.",
      "authors": [
        "Isabella Pu",
        "Prerna Ravi",
        "Linh Dieu Dinh",
        "Chelsea Joe",
        "Caitlin Ogoe",
        "Zixuan Li",
        "Cynthia Breazeal",
        "Anastasia K. Ostrowski"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.15525",
        "HTML": "https://arxiv.org/html/2506.15525",
        "PDF": "https://arxiv.org/pdf/2506.15525"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 18 Jun 2025 14:58:50 GMT",
          "size": "19724kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 12:16:23 GMT",
          "size": "19724kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "\"How can we learn and use AI at the same time?\": Participatory Design of GenAI with High School Students",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "This paper involves participatory design around the use of generative AI in education, addressing issues like bias and misinformation that can be tangentially related to the ethics of training data, but it does not focus on LLM training data directly."
      }
    },
    {
      "id": "2506.15961",
      "abstract": "Training large language models (LLMs) at scale requires parallel execution across thousands of devices, incurring enormous computational costs. Yet, these costly distributed trainings are rarely verified, leaving them prone to silent errors and potentially wasting millions of GPU hours. We introduce TrainVerify, a system for verifiable distributed training of LLMs. Given a deep learning model's logical specification as the ground truth, TrainVerify formally verifies that a distributed parallel execution plan is mathematically equivalent to it. Direct verification is notoriously difficult due to the sheer scale of LLMs which often involves billions of variables and highly intricate computation graphs. Therefore, TrainVerify introduces shape-reduction techniques and a stage-wise parallel verification algorithm that significantly reduces complexity while preserving formal correctness. TrainVerify scales to frontier LLMs, including the successful verification of the Llama3 (405B) and DeepSeek-V3 (671B) training plans.",
      "authors": [
        "Yunchi Lu",
        "Youshan Miao",
        "Cheng Tan",
        "Peng Huang",
        "Yi Zhu",
        "Xian Zhang",
        "Fan Yang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.15961",
        "HTML": "https://arxiv.org/html/2506.15961",
        "PDF": "https://arxiv.org/pdf/2506.15961"
      },
      "subjects": [
        "Distributed, Parallel, and Cluster Computing (cs.DC)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Thu, 19 Jun 2025 02:10:06 GMT",
          "size": "1181kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 10:50:28 GMT",
          "size": "1181kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "TrainVerify: Equivalence-Based Verification for Distributed LLM Training",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper addresses distributed training verification for LLMs, which is tangentially related to LLM training data but is primarily focused on computational verification rather than data."
      },
      "tasks": []
    },
    {
      "id": "2506.17539",
      "abstract": "The growing dependence on mobile phones and their apps has made multi-user interactive features, like chat calls, live streaming, and video conferencing, indispensable for bridging the gaps in social connectivity caused by physical and situational barriers. However, automating these interactive features for testing is fraught with challenges, owing to their inherent need for timely, dynamic, and collaborative user interactions, which current automated testing methods inadequately address. Inspired by the concept of agents designed to autonomously and collaboratively tackle problems, we propose MAdroid, a novel multi-agent approach powered by the Large Language Models (LLMs) to automate the multi-user interactive task for app feature testing. Specifically, MAdroid employs two functional types of multi-agents: user agents (Operator) and supervisor agents (Coordinator and Observer). Each agent takes a specific role: the Coordinator directs the interactive task; the Operator mimics user interactions on the device; and the Observer monitors and reviews the task automation process. Our evaluation, which included 41 multi-user interactive tasks, demonstrates the effectiveness of our approach, achieving 82.9% of the tasks with 96.8% action similarity, outperforming the ablation studies and state-of-the-art baselines. Additionally, a preliminary investigation underscores MAdroid's practicality by helping identify 11 multi-user interactive bugs during regression app testing, confirming its potential value in real-world software development contexts.",
      "authors": [
        "Sidong Feng",
        "Changhao Du",
        "Huaxiao Liu",
        "Qingnan Wang",
        "Zhengwei Lv",
        "Mengfei Wang",
        "Chunyang Chen"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.17539",
        "HTML": "https://arxiv.org/html/2506.17539",
        "PDF": "https://arxiv.org/pdf/2506.17539"
      },
      "subjects": [
        "Software Engineering (cs.SE)"
      ],
      "submission_historys": [
        {
          "date": "Sat, 21 Jun 2025 01:38:53 GMT",
          "size": "30615kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 00:54:08 GMT",
          "size": "15308kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Breaking Single-Tester Limits: Multi-Agent LLMs for Multi-User Feature Testing",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "Discusses using LLMs in a novel testing approach for app features, hinting at LLM application and potential data use but not specifically focusing on LLM training data."
      }
    },
    {
      "id": "2506.17551",
      "abstract": "With the rapid adoption of large language models (LLMs) in recommendation systems, the computational and communication bottlenecks caused by their massive parameter sizes and large data volumes have become increasingly prominent. This paper systematically investigates two classes of optimization methods-model parallelism and data parallelism-for distributed training of LLMs in recommendation scenarios. For model parallelism, we implement both tensor parallelism and pipeline parallelism, and introduce an adaptive load-balancing mechanism to reduce cross-device communication overhead. For data parallelism, we compare synchronous and asynchronous modes, combining gradient compression and sparsification techniques with an efficient aggregation communication framework to significantly improve bandwidth utilization. Experiments conducted on a real-world recommendation dataset in a simulated service environment demonstrate that our proposed hybrid parallelism scheme increases training throughput by over 30% and improves resource utilization by approximately 20% compared to traditional single-mode parallelism, while maintaining strong scalability and robustness. Finally, we discuss trade-offs among different parallel strategies in online deployment and outline future directions involving heterogeneous hardware integration and automated scheduling technologies.",
      "authors": [
        "Haowei Yang",
        "Yu Tian",
        "Zhongheng Yang",
        "Zhao Wang",
        "Chengrui Zhou",
        "Dannier Li"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.17551",
        "HTML": "https://arxiv.org/html/2506.17551",
        "PDF": "https://arxiv.org/pdf/2506.17551"
      },
      "subjects": [
        "Distributed, Parallel, and Cluster Computing (cs.DC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Sat, 21 Jun 2025 02:37:25 GMT",
          "size": "1384kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 02:28:50 GMT",
          "size": "1385kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Research on Model Parallelism and Data Parallelism Optimization Methods in Large Language Model-Based Recommendation Systems",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While focusing on optimization methods for parallelism in LLM-based recommendation systems, it indirectly mentions large data volumes in the context of LLM training, but does not focus on the data itself."
      }
    },
    {
      "id": "2506.17728",
      "abstract": "In this paper, we introduce KAG-Thinker, which upgrade KAG to a multi-turn interactive thinking and deep reasoning framework powered by a dedicated parameter-light large language model (LLM). Our approach constructs a structured thinking process for solving complex problems, enhancing the the logical coherence and contextual consistency of the reasoning process in question-answering (Q&A) tasks on domain-specific knowledge bases (KBs) within LLMs. Following the \\textbf{Logical Form} guided retrieval and reasoning technology route of KAG, this framework first decomposes complex questions into independently solvable sub-problems (which are also referred to as logical forms) through \\textbf{breadth decomposition}. Each such logical form is represented in two equivalent forms-natural language and logical function-and subsequently classified as either a Knowledge Retrieval or Reasoning Analysis task. Dependencies and parameter passing between these tasks are explicitly modeled via logical function interfaces. In the solving process, the Retrieval function performs retrieval tasks. It retrieves one-hop structured and unstructured information of specified knowledge unit. While the Math and Deduce functions are used to perform reasoning analysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external knowledge sources are regarded as equivalent KBs. We use the \\textbf{knowledge boundary} module to determine the optimal source using self-regulatory mechanisms such as confidence calibration and reflective reasoning, and use the \\textbf{depth solving} module to enhance the comprehensiveness of knowledge acquisition...",
      "authors": [
        "Dalong Zhang",
        "Jun Xu",
        "Jun Zhou",
        "Lei Liang",
        "Lin Yuan",
        "Ling Zhong",
        "Mengshu Sun",
        "Peilong Zhao",
        "QiWei Wang",
        "Xiaorui Wang",
        "Xinkai Du",
        "YangYang Hou",
        "Yu Ao",
        "ZhaoYang Wang",
        "Zhengke Gui",
        "ZhiYing Yi",
        "Zhongpu Bo"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.17728",
        "HTML": "https://arxiv.org/html/2506.17728",
        "PDF": "https://arxiv.org/pdf/2506.17728"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Sat, 21 Jun 2025 14:58:53 GMT",
          "size": "922kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 12:50:57 GMT",
          "size": "938kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "KAG-Thinker: Interactive Thinking and Deep Reasoning in LLMs via Knowledge-Augmented Generation",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper introduces a framework for interactive reasoning with LLMs. It involves knowledge retrieval but does not focus on LLM training data or datasets explicitly."
      }
    },
    {
      "id": "2506.17789",
      "abstract": "Tokenization plays a pivotal role in multilingual NLP. However, existing tokenizers are often skewed towards high-resource languages, limiting their effectiveness for linguistically diverse and morphologically rich languages such as those in the Indian subcontinent. This paper presents a comprehensive intrinsic evaluation of tokenization strategies across 17 Indian languages. We quantify the trade-offs between bottom-up and top-down tokenizer algorithms (BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of multilingual vocabulary construction such as joint and cluster-based training. We also show that extremely low-resource languages can benefit from tokenizers trained on related high-resource languages. Our study provides practical insights for building more fair, efficient, and linguistically informed tokenizers for multilingual NLP.",
      "authors": [
        "N J Karthika",
        "Maharaj Brahma",
        "Rohit Saluja",
        "Ganesh Ramakrishnan",
        "Maunendra Sankar Desarkar"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.17789",
        "HTML": "https://arxiv.org/html/2506.17789",
        "PDF": "https://arxiv.org/pdf/2506.17789"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Sat, 21 Jun 2025 18:47:33 GMT",
          "size": "61kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 09:35:36 GMT",
          "size": "61kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "Tokenization techniques for multilingual NLP are evaluated, which touches on preprocessing and data aspects but not specifically on LLM training data."
      }
    },
    {
      "id": "2506.17886",
      "abstract": "Multimodal contrastive models have achieved strong performance in text-audio retrieval and zero-shot settings, but improving joint embedding spaces remains an active research area. Less attention has been given to making these systems controllable and interactive for users. In text-music retrieval, the ambiguity of freeform language creates a many-to-many mapping, often resulting in inflexible or unsatisfying results.\n  We introduce Generative Diffusion Retriever (GDR), a novel framework that leverages diffusion models to generate queries in a retrieval-optimized latent space. This enables controllability through generative tools such as negative prompting and denoising diffusion implicit models (DDIM) inversion, opening a new direction in retrieval control. GDR improves retrieval performance over contrastive teacher models and supports retrieval in audio-only latent spaces using non-jointly trained encoders. Finally, we demonstrate that GDR enables effective post-hoc manipulation of retrieval behavior, enhancing interactive control for text-music retrieval tasks.",
      "authors": [
        "Julien Guinot",
        "Elio Quinton",
        "Gy\\\"orgy Fazekas"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.17886",
        "HTML": "https://arxiv.org/html/2506.17886",
        "PDF": "https://arxiv.org/pdf/2506.17886"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "date": "Sun, 22 Jun 2025 03:30:27 GMT",
          "size": "4179kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 14:25:12 GMT",
          "size": "4177kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "GD-Retriever: Controllable Generative Text-Music Retrieval with Diffusion Models",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper addresses controllability in generative models for text-music retrieval, mentioning concepts like data embedding spaces which might indirectly relate to LLM training data but is not the focus."
      }
    },
    {
      "id": "2506.18069",
      "abstract": "We developed a proof-of-concept method for the automatic analysis of the structure and content of incunabula pages. A custom dataset comprising 500 annotated pages from five different incunabula was created using resources from the Jagiellonian Digital Library. Each page was manually labeled with five predefined classes: Text, Title, Picture, Table, and Handwriting. Additionally, the publicly available DocLayNet dataset was utilized as supplementary training data. To perform object detection, YOLO11n and YOLO11s models were employed and trained using two strategies: a combined dataset (DocLayNet and the custom dataset) and the custom dataset alone. The highest performance (F1 = 0.94) was achieved by the YOLO11n model trained exclusively on the custom data. Optical character recognition was then conducted on regions classified as Text, using both Tesseract and Kraken OCR, with Tesseract demonstrating superior results. Subsequently, image classification was applied to the Picture class using a ResNet18 model, achieving an accuracy of 98.7% across five subclasses: Decorative_letter, Illustration, Other, Stamp, and Wrong_detection. Furthermore, the CLIP model was utilized to generate semantic descriptions of illustrations. The results confirm the potential of machine learning in the analysis of early printed books, while emphasizing the need for further advancements in OCR performance and visual content interpretation.",
      "authors": [
        "Klaudia Ropel",
        "Krzysztof Kutt",
        "Luiz do Valle Miranda",
        "Grzegorz J. Nalepa"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18069",
        "HTML": "https://arxiv.org/html/2506.18069",
        "PDF": "https://arxiv.org/pdf/2506.18069"
      },
      "subjects": [
        "Digital Libraries (cs.DL)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Sun, 22 Jun 2025 15:33:20 GMT",
          "size": "2169kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 11:19:13 GMT",
          "size": "2169kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Unfolding the Past: A Comprehensive Deep Learning Approach to Analyzing Incunabula Pages",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper mentions the creation and use of a custom dataset for image classification and OCR; however, it does not focus on LLM training data or issues specifically relevant to LLMs."
      }
    },
    {
      "id": "2506.18246",
      "abstract": "Natural language querying of visual content underpins many vision-language tasks, typically categorized by text granularity and visual search scope. Text-Image Retrieval (TIR) retrieves whole images using coarse descriptions, while Referring Expression Comprehension (REC) localizes objects using fine-grained expressions within a single image. However, real-world scenarios often require both instance-level retrieval and localization across large galleries -- tasks where TIR lacks precision and REC lacks scalability. To address this gap, we propose a new task: Referring Expression Instance Retrieval (REIR), which jointly supports instance-level retrieval and localization. We introduce REIRCOCO, a large-scale benchmark constructed by prompting vision-language models to generate fine-grained expressions for MSCOCO and RefCOCO instances. We also present a baseline method, CLARE, featuring a dual-stream architecture with a Mix of Relation Experts (MORE) module for capturing inter-instance relationships. CLARE integrates object detection and REC pretraining with Contrastive Language-Instance Alignment (CLIA) for end-to-end optimization. Experiments show that CLARE achieves state-of-the-art performance on REIR and generalizes well to TIR and REC, highlighting its effectiveness and versatility.",
      "authors": [
        "Xiangzhao Hao",
        "Kuan Zhu",
        "Hongyu Guo",
        "Haiyun Guo",
        "Ning Jiang",
        "Quan Lu",
        "Ming Tang",
        "JinQiao Wang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18246",
        "HTML": "https://arxiv.org/html/2506.18246",
        "PDF": "https://arxiv.org/pdf/2506.18246"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 02:28:44 GMT",
          "size": "7895kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 03:38:39 GMT",
          "size": "7895kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Referring Expression Instance Retrieval and A Strong End-to-End Baseline",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper proposes a benchmark and method for visual content retrieval using language, which indirectly relates to dataset discussions in multimodal learning but does not directly address LLM training data."
      }
    },
    {
      "id": "2506.18405",
      "abstract": "In this paper, we consider the problem of degradation of anonymity upon linkages of anonymized datasets. We work in the setting where an adversary links together $t\\geq 2$ anonymized datasets in which a user of interest participates, based on the user's known quasi-identifiers, which motivates the use of $\\ell$-diversity as the notion of dataset anonymity. We first argue that in the worst case, such linkage attacks can reveal the exact sensitive attribute of the user, even when each dataset respects $\\ell$-diversity, for moderately large values of $\\ell$. This issue motivates our definition of (approximate) $(\\ell,\\delta)$-diversity -- a parallel of (approximate) $(\\epsilon,\\delta)$-differential privacy (DP) -- which simply requires that a dataset respect $\\ell$-diversity, with high probability. We then present a mechanism for achieving $(\\ell,\\delta)$-diversity, in the setting of independent and identically distributed samples. Next, we establish bounds on the degradation of $(\\ell,\\delta)$-diversity, via a simple ``composition theorem,'' similar in spirit to those in the DP literature, thereby showing that approximate diversity, unlike standard diversity, is roughly preserved upon linkage. Finally, we describe simple algorithms for maximizing utility, measured in terms of the number of anonymized ``equivalence classes,'' and derive explicit lower bounds on the utility, for special sample distributions.",
      "authors": [
        "V. Arvind Rameshwar and Anshoo Tandon"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18405",
        "HTML": "https://arxiv.org/html/2506.18405",
        "PDF": "https://arxiv.org/pdf/2506.18405"
      },
      "subjects": [
        "Information Theory (cs.IT)",
        "Information Theory (math.IT)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 08:42:14 GMT",
          "size": "278kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 05:27:22 GMT",
          "size": "278kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "$(\\ell,\\delta)$-Diversity: Linkage-Robustness via a Composition Theorem",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While the paper discusses anonymity and linkage-robustness of datasets using concepts like diversity and privacy, which are indirectly relevant to data curation and protection, it does not focus on LLM training data."
      }
    },
    {
      "id": "2506.18710",
      "abstract": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a pivotal role in evaluating AI's knowledge and abilities across diverse domains. However, existing benchmarks predominantly focus on content knowledge, leaving a critical gap in assessing models' understanding of pedagogy - the method and practice of teaching. This paper introduces The Pedagogy Benchmark, a novel dataset designed to evaluate large language models on their Cross-Domain Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND) pedagogical knowledge. These benchmarks are built on a carefully curated set of questions sourced from professional development exams for teachers, which cover a range of pedagogical subdomains such as teaching strategies and assessment methods. Here we outline the methodology and development of these benchmarks. We report results for 97 models, with accuracies spanning a range from 28% to 89% on the pedagogical knowledge questions. We consider the relationship between cost and accuracy and chart the progression of the Pareto value frontier over time. We provide online leaderboards at https://rebrand.ly/pedagogy which are updated with new models and allow interactive exploration and filtering based on various model properties, such as cost per token and open-vs-closed weights, as well as looking at performance in different subjects. LLMs and generative AI have tremendous potential to influence education and help to address the global learning crisis. Education-focused benchmarks are crucial to measure models' capacities to understand pedagogical concepts, respond appropriately to learners' needs, and support effective teaching practices across diverse contexts. They are needed for informing the responsible and evidence-based deployment of LLMs and LLM-based tools in educational settings, and for guiding both development and policy decisions.",
      "authors": [
        "Maxime Leli\\`evre",
        "Amy Waldock",
        "Meng Liu",
        "Natalia Vald\\'es Aspillaga",
        "Alasdair Mackintosh",
        "Mar\\'ia Jos\\'e Ogando Portela",
        "Jared Lee",
        "Paul Atherton",
        "Robin A. A. Ince",
        "Oliver G. B. Garrod"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18710",
        "HTML": "https://arxiv.org/html/2506.18710",
        "PDF": "https://arxiv.org/pdf/2506.18710"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 14:49:01 GMT",
          "size": "3243kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 12:36:22 GMT",
          "size": "3243kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Benchmarking the Pedagogical Knowledge of Large Language Models",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "Although the paper introduces a benchmark dataset for evaluating LLMs, it focuses on assessing pedagogical knowledge rather than directly addressing LLM training data topics such as data collection or quality."
      },
      "datasets": [
        {
          "dataset_name": "AI-for-Education/pedagogy-benchmark",
          "downloads": "0",
          "likes": "0",
          "link": "https://huggingface.co/datasets/AI-for-Education/pedagogy-benchmark"
        }
      ]
    },
    {
      "id": "2506.18902",
      "abstract": "We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-document retrieval, semantic text similarity, and code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single-modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.",
      "authors": [
        "Michael G\\\"unther",
        "Saba Sturua",
        "Mohammad Kalim Akram",
        "Isabelle Mohr",
        "Andrei Ungureanu",
        "Bo Wang",
        "Sedigheh Eslami",
        "Scott Martens",
        "Maximilian Werk",
        "Nan Wang",
        "Han Xiao"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18902",
        "HTML": "https://arxiv.org/html/2506.18902",
        "PDF": "https://arxiv.org/pdf/2506.18902"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Information Retrieval (cs.IR)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 17:59:55 GMT",
          "size": "174kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 15:52:37 GMT",
          "size": "171kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "While this paper is about embeddings and multimodal retrieval, it mentions task-specific adaptations, which could suggest some minor implications about data used in training the models."
      },
      "models": [
        {
          "model_path": "jinaai/jina-embeddings-v4",
          "downloads": "669",
          "likes": "25",
          "trending_score": "25.0",
          "link": "https://huggingface.co/jinaai/jina-embeddings-v4"
        }
      ]
    },
    {
      "id": "2504.03784",
      "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset.",
      "authors": [
        "Kai Ye",
        "Hongyi Zhou",
        "Jin Zhu",
        "Francesco Quinzan",
        "Chengchun Shi"
      ],
      "last_revised_date": "2025/06/23",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2504.03784",
        "HTML": "https://arxiv.org/html/2504.03784",
        "PDF": "https://arxiv.org/pdf/2504.03784"
      },
      "subjects": [
        "Machine Learning (stat.ML)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Thu, 03 Apr 2025 16:16:35 GMT",
          "size": "2970kb",
          "version": "v1"
        },
        {
          "date": "Wed, 09 Apr 2025 03:41:09 GMT",
          "size": "2970kb",
          "version": "v2"
        },
        {
          "date": "Tue, 15 Apr 2025 09:29:06 GMT",
          "size": "2970kb",
          "version": "v3"
        },
        {
          "date": "Mon, 23 Jun 2025 18:51:33 GMT",
          "size": "2970kb",
          "version": "v4"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses reinforcement learning from human feedback applied to LLM fine-tuning, touching on LLM datasets used for evaluations, like the Anthropic Helpful and Harmless dataset. However, it is more focused on RL algorithms rather than training data itself."
      },
      "tasks": []
    },
    {
      "id": "2505.19447",
      "abstract": "Self-Supervised Learning (SSL) enables us to pre-train foundation models without costly labeled data. Among SSL methods, Contrastive Learning (CL) methods are better at obtaining accurate semantic representations in noise interference. However, due to the significant domain gap, while CL methods have achieved great success in many computer vision tasks, they still require specific adaptation for Remote Sensing (RS) images. To this end, we present a novel self-supervised method called PerA, which produces all-purpose RS features through semantically Perfectly Aligned sample pairs. Specifically, PerA obtains features from sampled views by applying spatially disjoint masks to augmented images rather than random cropping. Our framework provides high-quality features by ensuring consistency between teacher and student and predicting learnable mask tokens. Compared to previous contrastive methods, our method demonstrates higher memory efficiency and can be trained with larger batches due to its sparse inputs. Additionally, the proposed method demonstrates remarkable adaptability to uncurated RS data and reduce the impact of the potential semantic inconsistency. We also collect an unlabeled pre-training dataset, which contains about 5 million RS images. We conducted experiments on multiple downstream task datasets and achieved performance comparable to previous state-of-the-art methods with a limited model scale, demonstrating the effectiveness of our approach. We hope this work will contribute to practical remote sensing interpretation works.",
      "authors": [
        "Hengtong Shen",
        "Haiyan Gu",
        "Haitao Li",
        "Yi Yang",
        "Agen Qiu"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.19447",
        "HTML": "https://arxiv.org/html/2505.19447",
        "PDF": "https://arxiv.org/pdf/2505.19447"
      },
      "subjects": [
        "Image and Video Processing (eess.IV)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 26 May 2025 03:12:49 GMT",
          "size": "1558kb",
          "version": "v1"
        },
        {
          "date": "Tue, 24 Jun 2025 02:04:10 GMT",
          "size": "1629kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "A Contrastive Learning Foundation Model Based on Perfectly Aligned Sample Pairs for Remote Sensing Images",
      "relevance": {
        "keyword": "creativity",
        "level": "weak",
        "reason": "The paper discusses self-supervised learning, pre-training data, and data quality for remote sensing, which indirectly relates to training data in language models. However, the focus is not specifically on LLM training data."
      },
      "tasks": [
        "Contrastive Learning",
        "Self-Supervised Learning"
      ]
    }
  ],
  "subjects": [
    "Geophysics (physics.geo-ph)",
    "Logic in Computer Science (cs.LO)",
    "Computer Vision and Pattern Recognition (cs.CV)",
    "Networking and Internet Architecture (cs.NI)",
    "Multimedia (cs.MM)",
    "Quantum Physics (quant-ph)",
    "Physics and Society (physics.soc-ph)",
    "Computational Engineering, Finance, and Science (cs.CE)",
    "Computation and Language (cs.CL)",
    "Databases (cs.DB)",
    "Sound (cs.SD)",
    "Image and Video Processing (eess.IV)",
    "Systems and Control (eess.SY)",
    "Cryptography and Security (cs.CR)",
    "Robotics (cs.RO)",
    "Information Retrieval (cs.IR)",
    "Artificial Intelligence (cs.AI)",
    "Neurons and Cognition (q-bio.NC)",
    "Data Analysis, Statistics and Probability (physics.data-an)",
    "Systems and Control (cs.SY)",
    "Programming Languages (cs.PL)",
    "Mathematical Software (cs.MS)",
    "Probability (math.PR)",
    "Information Theory (math.IT)",
    "Emerging Technologies (cs.ET)",
    "Signal Processing (eess.SP)",
    "Audio and Speech Processing (eess.AS)",
    "Social and Information Networks (cs.SI)",
    "Machine Learning (stat.ML)",
    "Information Theory (cs.IT)",
    "Computers and Society (cs.CY)",
    "Tissues and Organs (q-bio.TO)",
    "Multiagent Systems (cs.MA)",
    "General Finance (q-fin.GN)",
    "Software Engineering (cs.SE)",
    "Distributed, Parallel, and Cluster Computing (cs.DC)",
    "Human-Computer Interaction (cs.HC)",
    "Graphics (cs.GR)",
    "Digital Libraries (cs.DL)",
    "Machine Learning (cs.LG)",
    "General Economics (econ.GN)",
    "Economics (q-fin.EC)"
  ],
  "prompt": {
    "creativity": "\nYou are an expert in information retrieval. I will provide you with a list of research papers from arXiv, specifically in the *cs* (Computer Science) category.\n\nYour task is to analyze each paper and determine its relevance to the topic of **LLM training data** \u2014 that is, any content, methodology, or discussion related to data used in training large language models (LLMs), such as data collection, curation, filtering, privacy, quality, or representativeness.\n\nClassify each paper into one of the following relevance levels:\n\n* `\"strong\"`: The paper is explicitly focused on LLM training data (e.g., it proposes a dataset, evaluates dataset quality, or discusses training data's impact on LLMs).\n* `\"weak\"`: The paper touches on topics indirectly related to LLM training data (e.g., discusses user-generated content, data annotation, or ethics in data use) but it's not the main focus.\n* `\"none\"`: The paper is not related to LLM training data.\n\nReturn your results in the following JSON format:\n\n```json\n{\n  \"result\": [\n    {\n      \"id\": \"paper id\",\n      \"level\": \"strong | weak | none\",\n      \"reason\": \"Brief justification based on the paper content\"\n    },\n    {\n      \"id\": \"paper id\",\n      \"level\": \"strong | weak | none\",\n      \"reason\": \"Brief justification based on the paper content\"\n    }\n  ]\n}\n```\n\nBe concise but specific in your reasoning. Mention key terms (e.g., \u201ctraining data\u201d, \u201cdataset\u201d, \u201ccorpus\u201d, \u201cdata collection\u201d) or specific sections (e.g., abstract, methodology) that informed your decision when applicable.\n"
  },
  "description": "Data source: https://arxiv.org/list/cs.HC/new"
}