{
  "data": [
    {
      "id": "2409.10394",
      "abstract": "Deep learning-based Magnetic Resonance (MR) reconstruction methods have focused on generating high-quality images but often overlook the impact on downstream tasks (e.g., segmentation) that utilize the reconstructed images. Cascading separately trained reconstruction network and downstream task network has been shown to introduce performance degradation due to error propagation and domain gaps between training datasets. To mitigate this issue, downstream task-oriented reconstruction optimization has been proposed for a single downstream task. Expanding this optimization to multi-task scenarios is not straightforward. In this work, we extended this optimization to sequentially introduced multiple downstream tasks and demonstrated that a single MR reconstruction network can be optimized for multiple downstream tasks by deploying continual learning (MOST). MOST integrated techniques from replay-based continual learning and image-guided loss to overcome catastrophic forgetting. Comparative experiments demonstrated that MOST outperformed a reconstruction network without finetuning, a reconstruction network with na\\\"ive finetuning, and conventional continual learning methods. The source code is available at: https://github.com/SNU-LIST/MOST.",
      "authors": [
        "Jeong, Hwihun",
        "Chun, Se Young",
        "Lee, Jongho"
      ],
      "last_revised_date": "2025/06/24",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2409.10394v3",
        "Other Formats": "https://arxiv.org/format/2409.10394",
        "TeX Source": "https://arxiv.org/src/2409.10394",
        "View PDF": "https://arxiv.org/pdf/2409.10394"
      },
      "subjects": [
        "Image and Video Processing (eess.IV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 16 Sep 2024 15:31:04 UTC (1,260 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 17:52:36 UTC (1,035 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Tue, 24 Jun 2025 14:38:57 UTC (1,040 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/09/16",
      "title": "MOST: MR reconstruction Optimization for multiple downStream Tasks via continual learning",
      "repo_urls": [
        "https://github.com/snu-list/most"
      ],
      "tasks": [
        "Continual Learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.13274",
      "abstract": "Learning rate is widely regarded as crucial for effective foundation model pretraining. Recent research explores and demonstrates the transferability of learning rate configurations across varying model and dataset sizes, etc. Nevertheless, these approaches are constrained to specific training scenarios and typically necessitate extensive hyperparameter tuning on proxy models. In this work, we propose \\textbf{AdaLRS}, a plug-in-and-play adaptive learning rate search algorithm that conducts online optimal learning rate search via optimizing loss descent velocities. We provide experiment results to show that the optimization of training loss and loss descent velocity in foundation model pretraining are both convex and share the same optimal learning rate. Relying solely on training loss dynamics, AdaLRS involves few extra computations to guide the search process, and its convergence is guaranteed via theoretical analysis. Experiments on both LLM and VLM pretraining show that AdaLRS adjusts suboptimal learning rates to the neighborhood of optimum with marked efficiency and effectiveness, with model performance improved accordingly. We also show the robust generalizability of AdaLRS across varying training scenarios, such as different model sizes, training paradigms, and base learning rate scheduler choices.",
      "authors": [
        "Dong, Hongyuan",
        "Yang, Dingkang",
        "Liang, Xiao",
        "Feng, Chao",
        "Ran, Jiao"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.13274v2",
        "Other Formats": "https://arxiv.org/format/2506.13274",
        "TeX Source": "https://arxiv.org/src/2506.13274",
        "View PDF": "https://arxiv.org/pdf/2506.13274"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 16 Jun 2025 09:14:01 UTC (3,511 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 03:18:17 UTC (3,511 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/06/16",
      "title": "AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient Foundation Model Pretraining",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2503.09283",
      "abstract": "Building on recent advances in Bayesian statistics and image denoising, we propose Noise2Score3D, a fully unsupervised framework for point cloud denoising. Noise2Score3D learns the score function of the underlying point cloud distribution directly from noisy data, eliminating the need for clean data during training. Using Tweedie's formula, our method performs denoising in a single step, avoiding the iterative processes used in existing unsupervised methods, thus improving both accuracy and efficiency. Additionally, we introduce Total Variation for Point Clouds as a denoising quality metric, which allows for the estimation of unknown noise parameters. Experimental results demonstrate that Noise2Score3D achieves state-of-the-art performance on standard benchmarks among unsupervised learning methods in Chamfer distance and point-to-mesh metrics. Noise2Score3D also demonstrates strong generalization ability beyond training datasets. Our method, by addressing the generalization issue and challenge of the absence of clean data in learning-based methods, paves the way for learning-based point cloud denoising methods in real-world applications.",
      "authors": [
        "Wei, Xiangbin",
        "Wang, Yuanfeng",
        "XU, Ao",
        "Zhu, Lingyu",
        "Sun, Dongyong",
        "Li, Keren",
        "Li, Yang",
        "Qin, Qi"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2503.09283v2",
        "Other Formats": "https://arxiv.org/format/2503.09283",
        "TeX Source": "https://arxiv.org/src/2503.09283",
        "View PDF": "https://arxiv.org/pdf/2503.09283"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 12 Mar 2025 11:28:04 UTC (34,797 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 08:47:30 UTC (34,795 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/03/12",
      "title": "Noise2Score3D: Tweedie's Approach for Unsupervised Point Cloud Denoising",
      "tasks": [
        "Denoising",
        "Image Denoising"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18504",
      "abstract": "Recently, vision-language pretraining has emerged as a transformative technique that integrates the strengths of both visual and textual modalities, resulting in powerful vision-language models (VLMs). Leveraging web-scale pretraining data, these models exhibit strong zero-shot capabilities. However, their performance often deteriorates when confronted with domain-specific or specialized generalization tasks. To address this, a growing body of research focuses on transferring or generalizing the rich knowledge embedded in VLMs to various downstream applications. This survey aims to comprehensively summarize the generalization settings, methodologies, benchmarking and results in VLM literatures. Delving into the typical VLM structures, current literatures are categorized into prompt-based, parameter-based and feature-based methods according to the transferred modules. The differences and characteristics in each category are furthered summarized and discussed by revisiting the typical transfer learning (TL) settings, providing novel interpretations for TL in the era of VLMs. Popular benchmarks for VLM generalization are further introduced with thorough performance comparisons among the reviewed methods. Following the advances in large-scale generalizable pretraining, this survey also discusses the relations and differences between VLMs and up-to-date multimodal large language models (MLLM), e.g., DeepSeek-VL. By systematically reviewing the surging literatures in vision-language research from a novel and practical generalization prospective, this survey contributes to a clear landscape of current and future multimodal researches.",
      "authors": [
        "Li, Xinyao",
        "Li, Jingjing",
        "Li, Fengling",
        "Zhu, Lei",
        "Yang, Yang",
        "Shen, Heng Tao"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18504v1",
        "Other Formats": "https://arxiv.org/format/2506.18504",
        "TeX Source": "https://arxiv.org/src/2506.18504",
        "View PDF": "https://arxiv.org/pdf/2506.18504"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 10:56:37 UTC (1,001 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.24446",
      "abstract": "This paper presents our system for the MISP-Meeting Challenge Track 2. The primary difficulty lies in the dataset, which contains strong background noise, reverberation, overlapping speech, and diverse meeting topics. To address these issues, we (a) designed G-SpatialNet, a speech enhancement (SE) model to improve Guided Source Separation (GSS) signals; (b) proposed TLS, a framework comprising time alignment, level alignment, and signal-to-noise ratio filtering, to generate signal-level pseudo labels for real-recorded far-field audio data, thereby facilitating SE models' training; and (c) explored fine-tuning strategies, data augmentation, and multimodal information to enhance the performance of pre-trained Automatic Speech Recognition (ASR) models in meeting scenarios. Finally, our system achieved character error rates (CERs) of 5.44% and 9.52% on the Dev and Eval sets, respectively, with relative improvements of 64.8% and 52.6% over the baseline, securing second place.",
      "authors": [
        "Luo, Longjie",
        "Lu, Shenghui",
        "Li, Lin",
        "Hong, Qingyang"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2505.24446v2",
        "Other Formats": "https://arxiv.org/format/2505.24446",
        "TeX Source": "https://arxiv.org/src/2505.24446",
        "View PDF": "https://arxiv.org/pdf/2505.24446"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 30 May 2025 10:33:54 UTC (9,079 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 07:54:08 UTC (9,079 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/30",
      "title": "Pseudo Labels-based Neural Speech Enhancement for the AVSR Task in the MISP-Meeting Challenge",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2409.17992",
      "abstract": "Reinforcement Learning (RL) has shown its remarkable and generalizable capability in legged locomotion through sim-to-real transfer. However, while adaptive methods like domain randomization are expected to enhance policy robustness across diverse environments, they potentially compromise the policy's performance in any specific environment, leading to suboptimal real-world deployment due to the No Free Lunch theorem. To address this, we propose LoopSR, a lifelong policy adaptation framework that continuously refines RL policies in the post-deployment stage. LoopSR employs a transformer-based encoder to map real-world trajectories into a latent space and reconstruct a digital twin of the real world for further improvement. Autoencoder architecture and contrastive learning methods are adopted to enhance feature extraction of real-world dynamics. Simulation parameters for continual training are derived by combining predicted values from the decoder with retrieved parameters from a pre-collected simulation trajectory dataset. By leveraging simulated continual training, LoopSR achieves superior data efficiency compared with strong baselines, yielding eminent performance with limited data in both sim-to-sim and sim-to-real experiments.",
      "authors": [
        "Wu, Peilin",
        "Xie, Weiji",
        "Cao, Jiahang",
        "Lai, Hang",
        "Zhang, Weinan"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2409.17992v2",
        "Other Formats": "https://arxiv.org/format/2409.17992",
        "TeX Source": "https://arxiv.org/src/2409.17992",
        "View PDF": "https://arxiv.org/pdf/2409.17992"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 26 Sep 2024 16:02:25 UTC (7,277 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 06:59:08 UTC (6,439 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/09/26",
      "title": "LoopSR: Looping Sim-and-Real for Lifelong Policy Adaptation of Legged Robots",
      "tasks": [
        "Contrastive Learning",
        "Decoder",
        "Reinforcement Learning (RL)"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18896",
      "abstract": "Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: https://github.com/Gen-Verse/ReasonFlux",
      "authors": [
        "Zou, Jiaru",
        "Yang, Ling",
        "Gu, Jingwen",
        "Qiu, Jiahao",
        "Shen, Ke",
        "He, Jingrui",
        "Wang, Mengdi"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.18896",
        "TeX Source": "https://arxiv.org/src/2506.18896",
        "View PDF": "https://arxiv.org/pdf/2506.18896"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 17:59:02 UTC (3,635 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs",
      "models": [
        {
          "model_path": "Gen-Verse/ReasonFlux-PRM-1.5B",
          "downloads": "11",
          "likes": "2",
          "trending_score": "2.0",
          "link": "https://huggingface.co/Gen-Verse/ReasonFlux-PRM-1.5B"
        },
        {
          "model_path": "Gen-Verse/ReasonFlux-PRM-Qwen-2.5-7B",
          "downloads": "8",
          "likes": "2",
          "trending_score": "2.0",
          "link": "https://huggingface.co/Gen-Verse/ReasonFlux-PRM-Qwen-2.5-7B"
        },
        {
          "model_path": "Gen-Verse/ReasonFlux-PRM-7B",
          "downloads": "8",
          "likes": "2",
          "trending_score": "2.0",
          "link": "https://huggingface.co/Gen-Verse/ReasonFlux-PRM-7B"
        }
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18598",
      "abstract": "Neural network classifiers trained on datasets with uneven group representation often inherit class biases and learn spurious correlations. These models may perform well on average but consistently fail on atypical groups. For example, in hair color classification, datasets may over-represent females with blond hair, reinforcing stereotypes. Although various algorithmic and data-centric methods have been proposed to address such biases, they often require retraining or significant compute. In this work, we propose a cheap, training-free method inspired by steering vectors used to edit behaviors in large language models. We compute the difference in mean activations between majority and minority groups to define a \"bias vector,\" which we subtract from the model's residual stream. This leads to reduced classification bias and improved worst-group accuracy. We explore multiple strategies for extracting and applying these vectors in transformer-like classifiers, showing that steering vectors, traditionally used in generative models, can also be effective in classification. More broadly, we showcase an extremely cheap, inference time, training free method to mitigate bias in classification models.",
      "authors": [
        "Gupta, Aviral",
        "Sethi, Armaan",
        "Sethi, Ameesh"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18598v1",
        "Other Formats": "https://arxiv.org/format/2506.18598",
        "TeX Source": "https://arxiv.org/src/2506.18598",
        "View PDF": "https://arxiv.org/pdf/2506.18598"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Computation and Language (cs.CL)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 12:58:54 UTC (38 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "No Training Wheels: Steering Vectors for Bias Correction at Inference Time",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18247",
      "abstract": "Quantifying and propagating modeling uncertainties is crucial for reliability analysis, robust optimization, and other model-based algorithmic processes in engineering design and control. Now, physics-informed machine learning (PIML) methods have emerged in recent years as a new alternative to traditional computational modeling and surrogate modeling methods, offering a balance between computing efficiency, modeling accuracy, and interpretability. However, their ability to predict and propagate modeling uncertainties remains mostly unexplored. In this paper, a promising class of auto-differentiable hybrid PIML architectures that combine partial physics and neural networks or ANNs (for input transformation or adaptive parameter estimation) is integrated with Bayesian Neural networks (replacing the ANNs); this is done with the goal to explore whether BNNs can successfully provision uncertainty propagation capabilities in the PIML architectures as well, further supported by the auto-differentiability of these architectures. A two-stage training process is used to alleviate the challenges traditionally encountered in training probabilistic ML models. The resulting BNN-integrated PIML architecture is evaluated on an analytical benchmark problem and flight experiments data for a fixed-wing RC aircraft, with prediction performance observed to be slightly worse or at par with purely data-driven ML and original PIML models. Moreover, Monte Carlo sampling of probabilistic BNN weights was found to be most effective in propagating uncertainty in the BNN-integrated PIML architectures.",
      "authors": [
        "Oddiraju, Manaswin",
        "Penumatsa, Bharath Varma",
        "Amin, Divyang",
        "Piedmonte, Michael",
        "Chowdhury, Souma"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18247v1",
        "Other Formats": "https://arxiv.org/format/2506.18247",
        "TeX Source": "https://arxiv.org/src/2506.18247",
        "View PDF": "https://arxiv.org/pdf/2506.18247"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 02:32:20 UTC (1,201 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Exploring Efficient Quantification of Modeling Uncertainties with Differentiable Physics-Informed Machine Learning Architectures",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2403.03639",
      "abstract": "Legged robots have become capable of performing highly dynamic maneuvers in the past few years. However, agile locomotion in highly constrained environments such as stepping stones is still a challenge. In this paper, we propose a combination of model-based control, search, and learning to design efficient control policies for agile locomotion on stepping stones. In our framework, we use nonlinear model predictive control (NMPC) to generate whole-body motions for a given contact plan. To efficiently search for an optimal contact plan, we propose to use Monte Carlo tree search (MCTS). While the combination of MCTS and NMPC can quickly find a feasible plan for a given environment (a few seconds), it is not yet suitable to be used as a reactive policy. Hence, we generate a dataset for optimal goal-conditioned policy for a given scene and learn it through supervised learning. In particular, we leverage the power of diffusion models in handling multi-modality in the dataset. We test our proposed framework on a scenario where our quadruped robot Solo12 successfully jumps to different goals in a highly constrained environment.",
      "authors": [
        "Dh\u00c3\u00a9din, Victor",
        "Ravi, Adithya Kumar Chinnakkonda",
        "Jordana, Armand",
        "Zhu, Huaijiang",
        "Meduri, Avadesh",
        "Righetti, Ludovic",
        "Sch\u00c3\u00b6lkopf, Bernhard",
        "Khadiv, Majid"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2403.03639v5",
        "Other Formats": "https://arxiv.org/format/2403.03639",
        "TeX Source": "https://arxiv.org/src/2403.03639",
        "View PDF": "https://arxiv.org/pdf/2403.03639"
      },
      "subjects": [
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 6 Mar 2024 11:52:33 UTC (2,597 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Tue, 16 Jul 2024 14:31:42 UTC (11,424 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Mon, 14 Oct 2024 12:29:26 UTC (3,792 KB)",
          "link": "/",
          "version": "[v3]"
        },
        {
          "details": "Mon, 26 May 2025 09:09:53 UTC (3,792 KB)",
          "link": "/",
          "version": "[v4]"
        },
        {
          "details": "Mon, 23 Jun 2025 09:14:54 UTC (2,635 KB)",
          "version": "[v5]"
        }
      ],
      "submitted_date": "2024/03/06",
      "title": "Diffusion-based learning of contact plans for agile locomotion",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18725",
      "abstract": "Point cloud-based object/place recognition remains a problem of interest in applications such as autonomous driving, scene reconstruction, and localization. Extracting meaningful local descriptors from a query point cloud that can be matched with the descriptors of the collected point clouds is a challenging problem. Furthermore, when the query point cloud is noisy or has been transformed (e.g., rotated), it adds to the complexity. To this end, we propose a novel methodology, named TDACloud, using Topological Data Analysis (TDA) for local descriptor extraction from a point cloud, which does not need resource-intensive GPU-based machine learning training. More specifically, we used the ATOL vectorization method to generate vectors for point clouds. Unlike voxelization, our proposed technique can take raw point clouds as inputs and outputs a fixed-size TDA-descriptor vector. To test the quality of the proposed TDACloud technique, we have implemented it on multiple real-world (e.g., Oxford RobotCar, KITTI-360) and realistic (e.g., ShapeNet) point cloud datasets for object and place recognition. We have also tested TDACloud on noisy and transformed test cases where the query point cloud has been scaled, translated, or rotated. Our results demonstrate high recognition accuracies in noisy conditions and large-scale real-world place recognition while outperforming the baselines by up to approximately 14%.",
      "authors": [
        "Ghosh, Anirban",
        "Dahlin, Ian",
        "Dutta, Ayan"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18725v1",
        "Other Formats": "https://arxiv.org/format/2506.18725",
        "TeX Source": "https://arxiv.org/src/2506.18725",
        "View PDF": "https://arxiv.org/pdf/2506.18725"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Computational Geometry (cs.CG)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 14:59:39 UTC (1,730 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "TDACloud: Point Cloud Recognition Using Topological Data Analysis",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18898",
      "abstract": "This paper presents a multimodal framework that attempts to unify visual understanding and generation within a shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from a large language model's (LLM) vocabulary. By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through a shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with a generative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. Code, models, and data are available at https://tar.csuhan.com",
      "authors": [
        "Han, Jiaming",
        "Chen, Hao",
        "Zhao, Yang",
        "Wang, Hanyu",
        "Zhao, Qi",
        "Yang, Ziyan",
        "He, Hao",
        "Yue, Xiangyu",
        "Jiang, Lu"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18898v1",
        "Other Formats": "https://arxiv.org/format/2506.18898",
        "TeX Source": "https://arxiv.org/src/2506.18898",
        "View PDF": "https://arxiv.org/pdf/2506.18898"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Multimedia (cs.MM)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 17:59:14 UTC (17,228 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18427",
      "abstract": "The finite element method (FEM) is a well-established numerical method for solving partial differential equations (PDEs). However, its mesh-based nature gives rise to substantial computational costs, especially for complex multiscale simulations. Emerging machine learning-based methods (e.g., neural operators) provide data-driven solutions to PDEs, yet they present challenges, including high training cost and low model reusability. Here, we propose the neural-operator element method (NOEM) by synergistically combining FEM with operator learning to address these challenges. NOEM leverages neural operators (NOs) to simulate subdomains where a large number of finite elements would be required if FEM was used. In each subdomain, an NO is used to build a single element, namely a neural-operator element (NOE). NOEs are then integrated with standard finite elements to represent the entire solution through the variational framework. Thereby, NOEM does not necessitate dense meshing and offers efficient simulations. We demonstrate the accuracy, efficiency, and scalability of NOEM by performing extensive and systematic numerical experiments, including nonlinear PDEs, multiscale problems, PDEs on complex geometries, and discontinuous coefficient fields.",
      "authors": [
        "Ouyang, Weihang",
        "Shin, Yeonjong",
        "Liu, Si-Wei",
        "Lu, Lu"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18427v1",
        "Other Formats": "https://arxiv.org/format/2506.18427",
        "TeX Source": "https://arxiv.org/src/2506.18427",
        "View PDF": "https://arxiv.org/pdf/2506.18427"
      },
      "subjects": [
        "Computational Engineering, Finance, and Science (cs.CE)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 09:08:10 UTC (25,998 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Neural-operator element method: Efficient and scalable finite element method enabled by reusable neural operators",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18309",
      "abstract": "User profiling is pivotal for recommendation systems, as it transforms raw user interaction data into concise and structured representations that drive personalized recommendations. While traditional embedding-based profiles lack interpretability and adaptability, recent advances with large language models (LLMs) enable text-based profiles that are semantically richer and more transparent. However, existing methods often adhere to fixed formats that limit their ability to capture the full diversity of user behaviors. In this paper, we introduce LettinGo, a novel framework for generating diverse and adaptive user profiles. By leveraging the expressive power of LLMs and incorporating direct feedback from downstream recommendation tasks, our approach avoids the rigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ Direct Preference Optimization (DPO) to align the profile generator with task-specific performance, ensuring that the profiles remain adaptive and effective. LettinGo operates in three stages: (1) exploring diverse user profiles via multiple LLMs, (2) evaluating profile quality based on their impact in recommendation systems, and (3) aligning the profile generation through pairwise preference data derived from task performance. Experimental results demonstrate that our framework significantly enhances recommendation accuracy, flexibility, and contextual awareness. This work enhances profile generation as a key innovation for next-generation recommendation systems.",
      "authors": [
        "Wang, Lu",
        "Zhang, Di",
        "Yang, Fangkai",
        "Zhao, Pu",
        "Liu, Jianfeng",
        "Zhan, Yuefeng",
        "Sun, Hao",
        "Lin, Qingwei",
        "Deng, Weiwei",
        "Zhang, Dongmei",
        "Sun, Feng",
        "Zhang, Qi"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18309v1",
        "Other Formats": "https://arxiv.org/format/2506.18309",
        "TeX Source": "https://arxiv.org/src/2506.18309",
        "View PDF": "https://arxiv.org/pdf/2506.18309"
      },
      "subjects": [
        "Information Retrieval (cs.IR)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 05:51:52 UTC (768 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "LettinGo: Explore User Profile Generation for Recommendation System",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2307.16714",
      "abstract": "Growth in system complexity increases the need for automated log analysis techniques, such as Log-based Anomaly Detection (LAD). While deep learning (DL) methods have been widely used for LAD, traditional machine learning (ML) techniques can also perform well depending on the context and dataset. Semi-supervised techniques deserve the same attention as they offer practical advantages over fully supervised methods. Current evaluations mainly focus on detection accuracy, but this alone is insufficient to determine the suitability of a technique for a given LAD task. Other aspects to consider include training and prediction times as well as the sensitivity to hyperparameter tuning, which in practice matters to engineers. This paper presents a comprehensive empirical study evaluating a wide range of supervised and semi-supervised, traditional and deep ML techniques across four criteria: detection accuracy, time performance, and sensitivity to hyperparameter tuning in both detection accuracy and time performance. The experimental results show that supervised traditional and deep ML techniques fare similarly in terms of their detection accuracy and prediction time on most of the benchmark datasets considered in our study. Moreover, overall, sensitivity analysis to hyperparameter tuning with respect to detection accuracy shows that supervised traditional ML techniques are less sensitive than deep learning techniques. Further, semi-supervised techniques yield significantly worse detection accuracy than supervised techniques.",
      "authors": [
        "Ali, Shan",
        "Boufaied, Chaima",
        "Bianculli, Domenico",
        "Branco, Paula",
        "Briand, Lionel"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2307.16714v5",
        "Other Formats": "https://arxiv.org/format/2307.16714",
        "TeX Source": "https://arxiv.org/src/2307.16714",
        "View PDF": "https://arxiv.org/pdf/2307.16714"
      },
      "subjects": [
        "Software Engineering (cs.SE)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 31 Jul 2023 14:34:33 UTC (149 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 20 May 2024 12:23:02 UTC (285 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Fri, 20 Dec 2024 21:52:14 UTC (454 KB)",
          "link": "/",
          "version": "[v3]"
        },
        {
          "details": "Wed, 16 Apr 2025 20:57:44 UTC (430 KB)",
          "link": "/",
          "version": "[v4]"
        },
        {
          "details": "Mon, 23 Jun 2025 16:53:44 UTC (392 KB)",
          "version": "[v5]"
        }
      ],
      "submitted_date": "2023/07/31",
      "title": "A Comprehensive Study of Machine Learning Techniques for Log-Based Anomaly Detection",
      "tasks": [
        "Anomaly Detection",
        "Deep Learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18270",
      "abstract": "As the deep learning revolution marches on, masked modeling has emerged as a distinctive approach that involves predicting parts of the original data that are proportionally masked during training, and has demonstrated exceptional performance in multiple fields. Magnetic Resonance Imaging (MRI) reconstruction is a critical task in medical imaging that seeks to recover high-quality images from under-sampled k-space data. However, previous MRI reconstruction strategies usually optimized the entire image domain or k-space, without considering the importance of different frequency regions in the k-space This work introduces a diffusion model based on adaptive masks (AMDM), which utilizes the adaptive adjustment of frequency distribution based on k-space data to develop a hybrid masks mechanism that adapts to different k-space inputs. This enables the effective separation of high-frequency and low-frequency components, producing diverse frequency-specific representations. Additionally, the k-space frequency distribution informs the generation of adaptive masks, which, in turn, guide a closed-loop diffusion process. Experimental results verified the ability of this method to learn specific frequency information and thereby improved the quality of MRI reconstruction, providing a flexible framework for optimizing k-space data using masks in the future.",
      "authors": [
        "Cai, Qinrong",
        "Guan, Yu",
        "Chen, Zhibo",
        "Liang, Dong",
        "Fan, Qiuyun",
        "Liu, Qiegen"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.18270",
        "View PDF": "https://arxiv.org/pdf/2506.18270"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 03:54:53 UTC (2,769 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Adaptive Mask-guided K-space Diffusion for Accelerated MRI Reconstruction",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18527",
      "abstract": "Generating multi-view images from human instructions is crucial for 3D content creation. The primary challenges involve maintaining consistency across multiple views and effectively synthesizing shapes and textures under diverse conditions. In this paper, we propose the Multi-View Auto-Regressive (MV-AR) method, which leverages an auto-regressive model to progressively generate consistent multi-view images from arbitrary prompts. Firstly, the next-token-prediction capability of the AR model significantly enhances its effectiveness in facilitating progressive multi-view synthesis. When generating widely-separated views, MV-AR can utilize all its preceding views to extract effective reference information. Subsequently, we propose a unified model that accommodates various prompts via architecture designing and training strategies. To address multiple conditions, we introduce condition injection modules for text, camera pose, image, and shape. To manage multi-modal conditions simultaneously, a progressive training strategy is employed. This strategy initially adopts the text-to-multi-view (t2mv) model as a baseline to enhance the development of a comprehensive X-to-multi-view (X2mv) model through the randomly dropping and combining conditions. Finally, to alleviate the overfitting problem caused by limited high-quality data, we propose the \"Shuffle View\" data augmentation technique, thus significantly expanding the training data by several magnitudes. Experiments demonstrate the performance and versatility of our MV-AR, which consistently generates consistent multi-view images across a range of conditions and performs on par with leading diffusion-based multi-view image generation models. Code and models will be released at https://github.com/MILab-PKU/MVAR.",
      "authors": [
        "Hu, JiaKui",
        "Yang, Yuxiao",
        "Liu, Jialun",
        "Wu, Jinbo",
        "Zhao, Chen",
        "Lu, Yanye"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18527v1",
        "Other Formats": "https://arxiv.org/format/2506.18527",
        "TeX Source": "https://arxiv.org/src/2506.18527",
        "View PDF": "https://arxiv.org/pdf/2506.18527"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 11:28:37 UTC (2,590 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Auto-Regressively Generating Multi-View Consistent Images",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18245",
      "abstract": "Smart contract vulnerability detection remains a major challenge in blockchain security. Existing vulnerability detection methods face two main issues: (1) Existing datasets lack comprehensive coverage and high-quality explanations for preference learning. (2) Large language models (LLMs) often struggle with accurately interpreting specific concepts in smart contract security. Empirical analysis shows that even after continual pre-training (CPT) and supervised fine-tuning (SFT), LLMs may misinterpret the execution order of state changes, resulting in incorrect explanations despite making correct detection decisions. To address these challenges, we propose Smart-LLaMA-DPO based on LLaMA-3.1-8B. We construct a comprehensive dataset covering four major vulnerability types and machine-unauditable vulnerabilities, including precise labels, explanations, and locations for SFT, as well as high-quality and low-quality output pairs for Direct Preference Optimization (DPO). Second, we perform CPT using large-scale smart contract to enhance the LLM's understanding of specific security practices in smart contracts. Futhermore, we conduct SFT with our comprehensive dataset. Finally, we apply DPO, leveraging human feedback and a specially designed loss function that increases the probability of preferred explanations while reducing the likelihood of non-preferred outputs. We evaluate Smart-LLaMA-DPO on four major vulnerability types: reentrancy, timestamp dependence, integer overflow/underflow, and delegatecall, as well as machine-unauditable vulnerabilities. Our method significantly outperforms state-of-the-art baselines, with average improvements of 10.43% in F1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human evaluation confirm that our method generates more correct, thorough, and clear explanations.",
      "authors": [
        "Yu, Lei",
        "Huang, Zhirong",
        "Yuan, Hang",
        "Cheng, Shiqi",
        "Yang, Li",
        "Zhang, Fengjun",
        "Shen, Chenjie",
        "Ma, Jiajia",
        "Zhang, Jingyuan",
        "Lu, Junyi",
        "Zuo, Chun"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18245v1",
        "Other Formats": "https://arxiv.org/format/2506.18245",
        "TeX Source": "https://arxiv.org/src/2506.18245",
        "View PDF": "https://arxiv.org/pdf/2506.18245"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Artificial Intelligence (cs.AI)",
        "Software Engineering (cs.SE)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 02:24:07 UTC (714 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18497",
      "abstract": "Large-scale foundation models, including neural network interatomic potentials (NIPs) in computational materials science, have demonstrated significant potential. However, despite their success in accelerating atomistic simulations, NIPs face challenges in directly predicting electronic properties and often require coupling to higher-scale models or extensive simulations for macroscopic properties. Machine learning (ML) offers alternatives for structure-to-property mapping but faces trade-offs: feature-based methods often lack generalizability, while deep neural networks require significant data and computational power. To address these trade-offs, we introduce HackNIP, a two-stage pipeline that leverages pretrained NIPs. This method first extracts fixed-length feature vectors (embeddings) from NIP foundation models and then uses these embeddings to train shallow ML models for downstream structure-to-property predictions. This study investigates whether such a hybridization approach, by ``hacking\" the NIP, can outperform end-to-end deep neural networks, determines the dataset size at which this transfer learning approach surpasses direct fine-tuning of the NIP, and identifies which NIP embedding depths yield the most informative features. HackNIP is benchmarked on Matbench, evaluated for data efficiency, and tested on diverse tasks including \\textit{ab initio}, experimental, and molecular properties. We also analyze how embedding depth impacts performance. This work demonstrates a hybridization strategy to overcome ML trade-offs in materials science, aiming to democratize high-performance predictive modeling.",
      "authors": [
        "Kim, So Yeon",
        "Park, Yang Jeong",
        "Li, Ju"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18497v1",
        "Other Formats": "https://arxiv.org/format/2506.18497",
        "TeX Source": "https://arxiv.org/src/2506.18497",
        "View PDF": "https://arxiv.org/pdf/2506.18497"
      },
      "subjects": [
        "Materials Science (cond-mat.mtrl-sci)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 10:49:19 UTC (4,496 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Leveraging neural network interatomic potentials for a foundation model of chemistry",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2411.12858",
      "abstract": "Diffusion Models (DMs) benefit from large and diverse datasets for their training. Since this data is often scraped from the Internet without permission from the data owners, this raises concerns about copyright and intellectual property protections. While (illicit) use of data is easily detected for training samples perfectly re-created by a DM at inference time, it is much harder for data owners to verify if their data was used for training when the outputs from the suspect DM are not close replicas. Conceptually, membership inference attacks (MIAs), which detect if a given data point was used during training, present themselves as a suitable tool to address this challenge. However, we demonstrate that existing MIAs are not strong enough to reliably determine the membership of individual images in large, state-of-the-art DMs. To overcome this limitation, we propose CDI, a framework for data owners to identify whether their dataset was used to train a given DM. CDI relies on dataset inference techniques, i.e., instead of using the membership signal from a single data point, CDI leverages the fact that most data owners, such as providers of stock photography, visual media companies, or even individual artists, own datasets with multiple publicly exposed data points which might all be included in the training of a given DM. By selectively aggregating signals from existing MIAs and using new handcrafted methods to extract features for these datasets, feeding them to a scoring model, and applying rigorous statistical testing, CDI allows data owners with as little as 70 data points to identify with a confidence of more than 99% whether their data was used to train a given DM. Thereby, CDI represents a valuable tool for data owners to claim illegitimate use of their copyrighted data. We make the code available at https://github.com/sprintml/copyrighted_data_identification",
      "authors": [
        "Dubi\u0144ski, Jan",
        "Kowalczuk, Antoni",
        "Boenisch, Franziska",
        "Dziedzic, Adam"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2411.12858v3",
        "Other Formats": "https://arxiv.org/format/2411.12858",
        "TeX Source": "https://arxiv.org/src/2411.12858",
        "View PDF": "https://arxiv.org/pdf/2411.12858"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "details": "Tue, 19 Nov 2024 21:02:09 UTC (19,767 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 24 Nov 2024 16:55:16 UTC (19,767 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Mon, 23 Jun 2025 17:31:25 UTC (19,793 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/11/19",
      "title": "CDI: Copyrighted Data Identification in Diffusion Models",
      "repo_urls": [
        "https://github.com/sprintml/copyrighted_data_identification"
      ],
      "conference_url_abs": "http://openaccess.thecvf.com//content/CVPR2025/html/Dubinski_CDI_Copyrighted_Data_Identification_in_Diffusion_Models_CVPR_2025_paper.html",
      "tasks": [],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18237",
      "abstract": "Reinforcement Learning (RL)-based post-training has significantly advanced the complex reasoning capabilities of language models, fostering sophisticated self-reflection processes. However, this ``slow thinking'' paradigm presents a critical challenge to reasoning efficiency: models may expend excessive computation on simple questions and shift reasoning prematurely for complex ones. Previous mechanisms typically rely on static length budgets or predefined rules, lacking the adaptability for varying question complexities and models' evolving capabilities. To this end, we propose AdapThink, an adaptive post-training framework designed to induce more efficient thinking while maintaining the performance of reasoning language models. Specifically, AdapThink incorporates two key mechanisms: 1) A group-relative reward function that leverages model confidence and response's characteristic to dynamically adjust the preference of reflection-related transition words without resorting to a fixed length preference. 2) A diversity-aware sampling mechanism that balances the training group's solution accuracy with reasoning diversity via an entropy-guided score. Experiments on several mathematical reasoning datasets with DeepSeek-distilled models demonstrate AdapThink's advantages in enabling adaptive reasoning patterns and mitigating the inefficiencies.",
      "authors": [
        "Wan, Xu",
        "Wang, Wei",
        "Xu, Wenyue",
        "Yin, Wotao",
        "Song, Jie",
        "Sun, Mingyang"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18237v1",
        "Other Formats": "https://arxiv.org/format/2506.18237",
        "TeX Source": "https://arxiv.org/src/2506.18237",
        "View PDF": "https://arxiv.org/pdf/2506.18237"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 02:06:04 UTC (4,962 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "AdapThink: Adaptive Thinking Preferences for Reasoning Language Model",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18398",
      "abstract": "Rug pull scams have emerged as a persistent threat to cryptocurrency, causing significant financial losses. A typical scenario involves scammers deploying honeypot contracts to attract investments, restricting token sales, and draining the funds, which leaves investors with worthless tokens. Current methods either rely on predefined patterns to detect code risks or utilize statistical transaction data to train detection models. However, real-world Rug Pull schemes often involve a complex interplay between malicious code and suspicious transaction behaviors. These methods, which solely focus on one aspect, fall short in detecting such schemes effectively. In this paper, we propose RPhunter, a novel technique that integrates code and transaction for Rug Pull detection. First, RPhunter establishes declarative rules and performs flow analysis to extract code risk information, further constructing a semantic risk code graph (SRCG). Meanwhile, to leverage transaction information, RPhunter formulates dynamic token transaction activities as a token flow behavior graph (TFBG) in which nodes and edges are characterized from network structure and market manipulation perspectives. Finally, RPhunter employs graph neural networks to extract complementary features from SRCG and TFBG, integrating them through an attention fusion model to enhance the detection of Rug Pull. We manually analyzed 645 Rug Pull incidents from code and transaction aspects and constructed a ground-truth dataset. We evaluated RPhunter on our dataset, achieving a precision of 95.3%, a recall of 93.8% and an F1 score of 94.5%, which highlights superior performance compared to existing state-of-the-art methods. Furthermore, when applied to the real-world scenarios, RPhunter has identified 4801 Rug Pull tokens, achieving a precision of 91%.",
      "authors": [
        "Wu, Hao",
        "Wang, Haijun",
        "Li, Shangwang",
        "Wu, Yin",
        "Fan, Ming",
        "Jin, Wuxia",
        "Zhao, Yitao",
        "Liu, Ting"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18398v1",
        "Other Formats": "https://arxiv.org/format/2506.18398",
        "TeX Source": "https://arxiv.org/src/2506.18398",
        "View PDF": "https://arxiv.org/pdf/2506.18398"
      },
      "subjects": [
        "Software Engineering (cs.SE)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 08:34:15 UTC (4,001 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Your Token Becomes Worthless: Unveiling Rug Pull Schemes in Crypto Token via Code-and-Transaction Fusion Analysis",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18580",
      "abstract": "Using 3D point clouds in odometry estimation in robotics often requires finding a set of correspondences between points in subsequent scans. While there are established methods for point clouds of sufficient quality, state-of-the-art still struggles when this quality drops. Thus, this paper presents a novel learning-based framework for predicting robust point correspondences between pairs of noisy, sparse and unstructured 3D point clouds from a light-weight, low-power, inexpensive, consumer-grade System-on-Chip (SoC) Frequency Modulated Continuous Wave (FMCW) radar sensor. Our network is based on the transformer architecture which allows leveraging the attention mechanism to discover pairs of points in consecutive scans with the greatest mutual affinity. The proposed network is trained in a self-supervised way using set-based multi-label classification cross-entropy loss, where the ground-truth set of matches is found by solving the Linear Sum Assignment (LSA) optimization problem, which avoids tedious hand annotation of the training data. Additionally, posing the loss calculation as multi-label classification permits supervising on point correspondences directly instead of on odometry error, which is not feasible for sparse and noisy data from the SoC radar we use. We evaluate our method with an open-source state-of-the-art Radar-Inertial Odometry (RIO) framework in real-world Unmanned Aerial Vehicle (UAV) flights and with the widely used public Coloradar dataset. Evaluation shows that the proposed method improves the position estimation accuracy by over 14 % and 19 % on average, respectively. The open source code and datasets can be found here: https://github.com/aau-cns/radar_transformer.",
      "authors": [
        "Michalczyk, Jan",
        "Weiss, Stephan",
        "Steinbrener, Jan"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18580v1",
        "Other Formats": "https://arxiv.org/format/2506.18580",
        "TeX Source": "https://arxiv.org/src/2506.18580",
        "View PDF": "https://arxiv.org/pdf/2506.18580"
      },
      "subjects": [
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 12:32:10 UTC (5,171 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Learning Point Correspondences In Radar 3D Point Clouds For Radar-Inertial Odometry",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2501.17690",
      "abstract": "We introduce a novel segmentation-aware joint training framework called generative reinforcement network (GRN) that integrates segmentation loss feedback to optimize both image generation and segmentation performance in a single stage. An image enhancement technique called segmentation-guided enhancement (SGE) is also developed, where the generator produces images tailored specifically for the segmentation model. Two variants of GRN were also developed, including GRN for sample-efficient learning (GRN-SEL) and GRN for semi-supervised learning (GRN-SSL). GRN's performance was evaluated using a dataset of 69 fully annotated 3D ultrasound scans from 29 subjects. The annotations included six anatomical structures: dermis, superficial fat, superficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and muscle. Our results show that GRN-SEL with SGE reduces labeling efforts by up to 70% while achieving a 1.98% improvement in the Dice Similarity Coefficient (DSC) compared to models trained on fully labeled datasets. GRN-SEL alone reduces labeling efforts by 60%, GRN-SSL with SGE decreases labeling requirements by 70%, and GRN-SSL alone by 60%, all while maintaining performance comparable to fully supervised models. These findings suggest the effectiveness of the GRN framework in optimizing segmentation performance with significantly less labeled data, offering a scalable and efficient solution for ultrasound image analysis and reducing the burdens associated with data annotation.",
      "authors": [
        "Zeng, Zixue",
        "Zhao, Xiaoyan",
        "Cartier, Matthew",
        "Yu, Tong",
        "Wang, Jing",
        "Meng, Xin",
        "Sheng, Zhiyu",
        "Satarpour, Maryam",
        "Cormack, John M",
        "Bean, Allison",
        "Nussbaum, Ryan",
        "Maurer, Maya",
        "Landis-Walkenhorst, Emily",
        "Kumbhare, Dinesh",
        "Kim, Kang",
        "Wasan, Ajay",
        "Pu, Jiantao"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "Other Formats": "https://arxiv.org/format/2501.17690",
        "View PDF": "https://arxiv.org/pdf/2501.17690"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 29 Jan 2025 14:58:48 UTC (1,095 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Wed, 30 Apr 2025 14:19:58 UTC (1,150 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Mon, 23 Jun 2025 17:08:10 UTC (1,450 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2025/01/29",
      "title": "Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP) Assessment",
      "repo_urls": [
        "https://github.com/Francisdadada/GRN"
      ],
      "tasks": [
        "Image Enhancement",
        "Image Generation",
        "Segmentation"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18544",
      "abstract": "Recently, detecting logical anomalies is becoming a more challenging task compared to detecting structural ones. Existing encoder decoder based methods typically compress inputs into low-dimensional bottlenecks on the assumption that the compression process can effectively suppress the transmission of logical anomalies to the decoder. However, logical anomalies present a particular difficulty because, while their local features often resemble normal semantics, their global semantics deviate significantly from normal patterns. Thanks to the generalisation capabilities inherent in neural networks, these abnormal semantic features can propagate through low-dimensional bottlenecks. This ultimately allows the decoder to reconstruct anomalous images with misleading fidelity. To tackle the above challenge, we propose a novel normality prior guided multi-semantic fusion network for unsupervised anomaly detection. Instead of feeding the compressed bottlenecks to the decoder directly, we introduce the multi-semantic features of normal samples into the reconstruction process. To this end, we first extract abstract global semantics of normal cases by a pre-trained vision-language network, then the learnable semantic codebooks are constructed to store representative feature vectors of normal samples by vector quantisation. Finally, the above multi-semantic features are fused and employed as input to the decoder to guide the reconstruction of anomalies to approximate normality. Extensive experiments are conducted to validate the effectiveness of our proposed method, and it achieves the SOTA performance on the MVTec LOCO AD dataset with improvements of 5.7% in pixel-sPRO and 2.6% in image-AUROC. The source code is available at https://github.com/Xmh-L/NPGMF.",
      "authors": [
        "Xu, Muhao",
        "Zhou, Xueying",
        "Gao, Xizhan",
        "Song, Weiye",
        "Feng, Guang",
        "Niu, Sijie"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18544v1",
        "Other Formats": "https://arxiv.org/format/2506.18544",
        "TeX Source": "https://arxiv.org/src/2506.18544",
        "View PDF": "https://arxiv.org/pdf/2506.18544"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 11:54:15 UTC (1,665 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Normality Prior Guided Multi-Semantic Fusion Network for Unsupervised Image Anomaly Detection",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18777",
      "abstract": "Training large language models (LLMs) on source code significantly enhances their general-purpose reasoning abilities, but the mechanisms underlying this generalisation are poorly understood. In this paper, we propose Programming by Backprop (PBB) as a potential driver of this effect - teaching a model to evaluate a program for inputs by training on its source code alone, without ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of programs representing simple maths problems and algorithms: one with source code and I/O examples (w/ IO), the other with source code only (w/o IO). We find evidence that LLMs have some ability to evaluate w/o IO programs for inputs in a range of experimental settings, and make several observations. Firstly, PBB works significantly better when programs are provided as code rather than semantically equivalent language descriptions. Secondly, LLMs can produce outputs for w/o IO programs directly, by implicitly evaluating the program within the forward pass, and more reliably when stepping through the program in-context via chain-of-thought. We further show that PBB leads to more robust evaluation of programs across inputs than training on I/O pairs drawn from a distribution that mirrors naturally occurring data. Our findings suggest a mechanism for enhanced reasoning through code training: it allows LLMs to internalise reusable algorithmic abstractions. Significant scope remains for future work to enable LLMs to more effectively learn from symbolic procedures, and progress in this direction opens other avenues like model alignment by training on formal constitutional principles.",
      "authors": [
        "Cook, Jonathan",
        "Sapora, Silvia",
        "Ahmadian, Arash",
        "Khan, Akbir",
        "Rocktaschel, Tim",
        "Foerster, Jakob",
        "Ruis, Laura"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18777v1",
        "Other Formats": "https://arxiv.org/format/2506.18777",
        "TeX Source": "https://arxiv.org/src/2506.18777",
        "View PDF": "https://arxiv.org/pdf/2506.18777"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 15:45:44 UTC (574 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18701",
      "abstract": "We introduce Matrix-Game, an interactive world foundation model for controllable game world generation. Matrix-Game is trained using a two-stage pipeline that first performs large-scale unlabeled pretraining for environment understanding, followed by action-labeled training for interactive video generation. To support this, we curate Matrix-Game-MC, a comprehensive Minecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips and over 1,000 hours of high-quality labeled clips with fine-grained keyboard and mouse action annotations. Our model adopts a controllable image-to-world generation paradigm, conditioned on a reference image, motion context, and user actions. With over 17 billion parameters, Matrix-Game enables precise control over character actions and camera movements, while maintaining high visual quality and temporal coherence. To evaluate performance, we develop GameWorld Score, a unified benchmark measuring visual quality, temporal quality, action controllability, and physical rule understanding for Minecraft world generation. Extensive experiments show that Matrix-Game consistently outperforms prior open-source Minecraft world models (including Oasis and MineWorld) across all metrics, with particularly strong gains in controllability and physical consistency. Double-blind human evaluations further confirm the superiority of Matrix-Game, highlighting its ability to generate perceptually realistic and precisely controllable videos across diverse game scenarios. To facilitate future research on interactive image-to-world generation, we will open-source the Matrix-Game model weights and the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.",
      "authors": [
        "Zhang, Yifan",
        "Peng, Chunli",
        "Wang, Boyang",
        "Wang, Puyi",
        "Zhu, Qingcheng",
        "Kang, Fei",
        "Jiang, Biao",
        "Gao, Zedong",
        "Li, Eric",
        "Liu, Yang",
        "Zhou, Yahui"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18701v1",
        "Other Formats": "https://arxiv.org/format/2506.18701",
        "TeX Source": "https://arxiv.org/src/2506.18701",
        "View PDF": "https://arxiv.org/pdf/2506.18701"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 14:40:49 UTC (18,338 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Matrix-Game: Interactive World Foundation Model",
      "models": [
        {
          "model_path": "Skywork/Matrix-Game",
          "downloads": "60",
          "likes": "26",
          "trending_score": "0.0",
          "link": "https://huggingface.co/Skywork/Matrix-Game"
        }
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.20674",
      "abstract": "Humans ponder before articulating complex sentence elements, enabling deeper cognitive processing through focused effort. In this work, we introduce this pondering process into language models by repeatedly invoking the forward process within a single token generation step. During pondering, instead of generating an actual token sampled from the prediction distribution, the model ponders by yielding a weighted sum of all token embeddings according to the predicted token distribution. The generated embedding is then fed back as input for another forward pass. We show that the model can learn to ponder in this way through self-supervised learning, without any human annotations. Experiments across three widely used open-source architectures-GPT-2, Pythia, and LLaMA-and extensive downstream task evaluations demonstrate the effectiveness and generality of our method. For language modeling tasks, pondering language models achieve performance comparable to vanilla models with twice the number of parameters. On 9 downstream benchmarks, our pondering-enhanced Pythia models significantly outperform the official Pythia models. Notably, PonderingPythia-2.8B surpasses Pythia-6.9B, and PonderingPythia-1B is comparable to TinyLlama-1.1B, which is trained on 10 times more data. The code is available at https://github.com/LUMIA-Group/PonderingLM.",
      "authors": [
        "Zeng, Boyi",
        "Song, Shixiang",
        "Huang, Siyuan",
        "Wang, Yixuan",
        "Li, He",
        "He, Ziwei",
        "Wang, Xinbing",
        "Li, Zhiyu",
        "Lin, Zhouhan"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2505.20674v2",
        "Other Formats": "https://arxiv.org/format/2505.20674",
        "TeX Source": "https://arxiv.org/src/2505.20674",
        "View PDF": "https://arxiv.org/pdf/2505.20674"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Tue, 27 May 2025 03:47:33 UTC (252 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 13:48:37 UTC (252 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/27",
      "title": "Pretraining Language Models to Ponder in Continuous Space",
      "repo_urls": [
        "https://github.com/lumia-group/ponderinglm"
      ],
      "tasks": [
        "Language Modeling",
        "Language Modelling",
        "Self-Supervised Learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18516",
      "abstract": "Adversarial examples are small and often imperceptible perturbations crafted to fool machine learning models. These attacks seriously threaten the reliability of deep neural networks, especially in security-sensitive domains. Evasion attacks, a form of adversarial attack where input is modified at test time to cause misclassification, are particularly insidious due to their transferability: adversarial examples crafted against one model often fool other models as well. This property, known as adversarial transferability, complicates defense strategies since it enables black-box attacks to succeed without direct access to the victim model. While adversarial training is one of the most widely adopted defense mechanisms, its effectiveness is typically evaluated on a narrow and homogeneous population of models. This limitation hinders the generalizability of empirical findings and restricts practical adoption. In this work, we introduce DUMBer, an attack framework built on the foundation of the DUMB (Dataset soUrces, Model architecture, and Balance) methodology, to systematically evaluate the resilience of adversarially trained models. Our testbed spans multiple adversarial training techniques evaluated across three diverse computer vision tasks, using a heterogeneous population of uniquely trained models to reflect real-world deployment variability. Our experimental pipeline comprises over 130k evaluations spanning 13 state-of-the-art attack algorithms, allowing us to capture nuanced behaviors of adversarial training under varying threat models and dataset conditions. Our findings offer practical, actionable insights for AI practitioners, identifying which defenses are most effective based on the model, dataset, and attacker setup.",
      "authors": [
        "Marchiori, Francesco",
        "Alecci, Marco",
        "Pajola, Luca",
        "Conti, Mauro"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18516v1",
        "Other Formats": "https://arxiv.org/format/2506.18516",
        "TeX Source": "https://arxiv.org/src/2506.18516",
        "View PDF": "https://arxiv.org/pdf/2506.18516"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 11:16:21 UTC (211 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "DUMB and DUMBer: Is Adversarial Training Worth It in the Real World?",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18668",
      "abstract": "Pretraining on large-scale, in-domain datasets grants histopathology foundation models (FM) the ability to learn task-agnostic data representations, enhancing transfer learning on downstream tasks. In computational pathology, automated whole slide image analysis requires multiple instance learning (MIL) frameworks due to the gigapixel scale of the slides. The diversity among histopathology FMs has highlighted the need to design real-world challenges for evaluating their effectiveness. To bridge this gap, our work presents a novel benchmark for evaluating histopathology FMs as patch-level feature extractors within a MIL classification framework. For that purpose, we leverage the AI4SkIN dataset, a multi-center cohort encompassing slides with challenging cutaneous spindle cell neoplasm subtypes. We also define the Foundation Model - Silhouette Index (FM-SI), a novel metric to measure model consistency against distribution shifts. Our experimentation shows that extracting less biased features enhances classification performance, especially in similarity-based MIL classifiers.",
      "authors": [
        "Meseguer, Pablo",
        "del Amor, Roc\u00edo",
        "Naranjo, Valery"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18668v1",
        "Other Formats": "https://arxiv.org/format/2506.18668",
        "TeX Source": "https://arxiv.org/src/2506.18668",
        "View PDF": "https://arxiv.org/pdf/2506.18668"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 14:12:16 UTC (354 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Benchmarking histopathology foundation models in a multi-center dataset for skin cancer subtyping",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18883",
      "abstract": "This paper presents a computational model for universal video temporal grounding, which accurately localizes temporal moments in videos based on natural language queries (e.g., questions or descriptions). Unlike existing methods that are often limited to specific video domains or durations, we propose UniTime, a robust and universal video grounding model leveraging the strong vision-language understanding capabilities of generative Multi-modal Large Language Models (MLLMs). Our model effectively handles videos of diverse views, genres, and lengths while comprehending complex language queries. The key contributions include: (i) We consider steering strong MLLMs for temporal grounding in videos. To enable precise timestamp outputs, we incorporate temporal information by interleaving timestamp tokens with video tokens. (ii) By training the model to handle videos with different input granularities through adaptive frame scaling, our approach achieves robust temporal grounding for both short and long videos. (iii) Comprehensive experiments show that UniTime outperforms state-of-the-art approaches in both zero-shot and dataset-specific finetuned settings across five public temporal grounding benchmarks. (iv) When employed as a preliminary moment retriever for long-form video question-answering (VideoQA), UniTime significantly improves VideoQA accuracy, highlighting its value for complex video understanding tasks.",
      "authors": [
        "Li, Zeqian",
        "Di, Shangzhe",
        "Zhai, Zhonghua",
        "Huang, Weilin",
        "Wang, Yanfeng",
        "Xie, Weidi"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18883v1",
        "Other Formats": "https://arxiv.org/format/2506.18883",
        "TeX Source": "https://arxiv.org/src/2506.18883",
        "View PDF": "https://arxiv.org/pdf/2506.18883"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 17:53:18 UTC (2,043 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Universal Video Temporal Grounding with Generative Multi-modal Large Language Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.12754",
      "abstract": "Asynchronous federated learning (AFL) accelerates training by eliminating the need to wait for stragglers, but its asynchronous nature introduces gradient staleness, where outdated gradients degrade performance. Existing solutions address this issue with gradient buffers, forming a semi-asynchronous framework. However, this approach struggles when buffers accumulate numerous stale gradients, as blindly aggregating all gradients can harm training. To address this, we propose AFBS (Asynchronous FL Buffer Selection), the first algorithm to perform gradient selection within buffers while ensuring privacy protection. Specifically, the client sends the random projection encrypted label distribution matrix before training, and the server performs client clustering based on it. During training, server scores and selects gradients within each cluster based on their informational value, discarding low-value gradients to enhance semi-asynchronous federated learning. Extensive experiments in highly heterogeneous system and data environments demonstrate AFBS's superior performance compared to state-of-the-art methods. Notably, on the most challenging task, CIFAR-100, AFBS improves accuracy by up to 4.8% over the previous best algorithm and reduces the time to reach target accuracy by 75%.",
      "authors": [
        "Lu, Chaoyi",
        "Sun, Yiding",
        "Chen, Jinqian",
        "Yang, Zhichuan",
        "Pan, Jiangming",
        "Zhu, Jihua"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.12754v2",
        "Other Formats": "https://arxiv.org/format/2506.12754",
        "TeX Source": "https://arxiv.org/src/2506.12754",
        "View PDF": "https://arxiv.org/pdf/2506.12754"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 15 Jun 2025 07:42:46 UTC (1,546 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 05:27:00 UTC (1,520 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/06/15",
      "title": "AFBS:Buffer Gradient Selection in Semi-asynchronous Federated Learning",
      "tasks": [
        "Federated Learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18314",
      "abstract": "Existing foundation models for neuroimaging are often prohibitively large and data-intensive. We introduce BrainSymphony, a lightweight, parameter-efficient foundation model that achieves state-of-the-art performance while being pre-trained on significantly smaller public datasets. BrainSymphony's strong multimodal architecture processes functional MRI data through parallel spatial and temporal transformer streams, which are then efficiently distilled into a unified representation by a Perceiver module. Concurrently, it models structural connectivity from diffusion MRI using a novel signed graph transformer to encode the brain's anatomical structure. These powerful, modality-specific representations are then integrated via an adaptive fusion gate. Despite its compact design, our model consistently outperforms larger models on a diverse range of downstream benchmarks, including classification, prediction, and unsupervised network identification tasks. Furthermore, our model revealed novel insights into brain dynamics using attention maps on a unique external psilocybin neuroimaging dataset (pre- and post-administration). BrainSymphony establishes that architecturally-aware, multimodal models can surpass their larger counterparts, paving the way for more accessible and powerful research in computational neuroscience.",
      "authors": [
        "Khajehnejad, Moein",
        "Habibollahi, Forough",
        "Razi, Adeel"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.18314",
        "TeX Source": "https://arxiv.org/src/2506.18314",
        "View PDF": "https://arxiv.org/pdf/2506.18314"
      },
      "subjects": [
        "Quantitative Methods (q-bio.QM)",
        "Machine Learning (cs.LG)",
        "Neurons and Cognition (q-bio.NC)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 06:00:21 UTC (25,435 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "BrainSymphony: A Transformer-Driven Fusion of fMRI Time Series and Structural Connectivity",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18812",
      "abstract": "Physics-informed deep learning has achieved remarkable progress by embedding geometric priors, such as Hamiltonian symmetries and variational principles, into neural networks, enabling structure-preserving models that extrapolate with high accuracy. However, in systems with dissipation and holonomic constraints, ubiquitous in legged locomotion and multibody robotics, the canonical symplectic form becomes degenerate, undermining the very invariants that guarantee stability and long-term prediction. In this work, we tackle this foundational limitation by introducing Presymplectification Networks (PSNs), the first framework to learn the symplectification lift via Dirac structures, restoring a non-degenerate symplectic geometry by embedding constrained systems into a higher-dimensional manifold. Our architecture combines a recurrent encoder with a flow-matching objective to learn the augmented phase-space dynamics end-to-end. We then attach a lightweight Symplectic Network (SympNet) to forecast constrained trajectories while preserving energy, momentum, and constraint satisfaction. We demonstrate our method on the dynamics of the ANYmal quadruped robot, a challenging contact-rich, multibody system. To the best of our knowledge, this is the first framework that effectively bridges the gap between constrained, dissipative mechanical systems and symplectic learning, unlocking a whole new class of geometric machine learning models, grounded in first principles yet adaptable from data.",
      "authors": [
        "Papatheodorou, Aristotelis",
        "Vaidhyanathan, Pranav",
        "Ares, Natalia",
        "Havoutis, Ioannis"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18812v1",
        "Other Formats": "https://arxiv.org/format/2506.18812",
        "TeX Source": "https://arxiv.org/src/2506.18812",
        "View PDF": "https://arxiv.org/pdf/2506.18812"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 16:23:37 UTC (6,017 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Learning Physical Systems: Symplectification via Gauge Fixing in Dirac Structures",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.06502",
      "abstract": "Machine Learning, particularly Generative Adversarial Networks (GANs), has revolutionised Super Resolution (SR). However, generated images often lack physical meaningfulness, which is essential for scientific applications. Our approach, PC-SRGAN, enhances image resolution while ensuring physical consistency for interpretable simulations. PC-SRGAN significantly improves both the Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure compared to conventional methods, even with limited training data (e.g., only 13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments physically meaningful machine learning, incorporating numerically justified time integrators and advanced quality metrics. These advancements promise reliable and causal machine-learning models in scientific domains. A significant advantage of PC-SRGAN over conventional SR techniques is its physical consistency, which makes it a viable surrogate model for time-dependent problems. PC-SRGAN advances scientific machine learning, offering improved accuracy and efficiency for image processing, enhanced process understanding, and broader applications to scientific research. We publicly release the complete source code at https://github.com/hasan-rakibul/PC-SRGAN.",
      "authors": [
        "Hasan, Md Rakibul",
        "Behnoudfar, Pouria",
        "MacKinlay, Dan",
        "Poulet, Thomas"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2505.06502v2",
        "Other Formats": "https://arxiv.org/format/2505.06502",
        "TeX Source": "https://arxiv.org/src/2505.06502",
        "View PDF": "https://arxiv.org/pdf/2505.06502"
      },
      "subjects": [
        "Image and Video Processing (eess.IV)",
        "Computational Engineering, Finance, and Science (cs.CE)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 10 May 2025 04:05:00 UTC (13,569 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 14:50:11 UTC (18,029 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/10",
      "title": "PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations",
      "tasks": [
        "Generative Adversarial Network",
        "Super-Resolution"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2410.11382",
      "abstract": "Recent advances in operator learning have produced two distinct approaches for solving partial differential equations (PDEs): attention-based methods offering point-level adaptability but lacking spectral constraints, and spectral-based methods providing domain-level continuity priors but limited in local flexibility. This dichotomy has hindered the development of PDE solvers with both strong flexibility and generalization capability. This work introduces Holistic Physics Mixer (HPM), a simple framework that bridges this gap by integrating spectral and physical information in a unified space. HPM unifies both approaches as special cases while enabling more powerful spectral-physical interactions beyond either method alone. This enables HPM to inherit both the strong generalization of spectral methods and the flexibility of attention mechanisms while avoiding their respective limitations. Through extensive experiments across diverse PDE problems, we demonstrate that HPM consistently outperforms state-of-the-art methods in both accuracy and computational efficiency, while maintaining strong generalization capabilities with limited training data and excellent zero-shot performance on unseen resolutions.",
      "authors": [
        "Yue, Xihang",
        "Yang, Yi",
        "Zhu, Linchao"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2410.11382v2",
        "Other Formats": "https://arxiv.org/format/2410.11382",
        "TeX Source": "https://arxiv.org/src/2410.11382",
        "View PDF": "https://arxiv.org/pdf/2410.11382"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Numerical Analysis (math.NA)"
      ],
      "submission_historys": [
        {
          "details": "Tue, 15 Oct 2024 08:19:39 UTC (5,983 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 08:07:36 UTC (6,287 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/10/15",
      "title": "Holistic Physics Solver: Learning PDEs in a Unified Spectral-Physical Space",
      "repo_urls": [
        "https://github.com/yuexihang/pcsm"
      ],
      "tasks": [
        "Operator learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18720",
      "abstract": "Synthetic contrast enhancement offers fast image acquisition and eliminates the need for intravenous injection of contrast agent. This is particularly beneficial for breast imaging, where long acquisition times and high cost are significantly limiting the applicability of magnetic resonance imaging (MRI) as a widespread screening modality. Recent studies have demonstrated the feasibility of synthetic contrast generation. However, current state-of-the-art (SOTA) methods lack sufficient measures for consistent temporal evolution. Neural cellular automata (NCA) offer a robust and lightweight architecture to model evolving patterns between neighboring cells or pixels. In this work we introduce TeNCA (Temporal Neural Cellular Automata), which extends and further refines NCAs to effectively model temporally sparse, non-uniformly sampled imaging data. To achieve this, we advance the training strategy by enabling adaptive loss computation and define the iterative nature of the method to resemble a physical progression in time. This conditions the model to learn a physiologically plausible evolution of contrast enhancement. We rigorously train and test TeNCA on a diverse breast MRI dataset and demonstrate its effectiveness, surpassing the performance of existing methods in generation of images that align with ground truth post-contrast sequences.",
      "authors": [
        "Lang, Daniel M.",
        "Osuala, Richard",
        "Spieker, Veronika",
        "Lekadir, Karim",
        "Braren, Rickmer",
        "Schnabel, Julia A."
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18720v1",
        "Other Formats": "https://arxiv.org/format/2506.18720",
        "TeX Source": "https://arxiv.org/src/2506.18720",
        "View PDF": "https://arxiv.org/pdf/2506.18720"
      },
      "subjects": [
        "Image and Video Processing (eess.IV)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 14:56:45 UTC (2,814 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Temporal Neural Cellular Automata: Application to modeling of contrast enhancement in breast MRI",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18851",
      "abstract": "Subject-to-video generation has witnessed substantial progress in recent years. However, existing models still face significant challenges in faithfully following textual instructions. This limitation, commonly known as the copy-paste problem, arises from the widely used in-pair training paradigm. This approach inherently entangles subject identity with background and contextual attributes by sampling reference images from the same scene as the target video. To address this issue, we introduce \\textbf{Phantom-Data, the first general-purpose cross-pair subject-to-video consistency dataset}, containing approximately one million identity-consistent pairs across diverse categories. Our dataset is constructed via a three-stage pipeline: (1) a general and input-aligned subject detection module, (2) large-scale cross-context subject retrieval from more than 53 million videos and 3 billion images, and (3) prior-guided identity verification to ensure visual consistency under contextual variation. Comprehensive experiments show that training with Phantom-Data significantly improves prompt alignment and visual quality while preserving identity consistency on par with in-pair baselines.",
      "authors": [
        "Chen, Zhuowei",
        "Li, Bingchuan",
        "Ma, Tianxiang",
        "Liu, Lijie",
        "Liu, Mingcong",
        "Zhang, Yi",
        "Li, Gen",
        "Li, Xinghui",
        "Zhou, Siyu",
        "He, Qian",
        "Wu, Xinglong"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18851v1",
        "Other Formats": "https://arxiv.org/format/2506.18851",
        "TeX Source": "https://arxiv.org/src/2506.18851",
        "View PDF": "https://arxiv.org/pdf/2506.18851"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 17:11:56 UTC (12,834 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Phantom-Data : Towards a General Subject-Consistent Video Generation Dataset",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18901",
      "abstract": "We introduce RealPlay, a neural network-based real-world game engine that enables interactive video generation from user control signals. Unlike prior works focused on game-style visuals, RealPlay aims to produce photorealistic, temporally consistent video sequences that resemble real-world footage. It operates in an interactive loop: users observe a generated scene, issue a control command, and receive a short video chunk in response. To enable such realistic and responsive generation, we address key challenges including iterative chunk-wise prediction for low-latency feedback, temporal consistency across iterations, and accurate control response. RealPlay is trained on a combination of labeled game data and unlabeled real-world videos, without requiring real-world action annotations. Notably, we observe two forms of generalization: (1) control transfer-RealPlay effectively maps control signals from virtual to real-world scenarios; and (2) entity transfer-although training labels originate solely from a car racing game, RealPlay generalizes to control diverse real-world entities, including bicycles and pedestrians, beyond vehicles. Project page can be found: https://wenqsun.github.io/RealPlay/",
      "authors": [
        "Sun, Wenqiang",
        "Wei, Fangyun",
        "Zhao, Jinjing",
        "Chen, Xi",
        "Chen, Zilong",
        "Zhang, Hongyang",
        "Zhang, Jun",
        "Lu, Yan"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18901v1",
        "Other Formats": "https://arxiv.org/format/2506.18901",
        "TeX Source": "https://arxiv.org/src/2506.18901",
        "View PDF": "https://arxiv.org/pdf/2506.18901"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 17:59:53 UTC (12,852 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "From Virtual Games to Real-World Play",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18640",
      "abstract": "Federated learning (FL) has emerged as a groundbreaking paradigm in machine learning (ML), offering privacy-preserving collaborative model training across diverse datasets. Despite its promise, FL faces significant hurdles in non-identically and independently distributed (non-IID) data scenarios, where most existing methods often struggle with data heterogeneity and lack robustness in performance. This paper introduces Federated Loss Exploration (FedLEx), an innovative approach specifically designed to tackle these challenges. FedLEx distinctively addresses the shortcomings of existing FL methods in non-IID settings by optimizing its learning behavior for scenarios in which assumptions about data heterogeneity are impractical or unknown. It employs a federated loss exploration technique, where clients contribute to a global guidance matrix by calculating gradient deviations for model parameters. This matrix serves as a strategic compass to guide clients' gradient updates in subsequent FL rounds, thereby fostering optimal parameter updates for the global model. FedLEx effectively navigates the complex loss surfaces inherent in non-IID data, enhancing knowledge transfer in an efficient manner, since only a small number of epochs and small amount of data are required to build a strong global guidance matrix that can achieve model convergence without the need for additional data sharing or data distribution statics in a large client scenario. Our extensive experiments with state-of-the art FL algorithms demonstrate significant improvements in performance, particularly under realistic non-IID conditions, thus highlighting FedLEx's potential to overcome critical barriers in diverse FL applications.",
      "authors": [
        "Intern\u00f2, Christian",
        "Olhofer, Markus",
        "Jin, Yaochu",
        "Hammer, Barbara"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18640v1",
        "Other Formats": "https://arxiv.org/format/2506.18640",
        "TeX Source": "https://arxiv.org/src/2506.18640",
        "View PDF": "https://arxiv.org/pdf/2506.18640"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 13:42:07 UTC (1,439 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Federated Loss Exploration for Improved Convergence on Non-IID Data",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18791",
      "abstract": "The evolution of Vision Transformers has led to their widespread adaptation to different domains. Despite large-scale success, there remain significant challenges including their reliance on extensive computational and memory resources for pre-training on huge datasets as well as difficulties in task-specific transfer learning. These limitations coupled with energy inefficiencies mainly arise due to the computation-intensive self-attention mechanism. To address these issues, we propose a novel Super-Pixel Based Patch Pooling (SPPP) technique that generates context-aware, semantically rich, patch embeddings to effectively reduce the architectural complexity and improve efficiency. Additionally, we introduce the Light Latent Attention (LLA) module in our pipeline by integrating latent tokens into the attention mechanism allowing cross-attention operations to significantly reduce the time and space complexity of the attention module. By leveraging the data-intuitive patch embeddings coupled with dynamic positional encodings, our approach adaptively modulates the cross-attention process to focus on informative regions while maintaining the global semantic structure. This targeted attention improves training efficiency and accelerates convergence. Notably, the SPPP module is lightweight and can be easily integrated into existing transformer architectures. Extensive experiments demonstrate that our proposed architecture provides significant improvements in terms of computational efficiency while achieving comparable results with the state-of-the-art approaches, highlighting its potential for energy-efficient transformers suitable for edge deployment. (The code is available on our GitHub repository: https://github.com/zser092/Focused-Attention-ViT).",
      "authors": [
        "Gaurav, Suyash",
        "Humayun, Muhammad Farhan",
        "Heikkonen, Jukka",
        "Chaudhary, Jatin"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18791v1",
        "Other Formats": "https://arxiv.org/format/2506.18791",
        "TeX Source": "https://arxiv.org/src/2506.18791",
        "View PDF": "https://arxiv.org/pdf/2506.18791"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 16:00:57 UTC (2,502 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Focus Your Attention: Towards Data-Intuitive Lightweight Vision Transformers",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2504.16359",
      "abstract": "This work introduces \\textbf{VideoMark}, a distortion-free robust watermarking framework for video diffusion models. As diffusion models excel in generating realistic videos, reliable content attribution is increasingly critical. However, existing video watermarking methods often introduce distortion by altering the initial distribution of diffusion variables and are vulnerable to temporal attacks, such as frame deletion, due to variable video lengths. VideoMark addresses these challenges by employing a \\textbf{pure pseudorandom initialization} to embed watermarks, avoiding distortion while ensuring uniform noise distribution in the latent space to preserve generation quality. To enhance robustness, we adopt a frame-wise watermarking strategy with pseudorandom error correction (PRC) codes, using a fixed watermark sequence with randomly selected starting indices for each video. For watermark extraction, we propose a Temporal Matching Module (TMM) that leverages edit distance to align decoded messages with the original watermark sequence, ensuring resilience against temporal attacks. Experimental results show that VideoMark achieves higher decoding accuracy than existing methods while maintaining video quality comparable to watermark-free generation. The watermark remains imperceptible to attackers without the secret key, offering superior invisibility compared to other frameworks. VideoMark provides a practical, training-free solution for content attribution in diffusion-based video generation. Code and data are available at \\href{https://github.com/KYRIE-LI11/VideoMark}{https://github.com/KYRIE-LI11/VideoMark}{Project Page}.",
      "authors": [
        "Hu, Xuming",
        "Li, Hanqian",
        "Li, Jungang",
        "Huang, Yu",
        "Liu, Aiwei"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2504.16359v2",
        "Other Formats": "https://arxiv.org/format/2504.16359",
        "TeX Source": "https://arxiv.org/src/2504.16359",
        "View PDF": "https://arxiv.org/pdf/2504.16359"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 23 Apr 2025 02:21:12 UTC (845 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 14:08:22 UTC (615 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/04/23",
      "title": "VideoMark: A Distortion-Free Robust Watermarking Framework for Video Diffusion Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18655",
      "abstract": "Video generation techniques have achieved remarkable advancements in visual quality, yet faithfully reproducing real-world physics remains elusive. Preference-based model post-training may improve physical consistency, but requires costly human-annotated datasets or reward models that are not yet feasible. To address these challenges, we present Real Data Preference Optimisation (RDPO), an annotation-free framework that distills physical priors directly from real-world videos. Specifically, the proposed RDPO reverse-samples real video sequences with a pre-trained generator to automatically build preference pairs that are statistically distinguishable in terms of physical correctness. A multi-stage iterative training schedule then guides the generator to obey physical laws increasingly well. Benefiting from the dynamic information explored from real videos, our proposed RDPO significantly improves the action coherence and physical realism of the generated videos. Evaluations on multiple benchmarks and human evaluations have demonstrated that RDPO achieves improvements across multiple dimensions. The source code and demonstration of this paper are available at: https://wwenxu.github.io/RDPO/",
      "authors": [
        "Qian, Wenxu",
        "Wang, Chaoyue",
        "Peng, Hou",
        "Tan, Zhiyu",
        "Li, Hao",
        "Zeng, Anxiang"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18655v1",
        "Other Formats": "https://arxiv.org/format/2506.18655",
        "TeX Source": "https://arxiv.org/src/2506.18655",
        "View PDF": "https://arxiv.org/pdf/2506.18655"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 13:55:24 UTC (10,042 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "RDPO: Real Data Preference Optimization for Physics Consistency Video Generation",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2408.09886",
      "abstract": "Large foundation models, known for their strong zero-shot generalization capabilities, can be applied to a wide range of downstream tasks. However, developing foundation models for medical image segmentation poses a significant challenge due to the domain gap between natural and medical images. While fine-tuning techniques based on the Segment Anything Model (SAM) have been explored, they primarily focus on scaling up data or refining inference strategies without incorporating domain-specific architectural designs, limiting their zero-shot performance. To optimize segmentation performance under standard inference settings and provide a strong baseline for future research, we introduce SyncSAM, which employs a synchronized dual-branch encoder that integrates convolution and Transformer features in a synchronized manner to enhance medical image encoding, and a multi-scale dual-branch decoder to preserve image details. SyncSAM is trained on two of the largest medical image segmentation datasets, SA-Med2D-20M and IMed-361M, resulting in a series of pre-trained models for universal medical image segmentation. Experimental results demonstrate that SyncSAM not only achieves state-of-the-art performance on test sets but also exhibits strong zero-shot capabilities on unseen datasets. Code and checkpoints are available at https://github.com/Hhankyangg/SyncSAM.",
      "authors": [
        "Yang, Sihan",
        "Feng, Jiadong",
        "Mi, Xuande",
        "Bi, Haixia",
        "Zhang, Hai",
        "Sun, Jian"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2408.09886v4",
        "Other Formats": "https://arxiv.org/format/2408.09886",
        "TeX Source": "https://arxiv.org/src/2408.09886",
        "View PDF": "https://arxiv.org/pdf/2408.09886"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 19 Aug 2024 11:01:00 UTC (2,118 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Thu, 27 Feb 2025 15:24:27 UTC (21,324 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Sun, 2 Mar 2025 11:32:04 UTC (21,324 KB)",
          "link": "/",
          "version": "[v3]"
        },
        {
          "details": "Mon, 23 Jun 2025 17:43:31 UTC (21,109 KB)",
          "version": "[v4]"
        }
      ],
      "submitted_date": "2024/08/19",
      "title": "Improved Baselines with Synchronized Encoding for Universal Medical Image Segmentation",
      "repo_urls": [
        "https://github.com/hhankyangg/sam-unet"
      ],
      "tasks": [
        "Image Segmentation",
        "Medical Image Segmentation",
        "Segmentation",
        "Semantic Segmentation",
        "Zero Shot Segmentation"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2504.06386",
      "abstract": "To apply reinforcement learning to safety-critical applications, we ought to provide safety guarantees during both policy training and deployment. In this work, we present theoretical results that place a bound on the probability of violating a safety property for a new task-specific policy in a model-free, episodic setting. This bound, based on a maximum policy ratio computed with respect to a 'safe' base policy, can also be applied to temporally-extended properties (beyond safety) and to robust control problems. To utilize these results, we introduce SPoRt, which provides a data-driven method for computing this bound for the base policy using the scenario approach, and includes Projected PPO, a new projection-based approach for training the task-specific policy while maintaining a user-specified bound on property violation. SPoRt thus enables users to trade off safety guarantees against task-specific performance. Complementing our theoretical results, we present experimental results demonstrating this trade-off and comparing the theoretical bound to posterior bounds derived from empirical violation rates.",
      "authors": [
        "Cloete, Jacques",
        "Vertovec, Nikolaus",
        "Abate, Alessandro"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2504.06386v2",
        "Other Formats": "https://arxiv.org/format/2504.06386",
        "TeX Source": "https://arxiv.org/src/2504.06386",
        "View PDF": "https://arxiv.org/pdf/2504.06386"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Tue, 8 Apr 2025 19:09:07 UTC (3,242 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 10:50:00 UTC (2,028 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/04/08",
      "title": "SPoRt -- Safe Policy Ratio: Certified Training and Deployment of Task Policies in Model-Free RL",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2307.08526",
      "abstract": "With the rapid development of Artificial Intelligence Generated Content (AIGC), it has become a common practice to train models on synthetic data due to data-scarcity and privacy leakage problems. Owing to massive and diverse information conveyed in real images, it is challenging for text-to-image generative models to synthesize informative training data with hand-crafted prompts. Considering the impressive ability of large generative models, could such models directly synthesize good training images for prediction tasks with proper prompts? We offer an affirmative response to this question by proposing a simple yet effective method, validated through ImageNet classification. Specifically, we caption each real image with the advanced captioning model to obtain informative and faithful prompts that extract class-relevant information and clarify the polysemy of class names. The image captions and class names are concatenated to prompt generative models for training image synthesis. We show that this simple caption incorporation significantly boosts the informativeness of synthetic data therefore enhancing downstream model generalization. More importantly, besides improvements in data augmentation and privacy preservation, our experiments demonstrate that synthesized images can exceed real data in terms of out-of-distribution robustness.",
      "authors": [
        "Lei, Shiye",
        "Chen, Hao",
        "Zhang, Sen",
        "Zhao, Bo",
        "Tao, Dacheng"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2307.08526v2",
        "Other Formats": "https://arxiv.org/format/2307.08526",
        "TeX Source": "https://arxiv.org/src/2307.08526",
        "View PDF": "https://arxiv.org/pdf/2307.08526"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 17 Jul 2023 14:38:11 UTC (7,787 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 16:21:02 UTC (3,559 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2023/07/17",
      "title": "Image Captions are Natural Prompts for Text-to-Image Models",
      "repo_urls": [
        "https://github.com/BAAI-DCAI/Training-Data-Synthesis"
      ],
      "tasks": [
        "Image Captioning",
        "Image Generation"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18385",
      "abstract": "Recent benchmarks and datasets have been proposed to improve spatial reasoning in vision-language models (VLMs), yet existing open resources remain limited in scale, visual diversity, and instruction expressiveness. In this work, we introduce InternSpatial, the largest open-source dataset for spatial reasoning in VLMs, along with InternSpatial-Bench, a corresponding evaluation benchmark designed to assess spatial understanding under diverse instruction formats. InternSpatial comprises 12 million QA pairs spanning both single-view and multi-view settings, drawn from diverse visual environments and supporting 19 instruction formats that reflect varied query styles. For evaluation, we propose InternSpatial-Bench for single-view tasks and expand multi-view reasoning by introducing a novel rotation angle prediction task that has not been explored in prior work. Experimental results show that models trained on InternSpatial achieve 12.1% improvement on InternSpatial-Bench and 10.7% on VSI-Bench, while maintaining strong performance on general-purpose benchmarks. We hope these resources will support the development of spatially capable VLMs in practical applications such as robotics and embodied AI.",
      "authors": [
        "Deng, Nianchen",
        "Gu, Lixin",
        "Ye, Shenglong",
        "He, Yinan",
        "Chen, Zhe",
        "Li, Songze",
        "Wang, Haomin",
        "Wei, Xingguang",
        "Yang, Tianshuo",
        "Dou, Min",
        "He, Tong",
        "Shao, Wenqi",
        "Zhang, Kaipeng",
        "Wang, Yi",
        "Shi, Botian",
        "Zhang, Yanting",
        "Dai, Jifeng",
        "Qiao, Yu",
        "Zhang, Hongjie",
        "Wang, Wenhai"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18385v1",
        "Other Formats": "https://arxiv.org/format/2506.18385",
        "TeX Source": "https://arxiv.org/src/2506.18385",
        "View PDF": "https://arxiv.org/pdf/2506.18385"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 08:17:22 UTC (4,816 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "InternSpatial: A Comprehensive Dataset for Spatial Reasoning in Vision-Language Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18739",
      "abstract": "Prior work on the learnability of transformers has established its capacity to approximate specific algorithmic patterns through training under restrictive architectural assumptions. Fundamentally, these arguments remain data-driven and therefore can only provide a probabilistic guarantee. Expressivity, on the contrary, has theoretically been explored to address the problems \\emph{computable} by such architecture. These results proved the Turing-completeness of transformers, investigated bounds focused on circuit complexity, and formal logic. Being at the crossroad between learnability and expressivity, the question remains: \\emph{can transformer architectures exactly simulate an arbitrary attention mechanism, or in particular, the underlying operations?} In this study, we investigate the transformer encoder's ability to simulate a vanilla attention mechanism. By constructing a universal simulator $\\mathcal{U}$ composed of transformer encoders, we present algorithmic solutions to identically replicate attention outputs and the underlying elementary matrix and activation operations via RASP, a formal framework for transformer computation. Our proofs, for the first time, show the existence of an algorithmically achievable data-agnostic solution, previously known to be approximated only by learning.",
      "authors": [
        "Dutta, Debanjan",
        "Ansari, Faizanuddin",
        "Chakrabarty, Anish",
        "Das, Swagatam"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18739v1",
        "Other Formats": "https://arxiv.org/format/2506.18739",
        "TeX Source": "https://arxiv.org/src/2506.18739",
        "View PDF": "https://arxiv.org/pdf/2506.18739"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 15:15:25 UTC (82 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "On the Existence of Universal Simulators of Attention",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18227",
      "abstract": "We propose an efficient framework for amortized conditional inference by leveraging exact conditional score-guided diffusion models to train a non-reversible neural network as a conditional generative model. Traditional normalizing flow methods require reversible architectures, which can limit their expressiveness and efficiency. Although diffusion models offer greater flexibility, they often suffer from high computational costs during inference. To combine the strengths of both approaches, we introduce a two-stage method. First, we construct a training-free conditional diffusion model by analytically deriving an exact score function under a Gaussian mixture prior formed from samples of the underlying joint distribution. This exact conditional score model allows us to efficiently generate noise-labeled data, consisting of initial diffusion Gaussian noise and posterior samples conditioned on various observation values, by solving a reverse-time ordinary differential equation. Second, we use this noise-labeled data to train a feedforward neural network that maps noise and observations directly to posterior samples, eliminating the need for reversibility or iterative sampling at inference time. The resulting model provides fast, accurate, and scalable conditional sampling for high-dimensional and multi-modal posterior distributions, making it well-suited for uncertainty quantification tasks, e.g., parameter estimation of complex physical systems. We demonstrate the effectiveness of our approach through a series of numerical experiments.",
      "authors": [
        "Zhang, Zezhong",
        "Tatsuoka, Caroline",
        "Xiu, Dongbin",
        "Zhang, Guannan"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18227v1",
        "Other Formats": "https://arxiv.org/format/2506.18227",
        "TeX Source": "https://arxiv.org/src/2506.18227",
        "View PDF": "https://arxiv.org/pdf/2506.18227"
      },
      "subjects": [
        "Computational Engineering, Finance, and Science (cs.CE)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 01:31:19 UTC (9,416 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Exact Conditional Score-Guided Generative Modeling for Amortized Inference in Uncertainty Quantification",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18354",
      "abstract": "In this paper, we study the problem of map matching with travel time constraints. Given a sequence of $k$ spatio-temporal measurements and an embedded path graph with travel time costs, the goal is to snap each measurement to a close-by location in the graph, such that consecutive locations can be reached from one another along the path within the timestamp difference of the respective measurements. This problem arises in public transit data processing as well as in map matching of movement trajectories to general graphs. We show that the classical approach for this problem, which relies on selecting a finite set of candidate locations in the graph for each measurement, cannot guarantee to find a consistent solution. We propose a new algorithm that can deal with an infinite set of candidate locations per measurement. We prove that our algorithm always detects a consistent map matching path (if one exists). Despite the enlarged candidate set, we also demonstrate that our algorithm has superior running time in theory and practice. For a path graph with $n$ nodes, we show that our algorithm runs in $\\mathcal{O}(k^2 n \\log {nk})$ and under mild assumptions in $\\mathcal{O}(k n ^\\lambda + n \\log^3 n)$ for $\\lambda \\approx 0.695$. This is a significant improvement over the baseline, which runs in $\\mathcal{O}(k n^2)$ and which might not even identify a correct solution. The performance of our algorithm hinges on an efficient segment-circle intersection data structure. We describe how to design and implement such a data structure for our application. In the experimental evaluation, we demonstrate the usefulness of our novel algorithm on a diverse set of generated measurements as well as GTFS data.",
      "authors": [
        "Bosch, Yannick",
        "Storandt, Sabine"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18354v1",
        "Other Formats": "https://arxiv.org/format/2506.18354",
        "TeX Source": "https://arxiv.org/src/2506.18354",
        "View PDF": "https://arxiv.org/pdf/2506.18354"
      },
      "subjects": [
        "Computational Geometry (cs.CG)",
        "Data Structures and Algorithms (cs.DS)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 07:30:17 UTC (2,810 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Continuous Map Matching to Paths under Travel Time Constraints",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18306",
      "abstract": "This paper presents a lightweight software-based approach for running spiking neural networks (SNNs) without relying on specialized neuromorphic hardware or frameworks. Instead, we implement a specific SNN architecture (CoLaNET) in Rust and optimize it for common computing platforms. As a case study, we demonstrate our implementation, called Spiffy, on a Raspberry Pi using the MNIST dataset. Spiffy achieves 92% accuracy with low latency - just 0.9 ms per training step and 0.45 ms per inference step. The code is open-source.",
      "authors": [
        "Derzhavin, Andrey",
        "Larionov, Denis"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18306v1",
        "Other Formats": "https://arxiv.org/format/2506.18306",
        "TeX Source": "https://arxiv.org/src/2506.18306",
        "View PDF": "https://arxiv.org/pdf/2506.18306"
      },
      "subjects": [
        "Neural and Evolutionary Computing (cs.NE)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 05:47:14 UTC (1,153 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Spiffy: Efficient Implementation of CoLaNET for Raspberry Pi",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18828",
      "abstract": "This work describes the participation of the MLLP-VRAIN research group in the shared task of the IWSLT 2025 Simultaneous Speech Translation track. Our submission addresses the unique challenges of real-time translation of long-form speech by developing a modular cascade system that adapts strong pre-trained models to streaming scenarios. We combine Whisper Large-V3-Turbo for ASR with the multilingual NLLB-3.3B model for MT, implementing lightweight adaptation techniques rather than training new end-to-end models from scratch. Our approach employs document-level adaptation with prefix training to enhance the MT model's ability to handle incomplete inputs, while incorporating adaptive emission policies including a wait-$k$ strategy and RALCP for managing the translation stream. Specialized buffer management techniques and segmentation strategies ensure coherent translations across long audio sequences. Experimental results on the ACL60/60 dataset demonstrate that our system achieves a favorable balance between translation quality and latency, with a BLEU score of 31.96 and non-computational-aware StreamLAAL latency of 2.94 seconds. Our final model achieves a preliminary score on the official test set (IWSLT25Instruct) of 29.8 BLEU. Our work demonstrates that carefully adapted pre-trained components can create effective simultaneous translation systems for long-form content without requiring extensive in-domain parallel data or specialized end-to-end training.",
      "authors": [
        "Iranzo-S\u00e1nchez, Jorge",
        "Iranzo-S\u00e1nchez, Javier",
        "Gim\u00e9nez, Adri\u00e0",
        "Civera, Jorge",
        "Juan, Alfons"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18828v1",
        "Other Formats": "https://arxiv.org/format/2506.18828",
        "TeX Source": "https://arxiv.org/src/2506.18828",
        "View PDF": "https://arxiv.org/pdf/2506.18828"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 16:44:01 UTC (85 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18871",
      "abstract": "In this work, we introduce OmniGen2, a versatile and open-source generative model designed to provide a unified solution for diverse generation tasks, including text-to-image, image editing, and in-context generation. Unlike OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer. This design enables OmniGen2 to build upon existing multimodal understanding models without the need to re-adapt VAE inputs, thereby preserving the original text generation capabilities. To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data. Additionally, we introduce a reflection mechanism tailored for image generation tasks and curate a dedicated reflection dataset based on OmniGen2. Despite its relatively modest parameter size, OmniGen2 achieves competitive results on multiple task benchmarks, including text-to-image and image editing. To further evaluate in-context generation, also referred to as subject-driven tasks, we introduce a new benchmark named OmniContext. OmniGen2 achieves state-of-the-art performance among open-source models in terms of consistency. We will release our models, training code, datasets, and data construction pipeline to support future research in this field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link: https://github.com/VectorSpaceLab/OmniGen2",
      "authors": [
        "Wu, Chenyuan",
        "Zheng, Pengfei",
        "Yan, Ruiran",
        "Xiao, Shitao",
        "Luo, Xin",
        "Wang, Yueze",
        "Li, Wanli",
        "Jiang, Xiyan",
        "Liu, Yexin",
        "Zhou, Junjie",
        "Liu, Ze",
        "Xia, Ziyi",
        "Li, Chaofan",
        "Deng, Haoge",
        "Wang, Jiahao",
        "Luo, Kun",
        "Zhang, Bo",
        "Lian, Defu",
        "Wang, Xinlong",
        "Wang, Zhongyuan",
        "Huang, Tiejun",
        "Liu, Zheng"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18871v1",
        "Other Formats": "https://arxiv.org/format/2506.18871",
        "TeX Source": "https://arxiv.org/src/2506.18871",
        "View PDF": "https://arxiv.org/pdf/2506.18871"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 17:38:54 UTC (13,204 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
      "models": [
        {
          "model_path": "OmniGen2/OmniGen2",
          "downloads": "1150",
          "likes": "54",
          "trending_score": "49.0",
          "link": "https://huggingface.co/OmniGen2/OmniGen2"
        }
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18602",
      "abstract": "Estimation of semantic similarity is an important research problem both in natural language processing and the natural language understanding, and that has tremendous application on various downstream tasks such as question answering, semantic search, information retrieval, document clustering, word-sense disambiguation and machine translation. In this work, we carry out the estimation of semantic similarity using different state-of-the-art techniques including the USE (Universal Sentence Encoder), InferSent and the most recent BERT, or Bidirectional Encoder Representations from Transformers, models. We use two question pairs datasets for the analysis, one is a domain specific in-house dataset and the other is a public dataset which is the Quora's question pairs dataset. We observe that the BERT model gave much superior performance as compared to the other methods. This should be because of the fine-tuning procedure that is involved in its training process, allowing it to learn patterns based on the training data that is used. This works demonstrates the applicability of BERT on domain specific datasets. We infer from the analysis that BERT is the best technique to use in the case of domain specific data.",
      "authors": [
        "Prashanth, R."
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18602v1",
        "Other Formats": "https://arxiv.org/format/2506.18602",
        "TeX Source": "https://arxiv.org/src/2506.18602",
        "View PDF": "https://arxiv.org/pdf/2506.18602"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Applications (stat.AP)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 13:03:59 UTC (94 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Semantic similarity estimation for domain specific data using BERT and other techniques",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2502.16826",
      "abstract": "Building on recent advances in Bayesian statistics and image denoising, we propose Noise2Score3D, a fully unsupervised framework for point cloud denoising that addresses the critical challenge of limited availability of clean data. Noise2Score3D learns the gradient of the underlying point cloud distribution directly from noisy data, eliminating the need for clean data during training. By leveraging Tweedie's formula, our method performs inference in a single step, avoiding the iterative processes used in existing unsupervised methods, thereby improving both performance and efficiency. Experimental results demonstrate that Noise2Score3D achieves state-of-the-art performance on standard benchmarks, outperforming other unsupervised methods in Chamfer distance and point-to-mesh metrics, and rivaling some supervised approaches. Furthermore, Noise2Score3D demonstrates strong generalization ability beyond training datasets. Additionally, we introduce Total Variation for Point Cloud, a criterion that allows for the estimation of unknown noise parameters, which further enhances the method's versatility and real-world utility.",
      "authors": [
        "Wei, Xiangbin"
      ],
      "last_revised_date": "2025/06/23",
      "links": {},
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 24 Feb 2025 04:23:21 UTC (41,186 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 3 Mar 2025 03:09:49 UTC (41,186 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Mon, 23 Jun 2025 06:58:26 UTC (1 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2025/02/24",
      "title": "Noise2Score3D:Unsupervised Tweedie's Approach for Point Cloud Denoising",
      "tasks": [
        "Denoising",
        "Image Denoising"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18291",
      "abstract": "This paper presents an architecture for selecting important neighboring people to predict the primary person's trajectory. To achieve effective neighboring people selection, we propose a people selection module called the Importance Estimator which outputs the importance of each neighboring person for predicting the primary person's future trajectory. To prevent gradients from being blocked by non-differentiable operations when sampling surrounding people based on their importance, we employ the Gumbel Softmax for training. Experiments conducted on the JRDB dataset show that our method speeds up the process with competitive prediction accuracy.",
      "authors": [
        "Urano, Yota",
        "Taketsugu, Hiromu",
        "Ukita, Norimichi"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18291v1",
        "Other Formats": "https://arxiv.org/format/2506.18291",
        "TeX Source": "https://arxiv.org/src/2506.18291",
        "View PDF": "https://arxiv.org/pdf/2506.18291"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 05:01:24 UTC (1,617 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Selective Social-Interaction via Individual Importance for Fast Human Trajectory Prediction",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.24616",
      "abstract": "We introduce POLLUX, a comprehensive open-source benchmark designed to evaluate the generative capabilities of large language models (LLMs) in Russian. Our main contribution is a novel evaluation methodology that enhances the interpretability of LLM assessment. For each task type, we define a set of detailed criteria and develop a scoring protocol where models evaluate responses and provide justifications for their ratings. This enables transparent, criteria-driven evaluation beyond traditional resource-consuming, side-by-side human comparisons. POLLUX includes a detailed, fine-grained taxonomy of 35 task types covering diverse generative domains such as code generation, creative writing, and practical assistant use cases, totaling 2,100 manually crafted and professionally authored prompts. Each task is categorized by difficulty (easy/medium/hard), with experts constructing the dataset entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B) evaluators trained for nuanced assessment of generative outputs. This approach provides scalable, interpretable evaluation and annotation tools for model development, effectively replacing costly and less precise human judgments.",
      "authors": [
        "Martynov, Nikita",
        "Mordasheva, Anastasia",
        "Gorbetskiy, Dmitriy",
        "Astafurov, Danil",
        "Isaeva, Ulyana",
        "Basyrova, Elina",
        "Skachkov, Sergey",
        "Berestova, Victoria",
        "Ivanov, Nikolay",
        "Zanina, Valeriia",
        "Fenogenova, Alena"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "Other Formats": "https://arxiv.org/format/2505.24616",
        "TeX Source": "https://arxiv.org/src/2505.24616",
        "View PDF": "https://arxiv.org/pdf/2505.24616"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 30 May 2025 14:08:17 UTC (12,953 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 15:01:31 UTC (31,523 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/30",
      "title": "Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX",
      "tasks": [
        "Code Generation"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18288",
      "abstract": "This paper addresses the dual challenge of improving anomaly detection and signal integrity in high-speed dynamic random access memory signals. To achieve this, we propose a joint training framework that integrates an autoencoder with a classifier to learn more distinctive latent representations by focusing on valid data features. Our approach is evaluated across three anomaly detection algorithms and consistently outperforms two baseline methods. Detailed ablation studies further support these findings. Furthermore, we introduce a signal integrity enhancement algorithm that improves signal integrity by an average of 11.3%. The source code and data used in this study are available at https://github.com/Usama1002/learning-latent-representations.",
      "authors": [
        "Usama, Muhammad",
        "Jang, Hee-Deok",
        "Shanbhag, Soham",
        "Sung, Yoo-Chang",
        "Bae, Seung-Jun",
        "Chang, Dong Eui"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18288v1",
        "Other Formats": "https://arxiv.org/format/2506.18288",
        "TeX Source": "https://arxiv.org/src/2506.18288",
        "View PDF": "https://arxiv.org/pdf/2506.18288"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 04:48:22 UTC (20,161 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Learning High-Quality Latent Representations for Anomaly Detection and Signal Integrity Enhancement in High-Speed Signals",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18484",
      "abstract": "Virtual staining is a promising technique that uses deep generative models to recreate histological stains, providing a faster and more cost-effective alternative to traditional tissue chemical staining. Specifically for H&E-HER2 staining transfer, despite a rising trend in publications, the lack of sufficient public datasets has hindered progress in the topic. Additionally, it is currently unclear which model frameworks perform best for this particular task. In this paper, we introduce the HER2match dataset, the first publicly available dataset with the same breast cancer tissue sections stained with both H&E and HER2. Furthermore, we compare the performance of several Generative Adversarial Networks (GANs) and Diffusion Models (DMs), and implement a novel Brownian Bridge Diffusion Model for H&E-HER2 translation. Our findings indicate that, overall, GANs perform better than DMs, with only the BBDM achieving comparable results. Furthermore, we emphasize the importance of data alignment, as all models trained on HER2match produced vastly improved visuals compared to the widely used consecutive-slide BCI dataset. This research provides a new high-quality dataset ([available upon publication acceptance]), improving both model training and evaluation. In addition, our comparison of frameworks offers valuable guidance for researchers working on the topic.",
      "authors": [
        "Kl\u00f6ckner, Pascal",
        "Teixeira, Jos\u00e9",
        "Montezuma, Diana",
        "Cardoso, Jaime S.",
        "Horlings, Hugo M.",
        "Oliveira, Sara P."
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18484v1",
        "Other Formats": "https://arxiv.org/format/2506.18484",
        "TeX Source": "https://arxiv.org/src/2506.18484",
        "View PDF": "https://arxiv.org/pdf/2506.18484"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 10:37:41 UTC (14,435 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "GANs vs. Diffusion Models for virtual staining with the HER2match dataset",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18283",
      "abstract": "Neural networks make accurate predictions but often fail to provide reliable uncertainty estimates, especially under covariate distribution shifts between training and testing. To address this problem, we propose a Bayesian framework for uncertainty estimation that explicitly accounts for covariate shifts. While conventional approaches rely on fixed priors, the key idea of our method is an adaptive prior, conditioned on both training and new covariates. This prior naturally increases uncertainty for inputs that lie far from the training distribution in regions where predictive performance is likely to degrade. To efficiently approximate the resulting posterior predictive distribution, we employ amortized variational inference. Finally, we construct synthetic environments by drawing small bootstrap samples from the training data, simulating a range of plausible covariate shift using only the original dataset. We evaluate our method on both synthetic and real-world data. It yields substantially improved uncertainty estimates under distribution shifts.",
      "authors": [
        "Slavutsky, Yuli",
        "Blei, David M."
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18283v1",
        "Other Formats": "https://arxiv.org/format/2506.18283",
        "TeX Source": "https://arxiv.org/src/2506.18283",
        "View PDF": "https://arxiv.org/pdf/2506.18283"
      },
      "subjects": [
        "Machine Learning (stat.ML)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 04:30:36 UTC (4,244 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Quantifying Uncertainty in the Presence of Distribution Shifts",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.24450",
      "abstract": "Due to the lack of target speech annotations in real-recorded far-field conversational datasets, speech enhancement (SE) models are typically trained on simulated data. However, the trained models often perform poorly in real-world conditions, hindering their application in far-field speech recognition. To address the issue, we (a) propose direct sound estimation (DSE) to estimate the oracle direct sound of real-recorded data for SE; and (b) present a novel pseudo-supervised learning method, SuPseudo, which leverages DSE-estimates as pseudo-labels and enables SE models to directly learn from and adapt to real-recorded data, thereby improving their generalization capability. Furthermore, an SE model called FARNET is designed to fully utilize SuPseudo. Experiments on the MISP2023 corpus demonstrate the effectiveness of SuPseudo, and our system significantly outperforms the previous state-of-the-art. A demo of our method can be found at https://EeLLJ.github.io/SuPseudo/.",
      "authors": [
        "Luo, Longjie",
        "Li, Lin",
        "Hong, Qingyang"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2505.24450v2",
        "Other Formats": "https://arxiv.org/format/2505.24450",
        "TeX Source": "https://arxiv.org/src/2505.24450",
        "View PDF": "https://arxiv.org/pdf/2505.24450"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 30 May 2025 10:36:32 UTC (540 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 07:42:55 UTC (893 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/30",
      "title": "SuPseudo: A Pseudo-supervised Learning Method for Neural Speech Enhancement in Far-field Speech Recognition",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18841",
      "abstract": "Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under https://huggingface.co/THU-KEG/LongWriter-Zero-32B",
      "authors": [
        "Wu, Yuhao",
        "Bai, Yushi",
        "Hu, Zhiqiang",
        "Lee, Roy Ka-Wei",
        "Li, Juanzi"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.18841",
        "TeX Source": "https://arxiv.org/src/2506.18841",
        "View PDF": "https://arxiv.org/pdf/2506.18841"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 16:59:02 UTC (3,485 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning",
      "datasets": [
        {
          "dataset_name": "THU-KEG/LongWriter-Zero-RLData",
          "downloads": "72",
          "likes": "2",
          "link": "https://huggingface.co/datasets/THU-KEG/LongWriter-Zero-RLData"
        },
        {
          "dataset_name": "THU-KEG/Arena-Write",
          "downloads": "70",
          "likes": "1",
          "link": "https://huggingface.co/datasets/THU-KEG/Arena-Write"
        }
      ],
      "models": [
        {
          "model_path": "THU-KEG/LongWriter-Zero-32B",
          "downloads": "85",
          "likes": "7",
          "trending_score": "7.0",
          "link": "https://huggingface.co/THU-KEG/LongWriter-Zero-32B"
        }
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18316",
      "abstract": "The Citation Discovery Shared Task focuses on predicting the correct citation from a given candidate pool for a given paragraph. The main challenges stem from the length of the abstract paragraphs and the high similarity among candidate abstracts, making it difficult to determine the exact paper to cite. To address this, we develop a system that first retrieves the top-k most similar abstracts based on extracted relational features from the given paragraph. From this subset, we leverage a Large Language Model (LLM) to accurately identify the most relevant citation. We evaluate our framework on the training dataset provided by the SCIDOCA 2025 organizers, demonstrating its effectiveness in citation prediction.",
      "authors": [
        "An, Trieu",
        "Nguyen, Long",
        "Nguyen, Minh Le"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18316v1",
        "Other Formats": "https://arxiv.org/format/2506.18316",
        "TeX Source": "https://arxiv.org/src/2506.18316",
        "View PDF": "https://arxiv.org/pdf/2506.18316"
      },
      "subjects": [
        "Information Retrieval (cs.IR)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 06:01:21 UTC (66 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Team LA at SCIDOCA shared task 2025: Citation Discovery via relation-based zero-shot retrieval",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18843",
      "abstract": "Self-supervised learning (SSL) has revolutionized audio representations, yet models often remain domain-specific, focusing on either speech or non-speech tasks. In this work, we present Universal Speech and Audio Distillation (USAD), a unified approach to audio representation learning that integrates diverse audio types - speech, sound, and music - into a single model. USAD employs efficient layer-to-layer distillation from domain-specific SSL models to train a student on a comprehensive audio dataset. USAD offers competitive performance across various benchmarks and datasets, including frame and instance-level speech processing tasks, audio tagging, and sound classification, achieving near state-of-the-art results with a single encoder on SUPERB and HEAR benchmarks.",
      "authors": [
        "Chang, Heng-Jui",
        "Bhati, Saurabhchand",
        "Glass, James",
        "Liu, Alexander H."
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18843v1",
        "Other Formats": "https://arxiv.org/format/2506.18843",
        "TeX Source": "https://arxiv.org/src/2506.18843",
        "View PDF": "https://arxiv.org/pdf/2506.18843"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Computation and Language (cs.CL)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 17:02:00 UTC (376 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "USAD: Universal Speech and Audio Representation via Distillation",
      "models": [
        {
          "model_path": "MIT-SLS/USAD-Small",
          "downloads": "10",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/MIT-SLS/USAD-Small"
        },
        {
          "model_path": "MIT-SLS/USAD-Base",
          "downloads": "5",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/MIT-SLS/USAD-Base"
        },
        {
          "model_path": "MIT-SLS/USAD-Large",
          "downloads": "5",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/MIT-SLS/USAD-Large"
        }
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18251",
      "abstract": "In this paper, we present Morse, a simple dual-sampling framework for accelerating diffusion models losslessly. The key insight of Morse is to reformulate the iterative generation (from noise to data) process via taking advantage of fast jump sampling and adaptive residual feedback strategies. Specifically, Morse involves two models called Dash and Dot that interact with each other. The Dash model is just the pre-trained diffusion model of any type, but operates in a jump sampling regime, creating sufficient space for sampling efficiency improvement. The Dot model is significantly faster than the Dash model, which is learnt to generate residual feedback conditioned on the observations at the current jump sampling point on the trajectory of the Dash model, lifting the noise estimate to easily match the next-step estimate of the Dash model without jump sampling. By chaining the outputs of the Dash and Dot models run in a time-interleaved fashion, Morse exhibits the merit of flexibly attaining desired image generation performance while improving overall runtime efficiency. With our proposed weight sharing strategy between the Dash and Dot models, Morse is efficient for training and inference. Our method shows a lossless speedup of 1.78X to 3.31X on average over a wide range of sampling step budgets relative to 9 baseline diffusion models on 6 image generation tasks. Furthermore, we show that our method can be also generalized to improve the Latent Consistency Model (LCM-SDXL, which is already accelerated with consistency distillation technique) tailored for few-step text-to-image synthesis. The code and models are available at https://github.com/deep-optimization/Morse.",
      "authors": [
        "Li, Chao",
        "Fan, Jiawei",
        "Yao, Anbang"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18251v1",
        "Other Formats": "https://arxiv.org/format/2506.18251",
        "TeX Source": "https://arxiv.org/src/2506.18251",
        "View PDF": "https://arxiv.org/pdf/2506.18251"
      },
      "subjects": [
        "Graphics (cs.GR)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 02:43:21 UTC (11,780 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18337",
      "abstract": "Machine translation (MT) post-editing and research data collection often rely on inefficient, disconnected workflows. We introduce TranslationCorrect, an integrated framework designed to streamline these tasks. TranslationCorrect combines MT generation using models like NLLB, automated error prediction using models like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive post-editing interface within a single environment. Built with human-computer interaction (HCI) principles in mind to minimize cognitive load, as confirmed by a user study. For translators, it enables them to correct errors and batch translate efficiently. For researchers, TranslationCorrect exports high-quality span-based annotations in the Error Span Annotation (ESA) format, using an error taxonomy inspired by Multidimensional Quality Metrics (MQM). These outputs are compatible with state-of-the-art error detection models and suitable for training MT or post-editing systems. Our user study confirms that TranslationCorrect significantly improves translation efficiency and user satisfaction over traditional annotation methods.",
      "authors": [
        "Wasti, Syed Mekael",
        "Hung, Shou-Yi",
        "Collins, Christopher",
        "Lee, En-Shiun Annie"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18337v1",
        "Other Formats": "https://arxiv.org/format/2506.18337",
        "TeX Source": "https://arxiv.org/src/2506.18337",
        "View PDF": "https://arxiv.org/pdf/2506.18337"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 06:38:49 UTC (5,424 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18583",
      "abstract": "LiDAR-Inertial Odometry (LIO) is widely used for accurate state estimation and mapping which is an essential requirement for autonomous robots. Conventional LIO methods typically rely on formulating constraints from the geometric structure sampled by the LiDAR. Hence, in the lack of geometric structure, these tend to become ill-conditioned (degenerate) and fail. Robustness of LIO to such conditions is a necessity for its broader deployment. To address this, we propose PG-LIO, a real-time LIO method that fuses photometric and geometric information sampled by the LiDAR along with inertial constraints from an Inertial Measurement Unit (IMU). This multi-modal information is integrated into a factor graph optimized over a sliding window for real-time operation. We evaluate PG-LIO on multiple datasets that include both geometrically well-conditioned as well as self-similar scenarios. Our method achieves accuracy on par with state-of-the-art LIO in geometrically well-structured settings while significantly improving accuracy in degenerate cases including against methods that also fuse intensity. Notably, we demonstrate only 1 m drift over a 1 km manually piloted aerial trajectory through a geometrically self-similar tunnel at an average speed of 7.5m/s (max speed 10.8 m/s). For the benefit of the community, we shall also release our source code https://github.com/ntnu-arl/mimosa",
      "authors": [
        "Khedekar, Nikhil",
        "Alexis, Kostas"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18583v1",
        "Other Formats": "https://arxiv.org/format/2506.18583",
        "TeX Source": "https://arxiv.org/src/2506.18583",
        "View PDF": "https://arxiv.org/pdf/2506.18583"
      },
      "subjects": [
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 12:37:01 UTC (6,288 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "PG-LIO: Photometric-Geometric fusion for Robust LiDAR-Inertial Odometry",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18560",
      "abstract": "Beamforming enhances signal strength and quality by focusing energy in specific directions. This capability is particularly crucial in cell-free integrated sensing and communication (ISAC) systems, where multiple distributed access points (APs) collaborate to provide both communication and sensing services. In this work, we first derive the distribution of joint target detection probabilities across multiple receiving APs under false alarm rate constraints, and then formulate the beam selection procedure as a Markov decision process (MDP). We establish a deep reinforcement learning (DRL) framework, in which reward shaping and sinusoidal embedding are introduced to facilitate agent learning. To eliminate the high costs and associated risks of real-time agent-environment interactions, we further propose a novel digital twin (DT)-assisted offline DRL approach. Different from traditional online DRL, a conditional generative adversarial network (cGAN)-based DT module, operating as a replica of the real world, is meticulously designed to generate virtual state-action transition pairs and enrich data diversity, enabling offline adjustment of the agent's policy. Additionally, we address the out-of-distribution issue by incorporating an extra penalty term into the loss function design. The convergency of agent-DT interaction and the upper bound of the Q-error function are theoretically derived. Numerical results demonstrate the remarkable performance of our proposed approach, which significantly reduces online interaction overhead while maintaining effective beam selection across diverse conditions including strict false alarm control, low signal-to-noise ratios, and high target velocities.",
      "authors": [
        "Zhang, Jiexin",
        "Xu, Shu",
        "Li, Chunguo",
        "Huang, Yongming",
        "Yang, Luxi"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18560v1",
        "Other Formats": "https://arxiv.org/format/2506.18560",
        "TeX Source": "https://arxiv.org/src/2506.18560",
        "View PDF": "https://arxiv.org/pdf/2506.18560"
      },
      "subjects": [
        "Emerging Technologies (cs.ET)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 12:17:57 UTC (1,120 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Efficient Beam Selection for ISAC in Cell-Free Massive MIMO via Digital Twin-Assisted Deep Reinforcement Learning",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18364",
      "abstract": "The objective of Few-shot learning is to fully leverage the limited data resources for exploring the latent correlations within the data by applying algorithms and training a model with outstanding performance that can adequately meet the demands of practical applications. In practical applications, the number of images in each category is usually less than that in traditional deep learning, which can lead to over-fitting and poor generalization performance. Currently, many Few-shot classification models pay more attention to spatial domain information while neglecting frequency domain information, which contains more feature information. Ignoring frequency domain information will prevent the model from fully exploiting feature information, which would effect the classification performance. Based on conventional data augmentation, this paper proposes an SFIFNet with innovative data preprocessing. The key of this method is enhancing the accuracy of image feature representation by integrating frequency domain information with spatial domain information. The experimental results demonstrate the effectiveness of this method in enhancing classification performance.",
      "authors": [
        "Zhao, Wenqing",
        "Xie, Guojia",
        "Pan, Han",
        "Yang, Biao",
        "Zhang, Weichuan"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18364v1",
        "Other Formats": "https://arxiv.org/format/2506.18364",
        "TeX Source": "https://arxiv.org/src/2506.18364",
        "View PDF": "https://arxiv.org/pdf/2506.18364"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 07:47:11 UTC (700 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Spatial frequency information fusion network for few-shot learning",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.17590",
      "abstract": "Recently, 3D GANs based on 3D Gaussian splatting have been proposed for high quality synthesis of human heads. However, existing methods stabilize training and enhance rendering quality from steep viewpoints by conditioning the random latent vector on the current camera position. This compromises 3D consistency, as we observe significant identity changes when re-synthesizing the 3D head with each camera shift. Conversely, fixing the camera to a single viewpoint yields high-quality renderings for that perspective but results in poor performance for novel views. Removing view-conditioning typically destabilizes GAN training, often causing the training to collapse. In response to these challenges, we introduce CGS-GAN, a novel 3D Gaussian Splatting GAN framework that enables stable training and high-quality 3D-consistent synthesis of human heads without relying on view-conditioning. To ensure training stability, we introduce a multi-view regularization technique that enhances generator convergence with minimal computational overhead. Additionally, we adapt the conditional loss used in existing 3D Gaussian splatting GANs and propose a generator architecture designed to not only stabilize training but also facilitate efficient rendering and straightforward scaling, enabling output resolutions up to $2048^2$. To evaluate the capabilities of CGS-GAN, we curate a new dataset derived from FFHQ. This dataset enables very high resolutions, focuses on larger portions of the human head, reduces view-dependent artifacts for improved 3D consistency, and excludes images where subjects are obscured by hands or other objects. As a result, our approach achieves very high rendering quality, supported by competitive FID scores, while ensuring consistent 3D scene generation. Check our our project page here: https://fraunhoferhhi.github.io/cgs-gan/",
      "authors": [
        "Barthel, Florian",
        "Morgenstern, Wieland",
        "Hinzer, Paul",
        "Hilsmann, Anna",
        "Eisert, Peter"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2505.17590v2",
        "Other Formats": "https://arxiv.org/format/2505.17590",
        "TeX Source": "https://arxiv.org/src/2505.17590",
        "View PDF": "https://arxiv.org/pdf/2505.17590"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 23 May 2025 07:56:25 UTC (35,027 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 16:26:46 UTC (35,030 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/23",
      "title": "CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18266",
      "abstract": "3D semantic occupancy prediction in the past was considered to require precise geometric relationships in order to enable effective training. However, in complex indoor environments, the large-scale and widespread collection of data, along with the necessity for fine-grained annotations, becomes impractical due to the complexity of data acquisition setups and privacy concerns. In this paper, we demonstrate that 3D spatially-accurate training can be achieved using only indoor Internet data, without the need for any pre-knowledge of intrinsic or extrinsic camera parameters. In our framework, we collect a web dataset, YouTube-Occ, which comprises house tour videos from YouTube, providing abundant real house scenes for 3D representation learning. Upon on this web dataset, we establish a fully self-supervised model to leverage accessible 2D prior knowledge for reaching powerful 3D indoor perception. Specifically, we harness the advantages of the prosperous vision foundation models, distilling the 2D region-level knowledge into the occupancy network by grouping the similar pixels into superpixels. Experimental results show that our method achieves state-of-the-art zero-shot performance on two popular benchmarks (NYUv2 and OccScanNet",
      "authors": [
        "Chen, Haoming",
        "Yuan, Lichen",
        "Sun, TianFang",
        "Gong, Jingyu",
        "Tan, Xin",
        "Zhang, Zhizhong",
        "Xie, Yuan"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18266v1",
        "Other Formats": "https://arxiv.org/format/2506.18266",
        "TeX Source": "https://arxiv.org/src/2506.18266",
        "View PDF": "https://arxiv.org/pdf/2506.18266"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 03:44:43 UTC (4,164 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "YouTube-Occ: Learning Indoor 3D Semantic Occupancy Prediction from YouTube Videos",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2503.04308",
      "abstract": "Datasets for object detection often do not account for enough variety of glasses, due to their transparent and reflective properties. Specifically, open-vocabulary object detectors, widely used in embodied robotic agents, fail to distinguish subclasses of glasses. This scientific gap poses an issue to robotic applications that suffer from accumulating errors between detection, planning, and action execution. The paper introduces a novel method for the acquisition of real-world data from RGB-D sensors that minimizes human effort. We propose an auto-labeling pipeline that generates labels for all the acquired frames based on the depth measurements. We provide a novel real-world glass object dataset that was collected on the Neuro-Inspired COLlaborator (NICOL), a humanoid robot platform. The data set consists of 7850 images recorded from five different cameras. We show that our trained baseline model outperforms state-of-the-art open-vocabulary approaches. In addition, we deploy our baseline model in an embodied agent approach to the NICOL platform, on which it achieves a success rate of 81% in a human-robot bartending scenario.",
      "authors": [
        "Gajdo\u0161ech, Luk\u00e1\u0161",
        "Ali, Hassan",
        "Habekost, Jan-Gerrit",
        "Madaras, Martin",
        "Kerzel, Matthias",
        "Wermter, Stefan"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2503.04308v2",
        "Other Formats": "https://arxiv.org/format/2503.04308",
        "TeX Source": "https://arxiv.org/src/2503.04308",
        "View PDF": "https://arxiv.org/pdf/2503.04308"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 6 Mar 2025 10:51:04 UTC (25,124 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 11:51:48 UTC (25,124 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/03/06",
      "title": "Shaken, Not Stirred: A Novel Dataset for Visual Understanding of Glasses in Human-Robot Bartending Tasks",
      "tasks": [
        "Object",
        "object-detection",
        "Object Detection"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18289",
      "abstract": "AI's exponential growth intensifies computational demands and energy challenges. While practitioners employ various optimization techniques, that we refer as \"knobs\" in this paper, to tune model efficiency, these are typically afterthoughts and reactive ad-hoc changes applied in isolation without understanding their combinatorial effects on energy efficiency. This paper emphasizes on treating energy efficiency as the first-class citizen and as a fundamental design consideration for a compute-intensive pipeline. We show that strategic selection across five AI pipeline phases (data, model, training, system, inference) creates cascading efficiency. Experimental validation shows orthogonal combinations reduce energy consumption by up to $94.6$% while preserving $95.95$% of the original F1 score of non-optimized pipelines. This curated approach provides actionable frameworks for informed sustainable AI that balance efficiency, performance, and environmental responsibility.",
      "authors": [
        "Rajput, Saurabhsingh",
        "Saad, Mootez",
        "Sharma, Tushar"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18289v1",
        "Other Formats": "https://arxiv.org/format/2506.18289",
        "TeX Source": "https://arxiv.org/src/2506.18289",
        "View PDF": "https://arxiv.org/pdf/2506.18289"
      },
      "subjects": [
        "Software Engineering (cs.SE)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 04:52:08 UTC (835 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Tu(r)ning AI Green: Exploring Energy Efficiency Cascading with Orthogonal Optimizations",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18335",
      "abstract": "Segmenting biomarkers in medical images is crucial for various biotech applications. Despite advances, Transformer and CNN based methods often struggle with variations in staining and morphology, limiting feature extraction. In medical image segmentation, where datasets often have limited sample availability, recent state-of-the-art (SOTA) methods achieve higher accuracy by leveraging pre-trained encoders, whereas end-to-end methods tend to underperform. This is due to challenges in effectively transferring rich multiscale features from encoders to decoders, as well as limitations in decoder efficiency. To address these issues, we propose an architecture that captures multi-scale local and global contextual information and a novel decoder design, which effectively integrates features from the encoder, emphasizes important channels and regions, and reconstructs spatial dimensions to enhance segmentation accuracy. Our method, compatible with various encoders, outperforms SOTA methods, as demonstrated by experiments on four datasets and ablation studies. Specifically, our method achieves absolute performance gains of 2.76% on MoNuSeg, 3.12% on DSB, 2.87% on Electron Microscopy, and 4.03% on TNBC datasets compared to existing SOTA methods. Code: https://github.com/saadwazir/MCADS-Decoder",
      "authors": [
        "Wazir, Saad",
        "Kim, Daeyoung"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18335v1",
        "Other Formats": "https://arxiv.org/format/2506.18335",
        "TeX Source": "https://arxiv.org/src/2506.18335",
        "View PDF": "https://arxiv.org/pdf/2506.18335"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 06:32:36 UTC (2,573 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Rethinking Decoder Design: Improving Biomarker Segmentation Using Depth-to-Space Restoration and Residual Linear Attention",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18825",
      "abstract": "Imitation learning (IL), particularly when leveraging high-dimensional visual inputs for policy training, has proven intuitive and effective in complex bimanual manipulation tasks. Nonetheless, the generalization capability of visuomotor policies remains limited, especially when small demonstration datasets are available. Accumulated errors in visuomotor policies significantly hinder their ability to complete long-horizon tasks. To address these limitations, we propose SViP, a framework that seamlessly integrates visuomotor policies into task and motion planning (TAMP). SViP partitions human demonstrations into bimanual and unimanual operations using a semantic scene graph monitor. Continuous decision variables from the key scene graph are employed to train a switching condition generator. This generator produces parameterized scripted primitives that ensure reliable performance even when encountering out-of-the-distribution observations. Using only 20 real-world demonstrations, we show that SViP enables visuomotor policies to generalize across out-of-distribution initial conditions without requiring object pose estimators. For previously unseen tasks, SViP automatically discovers effective solutions to achieve the goal, leveraging constraint modeling in TAMP formulism. In real-world experiments, SViP outperforms state-of-the-art generative IL methods, indicating wider applicability for more complex tasks. Project website: https://sites.google.com/view/svip-bimanual",
      "authors": [
        "Chen, Yizhou",
        "Xu, Hang",
        "Yu, Dongjie",
        "Zhang, Zeqing",
        "Ren, Yi",
        "Pan, Jia"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18825v1",
        "Other Formats": "https://arxiv.org/format/2506.18825",
        "TeX Source": "https://arxiv.org/src/2506.18825",
        "View PDF": "https://arxiv.org/pdf/2506.18825"
      },
      "subjects": [
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 16:38:29 UTC (31,532 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "SViP: Sequencing Bimanual Visuomotor Policies with Object-Centric Motion Primitives",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18312",
      "abstract": "This paper explores the use of unlearning methods for training data attribution (TDA) in music generative models trained on large-scale datasets. TDA aims to identify which specific training data points contributed to the generation of a particular output from a specific model. This is crucial in the context of AI-generated music, where proper recognition and credit for original artists are generally overlooked. By enabling white-box attribution, our work supports a fairer system for acknowledging artistic contributions and addresses pressing concerns related to AI ethics and copyright. We apply unlearning-based attribution to a text-to-music diffusion model trained on a large-scale dataset and investigate its feasibility and behavior in this setting. To validate the method, we perform a grid search over different hyperparameter configurations and quantitatively evaluate the consistency of the unlearning approach. We then compare attribution patterns from unlearning with those from a similarity-based approach. Our findings suggest that unlearning-based approaches can be effectively adapted to music generative models, introducing large-scale TDA to this domain and paving the way for more ethical and accountable AI systems for music creation.",
      "authors": [
        "Choi, Woosung",
        "Koo, Junghyun",
        "Cheuk, Kin Wai",
        "Serr\u00e0, Joan",
        "Mart\u00ednez-Ram\u00edrez, Marco A.",
        "Ikemiya, Yukara",
        "Murata, Naoki",
        "Takida, Yuhta",
        "Liao, Wei-Hsiang",
        "Mitsufuji, Yuki"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18312v1",
        "Other Formats": "https://arxiv.org/format/2506.18312",
        "TeX Source": "https://arxiv.org/src/2506.18312",
        "View PDF": "https://arxiv.org/pdf/2506.18312"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 05:58:43 UTC (307 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Large-Scale Training Data Attribution for Music Generative Models via Unlearning",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18463",
      "abstract": "We introduce DIP, a novel unsupervised post-training method designed to enhance dense image representations in large-scale pretrained vision encoders for in-context scene understanding. Unlike prior approaches that rely on complex self-distillation architectures, our method trains the vision encoder using pseudo-tasks that explicitly simulate downstream in-context scenarios, inspired by meta-learning principles. To enable post-training on unlabeled data, we propose an automatic mechanism for generating in-context tasks that combines a pretrained diffusion model and the vision encoder itself. DIP is simple, unsupervised, and computationally efficient, requiring less than 9 hours on a single A100 GPU. By learning dense representations through pseudo in-context tasks, it achieves strong performance across a wide variety of downstream real-world in-context scene understanding tasks. It outperforms both the initial vision encoder and prior methods, offering a practical and effective solution for improving dense representations. Code available here: https://github.com/sirkosophia/DIP",
      "authors": [
        "Sirko-Galouchenko, Sophia",
        "Gidaris, Spyros",
        "Vobecky, Antonin",
        "Bursuc, Andrei",
        "Thome, Nicolas"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18463v1",
        "Other Formats": "https://arxiv.org/format/2506.18463",
        "TeX Source": "https://arxiv.org/src/2506.18463",
        "View PDF": "https://arxiv.org/pdf/2506.18463"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 10:01:14 UTC (37,502 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "DIP: Unsupervised Dense In-Context Post-training of Visual Representations",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2503.15195",
      "abstract": "Traditional machine learning models for Handwritten Text Recognition (HTR) rely on supervised training, requiring extensive manual annotations, and often produce errors due to the separation between layout and text processing. In contrast, Multimodal Large Language Models (MLLMs) offer a general approach to recognizing diverse handwriting styles without the need for model-specific training. The study benchmarks various proprietary and open-source LLMs against Transkribus models, evaluating their performance on both modern and historical datasets written in English, French, German, and Italian. In addition, emphasis is placed on testing the models' ability to autonomously correct previously generated outputs. Findings indicate that proprietary models, especially Claude 3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs achieve excellent results in recognizing modern handwriting and exhibit a preference for the English language due to their pre-training dataset composition. Comparisons with Transkribus show no consistent advantage for either approach. Moreover, LLMs demonstrate limited ability to autonomously correct errors in zero-shot transcriptions.",
      "authors": [
        "Crosilla, Giorgia",
        "Klic, Lukas",
        "Colavizza, Giovanni"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "Other Formats": "https://arxiv.org/format/2503.15195",
        "View PDF": "https://arxiv.org/pdf/2503.15195"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 19 Mar 2025 13:33:29 UTC (562 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Thu, 20 Mar 2025 15:49:10 UTC (952 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Mon, 23 Jun 2025 09:48:02 UTC (988 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2025/03/19",
      "title": "Benchmarking Large Language Models for Handwritten Text Recognition",
      "tasks": [
        "Benchmarking",
        "Handwritten Text Recognition",
        "HTR"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.00751",
      "abstract": "Text-to-image (T2I) diffusion models are widely used in image editing due to their powerful generative capabilities. However, achieving fine-grained control over specific object attributes, such as color and material, remains a considerable challenge. Existing methods often fail to accurately modify these attributes or compromise structural integrity and overall image consistency. To fill this gap, we introduce Structure Preservation and Attribute Amplification (SPAA), a novel training-free framework that enables precise generation of color and material attributes for the same object by intelligently manipulating self-attention maps and cross-attention values within diffusion models. Building on SPAA, we integrate multi-modal large language models (MLLMs) to automate data curation and instruction generation. Leveraging this object attribute data collection engine, we construct the Attribute Dataset, encompassing a comprehensive range of colors and materials across diverse object categories. Using this generated dataset, we propose InstructAttribute, an instruction-tuned model that enables fine-grained and object-level attribute editing through natural language prompts. This capability holds significant practical implications for diverse fields, from accelerating product design and e-commerce visualization to enhancing virtual try-on experiences. Extensive experiments demonstrate that InstructAttribute outperforms existing instruction-based baselines, achieving a superior balance between attribute modification accuracy and structural preservation.",
      "authors": [
        "Yin, Xingxi",
        "Zhang, Jingfeng",
        "Deng, Yue",
        "Li, Zhi",
        "Li, Yicheng",
        "Zhang, Yin"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2505.00751v2",
        "Other Formats": "https://arxiv.org/format/2505.00751",
        "TeX Source": "https://arxiv.org/src/2505.00751",
        "View PDF": "https://arxiv.org/pdf/2505.00751"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 1 May 2025 03:24:28 UTC (25,131 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 13:49:33 UTC (45,074 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/01",
      "title": "InstructAttribute: Fine-grained Object Attributes editing with Instruction",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.18846",
      "abstract": "Sixth Generation (6G) wireless networks, which are expected to be deployed in the 2030s, have already created great excitement in academia and the private sector with their extremely high communication speed and low latency rates. However, despite the ultra-low latency, high throughput, and AI-assisted orchestration capabilities they promise, they are vulnerable to stealthy and long-term Advanced Persistent Threats (APTs). Large Language Models (LLMs) stand out as an ideal candidate to fill this gap with their high success in semantic reasoning and threat intelligence. In this paper, we present a comprehensive systematic review and taxonomy study for LLM-assisted APT detection in 6G networks. We address five research questions, namely, semantic merging of fragmented logs, encrypted traffic analysis, edge distribution constraints, dataset/modeling techniques, and reproducibility trends, by leveraging most recent studies on the intersection of LLMs, APTs, and 6G wireless networks. We identify open challenges such as explainability gaps, data scarcity, edge hardware limitations, and the need for real-time slicing-aware adaptation by presenting various taxonomies such as granularity, deployment models, and kill chain stages. We then conclude the paper by providing several research gaps in 6G infrastructures for future researchers. To the best of our knowledge, this paper is the first comprehensive systematic review and classification study on LLM-based APT detection in 6G networks.",
      "authors": [
        "Golec, Muhammed",
        "Khamayseh, Yaser",
        "Melhem, Suhib Bani",
        "Alwarafy, Abdulmalik"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2505.18846v2",
        "Other Formats": "https://arxiv.org/format/2505.18846",
        "TeX Source": "https://arxiv.org/src/2505.18846",
        "View PDF": "https://arxiv.org/pdf/2505.18846"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 24 May 2025 19:42:11 UTC (991 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 14:37:53 UTC (991 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/24",
      "title": "LLM-Driven APT Detection for 6G Wireless Networks: A Systematic Review and Taxonomy",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18217",
      "abstract": "Shape estimation for transparent objects is challenging due to their complex light transport. To circumvent these difficulties, we leverage the Shape from Polarization (SfP) technique in the Long-Wave Infrared (LWIR) spectrum, where most materials are opaque and emissive. While a few prior studies have explored LWIR SfP, these attempts suffered from significant errors due to inadequate polarimetric modeling, particularly the neglect of reflection. Addressing this gap, we formulated a polarization model that explicitly accounts for the combined effects of emission and reflection. Based on this model, we estimated surface normals using not only a direct model-based method but also a learning-based approach employing a neural network trained on a physically-grounded synthetic dataset. Furthermore, we modeled the LWIR polarimetric imaging process, accounting for inherent systematic errors to ensure accurate polarimetry. We implemented a prototype system and created ThermoPol, the first real-world benchmark dataset for LWIR SfP. Through comprehensive experiments, we demonstrated the high accuracy and broad applicability of our method across various materials, including those transparent in the visible spectrum.",
      "authors": [
        "Kitazawa, Kazuma",
        "Takatani, Tsuyoshi"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18217v1",
        "Other Formats": "https://arxiv.org/format/2506.18217",
        "TeX Source": "https://arxiv.org/src/2506.18217",
        "View PDF": "https://arxiv.org/pdf/2506.18217"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 00:33:17 UTC (3,312 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Shape from Polarization of Thermal Emission and Reflection",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2405.14239",
      "abstract": "Vision-language contrastive learning frameworks such as CLIP enable learning representations from natural language supervision and provide strong zero-shot classification capabilities. However, due to the nature of the supervisory signal in these paradigms, they lack the ability to learn localized features, leading to degraded performance on dense prediction tasks such as segmentation and detection. On the other hand, self-supervised learning methods have shown the ability to learn granular representations, complementing the high-level features in vision-language training. In this work, we present Harmony, a framework that combines vision-language training with discriminative and generative self-supervision to learn visual features that can be generalized across different downstream vision tasks. Our framework is specifically designed to work on web-scraped data by not relying on negative examples in the self-supervised learning path and addressing the one-to-one correspondence issue using soft CLIP targets generated by an EMA model. Moreover, Harmony optimizes for five different objectives simultaneously, efficiently utilizing the supervision in each data example, making it even more suited in data-constrained settings. We comprehensively evaluate Harmony across various vision downstream tasks and find that it significantly outperforms the baseline CLIP and outperforms the previously leading joint self- and weakly supervised methods, SLIP, MaskCLIP, and DetailCLIP.",
      "authors": [
        "Baharoon, Mohammed",
        "Klein, Jonathan",
        "Michels, Dominik L."
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2405.14239v3",
        "Other Formats": "https://arxiv.org/format/2405.14239",
        "TeX Source": "https://arxiv.org/src/2405.14239",
        "View PDF": "https://arxiv.org/pdf/2405.14239"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 23 May 2024 07:18:08 UTC (1,142 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Wed, 26 Mar 2025 16:23:16 UTC (4,268 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Mon, 23 Jun 2025 09:13:37 UTC (3,379 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/05/23",
      "title": "Harmony: A Joint Self-Supervised and Weakly-Supervised Framework for Learning General Purpose Visual Representations",
      "repo_urls": [
        "https://github.com/mohammedsb/harmony"
      ],
      "tasks": [
        "Contrastive Learning",
        "Instance Segmentation",
        "object-detection",
        "Object Detection",
        "Segmentation",
        "Self-Supervised Learning",
        "Semantic Segmentation",
        "Zero-Shot Learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.14568",
      "abstract": "Automating table extraction (TE) from business documents is critical for industrial workflows but remains challenging due to sparse annotations and error-prone multi-stage pipelines. While semi-supervised learning (SSL) can leverage unlabeled data, existing methods rely on confidence scores that poorly reflect extraction quality. We propose QUEST, a Quality-aware Semi-supervised Table extraction framework designed for business documents. QUEST introduces a novel quality assessment model that evaluates structural and contextual features of extracted tables, trained to predict F1 scores instead of relying on confidence metrics. This quality-aware approach guides pseudo-label selection during iterative SSL training, while diversity measures (DPP, Vendi score, IntDiv) mitigate confirmation bias. Experiments on a proprietary business dataset (1000 annotated + 10000 unannotated documents) show QUEST improves F1 from 64% to 74% and reduces empty predictions by 45% (from 12% to 6.5%). On the DocILE benchmark (600 annotated + 20000 unannotated documents), QUEST achieves a 50% F1 score (up from 42%) and reduces empty predictions by 19% (from 27% to 22%). The framework's interpretable quality assessments and robustness to annotation scarcity make it particularly suited for business documents, where structural consistency and data completeness are paramount.",
      "authors": [
        "Thomas, Eliott",
        "Coustaty, Mickael",
        "Joseph, Aurelie",
        "Deloin, Gaspar",
        "Carel, Elodie",
        "D'Andecy, Vincent Poulain",
        "Ogier, Jean-Marc"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.14568v2",
        "Other Formats": "https://arxiv.org/format/2506.14568",
        "TeX Source": "https://arxiv.org/src/2506.14568",
        "View PDF": "https://arxiv.org/pdf/2506.14568"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Tue, 17 Jun 2025 14:25:44 UTC (8,834 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 09:53:21 UTC (8,833 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/06/17",
      "title": "QUEST: Quality-aware Semi-supervised Table Extraction for Business Documents",
      "tasks": [
        "Pseudo Label",
        "Table Extraction"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18679",
      "abstract": "We introduce MARL-MambaContour, the first contour-based medical image segmentation framework based on Multi-Agent Reinforcement Learning (MARL). Our approach reframes segmentation as a multi-agent cooperation task focused on generate topologically consistent object-level contours, addressing the limitations of traditional pixel-based methods which could lack topological constraints and holistic structural awareness of anatomical regions. Each contour point is modeled as an autonomous agent that iteratively adjusts its position to align precisely with the target boundary, enabling adaptation to blurred edges and intricate morphologies common in medical images. This iterative adjustment process is optimized by a contour-specific Soft Actor-Critic (SAC) algorithm, further enhanced with the Entropy Regularization Adjustment Mechanism (ERAM) which dynamically balance agent exploration with contour smoothness. Furthermore, the framework incorporates a Mamba-based policy network featuring a novel Bidirectional Cross-attention Hidden-state Fusion Mechanism (BCHFM). This mechanism mitigates potential memory confusion limitations associated with long-range modeling in state space models, thereby facilitating more accurate inter-agent information exchange and informed decision-making. Extensive experiments on five diverse medical imaging datasets demonstrate the state-of-the-art performance of MARL-MambaContour, highlighting its potential as an accurate and robust clinical application.",
      "authors": [
        "Zhang, Ruicheng",
        "Sun, Yu",
        "Zhang, Zeyu",
        "Li, Jinai",
        "Liu, Xiaofan",
        "Fan, Au Hoi",
        "Guo, Haowei",
        "Yan, Puxin"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18679v1",
        "Other Formats": "https://arxiv.org/format/2506.18679",
        "TeX Source": "https://arxiv.org/src/2506.18679",
        "View PDF": "https://arxiv.org/pdf/2506.18679"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 14:22:49 UTC (8,732 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning for Active Contour Optimization in Medical Image Segmentation",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18219",
      "abstract": "Context: There is an increase in the investment and development of data-intensive (DI) solutions, systems that manage large amounts of data. Without careful management, this growing investment will also grow associated technical debt (TD). Delivery of DI solutions requires a multidisciplinary skill set, but there is limited knowledge about how multidisciplinary teams develop DI systems and manage TD. Objective: This research contributes empirical, practice based insights about multidisciplinary DI team TD management practices. Method: This research was conducted as an exploratory observation case study. We used socio-technical grounded theory (STGT) for data analysis to develop concepts and categories that articulate TD and TDs debt management practices. Results: We identify TD that the DI team deals with, in particular technical data components debt and pipeline debt. We explain how the team manages the TD, assesses TD, what TD treatments they consider and how they implement TD treatments to fit sprint capacity constraints. Conclusion: We align our findings to existing TD and TDM taxonomies, discuss their implications and highlight the need for new implementation patterns and tool support for multidisciplinary DI teams.",
      "authors": [
        "Graetsch, Ulrike M.",
        "Hoda, Rashina",
        "Khalazjadeh, Hourieh",
        "Shahin, Mojtaba",
        "Grundy, John"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18219v1",
        "Other Formats": "https://arxiv.org/format/2506.18219",
        "TeX Source": "https://arxiv.org/src/2506.18219",
        "View PDF": "https://arxiv.org/pdf/2506.18219"
      },
      "subjects": [
        "Software Engineering (cs.SE)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 00:53:45 UTC (3,945 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Managing Technical Debt in a Multidisciplinary Data Intensive Software Team: an Observational Case Study",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18604",
      "abstract": "We present a novel simulation-free framework for training continuous-time diffusion processes over very general objective functions. Existing methods typically involve either prescribing the optimal diffusion process -- which only works for heavily restricted problem formulations -- or require expensive simulation to numerically obtain the time-dependent densities and sample from the diffusion process. In contrast, we propose a coupled parameterization which jointly models a time-dependent density function, or probability path, and the dynamics of a diffusion process that generates this probability path. To accomplish this, our approach directly bakes in the Fokker-Planck equation and density function requirements as hard constraints, by extending and greatly simplifying the construction of Neural Conservation Laws. This enables simulation-free training for a large variety of problem formulations, from data-driven objectives as in generative modeling and dynamical optimal transport, to optimality-based objectives as in stochastic optimal control, with straightforward extensions to mean-field objectives due to the ease of accessing exact density functions. We validate our method in a diverse range of application domains from modeling spatio-temporal events to learning optimal dynamics from population data.",
      "authors": [
        "Hua, Mengjian",
        "Vanden-Eijnden, Eric",
        "Chen, Ricky T. Q."
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18604v1",
        "Other Formats": "https://arxiv.org/format/2506.18604",
        "TeX Source": "https://arxiv.org/src/2506.18604",
        "View PDF": "https://arxiv.org/pdf/2506.18604"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 13:04:23 UTC (2,124 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Simulation-Free Differential Dynamics through Neural Conservation Laws",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2408.14505",
      "abstract": "Spatio-temporal forecasting is pivotal in numerous real-world applications, including transportation planning, energy management, and climate monitoring. In this work, we aim to harness the reasoning and generalization abilities of Pre-trained Language Models (PLMs) for more effective spatio-temporal forecasting, particularly in data-scarce scenarios. However, recent studies uncover that PLMs, which are primarily trained on textual data, often falter when tasked with modeling the intricate correlations in numerical time series, thereby limiting their effectiveness in comprehending spatio-temporal data. To bridge the gap, we propose RePST, a semantic-oriented PLM reprogramming framework tailored for spatio-temporal forecasting. Specifically, we first propose a semantic-oriented decomposer that adaptively disentangles spatially correlated time series into interpretable sub-components, which facilitates PLM to understand sophisticated spatio-temporal dynamics via a divide-and-conquer strategy. Moreover, we propose a selective discrete reprogramming scheme, which introduces an expanded spatio-temporal vocabulary space to project spatio-temporal series into discrete representations. This scheme minimizes the information loss during reprogramming and enriches the representations derived by PLMs. Extensive experiments on real-world datasets show that the proposed RePST outperforms twelve state-of-the-art baseline methods, particularly in data-scarce scenarios, highlighting the effectiveness and superior generalization capabilities of PLMs for spatio-temporal forecasting. Our codes can be found at https://github.com/usail-hkust/REPST.",
      "authors": [
        "Wang, Hao",
        "Han, Jindong",
        "Fan, Wei",
        "Sun, Leilei",
        "Liu, Hao"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2408.14505v3",
        "Other Formats": "https://arxiv.org/format/2408.14505",
        "TeX Source": "https://arxiv.org/src/2408.14505",
        "View PDF": "https://arxiv.org/pdf/2408.14505"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 24 Aug 2024 07:59:36 UTC (2,139 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Fri, 4 Oct 2024 17:08:17 UTC (514 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Mon, 23 Jun 2025 07:42:58 UTC (1,172 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/08/24",
      "title": "RePST: Language Model Empowered Spatio-Temporal Forecasting via Semantic-Oriented Reprogramming",
      "tasks": [
        "energy management",
        "Language Modeling",
        "Language Modelling",
        "Spatio-Temporal Forecasting",
        "Time Series",
        "Time Series Forecasting"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2503.18665",
      "abstract": "The development of Generalist Virtual Agents (GVAs) has shown significant promise in autonomous task execution. However, current training paradigms face critical limitations, including reliance on outcome supervision and labor-intensive human annotations. To address these challenges, we propose Similar, a Step-Wise Multi-Dimensional Generalist Reward Model, which offers fine-grained signals for agent training and can choose better action for inference-time scaling. Specifically, we begin by systematically defining five dimensions for evaluating agent actions. Building on this framework, we design an MCTS-P algorithm to automatically collect and annotate step-wise, five-dimensional agent execution data. Using this data, we train Similar with the Triple-M strategy. Furthermore, we introduce the first benchmark in the virtual agent domain for step-wise, multi-dimensional reward model training and evaluation, named SRM. This benchmark consists of two components: SRMTrain, which serves as the training set for Similar, and SRMEval, a manually selected test set for evaluating the reward model. Experimental results demonstrate that Similar, through its step-wise, multi-dimensional assessment and synergistic gain, provides GVAs with effective intermediate signals during both training and inference-time scaling. The project is available at https://github.com/antgroup/Similar.",
      "authors": [
        "Miao, Bingchen",
        "Wu, Yang",
        "Gao, Minghe",
        "Yu, Qifan",
        "Bu, Wendong",
        "Zhang, Wenqiao",
        "Li, Yunfei",
        "Tang, Siliang",
        "Chua, Tat-Seng",
        "Li, Juncheng"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "Other Formats": "https://arxiv.org/format/2503.18665",
        "TeX Source": "https://arxiv.org/src/2503.18665",
        "View PDF": "https://arxiv.org/pdf/2503.18665"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 24 Mar 2025 13:30:47 UTC (7,906 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 11:17:25 UTC (4,624 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/03/24",
      "title": "Boosting Virtual Agent Learning and Reasoning: A Step-Wise, Multi-Dimensional, and Generalist Reward Model with Benchmark",
      "repo_urls": [
        "https://github.com/galery23/similar-v1"
      ],
      "tasks": [],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2501.11299",
      "abstract": "Many keypoint detection and description methods have been proposed for image matching or registration. While these methods demonstrate promising performance for single-modality image matching, they often struggle with multimodal data because the descriptors trained on single-modality data tend to lack robustness against the non-linear variations present in multimodal data. Extending such methods to multimodal image matching often requires well-aligned multimodal data to learn modality-invariant descriptors. However, acquiring such data is often costly and impractical in many real-world scenarios. To address this challenge, we propose a modality-invariant feature learning network (MIFNet) to compute modality-invariant features for keypoint descriptions in multimodal image matching using only single-modality training data. Specifically, we propose a novel latent feature aggregation module and a cumulative hybrid aggregation module to enhance the base keypoint descriptors trained on single-modality data by leveraging pre-trained features from Stable Diffusion models. %, our approach generates robust and invariant features across diverse and unknown modalities. We validate our method with recent keypoint detection and description methods in three multimodal retinal image datasets (CF-FA, CF-OCT, EMA-OCTA) and two remote sensing datasets (Optical-SAR and Optical-NIR). Extensive experiments demonstrate that the proposed MIFNet is able to learn modality-invariant feature for multimodal image matching without accessing the targeted modality and has good zero-shot generalization ability. The code will be released at https://github.com/lyp-deeplearning/MIFNet.",
      "authors": [
        "Liu, Yepeng",
        "Sun, Zhichao",
        "Yu, Baosheng",
        "Zhao, Yitian",
        "Du, Bo",
        "Xu, Yongchao",
        "Cheng, Jun"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2501.11299v2",
        "Other Formats": "https://arxiv.org/format/2501.11299",
        "TeX Source": "https://arxiv.org/src/2501.11299",
        "View PDF": "https://arxiv.org/pdf/2501.11299"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 20 Jan 2025 06:56:30 UTC (21,816 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 10:56:43 UTC (21,247 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/01/20",
      "title": "MIFNet: Learning Modality-Invariant Features for Generalizable Multimodal Image Matching",
      "tasks": [
        "Keypoint Detection",
        "Zero-shot Generalization"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18499",
      "abstract": "The increasing reliance on machine learning (ML) models for decision-making requires high-quality training data. However, access to real-world datasets is often restricted due to privacy concerns, proprietary restrictions, and incomplete data availability. As a result, synthetic data generation (SDG) has emerged as a viable alternative, enabling the creation of artificial datasets that preserve the statistical properties of real data while ensuring privacy compliance. Despite its advantages, synthetic data is often overly clean and lacks real-world imperfections, such as missing values, noise, outliers, and misclassified labels, which can significantly impact model generalization and robustness. To address this limitation, we introduce Pucktrick, a Python library designed to systematically contaminate synthetic datasets by introducing controlled errors. The library supports multiple error types, including missing data, noisy values, outliers, label misclassification, duplication, and class imbalance, offering a structured approach to evaluating ML model resilience under real-world data imperfections. Pucktrick provides two contamination modes: one for injecting errors into clean datasets and another for further corrupting already contaminated datasets. Through extensive experiments on real-world financial datasets, we evaluate the impact of systematic data contamination on model performance. Our findings demonstrate that ML models trained on contaminated synthetic data outperform those trained on purely synthetic, error-free data, particularly for tree-based and linear models such as SVMs and Extra Trees.",
      "authors": [
        "Agostini, Alessandra",
        "Maurino, Andrea",
        "Spahiu, Blerina"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18499v1",
        "Other Formats": "https://arxiv.org/format/2506.18499",
        "TeX Source": "https://arxiv.org/src/2506.18499",
        "View PDF": "https://arxiv.org/pdf/2506.18499"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Databases (cs.DB)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 10:51:45 UTC (1,067 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "PuckTrick: A Library for Making Synthetic Data More Realistic",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18525",
      "abstract": "We present a perspective on federated learning in chemical engineering that envisions collaborative efforts in machine learning (ML) developments within the chemical industry. Large amounts of chemical and process data are proprietary to chemical companies and are therefore locked in data silos, hindering the training of ML models on large data sets in chemical engineering. Recently, the concept of federated learning has gained increasing attention in ML research, enabling organizations to jointly train machine learning models without disclosure of their individual data. We discuss potential applications of federated learning in several fields of chemical engineering, from the molecular to the process scale. In addition, we apply federated learning in two exemplary case studies that simulate practical scenarios of multiple chemical companies holding proprietary data sets: (i) prediction of binary mixture activity coefficients with graph neural networks and (ii) system identification of a distillation column with autoencoders. Our results indicate that ML models jointly trained with federated learning yield significantly higher accuracy than models trained by each chemical company individually and can perform similarly to models trained on combined datasets from all companies. Federated learning has therefore great potential to advance ML models in chemical engineering while respecting corporate data privacy, making it promising for future industrial applications.",
      "authors": [
        "Rittig, Jan G.",
        "Kortmann, Clemens"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18525v1",
        "Other Formats": "https://arxiv.org/format/2506.18525",
        "TeX Source": "https://arxiv.org/src/2506.18525",
        "View PDF": "https://arxiv.org/pdf/2506.18525"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Chemical Physics (physics.chem-ph)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 11:27:34 UTC (727 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Federated Learning from Molecules to Processes: A Perspective",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18600",
      "abstract": "A potential concern when simulating populations of large language models (LLMs) is data contamination, i.e. the possibility that training data may shape outcomes in unintended ways. While this concern is important and may hinder certain experiments with multi-agent models, it does not preclude the study of genuinely emergent dynamics in LLM populations. The recent critique by Barrie and T\\\"ornberg [1] of the results of Flint Ashery et al. [2] offers an opportunity to clarify that self-organisation and model-dependent emergent dynamics can be studied in LLM populations, highlighting how such dynamics have been empirically observed in the specific case of social conventions.",
      "authors": [
        "Ashery, Ariel Flint",
        "Aiello, Luca Maria",
        "Baronchelli, Andrea"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18600v1",
        "Other Formats": "https://arxiv.org/format/2506.18600",
        "TeX Source": "https://arxiv.org/src/2506.18600",
        "View PDF": "https://arxiv.org/pdf/2506.18600"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Computer Science and Game Theory (cs.GT)",
        "Multiagent Systems (cs.MA)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 12:59:34 UTC (12 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Reply to \"Emergent LLM behaviors are observationally equivalent to data leakage\"",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18496",
      "abstract": "Knowledge Distillation (KD) is a widely adopted model compression technique where a compact student model learns from the output of a larger, pre-trained teacher. While effective in balanced settings, conventional KD suffers significantly when applied to long-tailed data distributions, as the teacher model tends to be biased toward head classes and provides limited supervision for tail classes. In this paper, we propose Long-Tailed Knowledge Distillation (LTKD), a novel framework tailored for class-imbalanced scenarios. We begin by reformulating the standard KD objective into two components: inter-group and intra-group Kullback-Leibler (KL) divergence, corresponding to the prediction distributions across and within class groups (head, medium, tail), respectively. This decomposition allows us to identify and quantify the sources of teacher bias. To address them, we introduce (1) a rebalanced inter-group loss that calibrates the teacher's group-level predictions and (2) a uniform intra-group loss that ensures equal contribution from all groups during distillation. Extensive experiments on CIFAR-100-LT, TinyImageNet-LT, and ImageNet-LT show that LTKD consistently outperforms existing KD methods, achieving significant gains in both overall accuracy and tail-class performance. Our results demonstrate that LTKD enables effective knowledge transfer even from biased teachers, making it a strong candidate for real-world deployment in resource-constrained and imbalanced settings.",
      "authors": [
        "Kim, Seonghak"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18496v1",
        "Other Formats": "https://arxiv.org/format/2506.18496",
        "TeX Source": "https://arxiv.org/src/2506.18496",
        "View PDF": "https://arxiv.org/pdf/2506.18496"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 10:46:44 UTC (845 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Biased Teacher, Balanced Student",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2303.14658",
      "abstract": "The generalization error of a learning algorithm refers to the discrepancy between the loss of a learning algorithm on training data and that on unseen testing data. Various information-theoretic bounds on the generalization error have been derived in the literature, where the mutual information between the training data and the hypothesis (the output of the learning algorithm) plays an important role. Focusing on the individual sample mutual information bound by Bu et al., which itself is a tightened version of the first bound on the topic by Russo et al. and Xu et al., this paper investigates the tightness of these bounds, in terms of the dependence of their convergence rates on the sample size $n$. It has been recognized that these bounds are in general not tight, readily verified for the exemplary quadratic Gaussian mean estimation problem, where the individual sample mutual information bound scales as $O(\\sqrt{1/n})$ while the true generalization error scales as $O(1/n)$. The first contribution of this paper is to show that the same bound can in fact be asymptotically tight if an appropriate assumption is made. In particular, we show that the fast rate can be recovered when the assumption is made on the excess risk instead of the loss function, which was usually done in existing literature. A theoretical justification is given for this choice. The second contribution of the paper is a new set of generalization error bounds based on the $(\\eta, c)$-central condition, a condition relatively easy to verify and has the property that the mutual information term directly determines the convergence rate of the bound. Several analytical and numerical examples are given to show the effectiveness of these bounds.",
      "authors": [
        "Wu, Xuetong",
        "Manton, Jonathan H.",
        "Aickelin, Uwe",
        "Zhu, Jingge"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2303.14658v3",
        "Other Formats": "https://arxiv.org/format/2303.14658",
        "TeX Source": "https://arxiv.org/src/2303.14658",
        "View PDF": "https://arxiv.org/pdf/2303.14658"
      },
      "subjects": [
        "Information Theory (cs.IT)",
        "Machine Learning (cs.LG)",
        "Machine Learning (stat.ML)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 26 Mar 2023 08:59:05 UTC (835 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Thu, 31 Oct 2024 09:16:01 UTC (862 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Mon, 23 Jun 2025 04:15:18 UTC (150 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2023/03/26",
      "title": "Fast Rate Information-theoretic Bounds on Generalization Errors",
      "tasks": [],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2411.16301",
      "abstract": "Interior design is a complex and creative discipline involving aesthetics, functionality, ergonomics, and materials science. Effective solutions must meet diverse requirements, typically producing multiple deliverables such as renderings and design drawings from various perspectives. Consequently, interior design processes are often inefficient and demand significant creativity. With advances in machine learning, generative models have emerged as a promising means of improving efficiency by creating designs from text descriptions or sketches. However, few generative works focus on interior design, leading to substantial discrepancies between outputs and practical needs, such as differences in size, spatial scope, and the lack of controllable generation quality. To address these challenges, we propose DiffDesign, a controllable diffusion model with meta priors for efficient interior design generation. Specifically, we utilize the generative priors of a 2D diffusion model pre-trained on a large image dataset as our rendering backbone. We further guide the denoising process by disentangling cross-attention control over design attributes, such as appearance, pose, and size, and introduce an optimal transfer-based alignment module to enforce view consistency. Simultaneously, we construct an interior design-specific dataset, DesignHelper, consisting of over 400 solutions across more than 15 spatial types and 15 design styles. This dataset helps fine-tune DiffDesign. Extensive experiments conducted on various benchmark datasets demonstrate the effectiveness and robustness of DiffDesign.",
      "authors": [
        "Yang, Yuxuan",
        "Geng, Tao"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2411.16301v3",
        "Other Formats": "https://arxiv.org/format/2411.16301",
        "TeX Source": "https://arxiv.org/src/2411.16301",
        "View PDF": "https://arxiv.org/pdf/2411.16301"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 25 Nov 2024 11:36:34 UTC (5,541 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Tue, 20 May 2025 08:47:10 UTC (5,541 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Mon, 23 Jun 2025 15:20:13 UTC (5,794 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/11/25",
      "title": "DiffDesign: Controllable Diffusion with Meta Prior for Efficient Interior Design Generation",
      "tasks": [
        "Denoising"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18234",
      "abstract": "Large vision-language models (VLMs) for autonomous driving (AD) are evolving beyond perception and cognition tasks toward motion planning. However, we identify two critical challenges in this direction: (1) VLMs tend to learn shortcuts by relying heavily on history input information, achieving seemingly strong planning results without genuinely understanding the visual inputs; and (2) the chain-ofthought (COT) reasoning processes are always misaligned with the motion planning outcomes, and how to effectively leverage the complex reasoning capability to enhance planning remains largely underexplored. In this paper, we start from a small-scale domain-specific VLM and propose Drive-R1 designed to bridges the scenario reasoning and motion planning for AD. Drive-R1 first undergoes the supervised finetuning on a elaborate dataset containing both long and short COT data. Drive-R1 is encouraged to reason step-by-step from visual input to final planning decisions. Subsequently, Drive-R1 is trained within a reinforcement learning framework that incentivizes the discovery of reasoning paths that are more informative for planning, guided by rewards based on predicted trajectories and meta actions. Experimental evaluations on the nuScenes and DriveLM-nuScenes benchmarks demonstrate that Drive-R1 achieves superior performance compared to existing state-of-the-art VLMs. We believe that Drive-R1 presents a promising direction for bridging reasoning and planning in AD, offering methodological insights for future research and applications.",
      "authors": [
        "Li, Yue",
        "Tian, Meng",
        "Zhu, Dechang",
        "Zhu, Jiangtong",
        "Lin, Zhenyu",
        "Xiong, Zhiwei",
        "Zhao, Xinhai"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18234v1",
        "Other Formats": "https://arxiv.org/format/2506.18234",
        "TeX Source": "https://arxiv.org/src/2506.18234",
        "View PDF": "https://arxiv.org/pdf/2506.18234"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 01:57:14 UTC (1,685 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2310.17173",
      "abstract": "We present a novel extension to the family of Soft Actor-Critic (SAC) algorithms. We argue that based on the Maximum Entropy Principle, discrete SAC can be further improved via additional statistical constraints derived from a surrogate critic policy. Furthermore, our findings suggests that these constraints provide an added robustness against potential domain shifts, which are essential for safe deployment of reinforcement learning agents in the real-world. We provide theoretical analysis and show empirical results on low data regimes for both in-distribution and out-of-distribution variants of Atari 2600 games.",
      "authors": [
        "Neo, Dexter",
        "Chen, Tsuhan"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2310.17173v2",
        "Other Formats": "https://arxiv.org/format/2310.17173",
        "TeX Source": "https://arxiv.org/src/2310.17173",
        "View PDF": "https://arxiv.org/pdf/2310.17173"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 26 Oct 2023 05:54:51 UTC (2,195 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 02:45:04 UTC (1,316 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2023/10/26",
      "title": "DSAC-C: Constrained Maximum Entropy for Robust Discrete Soft-Actor Critic",
      "tasks": [
        "Atari Games",
        "reinforcement-learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2401.01752",
      "abstract": "In recent years, the Vision Transformer (ViT) model has gradually become mainstream in various computer vision tasks, and the robustness of the model has received increasing attention. However, existing large models tend to prioritize performance during training, potentially neglecting the robustness, which may lead to serious security concerns. In this paper, we establish a new challenge: exploring how to use a small number of additional parameters for adversarial finetuning to quickly and effectively enhance the adversarial robustness of a standardly trained model. To address this challenge, we develop novel LNLoRA module, incorporating a learnable layer normalization before the conventional LoRA module, which helps mitigate magnitude differences in parameters between the adversarial and standard training paradigms. Furthermore, we propose the FullLoRA framework by integrating the learnable LNLoRA modules into all key components of ViT-based models while keeping the pretrained model frozen, which can significantly improve the model robustness via adversarial finetuning in a parameter-efficient manner. Extensive experiments on several datasets demonstrate the superiority of our proposed FullLoRA framework. It achieves comparable robustness with full finetuning while only requiring about 5\\% of the learnable parameters. This also effectively addresses concerns regarding extra model storage space and enormous training time caused by adversarial finetuning.",
      "authors": [
        "Yuan, Zheng",
        "Zhang, Jie",
        "Shan, Shiguang",
        "Chen, Xilin"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2401.01752v2",
        "Other Formats": "https://arxiv.org/format/2401.01752",
        "TeX Source": "https://arxiv.org/src/2401.01752",
        "View PDF": "https://arxiv.org/pdf/2401.01752"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 3 Jan 2024 14:08:39 UTC (296 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 16:57:04 UTC (845 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/01/03",
      "title": "FullLoRA: Efficiently Boosting the Robustness of Pretrained Vision Transformers",
      "tasks": [
        "Adversarial Robustness"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18259",
      "abstract": "In recent years, Federated Learning (FL) has emerged as a widely adopted privacy-preserving distributed training approach, attracting significant interest from both academia and industry. Research efforts have been dedicated to improving different aspects of FL, such as algorithm improvement, resource allocation, and client selection, to enable its deployment in distributed edge networks for practical applications. One of the reasons for the poor FL model performance is due to the worker dropout during training as the FL server may be located far away from the FL workers. To address this issue, an Hierarchical Federated Learning (HFL) framework has been introduced, incorporating an additional layer of edge servers to relay communication between the FL server and workers. While the HFL framework improves the communication between the FL server and workers, large number of communication rounds may still be required for model convergence, particularly when FL workers have non-independent and identically distributed (non-IID) data. Moreover, the FL workers are assumed to fully cooperate in the FL training process, which may not always be true in practical situations. To overcome these challenges, we propose a synthetic-data-empowered HFL framework that mitigates the statistical issues arising from non-IID local datasets while also incentivizing FL worker participation. In our proposed framework, the edge servers reward the FL workers in their clusters for facilitating the FL training process. To improve the performance of the FL model given the non-IID local datasets of the FL workers, the edge servers generate and distribute synthetic datasets to FL workers within their clusters. FL workers determine which edge server to associate with, considering the computational resources required to train on both their local datasets and the synthetic datasets.",
      "authors": [
        "Ng, Jer Shyuan",
        "Kalapaaking, Aditya Pribadi",
        "Xia, Xiaoyu",
        "Niyato, Dusit",
        "Khalil, Ibrahim",
        "Gondal, Iqbal"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18259v1",
        "Other Formats": "https://arxiv.org/format/2506.18259",
        "TeX Source": "https://arxiv.org/src/2506.18259",
        "View PDF": "https://arxiv.org/pdf/2506.18259"
      },
      "subjects": [
        "Distributed, Parallel, and Cluster Computing (cs.DC)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 03:19:05 UTC (385 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Edge Association Strategies for Synthetic Data Empowered Hierarchical Federated Learning with Non-IID Data",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18434",
      "abstract": "Artificial Intelligence (AI) holds significant promise for improving prognosis prediction in medical imaging, yet its effective application remains challenging. In this work, we introduce a structured benchmark explicitly designed to evaluate and compare the transferability of Convolutional Neural Networks and Foundation Models in predicting clinical outcomes in COVID-19 patients, leveraging diverse publicly available Chest X-ray datasets. Our experimental methodology extensively explores a wide set of fine-tuning strategies, encompassing traditional approaches such as Full Fine-Tuning and Linear Probing, as well as advanced Parameter-Efficient Fine-Tuning methods including Low-Rank Adaptation, BitFit, VeRA, and IA3. The evaluations were conducted across multiple learning paradigms, including both extensive full-data scenarios and more clinically realistic Few-Shot Learning settings, which are critical for modeling rare disease outcomes and rapidly emerging health threats. By implementing a large-scale comparative analysis involving a diverse selection of pretrained models, including general-purpose architectures pretrained on large-scale datasets such as CLIP and DINOv2, to biomedical-specific models like MedCLIP, BioMedCLIP, and PubMedCLIP, we rigorously assess each model's capacity to effectively adapt and generalize to prognosis tasks, particularly under conditions of severe data scarcity and pronounced class imbalance. The benchmark was designed to capture critical conditions common in prognosis tasks, including variations in dataset size and class distribution, providing detailed insights into the strengths and limitations of each fine-tuning strategy. This extensive and structured evaluation aims to inform the practical deployment and adoption of robust, efficient, and generalizable AI-driven solutions in real-world clinical prognosis prediction workflows.",
      "authors": [
        "Ruffini, Filippo",
        "Ayllon, Elena Mulero",
        "Shen, Linlin",
        "Soda, Paolo",
        "Guarrasi, Valerio"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18434v1",
        "Other Formats": "https://arxiv.org/format/2506.18434",
        "TeX Source": "https://arxiv.org/src/2506.18434",
        "View PDF": "https://arxiv.org/pdf/2506.18434"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 09:16:04 UTC (5,960 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for Prognosis Prediction in Medical Imaging",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18448",
      "abstract": "Language-driven grasp detection has the potential to revolutionize human-robot interaction by allowing robots to understand and execute grasping tasks based on natural language commands. However, existing approaches face two key challenges. First, they often struggle to interpret complex text instructions or operate ineffectively in densely cluttered environments. Second, most methods require a training or finetuning step to adapt to new domains, limiting their generation in real-world applications. In this paper, we introduce GraspMAS, a new multi-agent system framework for language-driven grasp detection. GraspMAS is designed to reason through ambiguities and improve decision-making in real-world scenarios. Our framework consists of three specialized agents: Planner, responsible for strategizing complex queries; Coder, which generates and executes source code; and Observer, which evaluates the outcomes and provides feedback. Intensive experiments on two large-scale datasets demonstrate that our GraspMAS significantly outperforms existing baselines. Additionally, robot experiments conducted in both simulation and real-world settings further validate the effectiveness of our approach.",
      "authors": [
        "Nguyen, Quang",
        "Le, Tri",
        "Nguyen, Huy",
        "Vo, Thieu",
        "Ta, Tung D.",
        "Huang, Baoru",
        "Vu, Minh N.",
        "Nguyen, Anh"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18448v1",
        "Other Formats": "https://arxiv.org/format/2506.18448",
        "TeX Source": "https://arxiv.org/src/2506.18448",
        "View PDF": "https://arxiv.org/pdf/2506.18448"
      },
      "subjects": [
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 09:34:50 UTC (3,028 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "GraspMAS: Zero-Shot Language-driven Grasp Detection with Multi-Agent System",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2410.04520",
      "abstract": "Ensemble methods are known for enhancing the accuracy and robustness of machine learning models by combining multiple base learners. However, standard approaches like greedy or random ensembling often fall short, as they assume a constant weight across samples for the ensemble members. This can limit expressiveness and hinder performance when aggregating the ensemble predictions. In this study, we explore employing regularized neural networks as ensemble methods, emphasizing the significance of dynamic ensembling to leverage diverse model predictions adaptively. Motivated by the risk of learning low-diversity ensembles, we propose regularizing the ensembling model by randomly dropping base model predictions during the training. We demonstrate this approach provides lower bounds for the diversity within the ensemble, reducing overfitting and improving generalization capabilities. Our experiments showcase that the regularized neural ensemblers yield competitive results compared to strong baselines across several modalities such as computer vision, natural language processing, and tabular data.",
      "authors": [
        "Arango, Sebastian Pineda",
        "Janowski, Maciej",
        "Purucker, Lennart",
        "Zela, Arber",
        "Hutter, Frank",
        "Grabocka, Josif"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2410.04520v2",
        "Other Formats": "https://arxiv.org/format/2410.04520",
        "TeX Source": "https://arxiv.org/src/2410.04520",
        "View PDF": "https://arxiv.org/pdf/2410.04520"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 6 Oct 2024 15:25:39 UTC (1,499 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 16:40:18 UTC (1,818 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/10/06",
      "title": "Regularized Neural Ensemblers",
      "tasks": [
        "Diversity"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18747",
      "abstract": "We introduce ContinualFlow, a principled framework for targeted unlearning in generative models via Flow Matching. Our method leverages an energy-based reweighting loss to softly subtract undesired regions of the data distribution without retraining from scratch or requiring direct access to the samples to be unlearned. Instead, it relies on energy-based proxies to guide the unlearning process. We prove that this induces gradients equivalent to Flow Matching toward a soft mass-subtracted target, and validate the framework through experiments on 2D and image domains, supported by interpretable visualizations and quantitative evaluations.",
      "authors": [
        "Simone, Lorenzo",
        "Bacciu, Davide",
        "Ma, Shuangge"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18747v1",
        "Other Formats": "https://arxiv.org/format/2506.18747",
        "TeX Source": "https://arxiv.org/src/2506.18747",
        "View PDF": "https://arxiv.org/pdf/2506.18747"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 15:20:58 UTC (8,158 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "ContinualFlow: Learning and Unlearning with Neural Flow Matching",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18685",
      "abstract": "In this study, we conducted an in-depth examination of the utility analysis of the differentially private mechanism (DPM). The authors of DPM have already established the probability of a good split being selected and of DPM halting. In this study, we expanded the analysis of the stopping criterion and provided an interpretation of these guarantees in the context of realistic input distributions. Our findings revealed constraints on the minimum cluster size and the metric weight for the scoring function. Furthermore, we introduced an interpretation of the utility of DPM through the lens of the clustering metric, the silhouette score. Our findings indicate that even when an optimal DPM-based split is employed, the silhouette score of the resulting clustering may still decline. This observation calls into question the suitability of the silhouette score as a clustering metric. Finally, we examined the potential of the underlying concept of DPM by linking it to a more theoretical view, that of $(\\xi, \\rho)$-separability. This extensive analysis of the theoretical guarantees of DPM allows a better understanding of its behaviour for arbitrary inputs. From these guarantees, we can analyse the impact of different hyperparameters and different input data sets, thereby promoting the application of DPM in practice for unknown settings and data sets.",
      "authors": [
        "Sch\u00fctt, Yara",
        "Mohammadi, Esfandiar"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18685v1",
        "Other Formats": "https://arxiv.org/format/2506.18685",
        "TeX Source": "https://arxiv.org/src/2506.18685",
        "View PDF": "https://arxiv.org/pdf/2506.18685"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 14:27:19 UTC (433 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Understanding the Theoretical Guarantees of DPM",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18764",
      "abstract": "Detecting when public discourse shifts in response to major events is crucial for understanding societal dynamics. Real-world data is high-dimensional, sparse, and noisy, making changepoint detection in this domain a challenging endeavor. In this paper, we leverage neural networks for changepoint detection in news data, introducing a method based on the so-called learning-by-confusion scheme, which was originally developed for detecting phase transitions in physical systems. We train classifiers to distinguish between articles from different time periods. The resulting classification accuracy is used to estimate the total variation distance between underlying content distributions, where significant distances highlight changepoints. We demonstrate the effectiveness of this method on both synthetic datasets and real-world data from The Guardian newspaper, successfully identifying major historical events including 9/11, the COVID-19 pandemic, and presidential elections. Our approach requires minimal domain knowledge, can autonomously discover significant shifts in public discourse, and yields a quantitative measure of change in content, making it valuable for journalism, policy analysis, and crisis monitoring.",
      "authors": [
        "Zsolnai, Csaba",
        "L\u00f6rch, Niels",
        "Arnold, Julian"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18764v1",
        "Other Formats": "https://arxiv.org/format/2506.18764",
        "TeX Source": "https://arxiv.org/src/2506.18764",
        "View PDF": "https://arxiv.org/pdf/2506.18764"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Computation and Language (cs.CL)",
        "Computers and Society (cs.CY)",
        "Social and Information Networks (cs.SI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 15:33:30 UTC (660 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Neural Total Variation Distance Estimators for Changepoint Detection in News Data",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18394",
      "abstract": "Memory-related errors in C programming continue to pose significant challenges in software development, primarily due to the complexities of manual memory management inherent in the language. These errors frequently serve as vectors for severe vulnerabilities, while their repair requires extensive knowledge of program logic and C's memory model. Automated Program Repair (APR) has emerged as a critical research area to address these challenges. Traditional APR approaches rely on expert-designed strategies and predefined templates, which are labor-intensive and constrained by the effectiveness of manual specifications. Deep learning techniques offer a promising alternative by automatically extracting repair patterns, but they require substantial training datasets and often lack interpretability. This paper introduces LTFix, a novel approach that harnesses the potential of Large Language Models (LLMs) for automated memory error repair, especially for complex repository-level errors that span multiple functions and files. We address two fundamental challenges in LLM-based memory error repair: a limited understanding of interprocedural memory management patterns and context window limitations for repository-wide analysis. Our approach utilizes a finite typestate automaton to guide the tracking of error-propagation paths and context trace, capturing both spatial (memory states) and temporal (execution history) dimensions of error behavior. This typestate-guided context retrieval strategy provides the LLM with concise yet semantically rich information relevant to erroneous memory management, effectively addressing the token limitation of LLMs.",
      "authors": [
        "Cheng, Xiao",
        "Guo, Zhihao",
        "Huo, Huan",
        "Sui, Yulei"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18394v1",
        "Other Formats": "https://arxiv.org/format/2506.18394",
        "TeX Source": "https://arxiv.org/src/2506.18394",
        "View PDF": "https://arxiv.org/pdf/2506.18394"
      },
      "subjects": [
        "Software Engineering (cs.SE)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 08:30:00 UTC (1,065 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Tracing Errors, Constructing Fixes: Repository-Level Memory Error Repair via Typestate-Guided Context Retrieval",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2501.06572",
      "abstract": "Deep learning models trained on finite data lack a complete understanding of the physical world. On the other hand, physics-informed neural networks (PINNs) are infused with such knowledge through the incorporation of mathematically expressible laws of nature into their training loss function. By complying with physical laws, PINNs provide advantages over purely data-driven models in limited-data regimes and present as a promising route towards Physical AI. This feature has propelled them to the forefront of scientific machine learning, a domain characterized by scarce and costly data. However, the vision of accurate physics-informed learning comes with significant challenges. This work examines PINNs for the first time in terms of model optimization and generalization, shedding light on the need for new algorithmic advances to overcome issues pertaining to the training speed, precision, and generalizability of today's PINN models. Of particular interest are gradient-free evolutionary algorithms (EAs) for optimizing the uniquely complex loss landscapes arising in PINN training. Methods synergizing gradient descent and EAs for discovering bespoke neural architectures and balancing multiple terms in physics-informed learning objectives are positioned as important avenues for future research. Another exciting track is to cast evolutionary as a meta-learner of generalizable PINN models. To substantiate these proposed avenues, we further highlight results from recent literature to showcase the early success of such approaches in addressing the aforementioned challenges in PINN optimization and generalization.",
      "authors": [
        "Wong, Jian Cheng",
        "Gupta, Abhishek",
        "Ooi, Chin Chun",
        "Chiu, Pao-Hsiung",
        "Liu, Jiao",
        "Ong, Yew-Soon"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2501.06572v3",
        "Other Formats": "https://arxiv.org/format/2501.06572",
        "TeX Source": "https://arxiv.org/src/2501.06572",
        "View PDF": "https://arxiv.org/pdf/2501.06572"
      },
      "subjects": [
        "Neural and Evolutionary Computing (cs.NE)",
        "Computational Engineering, Finance, and Science (cs.CE)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 11 Jan 2025 15:45:11 UTC (2,395 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 31 Mar 2025 15:37:28 UTC (2,398 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Mon, 23 Jun 2025 04:04:45 UTC (2,708 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2025/01/11",
      "title": "Evolutionary Optimization of Physics-Informed Neural Networks: Evo-PINN Frontiers and Opportunities",
      "tasks": [
        "Model Optimization",
        "Survey"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18645",
      "abstract": "Stochastic Gradient Descent (SGD) is fundamental for training deep neural networks, especially in non-convex settings. Understanding SGD's generalization properties is crucial for ensuring robust model performance on unseen data. In this paper, we analyze the generalization error bounds of SGD for non-convex learning by introducing the Type II perturbed SGD (T2pm-SGD), which accommodates both sub-Gaussian and bounded loss functions. The generalization error bound is decomposed into two components: the trajectory term and the flatness term. Our analysis improves the trajectory term to $O(n^{-1})$, significantly enhancing the previous $O((nb)^{-1/2})$ bound for bounded losses, where n is the number of training samples and b is the batch size. By selecting an optimal variance for the perturbation noise, the overall bound is further refined to $O(n^{-2/3})$. For sub-Gaussian loss functions, a tighter trajectory term is also achieved. In both cases, the flatness term remains stable across iterations and is smaller than those reported in previous literature, which increase with iterations. This stability, ensured by T2pm-SGD, leads to tighter generalization error bounds for both loss function types. Our theoretical results are validated through extensive experiments on benchmark datasets, including MNIST and CIFAR-10, demonstrating the effectiveness of T2pm-SGD in establishing tighter generalization bounds.",
      "authors": [
        "Xiong, Wenjun",
        "Ding, Juan",
        "Zuo, Xinlei",
        "Li, Qizhai"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18645v1",
        "Other Formats": "https://arxiv.org/format/2506.18645",
        "TeX Source": "https://arxiv.org/src/2506.18645",
        "View PDF": "https://arxiv.org/pdf/2506.18645"
      },
      "subjects": [
        "Machine Learning (stat.ML)",
        "Machine Learning (cs.LG)",
        "Methodology (stat.ME)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 13:47:25 UTC (288 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Tight Generalization Error Bounds for Stochastic Gradient Descent in Non-convex Learning",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18330",
      "abstract": "We introduce Confucius3-Math, an open-source large language model with 14B parameters that (1) runs efficiently on a single consumer-grade GPU; (2) achieves SOTA performances on a range of mathematical reasoning tasks, outperforming many models with significantly larger sizes. In particular, as part of our mission to enhancing education and knowledge dissemination with AI, Confucius3-Math is specifically committed to mathematics learning for Chinese K-12 students and educators. Built via post-training with large-scale reinforcement learning (RL), Confucius3-Math aligns with national curriculum and excels at solving main-stream Chinese K-12 mathematical problems with low cost. In this report we share our development recipe, the challenges we encounter and the techniques we develop to overcome them. In particular, we introduce three technical innovations: Targeted Entropy Regularization, Recent Sample Recovery and Policy-Specific Hardness Weighting. These innovations encompass a new entropy regularization, a novel data scheduling policy, and an improved group-relative advantage estimator. Collectively, they significantly stabilize the RL training, improve data efficiency, and boost performance. Our work demonstrates the feasibility of building strong reasoning models in a particular domain at low cost. We open-source our model and code at https://github.com/netease-youdao/Confucius3-Math.",
      "authors": [
        "Wu, Lixin",
        "Cai, Na",
        "Cheng, Qiao",
        "Wang, Jiachen",
        "Duan, Yitao"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18330v1",
        "Other Formats": "https://arxiv.org/format/2506.18330",
        "TeX Source": "https://arxiv.org/src/2506.18330",
        "View PDF": "https://arxiv.org/pdf/2506.18330"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 06:23:53 UTC (108 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning",
      "models": [
        {
          "model_path": "netease-youdao/Confucius3-Math",
          "downloads": "5",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/netease-youdao/Confucius3-Math"
        },
        {
          "model_path": "netease-youdao/Confucius3-Math-GGUF",
          "downloads": "9",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/netease-youdao/Confucius3-Math-GGUF"
        }
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18221",
      "abstract": "Transfer learning is a cornerstone of modern machine learning, promising a way to adapt models pretrained on a broad mix of data to new tasks with minimal new data. However, a significant challenge remains in ensuring that transferred features are sufficient to handle unseen datasets, amplified by the difficulty of quantifying whether two tasks are \"related\". To address these challenges, we evaluate model transfer from a pretraining mixture to each of its component tasks, assessing whether pretrained features can match the performance of task-specific direct training. We identify a fundamental limitation in deep learning models -- an \"information saturation bottleneck\" -- where networks fail to learn new features once they encode similar competing features during training. When restricted to learning only a subset of key features during pretraining, models will permanently lose critical features for transfer and perform inconsistently on data distributions, even components of the training mixture. Empirical evidence from published studies suggests that this phenomenon is pervasive in deep learning architectures -- factors such as data distribution or ordering affect the features that current representation learning methods can learn over time. This study suggests that relying solely on large-scale networks may not be as effective as focusing on task-specific training, when available. We propose richer feature representations as a potential solution to better generalize across new datasets and, specifically, present existing methods alongside a novel approach, the initial steps towards addressing this challenge.",
      "authors": [
        "Yang, Xingyu Alice",
        "Zhang, Jianyu",
        "Bottou, L\u00e9on"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18221v1",
        "Other Formats": "https://arxiv.org/format/2506.18221",
        "TeX Source": "https://arxiv.org/src/2506.18221",
        "View PDF": "https://arxiv.org/pdf/2506.18221"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 01:04:29 UTC (658 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "These are Not All the Features You are Looking For: A Fundamental Bottleneck In Supervised Pretraining",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18369",
      "abstract": "Recent multi-modal large language models (MLLMs) often struggle to generate personalized image captions, even when trained on high-quality captions. In this work, we observe that such limitations persist in existing post-training-based MLLM personalization methods. Specifically, despite being post-tuned with large-scale caption data through supervised fine-tuning (SFT), these models frequently fail to produce faithful descriptions in real-world scenarios, such as multi-concept image captioning. However, acquiring large-scale, high-quality captions for such complex settings is both costly and difficult. To address the data-centric nature of SFT, we propose a reinforcement learning (RL)-based post-training framework. To the best of our knowledge, this is the first RL-based approach to post-train MLLMs for personalized image captioning. Our method significantly enhances both visual recognition and personalized generation capabilities of MLLMs, and consistently outperforms existing SFT-based baselines, especially in the challenging multi-concept image captioning task.",
      "authors": [
        "Oh, Yeongtak",
        "Mok, Jisoo",
        "Chung, Dohyun",
        "Shin, Juhyeon",
        "Park, Sangha",
        "Barthelemy, Johan",
        "Yoon, Sungroh"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18369v1",
        "Other Formats": "https://arxiv.org/format/2506.18369",
        "TeX Source": "https://arxiv.org/src/2506.18369",
        "View PDF": "https://arxiv.org/pdf/2506.18369"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 07:55:52 UTC (13,019 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18789",
      "abstract": "Federated Learning (FL) enables collaborative model training across decentralized clients without sharing raw data, yet faces significant challenges in real-world settings where client data distributions evolve dynamically over time. This paper tackles the critical problem of covariate and label shifts in streaming FL environments, where non-stationary data distributions degrade model performance and require adaptive middleware solutions. We introduce ShiftEx, a shift-aware mixture of experts framework that dynamically creates and trains specialized global models in response to detected distribution shifts using Maximum Mean Discrepancy for covariate shifts. The framework employs a latent memory mechanism for expert reuse and implements facility location-based optimization to jointly minimize covariate mismatch, expert creation costs, and label imbalance. Through theoretical analysis and comprehensive experiments on benchmark datasets, we demonstrate 5.5-12.9 percentage point accuracy improvements and 22-95 % faster adaptation compared to state-of-the-art FL baselines across diverse shift scenarios. The proposed approach offers a scalable, privacy-preserving middleware solution for FL systems operating in non-stationary, real-world conditions while minimizing communication and computational overhead.",
      "authors": [
        "Bhope, Rahul Atul",
        "Jayaram, K. R.",
        "Venkateswaran, Praveen",
        "Venkatasubramanian, Nalini"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18789v1",
        "Other Formats": "https://arxiv.org/format/2506.18789",
        "TeX Source": "https://arxiv.org/src/2506.18789",
        "View PDF": "https://arxiv.org/pdf/2506.18789"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 15:59:21 UTC (790 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Shift Happens: Mixture of Experts based Continual Adaptation in Federated Learning",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18508",
      "abstract": "Neural estimators are simulation-based estimators for the parameters of a family of statistical models, which build a direct mapping from the sample to the parameter vector. They benefit from the versatility of available network architectures and efficient training methods developed in the field of deep learning. Neural estimators are amortized in the sense that, once trained, they can be applied to any new data set with almost no computational cost. While many papers have shown very good performance of these methods in simulation studies and real-world applications, so far no statistical guarantees are available to support these observations theoretically. In this work, we study the risk of neural estimators by decomposing it into several terms that can be analyzed separately. We formulate easy-to-check assumptions ensuring that each term converges to zero, and we verify them for popular applications of neural estimators. Our results provide a general recipe to derive theoretical guarantees also for broader classes of architectures and estimation problems.",
      "authors": [
        "R\u00f6dder, Almut",
        "Hentschel, Manuel",
        "Engelke, Sebastian"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18508v1",
        "Other Formats": "https://arxiv.org/format/2506.18508",
        "TeX Source": "https://arxiv.org/src/2506.18508",
        "View PDF": "https://arxiv.org/pdf/2506.18508"
      },
      "subjects": [
        "Machine Learning (stat.ML)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 11:02:08 UTC (236 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Theoretical guarantees for neural estimators in parametric statistics",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18680",
      "abstract": "We present DuetGen, a novel framework for generating interactive two-person dances from music. The key challenge of this task lies in the inherent complexities of two-person dance interactions, where the partners need to synchronize both with each other and with the music. Inspired by the recent advances in motion synthesis, we propose a two-stage solution: encoding two-person motions into discrete tokens and then generating these tokens from music. To effectively capture intricate interactions, we represent both dancers' motions as a unified whole to learn the necessary motion tokens, and adopt a coarse-to-fine learning strategy in both the stages. Our first stage utilizes a VQ-VAE that hierarchically separates high-level semantic features at a coarse temporal resolution from low-level details at a finer resolution, producing two discrete token sequences at different abstraction levels. Subsequently, in the second stage, two generative masked transformers learn to map music signals to these dance tokens: the first producing high-level semantic tokens, and the second, conditioned on music and these semantic tokens, producing the low-level tokens. We train both transformers to learn to predict randomly masked tokens within the sequence, enabling them to iteratively generate motion tokens by filling an empty token sequence during inference. Through the hierarchical masked modeling and dedicated interaction representation, DuetGen achieves the generation of synchronized and interactive two-person dances across various genres. Extensive experiments and user studies on a benchmark duet dance dataset demonstrate state-of-the-art performance of DuetGen in motion realism, music-dance alignment, and partner coordination.",
      "authors": [
        "Ghosh, Anindita",
        "Zhou, Bing",
        "Dabral, Rishabh",
        "Wang, Jian",
        "Golyanik, Vladislav",
        "Theobalt, Christian",
        "Slusallek, Philipp",
        "Guo, Chuan"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18680v1",
        "Other Formats": "https://arxiv.org/format/2506.18680",
        "TeX Source": "https://arxiv.org/src/2506.18680",
        "View PDF": "https://arxiv.org/pdf/2506.18680"
      },
      "subjects": [
        "Graphics (cs.GR)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Sound (cs.SD)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 14:22:50 UTC (5,274 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18810",
      "abstract": "Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and OpenAI o1 series have achieved notable performance enhancements on complex reasoning tasks by scaling up the generation length by Chain-of-Thought (CoT). However, an emerging issue is their inclination to produce excessively verbose reasoning processes, leading to the inefficiency problem. Existing literature on improving efficiency mainly adheres to the before-reasoning paradigms such as prompting and reasoning or fine-tuning and reasoning, but ignores the promising direction of directly encouraging the model to speak concisely by intervening during the generation of reasoning. In order to fill the blank, we propose a framework dubbed ConciseHint, which continuously encourages the reasoning model to speak concisely by injecting the textual hint (manually designed or trained on the concise data) during the token generation of the reasoning process. Besides, ConciseHint is adaptive to the complexity of the query by adaptively adjusting the hint intensity, which ensures it will not undermine model performance. Experiments on the state-of-the-art LRMs, including DeepSeek-R1 and Qwen-3 series, demonstrate that our method can effectively produce concise reasoning processes while maintaining performance well. For instance, we achieve a reduction ratio of 65\\% for the reasoning length on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.",
      "authors": [
        "Tang, Siao",
        "Ma, Xinyin",
        "Fang, Gongfan",
        "Wang, Xinchao"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18810v1",
        "Other Formats": "https://arxiv.org/format/2506.18810",
        "TeX Source": "https://arxiv.org/src/2506.18810",
        "View PDF": "https://arxiv.org/pdf/2506.18810"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 16:20:44 UTC (464 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.15244",
      "abstract": "This work proposes a new algorithm to mitigate model generalization loss in Vertical Federated Learning (VFL) operating under client reliability constraints within 5G Core Networks (CNs). Recently studied and endorsed by 3GPP, VFL enables collaborative and load-balanced model training and inference across the CN. However, the performance of VFL significantly degrades when the Network Data Analytics Functions (NWDAFs) - which serve as primary clients for VFL model training and inference - experience reliability issues stemming from resource constraints and operational overhead. Unlike edge environments, CN environments adopt fundamentally different data management strategies, characterized by more centralized data orchestration capabilities. This presents opportunities to implement better distributed solutions that take full advantage of the CN data handling flexibility. Leveraging this flexibility, we propose a method that optimizes the vertical feature split among clients while centrally defining their local models based on reliability metrics. Our empirical evaluation demonstrates the effectiveness of our proposed algorithm, showing improved performance over traditional baseline methods.",
      "authors": [
        "Mestoukirdi, Mohamad",
        "Khanfouci, Mourad"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2505.15244v3",
        "Other Formats": "https://arxiv.org/format/2505.15244",
        "TeX Source": "https://arxiv.org/src/2505.15244",
        "View PDF": "https://arxiv.org/pdf/2505.15244"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Systems and Control (eess.SY)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 21 May 2025 08:20:16 UTC (262 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 14 Jun 2025 22:33:22 UTC (262 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Mon, 23 Jun 2025 08:29:22 UTC (160 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2025/05/21",
      "title": "Reliable Vertical Federated Learning in 5G Core Network Architecture",
      "repo_urls": [
        "https://github.com/merce-fra/Reliable-VFL-in-5G-Core-NW-Arch"
      ],
      "tasks": [
        "Federated Learning",
        "Vertical Federated Learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18297",
      "abstract": "Modern information retrieval systems often employ a two-stage pipeline: an efficient initial retrieval stage followed by a computationally intensive reranking stage. Cross-encoders have shown strong effectiveness for reranking due to their deep analysis of query-document pairs. This paper studies the impact of the Lion optimizer, a recent alternative to AdamW, during fine-tuning of cross-encoder rerankers. We fine-tune three transformer models-MiniLM, GTE, and ModernBERT-on the MS MARCO passage ranking dataset using both optimizers. GTE and ModernBERT support extended context lengths (up to 8192 tokens). We evaluate effectiveness using TREC 2019 Deep Learning Track and MS MARCO dev set (MRR@10). Experiments, run on the Modal cloud platform, reveal that ModernBERT with Lion achieves the best NDCG@10 (0.7225) and MAP (0.5121) on TREC DL 2019, while MiniLM with Lion ties ModernBERT for MRR@10 (0.5988) on MS MARCO dev. Lion also provides superior GPU efficiency, improving utilization by 2.67% to 10.33% across models. We analyze performance trends using standard IR metrics and discuss the optimizer's impact on training dynamics across architectures.",
      "authors": [
        "Kumar, Shahil",
        "Pande, Manu",
        "Damle, Anay Yatin"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18297v1",
        "Other Formats": "https://arxiv.org/format/2506.18297",
        "TeX Source": "https://arxiv.org/src/2506.18297",
        "View PDF": "https://arxiv.org/pdf/2506.18297"
      },
      "subjects": [
        "Information Retrieval (cs.IR)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 05:30:09 UTC (189 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Comparative Analysis of Lion and AdamW Optimizers for Cross-Encoder Reranking with MiniLM, GTE, and ModernBERT",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18322",
      "abstract": "Finetuning can cause spurious correlations to arise between non-essential features and the target labels, but benchmarks to study these effects involve contrived settings and narrow tasks. In contrast, we consider spurious correlations in multi-modal Large Vision Language Models (LVLMs) pretrained on extensive and diverse datasets without explicit task supervision. We develop a benchmark by sourcing GPT-4o errors on real-world visual-question-answering (VQA) benchmarks, then curating a subset through LVLM-human annotation and synthetic counterfactual evaluation to identify errors caused by spurious correlations. This process yields SpuriVerse, a novel benchmark comprised of 124 distinct types of spurious correlations extracted from real-world datasets, each containing 1 realistic and 10 synthetic VQA samples for a total of 1364 multiple choice questions. We evaluate 15 open and closed-source LVLMs on SpuriVerse, finding that even state-of-the-art closed-source models struggle significantly, achieving at best only 37.1% accuracy. Fine-tuning on synthetic examples that emphasize the spurious correlation improves performance to 78.40%, suggesting that training on diverse spurious patterns generalizes to unseen situations: models appear to learn to avoid \"shortcuts\" and attend to the overall image context.",
      "authors": [
        "Yang, Yiwei",
        "Lee, Chung Peng",
        "Feng, Shangbin",
        "Zhao, Dora",
        "Wen, Bingbing",
        "Liu, Anthony Z.",
        "Tsvetkov, Yulia",
        "Howe, Bill"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18322v1",
        "Other Formats": "https://arxiv.org/format/2506.18322",
        "TeX Source": "https://arxiv.org/src/2506.18322",
        "View PDF": "https://arxiv.org/pdf/2506.18322"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 06:11:43 UTC (19,894 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Escaping the SpuriVerse: Can Large Vision-Language Models Generalize Beyond Seen Spurious Correlations?",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18292",
      "abstract": "Quantitative descriptions of complete canopy architecture are crucial for evaluating crop photosynthesis and yield to guide ideotype design. Although three-dimensional (3D) sensing technologies have been developed for plant and canopy reconstruction, severe occlusion and complex architectures hinder accurate canopy descriptions. In this study, we propose a point cloud completion model for 3D reconstruction of rapeseed populations from seeding to silique stages using multi-view imaging. A complete point cloud generation framework was developed with the virtual-real integration (VRI) simulation method and occlusion point detection algorithm to annotate the training dataset by distinguishing surface from occluded points. The rapeseed population point cloud completion network (RP-PCN) was designed with a multi-resolution dynamic graph convolutional encoder (MRDG) and point pyramid decoder (PPD) to predict occluded points based on input surface point clouds. A dynamic graph convolutional feature extractor (DGCFE) was introduced to capture structural variations across the growth period. The effectiveness of point cloud completion was validated by predicting yield using architectural indicators from complete point clouds of rapeseed population. The results demonstrated that RP-PCN achieved chamfer distance (CD) values of 3.35 cm, 3.46 cm, 4.32 cm, and 4.51 cm at the seedling, bolting, flowering, and silique stages, respectively. Ablation studies showed the effectiveness of the MRDG and DGCFE modules, reducing CD values by 10% and 23%, respectively. The silique efficiency index (SEI) from RP-PCN improved yield prediction accuracy by 11.2% compared to incomplete point clouds. The RP-PCN pipeline proposed in this study has the potential to be extended to other crops, significantly enhancing the analysis of population canopy architectures in field environments.",
      "authors": [
        "Guo, Ziyue",
        "Yang, Xin",
        "Shen, Yutao",
        "Zhu, Yang",
        "Jiang, Lixi",
        "Cen, Haiyan"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.18292",
        "View PDF": "https://arxiv.org/pdf/2506.18292"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 05:02:31 UTC (3,479 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Rapeseed population point cloud completion network (RP-PCN) with dynamic graph convolution for 3D reconstruction of crop canopy occlusion architecture",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18530",
      "abstract": "Edge AI applications increasingly require models that can learn and adapt on-device with minimal energy budget. Traditional deep learning models, while powerful, are often overparameterized, energy-hungry, and dependent on cloud connectivity. Brain-Like Neural Networks (BLNNs), such as the Bayesian Confidence Propagation Neural Network (BCPNN), propose a neuromorphic alternative by mimicking cortical architecture and biologically-constrained learning. They offer sparse architectures with local learning rules and unsupervised/semi-supervised learning, making them well-suited for low-power edge intelligence. However, existing BCPNN implementations rely on GPUs or datacenter FPGAs, limiting their applicability to embedded systems. This work presents the first embedded FPGA accelerator for BCPNN on a Zynq UltraScale+ SoC using High-Level Synthesis. We implement both online learning and inference-only kernels with support for variable and mixed precision. Evaluated on MNIST, Pneumonia, and Breast Cancer datasets, our accelerator achieves up to 17.5x latency and 94% energy savings over ARM baselines, without sacrificing accuracy. This work enables practical neuromorphic computing on edge devices, bridging the gap between brain-like learning and real-world deployment.",
      "authors": [
        "Hafiz, Muhammad Ihsan Al",
        "Ravichandran, Naresh",
        "Lansner, Anders",
        "Herman, Pawel",
        "Podobas, Artur"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18530v1",
        "Other Formats": "https://arxiv.org/format/2506.18530",
        "TeX Source": "https://arxiv.org/src/2506.18530",
        "View PDF": "https://arxiv.org/pdf/2506.18530"
      },
      "subjects": [
        "Hardware Architecture (cs.AR)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 11:35:20 UTC (2,622 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Embedded FPGA Acceleration of Brain-Like Neural Networks: Online Learning to Scalable Inference",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18656",
      "abstract": "Attention mechanisms have revolutionized machine learning (ML) by enabling efficient modeling of global dependencies across inputs. Their inherently parallelizable structures allow for efficient scaling with the exponentially increasing size of both pretrained data and model parameters. Yet, despite their central role as the computational backbone of modern large language models (LLMs), the theoretical understanding of Attentions, especially in the nonlinear setting, remains limited. In this paper, we provide a precise characterization of the \\emph{in-context memorization error} of \\emph{nonlinear Attention}, in the high-dimensional proportional regime where the number of input tokens $n$ and their embedding dimension $p$ are both large and comparable. Leveraging recent advances in the theory of large kernel random matrices, we show that nonlinear Attention typically incurs higher memorization error than linear ridge regression on random inputs. However, this gap vanishes, and can even be reversed, when the input exhibits statistical structure, particularly when the Attention weights align with the input signal direction. Our results reveal how nonlinearity and input structure interact with each other to govern the memorization performance of nonlinear Attention. The theoretical insights are supported by numerical experiments.",
      "authors": [
        "Liao, Zhenyu",
        "Liu, Jiaqing",
        "Hou, TianQi",
        "Zou, Difan",
        "Ling, Zenan"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.18656",
        "TeX Source": "https://arxiv.org/src/2506.18656",
        "View PDF": "https://arxiv.org/pdf/2506.18656"
      },
      "subjects": [
        "Machine Learning (stat.ML)",
        "Machine Learning (cs.LG)",
        "Statistics Theory (math.ST)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 13:56:43 UTC (85 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "A Random Matrix Analysis of In-context Memorization for Nonlinear Attention",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2503.23980",
      "abstract": "We propose a flexible Semi-Automatic Labeling Tool (SALT) for general LiDAR point clouds with cross-scene adaptability and 4D consistency. Unlike recent approaches that rely on camera distillation, SALT operates directly on raw LiDAR data, automatically generating pre-segmentation results. To achieve this, we propose a novel zero-shot learning paradigm, termed data alignment, which transforms LiDAR data into pseudo-images by aligning with the training distribution of vision foundation models. Additionally, we design a 4D-consistent prompting strategy and 4D non-maximum suppression module to enhance SAM2, ensuring high-quality, temporally consistent presegmentation. SALT surpasses the latest zero-shot methods by 18.4% PQ on SemanticKITTI and achieves nearly 40-50% of human annotator performance on our newly collected low-resolution LiDAR data and on combined data from three LiDAR types, significantly boosting annotation efficiency. We anticipate that SALT's open-sourcing will catalyze substantial expansion of current LiDAR datasets and lay the groundwork for the future development of LiDAR foundation models. Code is available at https://github.com/Cavendish518/SALT.",
      "authors": [
        "Wang, Yanbo",
        "Chen, Yongtao",
        "Cao, Chuan",
        "Deng, Tianchen",
        "Zhao, Wentao",
        "Wang, Jingchuan",
        "Chen, Weidong"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2503.23980v2",
        "Other Formats": "https://arxiv.org/format/2503.23980",
        "TeX Source": "https://arxiv.org/src/2503.23980",
        "View PDF": "https://arxiv.org/pdf/2503.23980"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 31 Mar 2025 11:46:55 UTC (36,587 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 06:49:10 UTC (19,122 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/03/31",
      "title": "SALT: A Flexible Semi-Automatic Labeling Tool for General LiDAR Point Clouds with Cross-Scene Adaptability and 4D Consistency",
      "repo_urls": [
        "https://github.com/Cavendish518/SALT"
      ],
      "tasks": [
        "Zero-Shot Learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18329",
      "abstract": "Previous studies that used data from Stack Overflow to develop predictive models often employed limited benchmarks of 3-5 models or adopted arbitrary selection methods. Despite being insightful, their limited scope suggests the need to benchmark more models to avoid overlooking untested algorithms. Our study evaluates 21 algorithms across three tasks: predicting the number of question a user is likely to answer, their code quality violations, and their dropout status. We employed normalisation, standardisation, as well as logarithmic and power transformations paired with Bayesian hyperparameter optimisation and genetic algorithms. CodeBERT, a pre-trained language model for both natural and programming languages, was fine-tuned to classify user dropout given their posts (questions and answers) and code snippets. We found Bagging ensemble models combined with standardisation achieved the highest R2 value (0.821) in predicting user answers. The Stochastic Gradient Descent regressor, followed by Bagging and Epsilon Support Vector Machine models, consistently demonstrated superior performance to other benchmarked algorithms in predicting user code quality across multiple quality dimensions and languages. Extreme Gradient Boosting paired with log-transformation exhibited the highest F1-score (0.825) in predicting user dropout. CodeBERT was able to classify user dropout with a final F1-score of 0.809, validating the performance of Extreme Gradient Boosting that was solely based on numerical data. Overall, our benchmarking of 21 algorithms provides multiple insights. Researchers can leverage findings regarding the most suitable models for specific target variables, and practitioners can utilise the identified optimal hyperparameters to reduce the initial search space during their own hyperparameter tuning processes.",
      "authors": [
        "Zolduoarrati, Elijah",
        "Licorish, Sherlock A.",
        "Stanger, Nigel"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.18329",
        "View PDF": "https://arxiv.org/pdf/2506.18329"
      },
      "subjects": [
        "Software Engineering (cs.SE)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 06:23:12 UTC (1,507 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Predictive Analytics for Collaborators Answers, Code Quality, and Dropout on Stack Overflow",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2503.01837",
      "abstract": "Long-horizon tasks in robotic manipulation present significant challenges in reinforcement learning (RL) due to the difficulty of designing dense reward functions and effectively exploring the expansive state-action space. However, despite a lack of dense rewards, these tasks often have a multi-stage structure, which can be leveraged to decompose the overall objective into manageable subgoals. In this work, we propose DEMO3, a framework that exploits this structure for efficient learning from visual inputs. Specifically, our approach incorporates multi-stage dense reward learning, a bi-phasic training scheme, and world model learning into a carefully designed demonstration-augmented RL framework that strongly mitigates the challenge of exploration in long-horizon tasks. Our evaluations demonstrate that our method improves data-efficiency by an average of 40% and by 70% on particularly difficult tasks compared to state-of-the-art approaches. We validate this across 16 sparse-reward tasks spanning four domains, including challenging humanoid visual control tasks using as few as five demonstrations.",
      "authors": [
        "Escoriza, Adri\u00e0 L\u00f3pez",
        "Hansen, Nicklas",
        "Tao, Stone",
        "Mu, Tongzhou",
        "Su, Hao"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2503.01837v2",
        "Other Formats": "https://arxiv.org/format/2503.01837",
        "TeX Source": "https://arxiv.org/src/2503.01837",
        "View PDF": "https://arxiv.org/pdf/2503.01837"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 3 Mar 2025 18:57:08 UTC (4,728 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 11:41:17 UTC (6,648 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/03/03",
      "title": "Multi-Stage Manipulation with Demonstration-Augmented Reward, Policy, and World Model Learning",
      "repo_urls": [
        "https://github.com/adrialopezescoriza/demo3"
      ],
      "tasks": [
        "Reinforcement Learning (RL)"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18349",
      "abstract": "The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm for scaling large language models (LLMs) while maintaining inference efficiency. However, their enormous memory requirements make them prohibitively expensive to fine-tune or deploy in resource-constrained environments. To address this challenge, we introduce SlimMoE, a multi-stage compression framework for transforming large MoE models into much smaller, efficient variants without incurring the prohibitive costs of training from scratch. Our method systematically reduces parameter counts by slimming experts and transferring knowledge through intermediate stages, effectively mitigating the performance degradation common in one-shot pruning approaches. Using this framework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE (3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of the original model's training data. These compressed models can be fine-tuned on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them highly suitable for academic and resource-limited settings. Our experiments demonstrate that these compressed models outperform others of similar size and remain competitive with larger models. For instance, Phi-mini-MoE achieves similar or better performance to Phi-3-mini using only 2/3 of the activated parameters and yields comparable MMLU scores to Llama 3.1 8B despite having significantly lower latency. Our findings demonstrate that structured pruning combined with staged distillation offers an effective path to creating high-quality, compact MoE models, paving the way for broader adoption of MoE architectures. We make our models publicly available at https://huggingface.co/microsoft/Phi-mini-MoE-instruct and https://huggingface.co/microsoft/Phi-tiny-MoE-instruct .",
      "authors": [
        "Li, Zichong",
        "Liang, Chen",
        "Zhang, Zixuan",
        "Hong, Ilgee",
        "Kim, Young Jin",
        "Chen, Weizhu",
        "Zhao, Tuo"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18349v1",
        "Other Formats": "https://arxiv.org/format/2506.18349",
        "TeX Source": "https://arxiv.org/src/2506.18349",
        "View PDF": "https://arxiv.org/pdf/2506.18349"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 07:15:59 UTC (304 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation",
      "models": [
        {
          "model_path": "microsoft/Phi-mini-MoE-instruct",
          "downloads": "0",
          "likes": "4",
          "trending_score": "4.0",
          "link": "https://huggingface.co/microsoft/Phi-mini-MoE-instruct"
        },
        {
          "model_path": "microsoft/Phi-tiny-MoE-instruct",
          "downloads": "0",
          "likes": "2",
          "trending_score": "2.0",
          "link": "https://huggingface.co/microsoft/Phi-tiny-MoE-instruct"
        },
        {
          "model_path": "gabriellarson/Phi-mini-MoE-instruct-GGUF",
          "downloads": "0",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/gabriellarson/Phi-mini-MoE-instruct-GGUF"
        }
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2404.14249",
      "abstract": "Exploiting 3D Gaussian Splatting (3DGS) with Contrastive Language-Image Pre-Training (CLIP) models for open-vocabulary 3D semantic understanding of indoor scenes has emerged as an attractive research focus. Existing methods typically attach high-dimensional CLIP semantic embeddings to 3D Gaussians and leverage view-inconsistent 2D CLIP semantics as Gaussian supervision, resulting in efficiency bottlenecks and deficient 3D semantic consistency. To address these challenges, we present CLIP-GS, efficiently achieving a coherent semantic understanding of 3D indoor scenes via the proposed Semantic Attribute Compactness (SAC) and 3D Coherent Regularization (3DCR). SAC approach exploits the naturally unified semantics within objects to learn compact, yet effective, semantic Gaussian representations, enabling highly efficient rendering (>100 FPS). 3DCR enforces semantic consistency in 2D and 3D domains: In 2D, 3DCR utilizes refined view-consistent semantic outcomes derived from 3DGS to establish cross-view coherence constraints; in 3D, 3DCR encourages features similar among 3D Gaussian primitives associated with the same object, leading to more precise and coherent segmentation results. Extensive experimental results demonstrate that our method remarkably suppresses existing state-of-the-art approaches, achieving mIoU improvements of 21.20% and 13.05% on ScanNet and Replica datasets, respectively, while maintaining real-time rendering speed. Furthermore, our approach exhibits superior performance even with sparse input data, substantiating its robustness.",
      "authors": [
        "Liao, Guibiao",
        "Li, Jiankun",
        "Bao, Zhenyu",
        "Ye, Xiaoqing",
        "Li, Qing",
        "Liu, Kanglin"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2404.14249v2",
        "Other Formats": "https://arxiv.org/format/2404.14249",
        "TeX Source": "https://arxiv.org/src/2404.14249",
        "View PDF": "https://arxiv.org/pdf/2404.14249"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 22 Apr 2024 15:01:32 UTC (2,382 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 08:10:31 UTC (4,366 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/04/22",
      "title": "CLIP-GS: CLIP-Informed Gaussian Splatting for View-Consistent 3D Indoor Semantic Understanding",
      "repo_urls": [
        "https://github.com/gbliao/clip-gs"
      ],
      "tasks": [
        "Attribute"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2410.17264",
      "abstract": "Several studies have explored deep learning algorithms to predict large-scale signal fading, or path loss, in urban communication networks. The goal is to replace costly measurement campaigns, inaccurate statistical models, or computationally expensive ray-tracing simulations with machine learning models that deliver quick and accurate predictions. We focus on predicting path loss radio maps using convolutional neural networks, leveraging aerial images alone or in combination with supplementary height information. Notably, our approach does not rely on explicit classification of environmental objects, which is often unavailable for most locations worldwide. While the prediction of radio maps using complete 3D environmental data is well-studied, the use of only aerial images remains under-explored. We address this gap by showing that state-of-the-art models developed for existing radio map datasets can be effectively adapted to this task. Additionally, we introduce a new model dubbed UNetDCN that achieves on par or better performance compared to the state-of-the-art with reduced complexity. The trained models are differentiable, and therefore they can be incorporated in various network optimization algorithms. While an extensive discussion is beyond this paper's scope, we demonstrate this through an example optimizing the directivity of base stations in cellular networks via backpropagation to enhance coverage.",
      "authors": [
        "Jaensch, Fabian",
        "Caire, Giuseppe",
        "Demir, Beg\u00fcm"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2410.17264v2",
        "Other Formats": "https://arxiv.org/format/2410.17264",
        "TeX Source": "https://arxiv.org/src/2410.17264",
        "View PDF": "https://arxiv.org/pdf/2410.17264"
      },
      "subjects": [
        "Signal Processing (eess.SP)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 7 Oct 2024 09:19:20 UTC (4,688 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 12:42:36 UTC (5,125 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/10/07",
      "title": "Radio Map Prediction from Aerial Images and Application to Coverage Optimization",
      "tasks": [],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2408.01139",
      "abstract": "Perturbation robustness evaluates the vulnerabilities of models, arising from a variety of perturbations, such as data corruptions and adversarial attacks. Understanding the mechanisms of perturbation robustness is critical for global interpretability. We present a model-agnostic, global mechanistic interpretability method to interpret the perturbation robustness of image models. This research is motivated by two key aspects. First, previous global interpretability works, in tandem with robustness benchmarks, e.g. mean corruption error (mCE), are not designed to directly interpret the mechanisms of perturbation robustness within image models. Second, we notice that the spectral signal-to-noise ratios (SNR) of perturbed natural images exponentially decay over the frequency. This power-law-like decay implies that: Low-frequency signals are generally more robust than high-frequency signals -- yet high classification accuracy can not be achieved by low-frequency signals alone. By applying Shapley value theory, our method axiomatically quantifies the predictive powers of robust features and non-robust features within an information theory framework. Our method, dubbed as \\textbf{I-ASIDE} (\\textbf{I}mage \\textbf{A}xiomatic \\textbf{S}pectral \\textbf{I}mportance \\textbf{D}ecomposition \\textbf{E}xplanation), provides a unique insight into model robustness mechanisms. We conduct extensive experiments over a variety of vision models pre-trained on ImageNet to show that \\textbf{I-ASIDE} can not only \\textbf{measure} the perturbation robustness but also \\textbf{provide interpretations} of its mechanisms.",
      "authors": [
        "Luo, R\u00f3is\u00edn",
        "McDermott, James",
        "O'Riordan, Colm"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2408.01139v3",
        "Other Formats": "https://arxiv.org/format/2408.01139",
        "TeX Source": "https://arxiv.org/src/2408.01139",
        "View PDF": "https://arxiv.org/pdf/2408.01139"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 2 Aug 2024 09:35:06 UTC (9,374 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 18 Aug 2024 17:13:31 UTC (9,386 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Mon, 23 Jun 2025 13:00:34 UTC (5,671 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/08/02",
      "title": "Interpreting Global Perturbation Robustness of Image Models using Axiomatic Spectral Importance Decomposition",
      "tasks": [],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18258",
      "abstract": "Ground penetrating radar (GPR) provides a promising technology for accurate subsurface object detection. In particular, it has shown promise for detecting landmines with low metal content. However, the ground bounce (GB) that is present in GPR data, which is caused by the dielectric discontinuity between soil and air, is a major source of interference and degrades landmine detection performance. To mitigate this interference, GB tracking algorithms formulated using both a Kalman filter (KF) and a particle filter (PF) framework are proposed. In particular, the location of the GB in the radar signal is modeled as the hidden state in a stochastic system for the PF approach. The observations are the 2D radar images, which arrive scan by scan along the down-track direction. An initial training stage sets parameters automatically to accommodate different ground and weather conditions. The features associated with the GB description are updated adaptively with the arrival of new data. The prior distribution for a given location is predicted by propagating information from two adjacent channels/scans, which ensures that the overall GB surface remains smooth. The proposed algorithms are verified in experiments utilizing real data, and their performances are compared with other GB tracking approaches. We demonstrate that improved GB tracking contributes to improved performance for the landmine detection problem.",
      "authors": [
        "Tang, Li",
        "Torrione, Peter A.",
        "Eldeniz, Cihat",
        "Collins, Leslie M."
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18258v1",
        "Other Formats": "https://arxiv.org/format/2506.18258",
        "TeX Source": "https://arxiv.org/src/2506.18258",
        "View PDF": "https://arxiv.org/pdf/2506.18258"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 03:06:55 UTC (1,601 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Ground tracking for improved landmine detection in a GPR system",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2502.17323",
      "abstract": "Machine Unlearning (MU) aims at removing the influence of specific data points from a trained model, striving to achieve this at a fraction of the cost of full model retraining. In this paper, we analyze the efficiency of unlearning methods and establish the first upper and lower bounds on minimax computation times for this problem, characterizing the performance of the most efficient algorithm against the most difficult objective function. Specifically, for strongly convex objective functions and under the assumption that the forget data is inaccessible to the unlearning method, we provide a phase diagram for the unlearning complexity ratio -- a novel metric that compares the computational cost of the best unlearning method to full model retraining. The phase diagram reveals three distinct regimes: one where unlearning at a reduced cost is infeasible, another where unlearning is trivial because adding noise suffices, and a third where unlearning achieves significant computational advantages over retraining. These findings highlight the critical role of factors such as data dimensionality, the number of samples to forget, and privacy constraints in determining the practical feasibility of unlearning.",
      "authors": [
        "Van Waerebeke, Martin",
        "Lorenzi, Marco",
        "Neglia, Giovanni",
        "Scaman, Kevin"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2502.17323v2",
        "Other Formats": "https://arxiv.org/format/2502.17323",
        "TeX Source": "https://arxiv.org/src/2502.17323",
        "View PDF": "https://arxiv.org/pdf/2502.17323"
      },
      "subjects": [
        "Machine Learning (stat.ML)",
        "Machine Learning (cs.LG)",
        "Optimization and Control (math.OC)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 24 Feb 2025 16:56:27 UTC (121 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 15:08:08 UTC (205 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/02/24",
      "title": "When to Forget? Complexity Trade-offs in Machine Unlearning",
      "tasks": [
        "Machine Unlearning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18678",
      "abstract": "Neural implicit scene representations have recently shown promising results in dense visual SLAM. However, existing implicit SLAM algorithms are constrained to single-agent scenarios, and fall difficulties in large-scale scenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks cannot meet the constraints of communication bandwidth. To this end, we propose the first distributed multi-agent collaborative neural SLAM framework with hybrid scene representation, distributed camera tracking, intra-to-inter loop closure, and online distillation for multiple submap fusion. A novel triplane-grid joint scene representation method is proposed to improve scene reconstruction. A novel intra-to-inter loop closure method is designed to achieve local (single-agent) and global (multi-agent) consistency. We also design a novel online distillation method to fuse the information of different submaps to achieve global consistency. Furthermore, to the best of our knowledge, there is no real-world dataset for NeRF-based/GS-based SLAM that provides both continuous-time trajectories groundtruth and high-accuracy 3D meshes groundtruth. To this end, we propose the first real-world Dense slam (DES) dataset covering both single-agent and multi-agent scenarios, ranging from small rooms to large-scale outdoor scenes, with high-accuracy ground truth for both 3D mesh and continuous-time camera trajectory. This dataset can advance the development of the research in both SLAM, 3D reconstruction, and visual foundation model. Experiments on various datasets demonstrate the superiority of the proposed method in both mapping, tracking, and communication. The dataset and code will open-source on https://github.com/dtc111111/mcnslam.",
      "authors": [
        "Deng, Tianchen",
        "Shen, Guole",
        "Chen, Xun",
        "Yuan, Shenghai",
        "Shen, Hongming",
        "Peng, Guohao",
        "Wu, Zhenyu",
        "Wang, Jingchuan",
        "Xie, Lihua",
        "Wang, Danwei",
        "Wang, Hesheng",
        "Chen, Weidong"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.18678",
        "TeX Source": "https://arxiv.org/src/2506.18678",
        "View PDF": "https://arxiv.org/pdf/2506.18678"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 14:22:29 UTC (19,518 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18587",
      "abstract": "Given the abundance of unlabeled Satellite Image Time Series (SITS) and the scarcity of labeled data, contrastive self-supervised pretraining emerges as a natural tool to leverage this vast quantity of unlabeled data. However, designing effective data augmentations for contrastive learning remains challenging for time series. We introduce a novel resampling-based augmentation strategy that generates positive pairs by upsampling time series and extracting disjoint subsequences while preserving temporal coverage. We validate our approach on multiple agricultural classification benchmarks using Sentinel-2 imagery, showing that it outperforms common alternatives such as jittering, resizing, and masking. Further, we achieve state-of-the-art performance on the S2-Agri100 dataset without employing spatial information or temporal encodings, surpassing more complex masked-based SSL frameworks. Our method offers a simple, yet effective, contrastive learning augmentation for remote sensing time series.",
      "authors": [
        "Saget, Antoine",
        "Lafabregue, Baptiste",
        "Cornu\u00e9jols, Antoine",
        "Gan\u00e7arski, Pierre"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18587v1",
        "Other Formats": "https://arxiv.org/format/2506.18587",
        "TeX Source": "https://arxiv.org/src/2506.18587",
        "View PDF": "https://arxiv.org/pdf/2506.18587"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 12:48:19 UTC (39 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Resampling Augmentation for Time Series Contrastive Learning: Application to Remote Sensing",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.14020",
      "abstract": "Graph generation has emerged as a critical task in fields ranging from molecule design to drug discovery. Contemporary approaches, notably diffusion and flow-based models, have achieved solid graph generative performance through constructing a probability path that interpolates between a reference distribution and the data distribution. However, these methods typically model the evolution of individual nodes and edges independently and use linear interpolations to build the path assuming that the data lie in Euclidean space. We show that this is suboptimal given the intrinsic non-Euclidean structure and interconnected patterns of graphs, and it poses risks to the sampling convergence. To build a better probability path, we model the joint evolution of the nodes and edges by representing graphs as connected systems parameterized by Markov random fields (MRF). We then leverage the optimal transport displacement between MRF objects to design the probability path for graph generation. Based on this, we introduce BWFlow, a flow-matching framework for graph generation that respects the underlying geometry of graphs and provides smooth velocities in the probability path. The novel framework can be adapted to both continuous and discrete flow-matching algorithms. Experimental evaluations in plain graph generation and 2D/3D molecule generation validate the effectiveness of BWFlow in graph generation with competitive performance, stable training, and guaranteed sampling convergence.",
      "authors": [
        "Jiang, Keyue",
        "Cui, Jiahao",
        "Dong, Xiaowen",
        "Toni, Laura"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.14020v2",
        "Other Formats": "https://arxiv.org/format/2506.14020",
        "TeX Source": "https://arxiv.org/src/2506.14020",
        "View PDF": "https://arxiv.org/pdf/2506.14020"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (stat.ML)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 16 Jun 2025 21:36:56 UTC (3,077 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 23 Jun 2025 13:31:42 UTC (3,074 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/06/16",
      "title": "Bures-Wasserstein Flow Matching for Graph Generation",
      "tasks": [
        "3D Molecule Generation",
        "Drug Discovery",
        "Graph Generation"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18285",
      "abstract": "Due to its human-interpretability and invariance properties, Directed Acyclic Graph (DAG) has been a foundational tool across various areas of AI research, leading to significant advancements. However, DAG learning remains highly challenging, due to its super-exponential growth in computational cost and identifiability issues, particularly in small-sample regimes. To address these two challenges, in this work we leverage the recent success of linear transformers and develop a foundation model approach for discovering multiple order-consistent DAGs across tasks. In particular, we propose Attention-DAG (ADAG), a novel attention-mechanism-based architecture for learning multiple linear Structural Equation Models (SEMs). ADAG learns the mapping from observed data to both graph structure and parameters via a nonlinear attention-based kernel, enabling efficient multi-task estimation of the underlying linear SEMs. By formulating the learning process across multiple tasks as a continuous optimization problem, the pre-trained ADAG model captures the common structural properties as a shared low-dimensional prior, thereby reducing the ill-posedness of downstream DAG learning tasks in small-sample regimes. We evaluate our proposed approach on benchmark synthetic datasets and find that ADAG achieves substantial improvements in both DAG learning accuracy and zero-shot inference efficiency. To the best of our knowledge, this is the first practical approach for pre-training a foundation model specifically designed for DAG learning, representing a step toward more efficient and generalizable down-stream applications in causal discovery.",
      "authors": [
        "Yin, Naiyu",
        "Gao, Tian",
        "Yu, Yue"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18285v1",
        "Other Formats": "https://arxiv.org/format/2506.18285",
        "TeX Source": "https://arxiv.org/src/2506.18285",
        "View PDF": "https://arxiv.org/pdf/2506.18285"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 04:41:02 UTC (2,599 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Learning Causal Graphs at Scale: A Foundation Model Approach",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18323",
      "abstract": "Low-light image enhancement remains a challenging task, particularly in the absence of paired training data. In this study, we present LucentVisionNet, a novel zero-shot learning framework that addresses the limitations of traditional and deep learning-based enhancement methods. The proposed approach integrates multi-scale spatial attention with a deep curve estimation network, enabling fine-grained enhancement while preserving semantic and perceptual fidelity. To further improve generalization, we adopt a recurrent enhancement strategy and optimize the model using a composite loss function comprising six tailored components, including a novel no-reference image quality loss inspired by human visual perception. Extensive experiments on both paired and unpaired benchmark datasets demonstrate that LucentVisionNet consistently outperforms state-of-the-art supervised, unsupervised, and zero-shot methods across multiple full-reference and no-reference image quality metrics. Our framework achieves high visual quality, structural consistency, and computational efficiency, making it well-suited for deployment in real-world applications such as mobile photography, surveillance, and autonomous navigation.",
      "authors": [
        "Aslam, Muhammad Azeem",
        "Khalid, Hassan",
        "Ahmed, Nisar"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18323v1",
        "Other Formats": "https://arxiv.org/format/2506.18323",
        "TeX Source": "https://arxiv.org/src/2506.18323",
        "View PDF": "https://arxiv.org/pdf/2506.18323"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 06:11:55 UTC (13,956 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "A Multi-Scale Spatial Attention-Based Zero-Shot Learning Framework for Low-Light Image Enhancement",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2502.13347",
      "abstract": "Web crawl is a main source of large language models' (LLMs) pretraining data, but the majority of crawled web pages are discarded in pretraining due to low data quality. This paper presents Craw4LLM, an efficient web crawling method that explores the web graph based on the preference of LLM pretraining. Specifically, it leverages the influence of a webpage in LLM pretraining as the priority score of the web crawler's scheduler, replacing the standard graph connectivity based priority. Our experiments on a web graph containing 900 million webpages from a commercial search engine's index demonstrate the efficiency of Craw4LLM in obtaining high-quality pretraining data. With just 21% URLs crawled, LLMs pretrained on Craw4LLM data reach the same downstream performances of previous crawls, significantly reducing the crawling waste and alleviating the burdens on websites. Our code is publicly available at https://github.com/cxcscmu/Craw4LLM.",
      "authors": [
        "Yu, Shi",
        "Liu, Zhiyuan",
        "Xiong, Chenyan"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2502.13347v3",
        "Other Formats": "https://arxiv.org/format/2502.13347",
        "TeX Source": "https://arxiv.org/src/2502.13347",
        "View PDF": "https://arxiv.org/pdf/2502.13347"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 19 Feb 2025 00:31:43 UTC (327 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 24 Feb 2025 20:12:15 UTC (327 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Mon, 23 Jun 2025 02:24:30 UTC (306 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2025/02/19",
      "title": "Craw4LLM: Efficient Web Crawling for LLM Pretraining",
      "repo_urls": [
        "https://github.com/cxcscmu/crawl4llm"
      ],
      "tasks": [
        "10-shot image generation"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18326",
      "abstract": "The automatic speech quality assessment (SQA) has been extensively studied to predict the speech quality without time-consuming questionnaires. Recently, neural-based SQA models have been actively developed for speech samples produced by text-to-speech or voice conversion, with a primary focus on training mean opinion score (MOS) prediction models. The quality of each speech sample may not be consistent across the entire duration, and it remains unclear which segments of the speech receive the primary focus from humans when assigning subjective evaluation for MOS calculation. We hypothesize that when humans rate speech, they tend to assign more weight to low-quality speech segments, and the variance in ratings for each sample is mainly due to accidental assignment of higher scores when overlooking the poor quality speech segments. Motivated by the hypothesis, we analyze the VCC2018 and BVCC datasets. Based on the hypothesis, we propose the more reliable representative value N_low-MOS, the mean of the $N$-lowest opinion scores. Our experiments show that LCC and SRCC improve compared to regular MOS when employing N_low-MOS to MOSNet training. This result suggests that N_low-MOS is a more intrinsic representative value of subjective speech quality and makes MOSNet a better comparator of VC models.",
      "authors": [
        "Kondo, Yuto",
        "Kameoka, Hirokazu",
        "Tanaka, Kou",
        "Kaneko, Takuhiro"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18326v1",
        "Other Formats": "https://arxiv.org/format/2506.18326",
        "TeX Source": "https://arxiv.org/src/2506.18326",
        "View PDF": "https://arxiv.org/pdf/2506.18326"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 06:18:59 UTC (411 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "Selecting N-lowest scores for training MOS prediction models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18532",
      "abstract": "Grammatical Error Correction (GEC) and feedback play a vital role in supporting second language (L2) learners, educators, and examiners. While written GEC is well-established, spoken GEC (SGEC), aiming to provide feedback based on learners' speech, poses additional challenges due to disfluencies, transcription errors, and the lack of structured input. SGEC systems typically follow a cascaded pipeline consisting of Automatic Speech Recognition (ASR), disfluency detection, and GEC, making them vulnerable to error propagation across modules. This work examines an End-to-End (E2E) framework for SGEC and feedback generation, highlighting challenges and possible solutions when developing these systems. Cascaded, partial-cascaded and E2E architectures are compared, all built on the Whisper foundation model. A challenge for E2E systems is the scarcity of GEC labeled spoken data. To address this, an automatic pseudo-labeling framework is examined, increasing the training data from 77 to over 2500 hours. To improve the accuracy of the SGEC system, additional contextual information, exploiting the ASR output, is investigated. Candidate feedback of their mistakes is an essential step to improving performance. In E2E systems the SGEC output must be compared with an estimate of the fluent transcription to obtain the feedback. To improve the precision of this feedback, a novel reference alignment process is proposed that aims to remove hypothesised edits that results from fluent transcription errors. Finally, these approaches are combined with an edit confidence estimation approach, to exclude low-confidence edits. Experiments on the in-house Linguaskill (LNG) corpora and the publicly available Speak & Improve (S&I) corpus show that the proposed approaches significantly boost E2E SGEC performance.",
      "authors": [
        "Qian, Mengjie",
        "Ma, Rao",
        "Bann\u00f2, Stefano",
        "Gales, Mark J. F.",
        "Knill, Kate M."
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18532v1",
        "Other Formats": "https://arxiv.org/format/2506.18532",
        "TeX Source": "https://arxiv.org/src/2506.18532",
        "View PDF": "https://arxiv.org/pdf/2506.18532"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 11:40:04 UTC (531 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "End-to-End Spoken Grammatical Error Correction",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2410.12557",
      "abstract": "Diffusion models and flow-matching models have enabled generating diverse and realistic images by learning to transfer noise to data. However, sampling from these models involves iterative denoising over many neural network passes, making generation slow and expensive. Previous approaches for speeding up sampling require complex training regimes, such as multiple training phases, multiple networks, or fragile scheduling. We introduce shortcut models, a family of generative models that use a single network and training phase to produce high-quality samples in a single or multiple sampling steps. Shortcut models condition the network not only on the current noise level but also on the desired step size, allowing the model to skip ahead in the generation process. Across a wide range of sampling step budgets, shortcut models consistently produce higher quality samples than previous approaches, such as consistency models and reflow. Compared to distillation, shortcut models reduce complexity to a single network and training phase and additionally allow varying step budgets at inference time.",
      "authors": [
        "Frans, Kevin",
        "Hafner, Danijar",
        "Levine, Sergey",
        "Abbeel, Pieter"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2410.12557v3",
        "Other Formats": "https://arxiv.org/format/2410.12557",
        "TeX Source": "https://arxiv.org/src/2410.12557",
        "View PDF": "https://arxiv.org/pdf/2410.12557"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 16 Oct 2024 13:34:40 UTC (17,816 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 25 May 2025 23:37:47 UTC (17,816 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Mon, 23 Jun 2025 14:26:35 UTC (9,513 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/10/16",
      "title": "One Step Diffusion via Shortcut Models",
      "repo_urls": [
        "https://github.com/kvfrans/shortcut-models",
        "https://github.com/leffff/FlowModels"
      ],
      "tasks": [
        "Denoising",
        "Scheduling"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18842",
      "abstract": "We introduce a new dataset and algorithm for fast and efficient coastal distance calculations from Anywhere on Earth (AoE). Existing global coastal datasets are only available at coarse resolution (e.g. 1-4 km) which limits their utility. Publicly available satellite imagery combined with computer vision enable much higher precision. We provide a global coastline dataset at 10 meter resolution, a 100+ fold improvement in precision over existing data. To handle the computational challenge of querying at such an increased scale, we introduce a new library: Layered Iterative Geospatial Hierarchical Terrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both exceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM to achieve millisecond online inference, making it well suited for real-time applications in resource-constrained environments.",
      "authors": [
        "Beukema, Patrick",
        "Herzog, Henry",
        "Zhang, Yawen",
        "Pitelka, Hunter",
        "Bastani, Favyen"
      ],
      "last_revised_date": "2025/06/23",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18842v1",
        "Other Formats": "https://arxiv.org/format/2506.18842",
        "TeX Source": "https://arxiv.org/src/2506.18842",
        "View PDF": "https://arxiv.org/pdf/2506.18842"
      },
      "subjects": [
        "Databases (cs.DB)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 23 Jun 2025 17:00:34 UTC (7,922 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/23",
      "title": "LIGHTHOUSE: Fast and precise distance to shoreline calculations from anywhere on earth",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2212.09044",
      "abstract": "Many analysis and prediction tasks require the extraction of structured data from unstructured texts. However, an annotation scheme and a training dataset have not been available for training machine learning models to mine structured data from text without special templates and patterns. To solve it, this paper presents an end-to-end machine learning pipeline, Text2Struct, including a text annotation scheme, training data processing, and machine learning implementation. We formulated the mining problem as the extraction of metrics and units associated with numerals in the text. Text2Struct was trained and evaluated using an annotated text dataset collected from abstracts of medical publications regarding thrombectomy. In terms of prediction performance, a dice coefficient of 0.82 was achieved on the test dataset. By random sampling, most predicted relations between numerals and entities were well matched to the ground-truth annotations. These results show that Text2Struct is viable for the mining of structured data from text without special templates or patterns. It is anticipated to further improve the pipeline by expanding the dataset and investigating other machine learning models. A code demonstration can be found at: https://github.com/zcc861007/Text2Struct",
      "authors": [
        "Zhou, Chaochao",
        "Yang, Bo"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "Other Formats": "https://arxiv.org/format/2212.09044",
        "View PDF": "https://arxiv.org/pdf/2212.09044"
      },
      "subjects": [
        "Information Retrieval (cs.IR)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 18 Dec 2022 09:31:36 UTC (673 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Tue, 20 Dec 2022 21:49:18 UTC (673 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Fri, 13 Jan 2023 23:30:39 UTC (711 KB)",
          "link": "/",
          "version": "[v3]"
        },
        {
          "details": "Sun, 22 Jun 2025 04:58:27 UTC (743 KB)",
          "version": "[v4]"
        }
      ],
      "submitted_date": "2022/12/18",
      "title": "Text2Struct: A Machine Learning Pipeline for Mining Structured Data from Text",
      "repo_urls": [
        "https://github.com/zcc861007/courseproject"
      ],
      "tasks": [
        "text annotation"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18134",
      "abstract": "Polyp detection is crucial for colorectal cancer screening, yet existing models are limited by the scale and diversity of available data. While generative models show promise for data augmentation, current methods mainly focus on enhancing polyp diversity, often overlooking the critical issue of false positives. In this paper, we address this gap by proposing an adversarial diffusion framework to synthesize high-value false positives. The extensive variability of negative backgrounds presents a significant challenge in false positive synthesis. To overcome this, we introduce two key innovations: First, we design a regional noise matching strategy to construct a negative synthesis space using polyp detection datasets. This strategy trains a negative-centric diffusion model by masking polyp regions, ensuring the model focuses exclusively on learning diverse background patterns. Second, we introduce the Detector-guided Adversarial Diffusion Attacker (DADA) module, which perturbs the negative synthesis process to disrupt a pre-trained detector's decision, guiding the negative-centric diffusion model to generate high-value, detector-confusing false positives instead of low-value, ordinary backgrounds. Our approach is the first to apply adversarial diffusion to lesion detection, establishing a new paradigm for targeted false positive synthesis and paving the way for more reliable clinical applications in colorectal cancer screening. Extensive results on public and in-house datasets verify the superiority of our method over the current state-of-the-arts, with our synthesized data improving the detectors by at least 2.6% and 2.7% in F1-score, respectively, over the baselines. Codes are at https://github.com/Huster-Hq/DADA.",
      "authors": [
        "Zhou, Quan",
        "Luo, Gan",
        "Hu, Qiang",
        "Zhang, Qingyong",
        "Zhang, Jinhua",
        "Tian, Yinjiao",
        "Li, Qiang",
        "Wang, Zhiwei"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18134v1",
        "Other Formats": "https://arxiv.org/format/2506.18134",
        "TeX Source": "https://arxiv.org/src/2506.18134",
        "View PDF": "https://arxiv.org/pdf/2506.18134"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 18:36:44 UTC (1,462 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Targeted False Positive Synthesis via Detector-guided Adversarial Diffusion Attacker for Robust Polyp Detection",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18173",
      "abstract": "While deep learning-based architectures have been widely used for correctly detecting and classifying plant diseases, they require large-scale datasets to learn generalized features and achieve state-of-the-art performance. This poses a challenge for such models to obtain satisfactory performance in classifying leaf diseases with limited samples. This work proposes a few-shot learning framework, Domain-adapted Expert Network (DExNet), for plant disease classification that compensates for the lack of sufficient training data by combining observations of a number of expert critics. It starts with extracting the feature embeddings as 'observations' from nine 'critics' that are state-of-the-art pre-trained CNN-based architectures. These critics are 'domain adapted' using a publicly available leaf disease dataset having no overlapping classes with the specific downstream task of interest. The observations are then passed to the 'Feature Fusion Block' and finally to a classifier network consisting of Bi-LSTM layers. The proposed pipeline is evaluated on the 10 classes of tomato leaf images from the PlantVillage dataset, achieving promising accuracies of 89.06%, 92.46%, and 94.07%, respectively, for 5-shot, 10-shot, and 15-shot classification. Furthermore, an accuracy of 98.09+-0.7% has been achieved in 80-shot classification, which is only 1.2% less than state-of-the-art, allowing a 94.5% reduction in the training data requirement. The proposed pipeline also outperforms existing works on leaf disease classification with limited data in both laboratory and real-life conditions in single-domain, mixed-domain, and cross-domain scenarios.",
      "authors": [
        "Ahmed, Sabbir",
        "Hasan, Md. Bakhtiar",
        "Ahmed, Tasnim",
        "Kabir, Md. Hasanul"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18173v1",
        "Other Formats": "https://arxiv.org/format/2506.18173",
        "TeX Source": "https://arxiv.org/src/2506.18173",
        "View PDF": "https://arxiv.org/pdf/2506.18173"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 21:15:54 UTC (6,946 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "DExNet: Combining Observations of Domain Adapted Critics for Leaf Disease Classification with Limited Data",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.09661",
      "abstract": "This paper focuses on explaining the timbre conveyed by speech signals and introduces a task termed voice timbre attribute detection (vTAD). In this task, voice timbre is explained with a set of sensory attributes describing its human perception. A pair of speech utterances is processed, and their intensity is compared in a designated timbre descriptor. Moreover, a framework is proposed, which is built upon the speaker embeddings extracted from the speech utterances. The investigation is conducted on the VCTK-RVA dataset. Experimental examinations on the ECAPA-TDNN and FACodec speaker encoders demonstrated that: 1) the ECAPA-TDNN speaker encoder was more capable in the seen scenario, where the testing speakers were included in the training set; 2) the FACodec speaker encoder was superior in the unseen scenario, where the testing speakers were not part of the training, indicating enhanced generalization capability. The VCTK-RVA dataset and open-source code are available on the website https://github.com/vTAD2025-Challenge/vTAD.",
      "authors": [
        "He, Jinghao",
        "Sheng, Zhengyan",
        "Chen, Liping",
        "Lee, Kong Aik",
        "Ling, Zhen-Hua"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "Other Formats": "https://arxiv.org/format/2505.09661",
        "TeX Source": "https://arxiv.org/src/2505.09661",
        "View PDF": "https://arxiv.org/pdf/2505.09661"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Artificial Intelligence (cs.AI)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 14 May 2025 13:46:46 UTC (365 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 11:25:43 UTC (354 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/14",
      "title": "Introducing voice timbre attribute detection",
      "repo_urls": [
        "https://github.com/vtad2025-challenge/vtad"
      ],
      "tasks": [
        "Attribute"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17939",
      "abstract": "Medical visual question answering aims to support clinical decision-making by enabling models to answer natural language questions based on medical images. While recent advances in multi-modal learning have significantly improved performance, current methods still suffer from limited answer reliability and poor interpretability, impairing the ability of clinicians and patients to understand and trust model-generated answers. To address this, this work first proposes a Thinking with Visual Grounding (ThinkVG) dataset wherein the answer generation is decomposed into intermediate reasoning steps that explicitly ground relevant visual regions of the medical image, thereby providing fine-grained explainability. Furthermore, we introduce a novel verifiable reward mechanism for reinforcement learning to guide post-training, improving the alignment between the model's reasoning process and its final answer. Remarkably, our method achieves comparable performance using only one-eighth of the training data, demonstrating the efficiency and effectiveness of the proposal. The dataset is available at https://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG.",
      "authors": [
        "Liu, Bo",
        "Zhao, Xiangyu",
        "He, Along",
        "Chen, Yidi",
        "Fu, Huazhu",
        "Wu, Xiao-Ming"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17939v1",
        "Other Formats": "https://arxiv.org/format/2506.17939",
        "TeX Source": "https://arxiv.org/src/2506.17939",
        "View PDF": "https://arxiv.org/pdf/2506.17939"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 08:09:58 UTC (672 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via Reinforcement Learning",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17931",
      "abstract": "We present a novel approach for unsupervised domain adaptation (UDA) for natural images. A commonly-used objective for UDA schemes is to enhance domain alignment in representation space even if there is a domain shift in the input space. Existing adversarial domain adaptation methods may not effectively align different domains of multimodal distributions associated with classification problems. Our approach has two main features. Firstly, its neural architecture uses the deep structure of ResNet and the effective separation of scales of feature pyramidal network (FPN) to work with both content and style features. Secondly, it uses a combination of a novel loss function and judiciously selected existing loss functions to train the network architecture. This tailored combination is designed to address challenges inherent to natural images, such as scale, noise, and style shifts, that occur on top of a multi-modal (multi-class) distribution. The combined loss function not only enhances model accuracy and robustness on the target domain but also speeds up training convergence. Our proposed UDA scheme generalizes better than state-of-the-art for CNN-based methods on Office-Home, Office-31, and VisDA-2017 datasets and comaparable for DomainNet dataset.",
      "authors": [
        "Gupta, Ravi Kant",
        "Das, Shounak",
        "Sethi, Amit"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17931v1",
        "Other Formats": "https://arxiv.org/format/2506.17931",
        "TeX Source": "https://arxiv.org/src/2506.17931",
        "View PDF": "https://arxiv.org/pdf/2506.17931"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 07:56:10 UTC (255 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "IDAL: Improved Domain Adaptive Learning for Natural Images Dataset",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17868",
      "abstract": "Accurately modeling and predicting complex dynamical systems, particularly those involving force exchange and dissipation, is crucial for applications ranging from fluid dynamics to robotics, but presents significant challenges due to the intricate interplay of geometric constraints and energy transfer. This paper introduces Geometric Contact Flows (GFC), a novel framework leveraging Riemannian and Contact geometry as inductive biases to learn such systems. GCF constructs a latent contact Hamiltonian model encoding desirable properties like stability or energy conservation. An ensemble of contactomorphisms then adapts this model to the target dynamics while preserving these properties. This ensemble allows for uncertainty-aware geodesics that attract the system's behavior toward the data support, enabling robust generalization and adaptation to unseen scenarios. Experiments on learning dynamics for physical systems and for controlling robots on interaction tasks demonstrate the effectiveness of our approach.",
      "authors": [
        "Testa, Andrea",
        "Hauberg, S\u00f8ren",
        "Asfour, Tamim",
        "Rozo, Leonel"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17868v1",
        "Other Formats": "https://arxiv.org/format/2506.17868",
        "TeX Source": "https://arxiv.org/src/2506.17868",
        "View PDF": "https://arxiv.org/pdf/2506.17868"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Machine Learning (cs.LG)",
        "Differential Geometry (math.DG)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 01:52:21 UTC (14,633 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Geometric Contact Flows: Contactomorphisms for Dynamics and Control",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17967",
      "abstract": "World models -- generative models that simulate environment dynamics conditioned on past observations and actions -- are gaining prominence in planning, simulation, and embodied AI. However, evaluating their rollouts remains a fundamental challenge, requiring fine-grained, temporally grounded assessment of action alignment and semantic consistency -- capabilities not captured by existing metrics. Vision-Language Models (VLMs) have shown promise as automatic evaluators of generative content due to their strong multimodal reasoning abilities. Yet, their use in fine-grained, temporally sensitive evaluation tasks remains limited and requires targeted adaptation. We introduce a evaluation protocol targeting two recognition tasks -- action recognition and character recognition -- each assessed across binary, multiple-choice, and open-ended formats. To support this, we present UNIVERSE (UNIfied Vision-language Evaluator for Rollouts in Simulated Environments), a method for adapting VLMs to rollout evaluation under data and compute constraints. We conduct a large-scale study comparing full, partial, and parameter-efficient finetuning across task formats, context lengths, sampling strategies, and data compositions. The resulting unified evaluator matches the performance of task-specific baselines using a single checkpoint. Human studies confirm strong alignment with human judgments, establishing UNIVERSE as a scalable, semantics-aware evaluator for world models.",
      "authors": [
        "Hendriksen, Mariya",
        "Rashid, Tabish",
        "Bignell, David",
        "Georgescu, Raluca",
        "Lemkhenter, Abdelhak",
        "Hofmann, Katja",
        "Devlin, Sam",
        "Parisot, Sarah"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17967v1",
        "Other Formats": "https://arxiv.org/format/2506.17967",
        "TeX Source": "https://arxiv.org/src/2506.17967",
        "View PDF": "https://arxiv.org/pdf/2506.17967"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 09:53:28 UTC (5,189 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Adapting Vision-Language Models for Evaluating World Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2503.23470",
      "abstract": "Proper recitation of the Quran, adhering to the rules of Tajweed, is crucial for preventing mistakes during recitation and requires significant effort to master. Traditional methods of teaching these rules are limited by the availability of qualified instructors and time constraints. Automatic evaluation of recitation can address these challenges by providing prompt feedback and supporting independent practice. This study focuses on developing a deep learning model to classify three Tajweed rules - separate stretching (Al Mad), tight noon (Ghunnah), and hide (Ikhfaa) - using the publicly available QDAT dataset, which contains over 1,500 audio recordings. The input data consisted of audio recordings from this dataset, transformed into normalized mel-spectrograms. For classification, the EfficientNet-B0 architecture was used, enhanced with a Squeeze-and-Excitation attention mechanism. The developed model achieved accuracy rates of 95.35%, 99.34%, and 97.01% for the respective rules. An analysis of the learning curves confirmed the model's robustness and absence of overfitting. The proposed approach demonstrates high efficiency and paves the way for developing interactive educational systems for Tajweed study.",
      "authors": [
        "Shaiakhmetov, Dim",
        "Gimaletdinova, Gulnaz",
        "Momunov, Kadyrmamat",
        "Cankurt, Selcuk"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2503.23470v2",
        "Other Formats": "https://arxiv.org/format/2503.23470",
        "TeX Source": "https://arxiv.org/src/2503.23470",
        "View PDF": "https://arxiv.org/pdf/2503.23470"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 30 Mar 2025 15:03:02 UTC (369 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 09:12:35 UTC (367 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/03/30",
      "title": "Evaluation of the Pronunciation of Tajweed Rules Based on DNN as a Step Towards Interactive Recitation Learning",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2504.14310",
      "abstract": "The widespread adoption of large artificial intelligence (AI) models has enabled numerous applications of the Internet of Things (IoT). However, large AI models require substantial computational and memory resources, which exceed the capabilities of resource-constrained IoT devices. End-edge collaboration paradigm is developed to address this issue, where a small model on the end device performs inference tasks, while a large model on the edge server assists with model updates. To improve the accuracy of the inference tasks, the data generated on the end devices will be periodically uploaded to edge server to update model, and a distilled model of the updated one will be transmitted back to the end device. Subjected to the limited bandwidth for the communication link between the end device and the edge server, it is important to investigate whether the system should allocate more bandwidth to data upload or to model transmission. In this paper, we characterize the impact of data upload and model transmission on inference accuracy. Subsequently, we formulate a bandwidth allocation problem. By solving this problem, we derive an efficient optimization framework for the end-edge collaboration system. The simulation results demonstrate our framework significantly enhances mean average precision (mAP) under various bandwidths and datasizes.",
      "authors": [
        "Yang, Dailin",
        "Zhang, Shuhang",
        "Zhang, Hongliang",
        "Song, Lingyang"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2504.14310v2",
        "Other Formats": "https://arxiv.org/format/2504.14310",
        "TeX Source": "https://arxiv.org/src/2504.14310",
        "View PDF": "https://arxiv.org/pdf/2504.14310"
      },
      "subjects": [
        "Emerging Technologies (cs.ET)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 19 Apr 2025 14:23:30 UTC (1,078 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 06:23:31 UTC (2,498 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/04/19",
      "title": "End-Edge Model Collaboration: Bandwidth Allocation for Data Upload and Model Transmission",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18088",
      "abstract": "Simulation-based data synthesis has emerged as a powerful paradigm for enhancing real-world robotic manipulation. However, existing synthetic datasets remain insufficient for robust bimanual manipulation due to two challenges: (1) the lack of an efficient, scalable data generation method for novel tasks, and (2) oversimplified simulation environments that fail to capture real-world complexity. We present RoboTwin 2.0, a scalable simulation framework that enables automated, large-scale generation of diverse and realistic data, along with unified evaluation protocols for dual-arm manipulation. We first construct RoboTwin-OD, a large-scale object library comprising 731 instances across 147 categories, each annotated with semantic and manipulation-relevant labels. Building on this foundation, we develop an expert data synthesis pipeline that combines multimodal large language models (MLLMs) with simulation-in-the-loop refinement to generate task-level execution code automatically. To improve sim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization along five axes: clutter, lighting, background, tabletop height and language instructions, thereby enhancing data diversity and policy robustness. We instantiate this framework across 50 dual-arm tasks spanning five robot embodiments, and pre-collect over 100,000 domain-randomized expert trajectories. Empirical results show a 10.9% gain in code generation success and improved generalization to novel real-world scenarios. A VLA model fine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%) on unseen scene real-world tasks, while zero-shot models trained solely on our synthetic data achieve a 228% relative gain, highlighting strong generalization without real-world supervision. We release the data generator, benchmark, dataset, and code to support scalable research in robust bimanual manipulation.",
      "authors": [
        "Chen, Tianxing",
        "Chen, Zanxin",
        "Chen, Baijun",
        "Cai, Zijian",
        "Liu, Yibin",
        "Liang, Qiwei",
        "Li, Zixuan",
        "Lin, Xianliang",
        "Ge, Yiheng",
        "Gu, Zhenyu",
        "Deng, Weiliang",
        "Guo, Yubin",
        "Nian, Tian",
        "Xie, Xuanbing",
        "Chen, Qiangyu",
        "Su, Kailun",
        "Xu, Tianling",
        "Liu, Guodong",
        "Hu, Mengkang",
        "Gao, Huan-ang",
        "Wang, Kaixuan",
        "Liang, Zhixuan",
        "Qin, Yusen",
        "Yang, Xiaokang",
        "Luo, Ping",
        "Mu, Yao"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18088v1",
        "Other Formats": "https://arxiv.org/format/2506.18088",
        "TeX Source": "https://arxiv.org/src/2506.18088",
        "View PDF": "https://arxiv.org/pdf/2506.18088"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Multiagent Systems (cs.MA)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 16:26:53 UTC (6,813 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18114",
      "abstract": "The rapid expansion of the Internet of Things (IoT) has introduced significant security challenges, necessitating efficient and adaptive Intrusion Detection Systems (IDS). Traditional IDS models often overlook the temporal characteristics of network traffic, limiting their effectiveness in early threat detection. We propose a Transformer-based Early Intrusion Detection System (EIDS) that incorporates dynamic temporal positional encodings to enhance detection accuracy while maintaining computational efficiency. By leveraging network flow timestamps, our approach captures both sequence structure and timing irregularities indicative of malicious behaviour. Additionally, we introduce a data augmentation pipeline to improve model robustness. Evaluated on the CICIoT2023 dataset, our method outperforms existing models in both accuracy and earliness. We further demonstrate its real-time feasibility on resource-constrained IoT devices, achieving low-latency inference and minimal memory footprint.",
      "authors": [
        "Panopoulos, Ioannis",
        "Bartsioka, Maria-Lamprini A.",
        "Nikolaidis, Sokratis",
        "Venieris, Stylianos I.",
        "Kaklamani, Dimitra I.",
        "Venieris, Iakovos S."
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18114v1",
        "Other Formats": "https://arxiv.org/format/2506.18114",
        "TeX Source": "https://arxiv.org/src/2506.18114",
        "View PDF": "https://arxiv.org/pdf/2506.18114"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 17:56:19 UTC (1,475 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Dynamic Temporal Positional Encodings for Early Intrusion Detection in IoT",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2407.09174",
      "abstract": "Accurate real-time object detection is vital across numerous industrial applications, from safety monitoring to quality control. Traditional approaches, however, are hindered by arduous manual annotation and data collection, struggling to adapt to ever-changing environments and novel target objects. To address these limitations, this paper presents DART, an innovative automated end-to-end pipeline that revolutionizes object detection workflows from data collection to model evaluation. It eliminates the need for laborious human labeling and extensive data collection while achieving outstanding accuracy across diverse scenarios. DART encompasses four key stages: (1) Data Diversification using subject-driven image generation (DreamBooth with SDXL), (2) Annotation via open-vocabulary object detection (Grounding DINO) to generate bounding box and class labels, (3) Review of generated images and pseudo-labels by large multimodal models (InternVL-1.5 and GPT-4o) to guarantee credibility, and (4) Training of real-time object detectors (YOLOv8 and YOLOv10) using the verified data. We apply DART to a self-collected dataset of construction machines named Liebherr Product, which contains over 15K high-quality images across 23 categories. The current instantiation of DART significantly increases average precision (AP) from 0.064 to 0.832. Its modular design ensures easy exchangeability and extensibility, allowing for future algorithm upgrades, seamless integration of new object categories, and adaptability to customized environments without manual labeling and additional data collection. The code and dataset are released at https://github.com/chen-xin-94/DART.",
      "authors": [
        "Xin, Chen",
        "Hartel, Andreas",
        "Kasneci, Enkelejda"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2407.09174v4",
        "Other Formats": "https://arxiv.org/format/2407.09174",
        "TeX Source": "https://arxiv.org/src/2407.09174",
        "View PDF": "https://arxiv.org/pdf/2407.09174"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 12 Jul 2024 11:16:44 UTC (29,614 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Fri, 26 Jul 2024 11:01:21 UTC (29,574 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Mon, 29 Jul 2024 09:14:07 UTC (29,574 KB)",
          "link": "/",
          "version": "[v3]"
        },
        {
          "details": "Sun, 22 Jun 2025 01:28:38 UTC (29,583 KB)",
          "version": "[v4]"
        }
      ],
      "submitted_date": "2024/07/12",
      "title": "DART: An Automated End-to-End Object Detection Pipeline with Data Diversification, Open-Vocabulary Bounding Box Annotation, Pseudo-Label Review, and Model Training",
      "repo_urls": [
        "https://github.com/chen-xin-94/dart"
      ],
      "tasks": [
        "Image Generation",
        "Object",
        "object-detection",
        "Object Detection",
        "Open-vocabulary object detection",
        "Open Vocabulary Object Detection",
        "Pseudo Label",
        "Real-Time Object Detection"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18194",
      "abstract": "Recent advances in machine learning (ML) have shown promise in accelerating the discovery of polymers with desired properties by aiding in tasks such as virtual screening via property prediction. However, progress in polymer ML is hampered by the scarcity of high-quality labeled datasets, which are necessary for training supervised ML models. In this work, we study the use of the very recent 'Joint Embedding Predictive Architecture' (JEPA), a type of architecture for self-supervised learning (SSL), on polymer molecular graphs to understand whether pretraining with the proposed SSL strategy improves downstream performance when labeled data is scarce. Our results indicate that JEPA-based self-supervised pretraining on polymer graphs enhances downstream performance, particularly when labeled data is very scarce, achieving improvements across all tested datasets.",
      "authors": [
        "Picolli, Francesco",
        "Vogel, Gabriel",
        "Weber, Jana M."
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18194v1",
        "Other Formats": "https://arxiv.org/format/2506.18194",
        "TeX Source": "https://arxiv.org/src/2506.18194",
        "View PDF": "https://arxiv.org/pdf/2506.18194"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 22:51:53 UTC (8,518 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Joint Embedding Predictive Architecture for self-supervised pretraining on polymer molecular graphs",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17960",
      "abstract": "Reliable navigation in unstructured, real-world environments remains a significant challenge for embodied agents, especially when operating across diverse terrains, weather conditions, and sensor configurations. In this paper, we introduce GeNIE (Generalizable Navigation System for In-the-Wild Environments), a robust navigation framework designed for global deployment. GeNIE integrates a generalizable traversability prediction model built on SAM2 with a novel path fusion strategy that enhances planning stability in noisy and ambiguous settings. We deployed GeNIE in the Earth Rover Challenge (ERC) at ICRA 2025, where it was evaluated across six countries spanning three continents. GeNIE took first place and achieved 79% of the maximum possible score, outperforming the second-best team by 17%, and completed the entire competition without a single human intervention. These results set a new benchmark for robust, generalizable outdoor robot navigation. We will release the codebase, pretrained model weights, and newly curated datasets to support future research in real-world navigation.",
      "authors": [
        "Wang, Jiaming",
        "Liu, Diwen",
        "Chen, Jizhuo",
        "Da, Jiaxuan",
        "Qian, Nuowen",
        "Man, Tram Minh",
        "Soh, Harold"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17960v1",
        "Other Formats": "https://arxiv.org/format/2506.17960",
        "TeX Source": "https://arxiv.org/src/2506.17960",
        "View PDF": "https://arxiv.org/pdf/2506.17960"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 09:36:05 UTC (7,855 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "GeNIE: A Generalizable Navigation System for In-the-Wild Environments",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17930",
      "abstract": "We propose a novel prompt design paradigm that challenges conventional wisdom in large language model (LLM) prompting. While conventional wisdom prioritizes well-crafted instructions and demonstrations for in-context learning (ICL), we show that pruning random demonstrations into seemingly incoherent \"gibberish\" can remarkably improve performance across diverse tasks. Notably, the \"gibberish\" always matches or surpasses state-of-the-art automatic prompt optimization techniques, achieving substantial gains regardless of LLM alignment. Nevertheless, discovering an effective pruning strategy is non-trivial, as existing attribution methods and prompt compression algorithms fail to deliver robust results, let alone human intuition. In terms of this, we propose a self-discover prompt optimization framework, PromptQuine, an evolutionary search framework that automatically searches for the pruning strategy by itself using only low-data regimes. Much like the emergent complexity in nature--such as symbiosis and self-organization--arising in response to resource constraints, our framework evolves and refines unconventional yet highly effective prompts by leveraging only the tokens present within the context. We demonstrate its effectiveness across classification, multi-choice question answering, generation and math reasoning tasks across LLMs, while achieving decent runtime efficiency. We hope our findings can guide mechanistic studies on in-context learning, and provide a call to action, to pave the way for more open-ended search algorithms for more effective LLM prompting.",
      "authors": [
        "Wang, Jianyu",
        "Hu, Zhiqiang",
        "Bing, Lidong"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17930v1",
        "Other Formats": "https://arxiv.org/format/2506.17930",
        "TeX Source": "https://arxiv.org/src/2506.17930",
        "View PDF": "https://arxiv.org/pdf/2506.17930"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)",
        "Neural and Evolutionary Computing (cs.NE)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 07:53:07 UTC (862 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2405.07332",
      "abstract": "Numerous applications have resulted from the automation of agricultural disease segmentation using deep learning techniques. However, when applied to new conditions, these applications frequently face the difficulty of overfitting, resulting in lower segmentation performance. In the context of potato farming, where diseases have a large influence on yields, it is critical for the agricultural economy to quickly and properly identify these diseases. Traditional data augmentation approaches, such as rotation, flip, and translation, have limitations and frequently fail to provide strong generalization results. To address these issues, our research employs a novel approach termed as PotatoGANs. In this novel data augmentation approach, two types of Generative Adversarial Networks (GANs) are utilized to generate synthetic potato disease images from healthy potato images. This approach not only expands the dataset but also adds variety, which helps to enhance model generalization. Using the Inception score as a measure, our experiments show the better quality and realisticness of the images created by PotatoGANs, emphasizing their capacity to resemble real disease images closely. The CycleGAN model outperforms the Pix2Pix GAN model in terms of image quality, as evidenced by its higher IS scores CycleGAN achieves higher Inception scores (IS) of 1.2001 and 1.0900 for black scurf and common scab, respectively. This synthetic data can significantly improve the training of large neural networks. It also reduces data collection costs while enhancing data diversity and generalization capabilities. Our work improves interpretability by combining three gradient-based Explainable AI algorithms (GradCAM, GradCAM++, and ScoreCAM) with three distinct CNN architectures (DenseNet169, Resnet152 V2, InceptionResNet V2) for potato disease classification.",
      "authors": [
        "Faria, Fatema Tuj Johora",
        "Moin, Mukaffi Bin",
        "Alam, Mohammad Shafiul",
        "Wase, Ahmed Al",
        "Sani, Md. Rabius",
        "Hasib, Khan Md"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2405.07332v2",
        "Other Formats": "https://arxiv.org/format/2405.07332",
        "TeX Source": "https://arxiv.org/src/2405.07332",
        "View PDF": "https://arxiv.org/pdf/2405.07332"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 12 May 2024 17:00:52 UTC (15,340 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 05:53:32 UTC (7,315 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/05/12",
      "title": "PotatoGANs: Utilizing Generative Adversarial Networks, Instance Segmentation, and Explainable AI for Enhanced Potato Disease Identification and Classification",
      "repo_urls": [
        "https://github.com/Mukaffi28/ExplainableAI-PotatoGAN-Cutting-Edge-Disease-Identification-for-Potatoes"
      ],
      "tasks": [
        "Data Augmentation",
        "Instance Segmentation",
        "Semantic Segmentation"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2410.03039",
      "abstract": "Diffusion Models (DMs) have become powerful image generation tools, especially for few-shot fine-tuning where a pretrained DM is fine-tuned on a small image set to capture specific styles or objects. Many people upload these personalized checkpoints online, fostering communities such as Civitai and HuggingFace. However, model owners may overlook the data leakage risks when releasing fine-tuned checkpoints. Moreover, concerns regarding copyright violations arise when unauthorized data is used during fine-tuning. In this paper, we ask: \"Can training data be extracted from these fine-tuned DMs shared online?\" A successful extraction would present not only data leakage threats but also offer tangible evidence of copyright infringement. To answer this, we propose FineXtract, a framework for extracting fine-tuning data. Our method approximates fine-tuning as a gradual shift in the model's learned distribution -- from the original pretrained DM toward the fine-tuning data. By extrapolating the models before and after fine-tuning, we guide the generation toward high-probability regions within the fine-tuned data distribution. We then apply a clustering algorithm to extract the most probable images from those generated using this extrapolated guidance. Experiments on DMs fine-tuned with datasets including WikiArt, DreamBooth, and real-world checkpoints posted online validate the effectiveness of our method, extracting about 20% of fine-tuning data in most cases. The code is available https://github.com/Nicholas0228/FineXtract.",
      "authors": [
        "Wu, Xiaoyu",
        "Zhang, Jiaru",
        "Wu, Zhiwei Steven"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2410.03039v2",
        "Other Formats": "https://arxiv.org/format/2410.03039",
        "TeX Source": "https://arxiv.org/src/2410.03039",
        "View PDF": "https://arxiv.org/pdf/2410.03039"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 3 Oct 2024 23:06:11 UTC (42,629 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 08:37:39 UTC (9,378 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/10/03",
      "title": "Leveraging Model Guidance to Extract Training Data from Personalized Diffusion Models",
      "tasks": [
        "Image Generation"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17894",
      "abstract": "Chip manufacturing is a complex process, and to achieve a faster time to market, an increasing number of untrusted third-party tools and designs from around the world are being utilized. The use of these untrusted third party intellectual properties (IPs) and tools increases the risk of adversaries inserting hardware trojans (HTs). The covert nature of HTs poses significant threats to cyberspace, potentially leading to severe consequences for national security, the economy, and personal privacy. Many graph neural network (GNN)-based HT detection methods have been proposed. However, they perform poorly on larger designs because they rely on training with smaller designs. Additionally, these methods do not explore different GNN models that are well-suited for HT detection or provide efficient training and inference processes. We propose a novel framework that generates graph embeddings for large designs (e.g., RISC-V) and incorporates various GNN models tailored for HT detection. Furthermore, our framework introduces domain-specific techniques for efficient training and inference by implementing model quantization. Model quantization reduces the precision of the weights, lowering the computational requirements, enhancing processing speed without significantly affecting detection accuracy. We evaluate our framework using a custom dataset, and our results demonstrate a precision of 98.66% and a recall (true positive rate) of 92.30%, highlighting the effectiveness and efficiency of our approach in detecting hardware trojans in large-scale chip designs",
      "authors": [
        "Thorat, Kiran",
        "Hasan, Amit",
        "Ding, Caiwen",
        "Shi, Zhijie"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17894v1",
        "Other Formats": "https://arxiv.org/format/2506.17894",
        "TeX Source": "https://arxiv.org/src/2506.17894",
        "View PDF": "https://arxiv.org/pdf/2506.17894"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 04:13:30 UTC (367 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "TROJAN-GUARD: Hardware Trojans Detection Using GNN in RTL Designs",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2503.07371",
      "abstract": "Accurate, real-time object detection on resource-constrained hardware is critical for anomaly-behavior monitoring. We introduce HGO-YOLO, a lightweight detector that combines GhostHGNetv2 with an optimized parameter-sharing head (OptiConvDetect) to deliver an outstanding accuracy-efficiency trade-off. By embedding GhostConv into the HGNetv2 backbone with multi-scale residual fusion, the receptive field is enlarged while redundant computation is reduced by 50%. OptiConvDetect shares a partial-convolution layer for the classification and regression branches, cutting detection-head FLOPs by 41% without accuracy loss. On three anomaly datasets (fall, fight, smoke), HGO-YOLO attains 87.4% mAP@0.5 and 81.1% recall at 56 FPS on a single CPU with just 4.3 GFLOPs and 4.6 MB-surpassing YOLOv8n by +3.0% mAP, -51.7% FLOPs, and 1.7* speed. Real-world tests on a Jetson Orin Nano further confirm a stable throughput gain of 42 FPS.",
      "authors": [
        "Zheng, Qizhi",
        "Luo, Zhongze",
        "Guo, Meiyan",
        "Wang, Xinzhu",
        "Wu, Renqimuge",
        "Meng, Qiu",
        "Dong, Guanghui"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "Other Formats": "https://arxiv.org/format/2503.07371",
        "View PDF": "https://arxiv.org/pdf/2503.07371"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 10 Mar 2025 14:29:12 UTC (1,025 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 11:56:12 UTC (1,092 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/03/10",
      "title": "HGO-YOLO: Advancing Anomaly Behavior Detection with Hierarchical Features and Lightweight Optimized Detection",
      "tasks": [
        "object-detection",
        "Object Detection",
        "Real-Time Object Detection"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17989",
      "abstract": "Training models on uncurated Text Embeddings (TEs) derived from raw tabular data can lead to a severe failure mode known as model collapse, where predictions converge to a single class regardless of input. By comparing models trained with identical hyper-parameter configurations on both raw tabular data and their TE-derived counterparts, we find that collapse is a consistent failure mode in the latter setting. We introduce a set of metrics that capture the extent of model collapse, offering a new perspective on TE quality as a proxy for data curation. Our results reveal that TE alone does not effectively function as a curation layer - and that their quality significantly influences downstream learning. More insidiously, we observe that the presence of model collapse can yield artificially inflated and spurious Accuracy-on-the-Line correlation. These findings highlight the need for more nuanced curation and evaluation of embedding-based representations, particularly in out-of-distribution settings.",
      "authors": [
        "Mattioli, Lucas",
        "Hadichou, Youness Ait",
        "Chaouche, Sabrina",
        "Gonzalez, Martin"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17989v1",
        "Other Formats": "https://arxiv.org/format/2506.17989",
        "TeX Source": "https://arxiv.org/src/2506.17989",
        "View PDF": "https://arxiv.org/pdf/2506.17989"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 11:01:41 UTC (7,374 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Data Curation Matters: Model Collapse and Spurious Shift Performance Prediction from Training on Uncurated Text Embeddings",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2402.01744",
      "abstract": "Background: Virtual Screening (VS) has become an essential tool in drug discovery, enabling the rapid and cost-effective identification of potential bioactive molecules. Among recent advancements, Graph Neural Networks (GNNs) have gained prominence for their ability to model complex molecular structures using graph-based representations. However, the integration of explainable methods to elucidate the specific contributions of molecular substructures to biological activity remains a significant challenge. This limitation hampers both the interpretability of predictive models and the rational design of novel therapeutics. Results: We trained 20 GNN models on a dataset of small molecules with the goal of predicting their activity on 20 distinct protein targets from the Kinase family. These classifiers achieved state-of-the-art performance in virtual screening tasks, demonstrating high accuracy and robustness on different targets. Building upon these models, we implemented the Hierarchical Grad-CAM graph Explainer (HGE) framework, enabling an in-depth analysis of the molecular moieties driving protein-ligand binding stabilization. HGE exploits Grad-CAM explanations at the atom, ring, and whole-molecule levels, leveraging the message-passing mechanism to highlight the most relevant chemical moieties. Validation against experimental data from the literature confirmed the ability of the explainer to recognize a molecular pattern of drugs and correctly annotate them to the known target. Conclusion: Our approach may represent a valid support to shorten both the screening and the hit discovery process. Detailed knowledge of the molecular substructures that play a role in the binding process can help the computational chemist to gain insights into the structure optimization, as well as in drug repurposing tasks.",
      "authors": [
        "Contino, Salvatore",
        "Sortino, Paolo",
        "Gulotta, Maria Rita",
        "Perricone, Ugo",
        "Pirrone, Roberto"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2402.01744v5",
        "Other Formats": "https://arxiv.org/format/2402.01744",
        "TeX Source": "https://arxiv.org/src/2402.01744",
        "View PDF": "https://arxiv.org/pdf/2402.01744"
      },
      "subjects": [
        "Quantitative Methods (q-bio.QM)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)",
        "Molecular Networks (q-bio.MN)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 29 Jan 2024 17:23:25 UTC (2,064 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Thu, 29 Feb 2024 16:05:32 UTC (2,065 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Wed, 8 May 2024 15:04:37 UTC (2,071 KB)",
          "link": "/",
          "version": "[v3]"
        },
        {
          "details": "Thu, 17 Apr 2025 12:16:35 UTC (3,494 KB)",
          "link": "/",
          "version": "[v4]"
        },
        {
          "details": "Sun, 22 Jun 2025 07:52:43 UTC (3,790 KB)",
          "version": "[v5]"
        }
      ],
      "submitted_date": "2024/01/29",
      "title": "Unveiling Molecular Moieties through Hierarchical Grad-CAM Graph Explainability",
      "repo_urls": [
        "https://github.com/chilab1/hge"
      ],
      "tasks": [
        "Drug Discovery",
        "Explainable artificial intelligence",
        "Explainable Artificial Intelligence (XAI)",
        "valid"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2405.07098",
      "abstract": "We explicitly construct zero loss neural network classifiers. We write the weight matrices and bias vectors in terms of cumulative parameters, which determine truncation maps acting recursively on input space. The configurations for the training data considered are (i) sufficiently small, well separated clusters corresponding to each class, and (ii) equivalence classes which are sequentially linearly separable. In the best case, for $Q$ classes of data in $\\mathbb{R}^M$, global minimizers can be described with $Q(M+2)$ parameters.",
      "authors": [
        "Chen, Thomas",
        "Ewald, Patr\u00edcia Mu\u00f1oz"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2405.07098v3",
        "Other Formats": "https://arxiv.org/format/2405.07098",
        "TeX Source": "https://arxiv.org/src/2405.07098",
        "View PDF": "https://arxiv.org/pdf/2405.07098"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Mathematical Physics (math-ph)",
        "Optimization and Control (math.OC)",
        "Machine Learning (stat.ML)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 11 May 2024 21:29:40 UTC (52 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 16 Sep 2024 18:55:22 UTC (51 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Sun, 22 Jun 2025 05:36:41 UTC (60 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/05/11",
      "title": "Interpretable global minima of deep ReLU neural networks on sequentially separable data",
      "tasks": [],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.12944",
      "abstract": "Risk stratification is a key tool in clinical decision-making, yet current approaches often fail to translate sophisticated survival analysis into actionable clinical criteria. We present a novel method for unsupervised machine learning that directly optimizes for survival heterogeneity across patient clusters through a differentiable adaptation of the multivariate logrank statistic. Unlike most existing methods that rely on proxy metrics, our approach represents novel methodology for training any neural network architecture on any data modality to identify prognostically distinct patient groups. We thoroughly evaluate the method in simulation experiments and demonstrate its utility in practice by applying it to two distinct cancer types: analyzing laboratory parameters from multiple myeloma patients and computed tomography images from non-small cell lung cancer patients, identifying prognostically distinct patient subgroups with significantly different survival outcomes in both cases. Post-hoc explainability analyses uncover clinically meaningful features determining the group assignments which align well with established risk factors and thus lend strong weight to the methods utility. This pan-cancer, model-agnostic approach represents a valuable advancement in clinical risk stratification, enabling the discovery of novel prognostic signatures across diverse data types while providing interpretable results that promise to complement treatment personalization and clinical decision-making in oncology and beyond.",
      "authors": [
        "Ferle, Maximilian",
        "Ader, Jonas",
        "Wiemers, Thomas",
        "Grieb, Nora",
        "Lindenmeyer, Adrian",
        "Meyer, Hans-Jonas",
        "Neumuth, Thomas",
        "Kreuz, Markus",
        "Reiche, Kristin",
        "Merz, Maximilian"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.12944",
        "TeX Source": "https://arxiv.org/src/2506.12944",
        "View PDF": "https://arxiv.org/pdf/2506.12944"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Tissues and Organs (q-bio.TO)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 15 Jun 2025 19:11:10 UTC (17,821 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 18:23:04 UTC (17,936 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/06/15",
      "title": "Unsupervised risk factor identification across cancer types and data modalities via explainable artificial intelligence",
      "tasks": [
        "Decision Making",
        "Explainable artificial intelligence",
        "Survival Analysis"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2504.01395",
      "abstract": "Differentially private (DP) image synthesis aims to generate synthetic images from a sensitive dataset, alleviating the privacy leakage concerns of organizations sharing and utilizing synthetic images. Although previous methods have significantly progressed, especially in training diffusion models on sensitive images with DP Stochastic Gradient Descent (DP-SGD), they still suffer from unsatisfactory performance. In this work, inspired by curriculum learning, we propose a two-stage DP image synthesis framework, where diffusion models learn to generate DP synthetic images from easy to hard. Unlike existing methods that directly use DP-SGD to train diffusion models, we propose an easy stage in the beginning, where diffusion models learn simple features of the sensitive images. To facilitate this easy stage, we propose to use `central images', simply aggregations of random samples of the sensitive dataset. Intuitively, although those central images do not show details, they demonstrate useful characteristics of all images and only incur minimal privacy costs, thus helping early-phase model training. We conduct experiments to present that on the average of four investigated image datasets, the fidelity and utility metrics of our synthetic images are 33.1% and 2.1% better than the state-of-the-art method.",
      "authors": [
        "Li, Kecen",
        "Gong, Chen",
        "Li, Xiaochen",
        "Zhao, Yuzhong",
        "Hou, Xinwen",
        "Wang, Tianhao"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2504.01395v2",
        "Other Formats": "https://arxiv.org/format/2504.01395",
        "TeX Source": "https://arxiv.org/src/2504.01395",
        "View PDF": "https://arxiv.org/pdf/2504.01395"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 2 Apr 2025 06:30:55 UTC (11,924 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 04:59:46 UTC (11,878 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/04/02",
      "title": "From Easy to Hard: Building a Shortcut for Differentially Private Image Synthesis",
      "repo_urls": [
        "https://github.com/sunnierlee/dp-feta"
      ],
      "tasks": [
        "Image Generation"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18084",
      "abstract": "Multi-task learning (MTL) can advance assistive driving by exploring inter-task correlations through shared representations. However, existing methods face two critical limitations: single-modality constraints limiting comprehensive scene understanding and inefficient architectures impeding real-time deployment. This paper proposes TEM^3-Learning (Time-Efficient Multimodal Multi-task Learning), a novel framework that jointly optimizes driver emotion recognition, driver behavior recognition, traffic context recognition, and vehicle behavior recognition through a two-stage architecture. The first component, the mamba-based multi-view temporal-spatial feature extraction subnetwork (MTS-Mamba), introduces a forward-backward temporal scanning mechanism and global-local spatial attention to efficiently extract low-cost temporal-spatial features from multi-view sequential images. The second component, the MTL-based gated multimodal feature integrator (MGMI), employs task-specific multi-gating modules to adaptively highlight the most relevant modality features for each task, effectively alleviating the negative transfer problem in MTL. Evaluation on the AIDE dataset, our proposed model achieves state-of-the-art accuracy across all four tasks, maintaining a lightweight architecture with fewer than 6 million parameters and delivering an impressive 142.32 FPS inference speed. Rigorous ablation studies further validate the effectiveness of the proposed framework and the independent contributions of each module. The code is available on https://github.com/Wenzhuo-Liu/TEM3-Learning.",
      "authors": [
        "Liu, Wenzhuo",
        "Qiao, Yicheng",
        "Wang, Zhen",
        "Guo, Qiannan",
        "Chen, Zilong",
        "Zhou, Meihua",
        "Li, Xinran",
        "Wang, Letian",
        "Li, Zhiwei",
        "Liu, Huaping",
        "Wang, Wenshuo"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18084v1",
        "Other Formats": "https://arxiv.org/format/2506.18084",
        "TeX Source": "https://arxiv.org/src/2506.18084",
        "View PDF": "https://arxiv.org/pdf/2506.18084"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 16:12:27 UTC (418 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "TEM^3-Learning: Time-Efficient Multimodal Multi-Task Learning for Advanced Assistive Driving",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2308.14048",
      "abstract": "We propose a novel generative model within the Bayesian non-parametric learning (BNPL) framework to address some notable failure modes in generative adversarial networks (GANs) and variational autoencoders (VAEs)--these being overfitting in the GAN case and noisy samples in the VAE case. We will demonstrate that the BNPL framework enhances training stability and provides robustness and accuracy guarantees when incorporating the Wasserstein distance and maximum mean discrepancy measure (WMMD) into our model's loss function. Moreover, we introduce a so-called ``triple model'' that combines the GAN, the VAE, and further incorporates a code-GAN (CGAN) to explore the latent space of the VAE. This triple model design generates high-quality, diverse samples, while the BNPL framework, leveraging the WMMD loss function, enhances training stability. Together, these components enable our model to achieve superior performance across various generative tasks. These claims are supported by both theoretical analyses and empirical validation on a wide variety of datasets.",
      "authors": [
        "Fazeli-Asl, Forough",
        "Zhang, Michael Minyi"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2308.14048v2",
        "Other Formats": "https://arxiv.org/format/2308.14048",
        "TeX Source": "https://arxiv.org/src/2308.14048",
        "View PDF": "https://arxiv.org/pdf/2308.14048"
      },
      "subjects": [
        "Machine Learning (stat.ML)",
        "Machine Learning (cs.LG)",
        "Applications (stat.AP)",
        "Computation (stat.CO)",
        "Methodology (stat.ME)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 27 Aug 2023 08:58:31 UTC (4,053 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 04:10:53 UTC (5,166 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2023/08/27",
      "title": "A Bayesian Non-parametric Approach to Generative Models: Integrating Variational Autoencoder and Generative Adversarial Networks using Wasserstein and Maximum Mean Discrepancy",
      "tasks": [
        "Anomaly Detection",
        "Data Augmentation"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2409.09111",
      "abstract": "Learning representations for structured data with certain geometries (e.g., observed or unobserved) is a fundamental challenge, wherein message passing neural networks (MPNNs) have become a de facto class of model solutions. In this paper, we propose an energy-constrained diffusion model as a principled mathematical framework for understanding the mechanism of MPNNs and navigating novel architectural designs. Inspired by physical systems, the model combines the inductive bias of diffusion on manifolds with layer-wise constraints of energy minimization. We identify that the diffusion operators have a one-to-one correspondence with the energy functions implicitly descended by the diffusion process, and the finite-difference iteration for solving the energy-constrained diffusion system induces the propagation layers of various types of MPNNs operating on observed or latent structures. This leads to a unified perspective on common neural architectures whose computational flows can be cast as message passing (or its special case), including MLP, GCN, GIN, APPNP, GCNII, GAT, and Transformers. Building on these insights, we devise a new class of neural message passing models, dubbed diffusion-inspired Transformers, whose global attention layers are derived from the principled energy-constrained diffusion framework. Across diverse datasets, ranging from real-world networks to images, texts, and physical particles, we demonstrate that the new model achieves promising performance in scenarios where the data structures are observed (as a graph), partially observed, or entirely unobserved.",
      "authors": [
        "Wu, Qitian",
        "Wipf, David",
        "Yan, Junchi"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2409.09111v2",
        "Other Formats": "https://arxiv.org/format/2409.09111",
        "TeX Source": "https://arxiv.org/src/2409.09111",
        "View PDF": "https://arxiv.org/pdf/2409.09111"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 13 Sep 2024 17:54:41 UTC (7,117 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 18:46:40 UTC (2,985 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/09/13",
      "title": "Bridging Geometric Diffusion and Energy Minimization: A Unified Framework for Neural Message Passing",
      "tasks": [
        "Inductive Bias"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17879",
      "abstract": "The color appearance of a pathological image is highly related to the imaging protocols, the proportion of different dyes, and the scanning devices. Computer-aided diagnostic systems may deteriorate when facing these color-variant pathological images. In this work, we propose a stain normalization method called StainPIDR. We try to eliminate this color discrepancy by decoupling the image into structure features and vector-quantized color features, restaining the structure features with the target color features, and decoding the stained structure features to normalized pathological images. We assume that color features decoupled by different images with the same color should be exactly the same. Under this assumption, we train a fixed color vector codebook to which the decoupled color features will map. In the restaining part, we utilize the cross-attention mechanism to efficiently stain the structure features. As the target color (decoupled from a selected template image) will also affect the performance of stain normalization, we further design a template image selection algorithm to select a template from a given dataset. In our extensive experiments, we validate the effectiveness of StainPIDR and the template image selection algorithm. All the results show that our method can perform well in the stain normalization task. The code of StainPIDR will be publicly available later.",
      "authors": [
        "Chen, Zheng"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17879v1",
        "Other Formats": "https://arxiv.org/format/2506.17879",
        "TeX Source": "https://arxiv.org/src/2506.17879",
        "View PDF": "https://arxiv.org/pdf/2506.17879"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 02:54:20 UTC (749 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "StainPIDR: A Pathological Image Decouplingand Reconstruction Method for StainNormalization Based on Color VectorQuantization and Structure Restaining",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17940",
      "abstract": "Progress of AI has led to a creation of very successful, but by no means humble models and tools, especially regarding (i) the huge and further exploding costs and resources they demand, and (ii) the over-confidence of these tools with the answers they provide. Here we introduce a novel mathematical framework for a non-equilibrium entropy-optimizing reformulation of Boltzmann machines based on the exact law of total probability. It results in the highly-performant, but much cheaper, gradient-descent-free learning framework with mathematically-justified existence and uniqueness criteria, and answer confidence/reliability measures. Comparisons to state-of-the-art AI tools in terms of performance, cost and the model descriptor lengths on a set of synthetic problems with varying complexity reveal that the proposed method results in more performant and slim models, with the descriptor lengths being very close to the intrinsic complexity scaling bounds for the underlying problems. Applying this framework to historical climate data results in models with systematically higher prediction skills for the onsets of La Ni\\~na and El Ni\\~no climate phenomena, requiring just few years of climate data for training - a small fraction of what is necessary for contemporary climate prediction tools.",
      "authors": [
        "Bassetti, Davide",
        "Posp\u00ed\u0161il, Luk\u00e1\u0161",
        "Groom, Michael",
        "O'Kane, Terence J.",
        "Horenko, Illia"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17940v1",
        "Other Formats": "https://arxiv.org/format/2506.17940",
        "TeX Source": "https://arxiv.org/src/2506.17940",
        "View PDF": "https://arxiv.org/pdf/2506.17940"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (stat.ML)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 08:13:22 UTC (1,304 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "An entropy-optimal path to humble AI",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18023",
      "abstract": "This report introduces PP-DocBee2, an advanced version of the PP-DocBee, designed to enhance multimodal document understanding. Built on a large multimodal model architecture, PP-DocBee2 addresses the limitations of its predecessor through key technological improvements, including enhanced synthetic data quality, improved visual feature fusion strategy, and optimized inference methodologies. These enhancements yield an $11.4\\%$ performance boost on internal benchmarks for Chinese business documents, and reduce inference latency by $73.0\\%$ to the vanilla version. A key innovation of our work is a data quality optimization strategy for multimodal document tasks. By employing a large-scale multimodal pre-trained model to evaluate data, we apply a novel statistical criterion to filter outliers, ensuring high-quality training data. Inspired by insights into underutilized intermediate features in multimodal models, we enhance the ViT representational capacity by decomposing it into layers and applying a novel feature fusion strategy to improve complex reasoning. The source code and pre-trained model are available at \\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.",
      "authors": [
        "Huang, Kui",
        "Chen, Xinrong",
        "Lv, Wenyu",
        "Liao, Jincheng",
        "Wang, Guanzhong",
        "Liu, Yi"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18023v1",
        "Other Formats": "https://arxiv.org/format/2506.18023",
        "TeX Source": "https://arxiv.org/src/2506.18023",
        "View PDF": "https://arxiv.org/pdf/2506.18023"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 13:06:13 UTC (133 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18172",
      "abstract": "Thyroid cancer is among the most common cancers in the United States. Thyroid nodules are frequently detected through ultrasound (US) imaging, and some require further evaluation via fine-needle aspiration (FNA) biopsy. Despite its effectiveness, FNA often leads to unnecessary biopsies of benign nodules, causing patient discomfort and anxiety. To address this, the American College of Radiology Thyroid Imaging Reporting and Data System (TI-RADS) has been developed to reduce benign biopsies. However, such systems are limited by interobserver variability. Recent deep learning approaches have sought to improve risk stratification, but they often fail to utilize the rich temporal and spatial context provided by US cine clips, which contain dynamic global information and surrounding structural changes across various views. In this work, we propose the Spatio-Temporal Cross Attention for Cine Thyroid Ultrasound Time Series Classification (STACT-Time) model, a novel representation learning framework that integrates imaging features from US cine clips with features from segmentation masks automatically generated by a pretrained model. By leveraging self-attention and cross-attention mechanisms, our model captures the rich temporal and spatial context of US cine clips while enhancing feature representation through segmentation-guided learning. Our model improves malignancy prediction compared to state-of-the-art models, achieving a cross-validation precision of 0.91 (plus or minus 0.02) and an F1 score of 0.89 (plus or minus 0.02). By reducing unnecessary biopsies of benign nodules while maintaining high sensitivity for malignancy detection, our model has the potential to enhance clinical decision-making and improve patient outcomes.",
      "authors": [
        "Adam, Irsyad",
        "Zhang, Tengyue",
        "Raman, Shrayes",
        "Qiu, Zhuyu",
        "Taraku, Brandon",
        "Feng, Hexiang",
        "Wang, Sile",
        "Radhachandran, Ashwath",
        "Athreya, Shreeram",
        "Ivezic, Vedrana",
        "Ping, Peipei",
        "Arnold, Corey",
        "Speier, William"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18172v1",
        "Other Formats": "https://arxiv.org/format/2506.18172",
        "TeX Source": "https://arxiv.org/src/2506.18172",
        "View PDF": "https://arxiv.org/pdf/2506.18172"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 21:14:04 UTC (833 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "STACT-Time: Spatio-Temporal Cross Attention for Cine Thyroid Ultrasound Time Series Classification",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2311.10873",
      "abstract": "The area of temporally fine-grained video representation learning focuses on generating frame-by-frame representations for temporally dense tasks, such as fine-grained action phase classification and frame retrieval. In this work, we advance the state-of-the-art for self-supervised models in this area by re-examining the design of transformer architectures for video representation learning. A key aspect of our approach is the improved sharing of scene information in the temporal pipeline by representing multiple salient entities per frame. Prior works use late-fusion architectures that reduce frames to a single-dimensional vector before modeling any cross-frame dynamics. In contrast, our Multi-entity Video Transformer (MV-Former) processes the frames as groups of entities represented as tokens linked across time. To achieve this, we propose a Learnable Spatial Token Pooling strategy to identify and extract features for multiple salient regions per frame. Through our experiments, we show that MV-Former outperforms previous self-supervised methods, and also surpasses some prior works that use additional supervision or training data. When combined with additional pre-training data from Kinetics-400, MV-Former achieves a further performance boost. Overall, our MV-Former achieves state-of-the-art results on multiple fine-grained video benchmarks and shows that parsing video scenes as collections of entities can enhance performance in video tasks.",
      "authors": [
        "Walmer, Matthew",
        "Kanjirathinkal, Rose",
        "Tai, Kai Sheng",
        "Muzumdar, Keyur",
        "Tian, Taipeng",
        "Shrivastava, Abhinav"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2311.10873v2",
        "Other Formats": "https://arxiv.org/format/2311.10873",
        "TeX Source": "https://arxiv.org/src/2311.10873",
        "View PDF": "https://arxiv.org/pdf/2311.10873"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 17 Nov 2023 21:23:12 UTC (903 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 18:20:08 UTC (371 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2023/11/17",
      "title": "Multi-entity Video Transformers for Fine-Grained Video Representation Learning",
      "repo_urls": [
        "https://github.com/facebookresearch/video_rep_learning"
      ],
      "tasks": [
        "Representation Learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2503.18177",
      "abstract": "The increasing number of autonomous vehicles and the rapid development of computer vision technologies underscore the particular importance of conducting research on the accuracy of traffic sign recognition. Numerous studies in this field have already achieved significant results, demonstrating high effectiveness in addressing traffic sign recognition tasks. However, the task becomes considerably more complex when a sign is partially obscured by surrounding objects, such as tree branches, billboards, or other elements of the urban environment. In our study, we investigated how partial occlusion of traffic signs affects their recognition. For this purpose, we collected a dataset comprising 5,746 images, including both fully visible and partially occluded signs, and made it publicly available. Using this dataset, we compared the performance of our custom convolutional neural network (CNN), which achieved 96% accuracy, with models trained using transfer learning. The best result was obtained by VGG16 with full layer unfreezing, reaching 99% accuracy. Additional experiments revealed that models trained solely on fully visible signs lose effectiveness when recognizing occluded signs. This highlights the critical importance of incorporating real-world data with partial occlusion into training sets to ensure robust model performance in complex practical scenarios and to enhance the safety of autonomous driving.",
      "authors": [
        "Gimaletdinova, Gulnaz",
        "Shaiakhmetov, Dim",
        "Akpaeva, Madina",
        "Abduzhabbarov, Mukhammadmuso",
        "Momunov, Kadyrmamat"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2503.18177v2",
        "Other Formats": "https://arxiv.org/format/2503.18177",
        "TeX Source": "https://arxiv.org/src/2503.18177",
        "View PDF": "https://arxiv.org/pdf/2503.18177"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 23 Mar 2025 19:25:56 UTC (3,276 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 09:19:11 UTC (3,276 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/03/23",
      "title": "Training A Neural Network For Partially Occluded Road Sign Identification In The Context Of Autonomous Vehicles",
      "tasks": [
        "Autonomous Driving",
        "Autonomous Vehicles",
        "Traffic Sign Recognition",
        "Transfer Learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2405.13094",
      "abstract": "The spread of rumors on social media, particularly during significant events like the US elections and the COVID-19 pandemic, poses a serious threat to social stability and public health. Current rumor detection methods primarily rely on propagation graphs to improve the model performance. However, the effectiveness of these methods is often compromised by noisy and irrelevant structures in the propagation process. To tackle this issue, techniques such as weight adjustment and data augmentation have been proposed. However, they depend heavily on rich original propagation structures, limiting their effectiveness in handling rumors that lack sufficient propagation information, especially in the early stages of dissemination. In this work, we introduce the Key Propagation Graph Generator (KPG), a novel reinforcement learning-based framework, that generates contextually coherent and informative propagation patterns for events with insufficient topology information and identifies significant substructures in events with redundant and noisy propagation structures. KPG comprises two key components: the Candidate Response Generator (CRG) and the Ending Node Selector (ENS). CRG learns latent variable distributions from refined propagation patterns to eliminate noise and generate new candidates for ENS, while ENS identifies the most influential substructures in propagation graphs and provides training data for CRG. Furthermore, we develop an end-to-end framework that utilizes rewards derived from a pre-trained graph neural network to guide the training process. The resulting key propagation graphs are then employed in downstream rumor detection tasks. Extensive experiments conducted on four datasets demonstrate that KPG outperforms current state-of-the-art methods.",
      "authors": [
        "Zhang, Yusong",
        "Xie, Kun",
        "Zhang, Xingyi",
        "Dong, Xiangyu",
        "Wang, Sibo"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2405.13094v2",
        "Other Formats": "https://arxiv.org/format/2405.13094",
        "TeX Source": "https://arxiv.org/src/2405.13094",
        "View PDF": "https://arxiv.org/pdf/2405.13094"
      },
      "subjects": [
        "Social and Information Networks (cs.SI)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Tue, 21 May 2024 13:13:43 UTC (221 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 14:47:11 UTC (297 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/05/21",
      "title": "Rumor Detection on Social Media with Reinforcement Learning-based Key Propagation Graph Generator",
      "tasks": [
        "Data Augmentation",
        "Graph Neural Network"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17951",
      "abstract": "Recent advancements in retrieval-augmented generation (RAG) have enhanced large language models in question answering by integrating external knowledge. However, challenges persist in achieving global understanding and aligning responses with human ethical and quality preferences. To address these issues, we propose GraphMPA, a comprehensive graph-based framework with mode-seeking preference alignment. Our approach constructs a hierarchical document graph using a general similarity measurement, mimicking human cognitive processes for information understanding and synthesis. Additionally, we introduce mode-seeking preference optimization to better align model outputs with human preferences through probability-matching constraints. Extensive experiments on six datasets demonstrate the effectiveness of our \\href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.",
      "authors": [
        "Tang, Quanwei",
        "Lee, Sophia Yat Mei",
        "Wu, Junshuang",
        "Zhang, Dong",
        "Li, Shoushan",
        "Cambria, Erik",
        "Zhou, Guodong"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17951",
        "TeX Source": "https://arxiv.org/src/2506.17951",
        "View PDF": "https://arxiv.org/pdf/2506.17951"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 09:08:44 UTC (3,461 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2501.15100",
      "abstract": "The rapid development of programmable network devices and the widespread use of machine learning (ML) in networking have facilitated efficient research into intelligent data plane (IDP). Offloading ML to programmable data plane (PDP) enables quick analysis and responses to network traffic dynamics, and efficient management of network links. However, PDP hardware pipeline has significant resource limitations. For instance, Intel Tofino ASIC has only 10Mb SRAM in each stage, and lacks support for multiplication, division and floating-point operations. These constraints significantly hinder the development of IDP. This paper presents \\quark, a framework that fully offloads convolutional neural network (CNN) inference onto PDP. \\quark employs model pruning to simplify the CNN model, and uses quantization to support floating-point operations. Additionally, \\quark divides the CNN into smaller units to improve resource utilization on the PDP. We have implemented a testbed prototype of \\quark on both P4 hardware switch (Intel Tofino ASIC) and software switch (i.e., BMv2). Extensive evaluation results demonstrate that \\quark achieves 97.3\\% accuracy in anomaly detection task while using only 22.7\\% of the SRAM resources on the Intel Tofino ASIC switch, completing inference tasks at line rate with an average latency of 42.66$\\mu s$.",
      "authors": [
        "Zhang, Mai",
        "Cui, Lin",
        "Zhang, Xiaoquan",
        "Tso, Fung Po",
        "Zhang, Zhen",
        "Deng, Yuhui",
        "Li, Zhetao"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2501.15100v2",
        "Other Formats": "https://arxiv.org/format/2501.15100",
        "TeX Source": "https://arxiv.org/src/2501.15100",
        "View PDF": "https://arxiv.org/pdf/2501.15100"
      },
      "subjects": [
        "Networking and Internet Architecture (cs.NI)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 25 Jan 2025 06:37:36 UTC (794 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 10:33:02 UTC (720 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/01/25",
      "title": "Quark: Implementing Convolutional Neural Networks Entirely on Programmable Data Plane",
      "repo_urls": [
        "https://github.com/antlab-repo/quark-cnn-p4"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18024",
      "abstract": "Industrial Cyber-Physical Systems (ICPS) technologies are foundational in driving maritime autonomy, particularly for Unmanned Surface Vehicles (USVs). However, onboard computational constraints and communication latency significantly restrict real-time data processing, analysis, and predictive modeling, hence limiting the scalability and responsiveness of maritime ICPS. To overcome these challenges, we propose a distributed Cloud-Edge-IoT architecture tailored for maritime ICPS by leveraging design principles from the recently proposed Cloud-Fog Automation paradigm. Our proposed architecture comprises three hierarchical layers: a Cloud Layer for centralized and decentralized data aggregation, advanced analytics, and future model refinement; an Edge Layer that executes localized AI-driven processing and decision-making; and an IoT Layer responsible for low-latency sensor data acquisition. Our experimental results demonstrated improvements in computational efficiency, responsiveness, and scalability. When compared with our conventional approaches, we achieved a classification accuracy of 86\\%, with an improved latency performance. By adopting Cloud-Fog Automation, we address the low-latency processing constraints and scalability challenges in maritime ICPS applications. Our work offers a practical, modular, and scalable framework to advance robust autonomy and AI-driven decision-making and autonomy for intelligent USVs in future maritime ICPS.",
      "authors": [
        "Tran, Thien",
        "Nguyen, Quang",
        "Kua, Jonathan",
        "Tran, Minh",
        "Luu, Toan",
        "Hoang, Thuong",
        "Jin, Jiong"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18024v1",
        "Other Formats": "https://arxiv.org/format/2506.18024",
        "TeX Source": "https://arxiv.org/src/2506.18024",
        "View PDF": "https://arxiv.org/pdf/2506.18024"
      },
      "subjects": [
        "Distributed, Parallel, and Cluster Computing (cs.DC)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 13:06:50 UTC (1,691 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Leveraging Cloud-Fog Automation for Autonomous Collision Detection and Classification in Intelligent Unmanned Surface Vehicles",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.06659",
      "abstract": "In complex driving environments, autonomous vehicles must navigate safely. Relying on a single predicted path, as in regression-based approaches, usually does not explicitly assess the safety of the predicted trajectory. Selection-based methods address this by generating and scoring multiple trajectory candidates and predicting the safety score for each, but face optimization challenges in precisely selecting the best option from thousands of possibilities and distinguishing subtle but safety-critical differences, especially in rare or underrepresented scenarios. We propose DriveSuprim to overcome these challenges and advance the selection-based paradigm through a coarse-to-fine paradigm for progressive candidate filtering, a rotation-based augmentation method to improve robustness in out-of-distribution scenarios, and a self-distillation framework to stabilize training. DriveSuprim achieves state-of-the-art performance, reaching 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS in NAVSIM v2 without extra data, demonstrating superior safetycritical capabilities, including collision avoidance and compliance with rules, while maintaining high trajectory quality in various driving scenarios.",
      "authors": [
        "Yao, Wenhao",
        "Li, Zhenxin",
        "Lan, Shiyi",
        "Wang, Zi",
        "Sun, Xinglong",
        "Alvarez, Jose M.",
        "Wu, Zuxuan"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.06659v2",
        "Other Formats": "https://arxiv.org/format/2506.06659",
        "TeX Source": "https://arxiv.org/src/2506.06659",
        "View PDF": "https://arxiv.org/pdf/2506.06659"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 7 Jun 2025 04:39:06 UTC (13,077 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 03:57:46 UTC (13,069 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/06/07",
      "title": "DriveSuprim: Towards Precise Trajectory Selection for End-to-End Planning",
      "tasks": [
        "Autonomous Vehicles",
        "Collision Avoidance",
        "Navigate",
        "NavSim"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2504.11170",
      "abstract": "The growing demand for robots to operate effectively in diverse environments necessitates the need for robust real-time anomaly detection techniques during robotic operations. However, deep learning-based models in robotics face significant challenges due to limited training data and highly noisy signal features. In this paper, we present Sparse Masked Autoregressive Flow-based Adversarial AutoEncoder model to address these problems. This approach integrates Masked Autoregressive Flow model into Adversarial AutoEncoders to construct a flexible latent space and utilize Sparse autoencoder to efficiently focus on important features, even in scenarios with limited feature space. Our experiments demonstrate that the proposed model achieves a 4.96% to 9.75% higher area under the receiver operating characteristic curve for pick-and-place robotic operations with randomly placed cans, compared to existing state-of-the-art methods. Notably, it showed up to 19.67% better performance in scenarios involving collisions with lightweight objects. Additionally, unlike the existing state-of-the-art model, our model performs inferences within 1 millisecond, ensuring real-time anomaly detection. These capabilities make our model highly applicable to machine learning-based robotic safety systems in dynamic environments. The code is available at https://github.com/twkang43/sparse-maf-aae.",
      "authors": [
        "Kang, Taewook",
        "You, Bum-Jae",
        "Park, Juyoun",
        "Lee, Yisoo"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2504.11170v3",
        "Other Formats": "https://arxiv.org/format/2504.11170",
        "TeX Source": "https://arxiv.org/src/2504.11170",
        "View PDF": "https://arxiv.org/pdf/2504.11170"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Tue, 15 Apr 2025 13:17:14 UTC (1,310 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Wed, 16 Apr 2025 08:50:55 UTC (1,310 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Sun, 22 Jun 2025 06:46:58 UTC (1,193 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2025/04/15",
      "title": "A real-time anomaly detection method for robots based on a flexible and sparse latent space",
      "tasks": [
        "Anomaly Detection"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18046",
      "abstract": "Time series anomaly detection (TSAD) plays an important role in many domains such as finance, transportation, and healthcare. With the ongoing instrumentation of reality, more time series data will be available, leading also to growing demands for TSAD. While many TSAD methods already exist, new and better methods are still desirable. However, effective progress hinges on the availability of reliable means of evaluating new methods and comparing them with existing methods. We address deficiencies in current evaluation procedures related to datasets and experimental settings and protocols. Specifically, we propose a new time series anomaly detection benchmark, called TAB. First, TAB encompasses 29 public multivariate datasets and 1,635 univariate time series from different domains to facilitate more comprehensive evaluations on diverse datasets. Second, TAB covers a variety of TSAD methods, including Non-learning, Machine learning, Deep learning, LLM-based, and Time-series pre-trained methods. Third, TAB features a unified and automated evaluation pipeline that enables fair and easy evaluation of TSAD methods. Finally, we employ TAB to evaluate existing TSAD methods and report on the outcomes, thereby offering a deeper insight into the performance of these methods. Besides, all datasets and code are available at https://github.com/decisionintelligence/TAB.",
      "authors": [
        "Qiu, Xiangfei",
        "Li, Zhe",
        "Qiu, Wanghui",
        "Hu, Shiyan",
        "Zhou, Lekui",
        "Wu, Xingjian",
        "Li, Zhengyu",
        "Guo, Chenjuan",
        "Zhou, Aoying",
        "Sheng, Zhenli",
        "Hu, Jilin",
        "Jensen, Christian S.",
        "Yang, Bin"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18046v1",
        "Other Formats": "https://arxiv.org/format/2506.18046",
        "TeX Source": "https://arxiv.org/src/2506.18046",
        "View PDF": "https://arxiv.org/pdf/2506.18046"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 14:19:36 UTC (3,333 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "TAB: Unified Benchmarking of Time Series Anomaly Detection Methods",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.12364",
      "abstract": "Multimodal document retrieval systems enable information access across text, images, and layouts, benefiting various domains like document-based question answering, report analysis, and interactive content summarization. Rerankers improve retrieval precision by reordering retrieved candidates. However, current multimodal reranking methods remain underexplored, with significant room for improvement in both training strategies and overall effectiveness. Moreover, the lack of explicit reasoning makes it difficult to analyze and optimize these methods further. In this paper, We propose MM-R5, a MultiModal Reasoning-Enhanced ReRanker via Reinforcement Learning for Document Retrieval, aiming to provide a more effective and reliable solution for multimodal reranking tasks. MM-R5 is trained in two stages: supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we focus on improving instruction-following and guiding the model to generate complete and high-quality reasoning chains. To support this, we introduce a novel data construction strategy that produces rich, high-quality reasoning data. In the RL stage, we design a task-specific reward framework, including a reranking reward tailored for multimodal candidates and a composite template-based reward to further refine reasoning quality. We conduct extensive experiments on MMDocIR, a challenging public benchmark spanning multiple domains. MM-R5 achieves state-of-the-art performance on most metrics and delivers comparable results to much larger models on the remaining ones. Moreover, compared to the best retrieval-only method, MM-R5 improves recall@1 by over 4%. These results validate the effectiveness of our reasoning-enhanced training pipeline. Our code is available at https://github.com/i2vec/MM-R5 .",
      "authors": [
        "Xu, Mingjun",
        "Dong, Jinhan",
        "Hou, Jue",
        "Wang, Zehui",
        "Li, Sihang",
        "Gao, Zhifeng",
        "Zhong, Renxin",
        "Cai, Hengxing"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.12364v2",
        "Other Formats": "https://arxiv.org/format/2506.12364",
        "TeX Source": "https://arxiv.org/src/2506.12364",
        "View PDF": "https://arxiv.org/pdf/2506.12364"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 14 Jun 2025 05:55:00 UTC (340 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 13:42:54 UTC (340 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/06/14",
      "title": "MM-R5: MultiModal Reasoning-Enhanced ReRanker via Reinforcement Learning for Document Retrieval",
      "models": [
        {
          "model_path": "i2vec/MM-R5",
          "downloads": "8",
          "likes": "1",
          "trending_score": "1.0",
          "link": "https://huggingface.co/i2vec/MM-R5"
        }
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18193",
      "abstract": "This paper introduces Decoupled Supervised Learning with Information Regularization (DeInfoReg), a novel approach that transforms a long gradient flow into multiple shorter ones, thereby mitigating the vanishing gradient problem. Integrating a pipeline strategy, DeInfoReg enables model parallelization across multiple GPUs, significantly improving training throughput. We compare our proposed method with standard backpropagation and other gradient flow decomposition techniques. Extensive experiments on diverse tasks and datasets demonstrate that DeInfoReg achieves superior performance and better noise resistance than traditional BP models and efficiently utilizes parallel computing resources. The code for reproducibility is available at: https://github.com/ianzih/Decoupled-Supervised-Learning-for-Information-Regularization/.",
      "authors": [
        "Huang, Zih-Hao",
        "Lin, You-Teng",
        "Chen, Hung-Hsuan"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18193v1",
        "Other Formats": "https://arxiv.org/format/2506.18193",
        "TeX Source": "https://arxiv.org/src/2506.18193",
        "View PDF": "https://arxiv.org/pdf/2506.18193"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Distributed, Parallel, and Cluster Computing (cs.DC)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 22:50:06 UTC (1,022 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "DeInfoReg: A Decoupled Learning Framework for Better Training Throughput",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17944",
      "abstract": "Remote sensing change detection is widely used in a variety of fields such as urban planning, terrain and geomorphology analysis, and environmental monitoring, mainly by analyzing the significant change differences of features (e.g., building changes) in the same spatial region at different time phases. In this paper, we propose a large language model (LLM) augmented inference approach (SegChange-R1), which enhances the detection capability by integrating textual descriptive information and aims at guiding the model to segment the more interested change regions, thus accelerating the convergence speed. Moreover, we design a spatial transformation module (BEV) based on linear attention, which solves the problem of modal misalignment in change detection by unifying features from different temporal perspectives onto the BEV space. In addition, we construct the first dataset for building change detection from UAV viewpoints (DVCD ), and our experiments on four widely-used change detection datasets show a significant improvement over existing methods. The code and pre-trained models are available in https://github.com/Yu-Zhouz/SegChange-R1.",
      "authors": [
        "Zhou, Fei"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17944v1",
        "Other Formats": "https://arxiv.org/format/2506.17944",
        "TeX Source": "https://arxiv.org/src/2506.17944",
        "View PDF": "https://arxiv.org/pdf/2506.17944"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 08:40:56 UTC (453 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "SegChange-R1:Augmented Reasoning for Remote Sensing Change Detection via Large Language Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17919",
      "abstract": "Reinforcement learning (RL) for auto-bidding has shifted from using simplistic offline simulators (Simulation-based RL Bidding, SRLB) to offline RL on fixed real datasets (Offline RL Bidding, ORLB). However, ORLB policies are limited by the dataset's state space coverage, offering modest gains. While SRLB expands state coverage, its simulator-reality gap risks misleading policies. This paper introduces Model-based RL Bidding (MRLB), which learns an environment model from real data to bridge this gap. MRLB trains policies using both real and model-generated data, expanding state coverage beyond ORLB. To ensure model reliability, we propose: 1) A permutation equivariant model architecture for better generalization, and 2) A robust offline Q-learning method that pessimistically penalizes model errors. These form the Permutation Equivariant Model-based Offline RL (PE-MORL) algorithm. Real-world experiments show that PE-MORL outperforms state-of-the-art auto-bidding methods.",
      "authors": [
        "Mou, Zhiyu",
        "Xu, Miao",
        "Chen, Wei",
        "Bai, Rongquan",
        "Yu, Chuan",
        "Xu, Jian"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17919v1",
        "Other Formats": "https://arxiv.org/format/2506.17919",
        "TeX Source": "https://arxiv.org/src/2506.17919",
        "View PDF": "https://arxiv.org/pdf/2506.17919"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 06:58:36 UTC (5,525 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Permutation Equivariant Model-based Offline Reinforcement Learning for Auto-bidding",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17946",
      "abstract": "This research paper proposes an improved deep learning model for classifying tents in street bazaars, comparing a custom Convolutional Neural Network (CNN) with EfficientNetB0. This is a critical task for market organization with a tent classification, but manual methods in the past have been inefficient. Street bazaars represent a vital economic hub in many regions, yet their unstructured nature poses significant challenges for the automated classification of market infrastructure, such as tents. In Kyrgyzstan, more than a quarter of the country's GDP is derived from bazaars. While CNNs have been widely applied to object recognition, their application to bazaar-specific tasks remains underexplored. Here, we build upon our original approach by training on an extended set of 126 original photographs that were augmented to generate additional images. This dataset is publicly available for download on Kaggle. A variety of performance metrics, such as accuracy, precision, recall, F1 score, and mean average precision (mAP), were used to assess the models comparatively, providing a more extensive analysis of classification performance. The results show that the CNN custom model achieved 92.8% accuracy, and EfficientNetB0 showed 98.4% accuracy results, confirming the effectiveness of transfer learning in the bazaar image classification. Also, when analyzing the confusion matrix, the analysis reveals the weaknesses and strengths of each model. These findings suggest that using a pre-trained model such as EfficientNetB0 significantly improves classification accuracy and generalization.",
      "authors": [
        "Ibragimov, Azamat",
        "Isaev, Ruslan",
        "Mekuria, Remudin Reshid",
        "Gimaletdinova, Gulnaz",
        "Shaiakhmetov, Dim"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17946v1",
        "Other Formats": "https://arxiv.org/format/2506.17946",
        "TeX Source": "https://arxiv.org/src/2506.17946",
        "View PDF": "https://arxiv.org/pdf/2506.17946"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 08:52:28 UTC (3,213 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Classification of Tents in Street Bazaars Using CNN",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18072",
      "abstract": "Medical image analysis increasingly relies on the integration of multiple imaging modalities to capture complementary anatomical and functional information, enabling more accurate diagnosis and treatment planning. Achieving aligned feature representations across these diverse modalities is therefore important for effective multimodal analysis. While contrastive language-image pre-training (CLIP) and its variant have enabled image-text alignments, they require explicitly paired data between arbitrary two modalities, which is difficult to acquire in medical contexts. To address the gap, we present Multimodal Medical Image Binding with Text (M\\textsuperscript{3}Bind), a novel pre-training framework that enables seamless alignment of multiple medical imaging modalities through a shared text representation space without requiring explicit paired data between any two medical image modalities. Specifically, based on the insight that different images can naturally bind with text, M\\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text models to align their modality-specific text embedding space while preserving their original image-text alignments. Subsequently, we distill these modality-specific text encoders into a unified model, creating a shared text embedding space. Experiments on X-ray, CT, retina, ECG, and pathological images on multiple downstream tasks demonstrate that M\\textsuperscript{3}Bind achieves state-of-the-art performance in zero-shot, few-shot classification and cross-modal retrieval tasks compared to its CLIP-like counterparts. These results validate M\\textsuperscript{3}Bind's effectiveness in achieving cross-image-modal alignment for medical analysis.",
      "authors": [
        "Liu, Yunhao",
        "Xi, Suyang",
        "Liu, Shiqi",
        "Ding, Hong",
        "Jin, Chicheng",
        "Yang, Chenxi",
        "He, Junjun",
        "Shen, Yiqing"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18072v1",
        "Other Formats": "https://arxiv.org/format/2506.18072",
        "TeX Source": "https://arxiv.org/src/2506.18072",
        "View PDF": "https://arxiv.org/pdf/2506.18072"
      },
      "subjects": [
        "Image and Video Processing (eess.IV)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 15:39:25 UTC (932 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Multimodal Medical Image Binding via Shared Text Embeddings",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18157",
      "abstract": "This work investigates the feasibility of a post-processing-based approach for phase separation in defocusing particle tracking velocimetry for dispersed two-phase flows. The method enables the simultaneous 3D localization determination of both tracer particles and particles of the dispersed phase, using a single-camera setup. The distinction between phases is based on pattern differences in defocused particle images, which arise from distinct light scattering behaviors of tracer particles and bubbles or droplets. Convolutional neural networks, including Faster R-CNN and YOLOv4 variants, are trained to detect and classify particle images based on these pattern features. To generate large, labeled training datasets, a generative adversarial network based framework is introduced, allowing the generation of auto-labeled data that more closely reflects experiment-specific visual appearance. Evaluation across six datasets, comprising synthetic two-phase and real single- and two-phase flows, demonstrates high detection precision and classification accuracy (95-100%), even under domain shifts. The results confirm the viability of using CNNs for robust phase separation in disperse two-phase DPTV, particularly in scenarios where traditional wavelength-, size-, or ensemble correlation-based methods are impractical.",
      "authors": [
        "Sax, Christian",
        "Kriegseis, Jochen"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18157v1",
        "Other Formats": "https://arxiv.org/format/2506.18157",
        "TeX Source": "https://arxiv.org/src/2506.18157",
        "View PDF": "https://arxiv.org/pdf/2506.18157"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Applied Physics (physics.app-ph)",
        "Fluid Dynamics (physics.flu-dyn)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 20:10:45 UTC (13,167 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Pattern-Based Phase-Separation of Tracer and Dispersed Phase Particles in Two-Phase Defocusing Particle Tracking Velocimetry",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.12447",
      "abstract": "This paper introduces a novel approach to person identification using hand images, designed specifically for criminal investigations. The method is particularly valuable in serious crimes such as sexual abuse, where hand images are often the only identifiable evidence available. Our proposed method, CLIP-HandID, leverages a pre-trained foundational vision-language model - CLIP - to efficiently learn discriminative deep feature representations from hand images (input to CLIP's image encoder) using textual prompts as semantic guidance. Since hand images are labeled with indexes rather than text descriptions, we employ a textual inversion network to learn pseudo-tokens that encode specific visual contexts or appearance attributes. These learned pseudo-tokens are then incorporated into textual prompts, which are fed into CLIP's text encoder to leverage its multi-modal reasoning and enhance generalization for identification. Through extensive evaluations on two large, publicly available hand datasets with multi-ethnic representation, we demonstrate that our method significantly outperforms existing approaches.",
      "authors": [
        "Baisa, Nathanael L.",
        "Pallam, Babu",
        "Jayavel, Amudhavel"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.12447v2",
        "Other Formats": "https://arxiv.org/format/2506.12447",
        "TeX Source": "https://arxiv.org/src/2506.12447",
        "View PDF": "https://arxiv.org/pdf/2506.12447"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 14 Jun 2025 10:59:00 UTC (633 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 21:19:24 UTC (633 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/06/14",
      "title": "CLIP-HandID: Vision-Language Model for Hand-Based Person Identification",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2403.06567",
      "abstract": "Content-based image retrieval (CBIR) has the potential to significantly improve diagnostic aid and medical research in radiology. However, current CBIR systems face limitations due to their specialization to certain pathologies, limiting their utility. On the other hand, several vision foundation models have been shown to produce general-purpose visual features. Therefore, in this work, we propose using vision foundation models as powerful and versatile off-the-shelf feature extractors for content-based image retrieval. Our contributions include: (1) benchmarking a diverse set of vision foundation models on an extensive dataset comprising 1.6 million 2D radiological images across four modalities and 161 pathologies; (2) identifying weakly-supervised models, particularly BiomedCLIP, as highly effective, achieving a achieving a P@1 of up to 0.594 (P@3: 0.590, P@5: 0.588, P@10: 0.583), comparable to specialized CBIR systems but without additional training; (3) conducting an in-depth analysis of the impact of index size on retrieval performance; (4) evaluating the quality of embedding spaces generated by different models; and (5) investigating specific challenges associated with retrieving anatomical versus pathological structures. Despite these challenges, our research underscores the vast potential of foundation models for CBIR in radiology, proposing a shift towards versatile, general-purpose medical image retrieval systems that do not require specific tuning. Our code, dataset splits and embeddings are publicly available under https://github.com/MIC-DKFZ/foundation-models-for-cbmir.",
      "authors": [
        "Denner, Stefan",
        "Zimmerer, David",
        "Bounias, Dimitrios",
        "Bujotzek, Markus",
        "Xiao, Shuhan",
        "Stock, Raphael",
        "Kausch, Lisa",
        "Schader, Philipp",
        "Penzkofer, Tobias",
        "J\u00c3\u00a4ger, Paul F.",
        "Maier-Hein, Klaus"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2403.06567v4",
        "Other Formats": "https://arxiv.org/format/2403.06567",
        "TeX Source": "https://arxiv.org/src/2403.06567",
        "View PDF": "https://arxiv.org/pdf/2403.06567"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Information Retrieval (cs.IR)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 11 Mar 2024 10:06:45 UTC (26,334 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Fri, 12 Apr 2024 08:52:24 UTC (26,334 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Wed, 17 Apr 2024 15:58:36 UTC (26,334 KB)",
          "link": "/",
          "version": "[v3]"
        },
        {
          "details": "Sun, 22 Jun 2025 12:33:24 UTC (23,470 KB)",
          "version": "[v4]"
        }
      ],
      "submitted_date": "2024/03/11",
      "title": "Leveraging Foundation Models for Content-Based Image Retrieval in Radiology",
      "repo_urls": [
        "https://github.com/mic-dkfz/foundation-models-for-cbmir"
      ],
      "tasks": [
        "Benchmarking",
        "Content-Based Image Retrieval",
        "Diagnostic",
        "Image Retrieval",
        "Medical Image Retrieval",
        "Retrieval"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2412.13722",
      "abstract": "The biophysical interactions between the T cell receptor (TCR) and its ligands determine the specificity of the cellular immune response. However, the immense diversity of receptors and ligands has made it challenging to discover generalizable rules across the distinct binding affinity landscapes created by different ligands. Here, we present an optimization framework for discovering biophysical rules that predict whether TCRs share specificity to a ligand. Applying this framework to TCRs associated with a collection of SARS-CoV-2 peptides we systematically characterize how co-specificity depends on the type and position of amino-acid differences between receptors. We also demonstrate that the inferred rules generalize to ligands highly dissimilar to any seen during training. Our analysis reveals that matching of steric properties between substituted amino acids is more important for receptor co-specificity than the hydrophobic properties that prominently determine evolutionary substitutability. Our analysis also quantifies the substantial importance of positions not in direct contact with the peptide for specificity. These findings highlight the potential for data-driven approaches to uncover the molecular mechanisms underpinning the specificity of adaptive immune responses.",
      "authors": [
        "Pyo, Andrew G. T.",
        "Nagano, Yuta",
        "Milighetti, Martina",
        "Henderson, James",
        "Callan Jr., Curtis G.",
        "Chain, Benny",
        "Wingreen, Ned S.",
        "Tiffeau-Mayer, Andreas"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2412.13722v3",
        "Other Formats": "https://arxiv.org/format/2412.13722",
        "TeX Source": "https://arxiv.org/src/2412.13722",
        "View PDF": "https://arxiv.org/pdf/2412.13722"
      },
      "subjects": [
        "Biomolecules (q-bio.BM)",
        "Machine Learning (cs.LG)",
        "Biological Physics (physics.bio-ph)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 18 Dec 2024 11:03:26 UTC (2,443 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Wed, 30 Apr 2025 13:59:34 UTC (2,635 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Sun, 22 Jun 2025 22:55:09 UTC (2,637 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/12/18",
      "title": "Data-driven Discovery of Biophysical T Cell Receptor Co-specificity Rules",
      "tasks": [
        "Diversity",
        "Specificity"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18078",
      "abstract": "We propose a novel nonparametric regression method that models complex input-output relationships as the sum of convex and concave components. The method-Identifiable Convex-Concave Nonparametric Least Squares (ICCNLS)-decomposes the target function into additive shape-constrained components, each represented via sub-gradient-constrained affine functions. To address the affine ambiguity inherent in convex-concave decompositions, we introduce global statistical orthogonality constraints, ensuring that residuals are uncorrelated with both intercept and input variables. This enforces decomposition identifiability and improves interpretability. We further incorporate L1, L2 and elastic net regularisation on sub-gradients to enhance generalisation and promote structural sparsity. The proposed method is evaluated on synthetic and real-world datasets, including healthcare pricing data, and demonstrates improved predictive accuracy and model simplicity compared to conventional CNLS and difference-of-convex (DC) regression approaches. Our results show that statistical identifiability, when paired with convex-concave structure and sub-gradient regularisation, yields interpretable models suited for forecasting, benchmarking, and policy evaluation.",
      "authors": [
        "Chung, William"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.18078",
        "View PDF": "https://arxiv.org/pdf/2506.18078"
      },
      "subjects": [
        "Machine Learning (stat.ML)",
        "Machine Learning (cs.LG)",
        "Statistics Theory (math.ST)",
        "Applications (stat.AP)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 15:53:12 UTC (1,605 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Identifiable Convex-Concave Regression via Sub-gradient Regularised Least Squares",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17873",
      "abstract": "Recent advances in Multimodal Large Language Models have demonstrated great potential in the medical domain, facilitating users to understand surgical scenes and procedures. Beyond image-based methods, the exploration of Video Large Language Models (Vid-LLMs) has emerged as a promising avenue for capturing the complex sequences of information involved in surgery. However, there is still a lack of Vid-LLMs specialized for fine-grained surgical video understanding tasks, which is crucial for analyzing specific processes or details within a surgical procedure. To bridge this gap, we propose SurgVidLM, the first video language model designed to address both full and fine-grained surgical video comprehension. To train our SurgVidLM, we construct the SVU-31K dataset which consists of over 31K video-instruction pairs, enabling both holistic understanding and detailed analysis of surgical procedures. Furthermore, we introduce the StageFocus mechanism which is a two-stage framework performing the multi-grained, progressive understanding of surgical videos. We also develop the Multi-frequency Fusion Attention to effectively integrate low and high-frequency visual tokens, ensuring the retention of critical information. Experimental results demonstrate that SurgVidLM significantly outperforms state-of-the-art Vid-LLMs in both full and fine-grained video understanding tasks, showcasing its superior capability in capturing complex procedural contexts.",
      "authors": [
        "Wang, Guankun",
        "Mo, Wenjin",
        "Wang, Junyi",
        "Bai, Long",
        "Yuan, Kun",
        "Hu, Ming",
        "Wu, Jinlin",
        "He, Junjun",
        "Huang, Yiming",
        "Padoy, Nicolas",
        "Lei, Zhen",
        "Liu, Hongbin",
        "Navab, Nassir",
        "Ren, Hongliang"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17873v1",
        "Other Formats": "https://arxiv.org/format/2506.17873",
        "TeX Source": "https://arxiv.org/src/2506.17873",
        "View PDF": "https://arxiv.org/pdf/2506.17873"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 02:16:18 UTC (2,706 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "SurgVidLM: Towards Multi-grained Surgical Video Understanding with Large Language Model",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17869",
      "abstract": "The integration of RGB and thermal data can significantly improve semantic segmentation performance in wild environments for field robots. Nevertheless, multi-source data processing (e.g. Transformer-based approaches) imposes significant computational overhead, presenting challenges for resource-constrained systems. To resolve this critical limitation, we introduced CM-SSM, an efficient RGB-thermal semantic segmentation architecture leveraging a cross-modal state space modeling (SSM) approach. Our framework comprises two key components. First, we introduced a cross-modal 2D-selective-scan (CM-SS2D) module to establish SSM between RGB and thermal modalities, which constructs cross-modal visual sequences and derives hidden state representations of one modality from the other. Second, we developed a cross-modal state space association (CM-SSA) module that effectively integrates global associations from CM-SS2D with local spatial features extracted through convolutional operations. In contrast with Transformer-based approaches, CM-SSM achieves linear computational complexity with respect to image resolution. Experimental results show that CM-SSM achieves state-of-the-art performance on the CART dataset with fewer parameters and lower computational cost. Further experiments on the PST900 dataset demonstrate its generalizability. Codes are available at https://github.com/xiaodonguo/CMSSM.",
      "authors": [
        "Guo, Xiaodong",
        "Lin, Zi'ang",
        "Hu, Luwen",
        "Deng, Zhihong",
        "Liu, Tong",
        "Zhou, Wujie"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17869v1",
        "Other Formats": "https://arxiv.org/format/2506.17869",
        "TeX Source": "https://arxiv.org/src/2506.17869",
        "View PDF": "https://arxiv.org/pdf/2506.17869"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 01:53:11 UTC (24,362 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Cross-modal State Space Modeling for Real-time RGB-thermal Wild Scene Semantic Segmentation",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2411.02385",
      "abstract": "OpenAI's Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. However, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned. A world model learning the true law should give predictions robust to nuances and correctly extrapolate on unseen scenarios. In this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial generalization. We developed a 2D simulation testbed for object movement and collisions to generate videos deterministically governed by one or more classical mechanics laws. This provides an unlimited supply of data for large-scale experimentation and enables quantitative evaluation of whether the generated videos adhere to physical laws. We trained diffusion-based video generation models to predict object movements based on initial frames. Our scaling experiments show perfect generalization within the distribution, measurable scaling behavior for combinatorial generalization, but failure in out-of-distribution scenarios. Further experiments reveal two key insights about the generalization mechanisms of these models: (1) the models fail to abstract general physical rules and instead exhibit \"case-based\" generalization behavior, i.e., mimicking the closest training example; (2) when generalizing to new cases, models are observed to prioritize different factors when referencing training data: color > size > velocity > shape. Our study suggests that scaling alone is insufficient for video generation models to uncover fundamental physical laws, despite its role in Sora's broader success. See our project page at https://phyworld.github.io",
      "authors": [
        "Kang, Bingyi",
        "Yue, Yang",
        "Lu, Rui",
        "Lin, Zhijie",
        "Zhao, Yang",
        "Wang, Kaixin",
        "Huang, Gao",
        "Feng, Jiashi"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2411.02385v2",
        "Other Formats": "https://arxiv.org/format/2411.02385",
        "TeX Source": "https://arxiv.org/src/2411.02385",
        "View PDF": "https://arxiv.org/pdf/2411.02385"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 4 Nov 2024 18:53:05 UTC (1,655 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 03:30:55 UTC (1,670 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/11/04",
      "title": "How Far is Video Generation from World Model: A Physical Law Perspective",
      "tasks": [
        "Video Generation"
      ],
      "datasets": [
        {
          "dataset_name": "magicr/phyworld",
          "downloads": "1070",
          "likes": "3",
          "link": "https://huggingface.co/datasets/magicr/phyworld"
        }
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18150",
      "abstract": "Fully Homomorphic Encryption (FHE) is an encryption scheme that not only encrypts data but also allows for computations to be applied directly on the encrypted data. While computationally expensive, FHE can enable privacy-preserving neural inference in the client-server setting: a client encrypts their input with FHE and sends it to an untrusted server. The server then runs neural inference on the encrypted data and returns the encrypted results. The client decrypts the output locally, keeping both the input and result private from the server. Private inference has focused on networks with dense inputs such as image classification, and less attention has been given to networks with sparse features. Unlike dense inputs, sparse features require efficient encrypted lookup operations into large embedding tables, which present computational and memory constraints for FHE. In this paper, we explore the challenges and opportunities when applying FHE to Deep Learning Recommendation Models (DLRM) from both a compiler and systems perspective. DLRMs utilize conventional MLPs for dense features and embedding tables to map sparse, categorical features to dense vector representations. We develop novel methods for performing compressed embedding lookups in order to reduce FHE computational costs while keeping the underlying model performant. Our embedding lookup improves upon a state-of-the-art approach by $77 \\times$. Furthermore, we present an efficient multi-embedding packing strategy that enables us to perform a 44 million parameter embedding lookup under FHE. Finally, we integrate our solutions into the open-source Orion framework and present HE-LRM, an end-to-end encrypted DLRM. We evaluate HE-LRM on UCI (health prediction) and Criteo (click prediction), demonstrating that with the right compression and packing strategies, encrypted inference for recommendation systems is practical.",
      "authors": [
        "Garimella, Karthik",
        "Ebel, Austin",
        "De Micheli, Gabrielle",
        "Reagen, Brandon"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18150v1",
        "Other Formats": "https://arxiv.org/format/2506.18150",
        "TeX Source": "https://arxiv.org/src/2506.18150",
        "View PDF": "https://arxiv.org/pdf/2506.18150"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 19:40:04 UTC (703 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "HE-LRM: Encrypted Deep Learning Recommendation Models using Fully Homomorphic Encryption",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18208",
      "abstract": "Neural Radiance Fields (NeRF) have revolutionized 3D scene reconstruction from sparse image collections. Recent work has explored integrating pre-trained vision features, particularly from DINO, to enhance few-shot reconstruction capabilities. However, the effectiveness of such approaches remains unclear, especially in extreme few-shot scenarios. In this paper, we present a systematic evaluation of DINO-enhanced NeRF models, comparing baseline NeRF, frozen DINO features, LoRA fine-tuned features, and multi-scale feature fusion. Surprisingly, our experiments reveal that all DINO variants perform worse than the baseline NeRF, achieving PSNR values around 12.9 to 13.0 compared to the baseline's 14.71. This counterintuitive result suggests that pre-trained vision features may not be beneficial for few-shot 3D reconstruction and may even introduce harmful biases. We analyze potential causes including feature-task mismatch, overfitting to limited data, and integration challenges. Our findings challenge common assumptions in the field and suggest that simpler architectures focusing on geometric consistency may be more effective for few-shot scenarios.",
      "authors": [
        "Sanjyal, Ankit"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18208v1",
        "Other Formats": "https://arxiv.org/format/2506.18208",
        "TeX Source": "https://arxiv.org/src/2506.18208",
        "View PDF": "https://arxiv.org/pdf/2506.18208"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 23:55:16 UTC (543 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Limitations of NERF with pre-trained Vision Features for Few-Shot 3D Reconstruction",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2411.02450",
      "abstract": "Quantum Neural Networks (QNNs) integrate quantum computing and deep neural networks, leveraging quantum properties like superposition and entanglement to enhance machine learning algorithms. These characteristics enable QNNs to outperform classical neural networks in tasks such as quantum chemistry simulations, optimization problems, and quantum-enhanced machine learning. Despite their early success, their reliability and safety issues have posed threats to their applicability. However, due to the inherently non-classical nature of quantum mechanics, verifying QNNs poses significant challenges. To address this, we propose QCov, a set of test coverage criteria specifically designed to systematically evaluate QNN state exploration during testing, with an emphasis on superposition. These criteria help evaluate test diversity and detect underlying defects within test suites. Extensive experiments on benchmark datasets and QNN models validate QCov's effectiveness in reflecting test quality, guiding fuzz testing efficiently, and thereby improving QNN robustness. We also evaluate sampling costs of QCov under realistic quantum scenarios to justify its practical feasibility. Finally, the effects of unrepresentative training data distribution and parameter choice are further explored.",
      "authors": [
        "Shao, Minqi",
        "Zhao, Jianjun"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2411.02450v2",
        "Other Formats": "https://arxiv.org/format/2411.02450",
        "TeX Source": "https://arxiv.org/src/2411.02450",
        "View PDF": "https://arxiv.org/pdf/2411.02450"
      },
      "subjects": [
        "Quantum Physics (quant-ph)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 3 Nov 2024 08:07:27 UTC (2,043 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 10:53:15 UTC (7,249 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/11/03",
      "title": "A Coverage-Guided Testing Framework for Quantum Neural Networks",
      "tasks": [],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18164",
      "abstract": "Learning dense correspondences, critical for application such as video label propagation, is hindered by tedious and unscalable manual annotation. Self-supervised methods address this by using a cross-view pretext task, often modeled with a masked autoencoder, where a masked target view is reconstructed from an anchor view. However, acquiring effective training data remains a challenge - collecting diverse video datasets is difficult and costly, while simple image crops lack necessary pose variations. This paper introduces CDG-MAE, a novel MAE-based self-supervised method that uses diverse synthetic views generated from static images via an image-conditioned diffusion model. These generated views exhibit substantial changes in pose and perspective, providing a rich training signal that overcomes the limitations of video and crop-based anchors. We present a quantitative method to evaluate local and global consistency of generated images, discussing their use for cross-view self-supervised pretraining. Furthermore, we enhance the standard single-anchor MAE setting to a multi-anchor strategy to effectively modulate the difficulty of pretext task. CDG-MAE significantly outperforms state-of-the-art MAE methods reliant only on images and substantially narrows the performance gap to video-based approaches.",
      "authors": [
        "Belagali, Varun",
        "Marza, Pierre",
        "Yellapragada, Srikar",
        "Li, Zilinghan",
        "Nandi, Tarak Nath",
        "Madduri, Ravi K",
        "Saltz, Joel",
        "Christodoulidis, Stergios",
        "Vakalopoulou, Maria",
        "Samaras, Dimitris"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18164v1",
        "Other Formats": "https://arxiv.org/format/2506.18164",
        "TeX Source": "https://arxiv.org/src/2506.18164",
        "View PDF": "https://arxiv.org/pdf/2506.18164"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 20:40:11 UTC (42,664 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "CDG-MAE: Learning Correspondences from Diffusion Generated Views",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.19133",
      "abstract": "Low-rank representation learning has emerged as a powerful tool for recovering missing values in power load data due to its ability to exploit the inherent low-dimensional structures of spatiotemporal measurements. Among various techniques, low-rank factorization models are favoured for their efficiency and interpretability. However, their performance is highly sensitive to the choice of regularization parameters, which are typically fixed or manually tuned, resulting in limited generalization capability or slow convergence in practical scenarios. In this paper, we propose a Regularization-optimized Low-Rank Factorization, which introduces a Proportional-Integral-Derivative controller to adaptively adjust the regularization coefficient. Furthermore, we provide a detailed algorithmic complexity analysis, showing that our method preserves the computational efficiency of stochastic gradient descent while improving adaptivity. Experimental results on real-world power load datasets validate the superiority of our method in both imputation accuracy and training efficiency compared to existing baselines.",
      "authors": [
        "Xia, Yan",
        "Feng, Hao",
        "Sun, Hongwei",
        "Wang, Junjie",
        "Hu, Qicong"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "Other Formats": "https://arxiv.org/format/2505.19133",
        "View PDF": "https://arxiv.org/pdf/2505.19133"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 25 May 2025 13:07:55 UTC (608 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 20:41:34 UTC (615 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/25",
      "title": "Fast and Accurate Power Load Data Completion via Regularization-optimized Low-Rank Factorization",
      "tasks": [
        "Computational Efficiency",
        "Imputation",
        "Missing Values",
        "Representation Learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18106",
      "abstract": "Aimed to develop and validate a CT radiomics-based explainable machine learning model for diagnosing malignancy and benignity specifically in endometrial cancer (EC) patients. A total of 83 EC patients from two centers, including 46 with malignant and 37 with benign conditions, were included, with data split into a training set (n=59) and a testing set (n=24). The regions of interest (ROIs) were manually segmented from pre-surgical CT scans, and 1132 radiomic features were extracted from the pre-surgical CT scans using Pyradiomics. Six explainable machine learning modeling algorithms were implemented respectively, for determining the optimal radiomics pipeline. The diagnostic performance of the radiomic model was evaluated by using sensitivity, specificity, accuracy, precision, F1 score, confusion matrices, and ROC curves. To enhance clinical understanding and usability, we separately implemented SHAP analysis and feature mapping visualization, and evaluated the calibration curve and decision curve. By comparing six modeling strategies, the Random Forest model emerged as the optimal choice for diagnosing EC, with a training AUC of 1.00 and a testing AUC of 0.96. SHAP identified the most important radiomic features, revealing that all selected features were significantly associated with EC (P < 0.05). Radiomics feature maps also provide a feasible assessment tool for clinical applications. DCA indicated a higher net benefit for our model compared to the \"All\" and \"None\" strategies, suggesting its clinical utility in identifying high-risk cases and reducing unnecessary interventions. In conclusion, the CT radiomics-based explainable machine learning model achieved high diagnostic performance, which could be used as an intelligent auxiliary tool for the diagnosis of endometrial cancer.",
      "authors": [
        "Zhang, Tingrui",
        "Wu, Honglin",
        "Jiang, Zekun",
        "Wang, Yingying",
        "Ye, Rui",
        "Ni, Huiming",
        "Liu, Chang",
        "Cao, Jin",
        "Sun, Xuan",
        "Shao, Rong",
        "Wei, Xiaorong",
        "Sun, Yingchun"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.18106",
        "View PDF": "https://arxiv.org/pdf/2506.18106"
      },
      "subjects": [
        "Image and Video Processing (eess.IV)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 17:31:06 UTC (762 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "CT Radiomics-Based Explainable Machine Learning Model for Accurate Differentiation of Malignant and Benign Endometrial Tumors: A Two-Center Study",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18155",
      "abstract": "This work introduces 4 novel probabilistic and reinforcement-driven methods for association rule mining (ARM): Gaussian process-based association rule mining (GPAR), Bayesian ARM (BARM), multi-armed bandit based ARM (MAB-ARM), and reinforcement learning based association rule mining (RLAR). These methods depart fundamentally from traditional frequency-based algorithms such as Apriori, FP-Growth, and Eclat, offering enhanced capabilities for incorporating prior knowledge, modeling uncertainty, item dependencies, probabilistic inference and adaptive search strategies. GPAR employs Gaussian processes to model item co-occurrence via feature representations, enabling principled inference, uncertainty quantification, and efficient generalization to unseen itemsets without retraining. BARM adopts a Bayesian framework with priors and optional correlation structures, yielding robust uncertainty quantification through full posterior distributions over item presence probabilities. MAB-ARM, including its Monte Carlo tree search (MCTS) companion, utilizes an upper confidence bound (UCB) strategy for efficient and adaptive exploration of the itemset space, while RLAR applies a deep Q-network (DQN) to learn a generalizable policy for identifying high-quality rules. Collectively, these approaches improve the flexibility and robustness of ARM, particularly for discovering rare or complex patterns and operating on small datasets. Empirical results on synthetic and real-world datasets demonstrate their effectiveness, while also highlighting trade-offs in computational complexity and interpretability. These innovations mark a significant shift from static, frequency-driven paradigms, offering some prior and dependency-informed, uncertainty-aware or scalable ARM frameworks for diverse application domains such as retail, geography, finance, medical diagnostics, and risk-sensitive scenarios.",
      "authors": [
        "Huang, Yongchao"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.18155",
        "TeX Source": "https://arxiv.org/src/2506.18155",
        "View PDF": "https://arxiv.org/pdf/2506.18155"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 19:51:15 UTC (1,715 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Probabilistic and reinforced mining of association rules",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18203",
      "abstract": "Verifiers can improve language model capabilities by scoring and ranking responses from generated candidates. Currently, high-quality verifiers are either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean). While LM judges and reward models have become broadly useful as general-purpose verifiers, a significant performance gap remains between them and oracle verifiers (verifiers with perfect accuracy). To help close this gap, we introduce Weaver, a framework for designing a strong verifier by combining multiple weak, imperfect verifiers. We find weighted ensembles of verifiers, which typically require learning from labeled data, significantly outperform unweighted combinations due to differences in verifier accuracies. To reduce dependency on labeled data, Weaver leverages weak supervision to estimate each verifier's accuracy and combines outputs into a unified score that better reflects true response quality. However, directly applying weak supervision algorithms poses challenges, including inconsistent verifier output formats and handling low-quality verifiers. Weaver addresses these using dataset statistics to normalize outputs and filter specific verifiers. We study Weaver's effectiveness in test-time repeated sampling, where a model generates multiple candidate responses and selects one. Our evaluations show Weaver significantly improves over Pass@1-performance when selecting the first candidate-across reasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B Instruct as generator, and an ensemble of 70B or smaller judge and reward models as verifiers (87.7% average). This gain mirrors the jump between GPT-4o and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and post-training. To reduce computational costs of verifier ensembles, we train a 400M cross-encoder using Weaver's combined output scores.",
      "authors": [
        "Saad-Falcon, Jon",
        "Buchanan, E. Kelly",
        "Chen, Mayee F.",
        "Huang, Tzu-Heng",
        "McLaughlin, Brendan",
        "Bhathal, Tanvir",
        "Zhu, Shang",
        "Athiwaratkun, Ben",
        "Sala, Frederic",
        "Linderman, Scott",
        "Mirhoseini, Azalia",
        "R\u00e9, Christopher"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18203v1",
        "Other Formats": "https://arxiv.org/format/2506.18203",
        "TeX Source": "https://arxiv.org/src/2506.18203",
        "View PDF": "https://arxiv.org/pdf/2506.18203"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 23:38:15 UTC (16,740 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Shrinking the Generation-Verification Gap with Weak Verifiers",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.12221",
      "abstract": "Neuromorphic hardware aims to leverage distributed computing and event-driven circuit design to achieve an energy-efficient AI system. The name \"neuromorphic\" is derived from its spiking and local computing nature, which mimics the fundamental activity of an animal's nervous system. In neuromorphic hardware, neurons, i.e., computing cores use single-bit, event-driven data (called spikes) for inter-communication, which differs substantially from conventional hardware. To leverage the advantages of neuromorphic hardware and implement a computing model, the conventional approach is to build spiking neural networks (SNNs). SNNs replace the nonlinearity part of artificial neural networks (ANNs) in the realm of deep learning with spiking neurons, where the spiking neuron mimics the basic behavior of bio-neurons. However, there is still a performance gap between SNNs and their ANN counterparts. In this paper, we explore a new way to map computing models onto neuromorphic hardware. We propose a Spiking-Driven ANN (SDANN) framework that directly implements quantized ANN on hardware, eliminating the need for tuning the trainable parameters or any performance degradation. With the power of quantized ANN, our SDANN ensures a lower bound of implementation performance on neuromorphic hardware. To address the limitation of bit width support on hardware, we propose bias calibration and scaled integration methods. Experiments on various tasks demonstrate that our SDANN achieves exactly the same accuracy as the quantized ANN. Beyond toy examples and software implementation, we successfully deployed and validated our spiking models on real neuromorphic hardware, demonstrating the feasibility of the SDANN framework.",
      "authors": [
        "Chen, Zhenhui",
        "Xu, Haoran",
        "Hu, Yangfan",
        "Jin, Xiaofei",
        "Li, Xinyu",
        "Kang, Ziyang",
        "Pan, Gang",
        "Ma, De"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2505.12221v2",
        "Other Formats": "https://arxiv.org/format/2505.12221",
        "TeX Source": "https://arxiv.org/src/2505.12221",
        "View PDF": "https://arxiv.org/pdf/2505.12221"
      },
      "subjects": [
        "Neural and Evolutionary Computing (cs.NE)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 18 May 2025 03:45:43 UTC (1,572 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 14:37:05 UTC (1,588 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/18",
      "title": "Bridging Quantized Artificial Neural Networks and Neuromorphic Hardware",
      "tasks": [
        "Distributed Computing"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17872",
      "abstract": "Federated learning has significantly advanced distributed training of machine learning models across decentralized data sources. However, existing frameworks often lack comprehensive solutions that combine uncertainty quantification, interpretability, and robustness. To address this, we propose FedNAM+, a federated learning framework that integrates Neural Additive Models (NAMs) with a novel conformal prediction method to enable interpretable and reliable uncertainty estimation. Our method introduces a dynamic level adjustment technique that utilizes gradient-based sensitivity maps to identify key input features influencing predictions. This facilitates both interpretability and pixel-wise uncertainty estimates. Unlike traditional interpretability methods such as LIME and SHAP, which do not provide confidence intervals, FedNAM+ offers visual insights into prediction reliability. We validate our approach through experiments on CT scan, MNIST, and CIFAR datasets, demonstrating high prediction accuracy with minimal loss (e.g., only 0.1% on MNIST), along with transparent uncertainty measures. Visual analysis highlights variable uncertainty intervals, revealing low-confidence regions where model performance can be improved with additional data. Compared to Monte Carlo Dropout, FedNAM+ delivers efficient and global uncertainty estimates with reduced computational overhead, making it particularly suitable for federated learning scenarios. Overall, FedNAM+ provides a robust, interpretable, and computationally efficient framework that enhances trust and transparency in decentralized predictive modeling.",
      "authors": [
        "Balija, Sree Bhargavi",
        "Nanda, Amitash",
        "Sahoo, Debashis"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17872v1",
        "Other Formats": "https://arxiv.org/format/2506.17872",
        "TeX Source": "https://arxiv.org/src/2506.17872",
        "View PDF": "https://arxiv.org/pdf/2506.17872"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 02:06:14 UTC (1,451 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Decoding Federated Learning: The FedNAM+ Conformal Revolution",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18067",
      "abstract": "The burgeoning low-altitude economy (LAE) necessitates integrated sensing and communication (ISAC) systems capable of high-accuracy multi-target localization and velocity estimation under hardware and coverage constraints inherent in conventional ISAC architectures. This paper addresses these challenges by proposing a cooperative bistatic ISAC framework within MIMO-OFDM cellular networks, enabling robust sensing services for LAE applications through standardized 5G New Radio (NR) infrastructure. We first develop a low-complexity parameter extraction algorithm employing CANDECOMP/PARAFAC (CP) tensor decomposition, which exploits the inherent Vandermonde structure in delay-related factor matrices to efficiently recover bistatic ranges, Doppler velocities, and angles-of-arrival (AoA) from multi-dimensional received signal tensors. To resolve data association ambiguity across distributed transmitter-receiver pairs and mitigate erroneous estimates, we further design a robust fusion scheme based on the minimum spanning tree (MST) method, enabling joint 3D position and velocity reconstruction. Comprehensive simulation results validate the framework's superiority in computational efficiency and sensing performance for low-altitude scenarios.",
      "authors": [
        "Zhang, Zhenkun",
        "Xu, Yining",
        "Pan, Cunhua",
        "Ren, Hong",
        "Yu, Yiming",
        "Wang, Jiangzhou"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18067v1",
        "Other Formats": "https://arxiv.org/format/2506.18067",
        "TeX Source": "https://arxiv.org/src/2506.18067",
        "View PDF": "https://arxiv.org/pdf/2506.18067"
      },
      "subjects": [
        "Signal Processing (eess.SP)",
        "Information Theory (cs.IT)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 15:32:07 UTC (6,420 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Cooperative Bistatic ISAC Systems for Low-Altitude Economy",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17870",
      "abstract": "Deploying quantized deep neural network (DNN) models with resource adaptation capabilities on ubiquitous Internet of Things (IoT) devices to provide high-quality AI services can leverage the benefits of compression and meet multi-scenario resource requirements. However, existing dynamic/mixed precision quantization requires retraining or special hardware, whereas post-training quantization (PTQ) has two limitations for resource adaptation: (i) The state-of-the-art PTQ methods only provide one fixed bitwidth model, which makes it challenging to adapt to the dynamic resources of IoT devices; (ii) Deploying multiple PTQ models with diverse bitwidths consumes large storage resources and switching overheads. To this end, this paper introduces a resource-friendly post-training integer-nesting quantization, i.e., NestQuant, for on-device quantized model switching on IoT devices. The proposed NestQuant incorporates the integer weight decomposition, which bit-wise splits quantized weights into higher-bit and lower-bit weights of integer data types. It also contains a decomposed weights nesting mechanism to optimize the higher-bit weights by adaptive rounding and nest them into the original quantized weights. In deployment, we can send and store only one NestQuant model and switch between the full-bit/part-bit model by paging in/out lower-bit weights to adapt to resource changes and reduce consumption. Experimental results on the ImageNet-1K pretrained DNNs demonstrated that the NestQuant model can achieve high performance in top-1 accuracy, and reduce in terms of data transmission, storage consumption, and switching overheads. In particular, the ResNet-101 with INT8 nesting INT6 can achieve 78.1% and 77.9% accuracy for full-bit and part-bit models, respectively, and reduce switching overheads by approximately 78.1% compared with diverse bitwidths PTQ models.",
      "authors": [
        "Xie, Jianhang",
        "Ding, Chuntao",
        "Li, Xiaqing",
        "Ren, Shenyuan",
        "Li, Yidong",
        "Lu, Zhichao"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17870v1",
        "Other Formats": "https://arxiv.org/format/2506.17870",
        "TeX Source": "https://arxiv.org/src/2506.17870",
        "View PDF": "https://arxiv.org/pdf/2506.17870"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Distributed, Parallel, and Cluster Computing (cs.DC)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 01:53:22 UTC (589 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "NestQuant: Post-Training Integer-Nesting Quantization for On-Device DNN",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18095",
      "abstract": "Recent advances in multimodal generative models have unlocked photorealistic, instruction-aligned image generation, yet leading systems like GPT-4o-Image remain proprietary and inaccessible. To democratize these capabilities, we present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and 46K text-and-image-to-image data, all synthesized using GPT-4o's image generation capabilities for distilling its advanced image generation abilities. Leveraging this dataset, we develop Janus-4o, a multimodal large language model capable of both text-to-image and text-and-image-to-image generation. Janus-4o not only significantly improves text-to-image generation over its predecessor, Janus-Pro, but also newly supports text-and-image-to-image generation. Notably, it achieves impressive performance in text-and-image-to-image generation from scratch, using only 91K synthetic samples and 6 hours of training on an 8 A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will foster open research in photorealistic, instruction-aligned image generation.",
      "authors": [
        "Chen, Junying",
        "Cai, Zhenyang",
        "Chen, Pengcheng",
        "Chen, Shunian",
        "Ji, Ke",
        "Wang, Xidong",
        "Yang, Yunjin",
        "Wang, Benyou"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18095v1",
        "Other Formats": "https://arxiv.org/format/2506.18095",
        "TeX Source": "https://arxiv.org/src/2506.18095",
        "View PDF": "https://arxiv.org/pdf/2506.18095"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 16:51:09 UTC (5,845 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation",
      "datasets": [
        {
          "dataset_name": "FreedomIntelligence/ShareGPT-4o-Image",
          "downloads": "75",
          "likes": "2",
          "link": "https://huggingface.co/datasets/FreedomIntelligence/ShareGPT-4o-Image"
        }
      ],
      "models": [
        {
          "model_path": "FreedomIntelligence/Janus-4o-7B",
          "downloads": "0",
          "likes": "2",
          "trending_score": "2.0",
          "link": "https://huggingface.co/FreedomIntelligence/Janus-4o-7B"
        }
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.07248",
      "abstract": "Long document classification poses challenges due to the computational limitations of transformer-based models, particularly BERT, which are constrained by fixed input lengths and quadratic attention complexity. Moreover, using the full document for classification is often redundant, as only a subset of sentences typically carries the necessary information. To address this, we propose a TF-IDF-based sentence ranking method that improves efficiency by selecting the most informative content. Our approach explores fixed-count and percentage-based sentence selection, along with an enhanced scoring strategy combining normalized TF-IDF scores and sentence length. Evaluated on the MahaNews LDC dataset of long Marathi news articles, the method consistently outperforms baselines such as first, last, and random sentence selection. With MahaBERT-v2, we achieve near-identical classification accuracy with just a 0.33 percent drop compared to the full-context baseline, while reducing input size by over 50 percent and inference latency by 43 percent. This demonstrates that significant context reduction is possible without sacrificing performance, making the method practical for real-world long document classification tasks.",
      "authors": [
        "Kokate, Prathamesh",
        "Sarnaik, Mitali",
        "Khopade, Manavi",
        "Joshi, Raviraj"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.07248v2",
        "Other Formats": "https://arxiv.org/format/2506.07248",
        "TeX Source": "https://arxiv.org/src/2506.07248",
        "View PDF": "https://arxiv.org/pdf/2506.07248"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 8 Jun 2025 18:09:43 UTC (7,467 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 07:31:53 UTC (7,467 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/06/08",
      "title": "Improving the Efficiency of Long Document Classification using Sentence Ranking Approach",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18020",
      "abstract": "Robust distributed learning algorithms aim to maintain good performance in distributed and federated settings, even in the presence of misbehaving workers. Two primary threat models have been studied: Byzantine attacks, where misbehaving workers can send arbitrarily corrupted updates, and data poisoning attacks, where misbehavior is limited to manipulation of local training data. While prior work has shown comparable optimization error under both threat models, a fundamental question remains open: How do these threat models impact generalization? Empirical evidence suggests a gap between the two threat models, yet it remains unclear whether it is fundamental or merely an artifact of suboptimal attacks. In this work, we present the first theoretical investigation into this problem, formally showing that Byzantine attacks are intrinsically more harmful to generalization than data poisoning. Specifically, we prove that: (i) under data poisoning, the uniform algorithmic stability of a robust distributed learning algorithm, with optimal optimization error, degrades by an additive factor of $\\varTheta ( \\frac{f}{n-f} )$, with $f$ the number of misbehaving workers out of $n$; and (ii) In contrast, under Byzantine attacks, the degradation is in $\\mathcal{O} \\big( \\sqrt{ \\frac{f}{n-2f}} \\big)$.This difference in stability leads to a generalization error gap that is especially significant as $f$ approaches its maximum value $\\frac{n}{2}$.",
      "authors": [
        "Boudou, Thomas",
        "Bars, Batiste Le",
        "Gupta, Nirupam",
        "Bellet, Aur\u00e9lien"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18020v1",
        "Other Formats": "https://arxiv.org/format/2506.18020",
        "TeX Source": "https://arxiv.org/src/2506.18020",
        "View PDF": "https://arxiv.org/pdf/2506.18020"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Cryptography and Security (cs.CR)",
        "Machine Learning (stat.ML)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 12:59:15 UTC (113 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Generalization under Byzantine & Poisoning Attacks: Tight Stability Bounds in Robust Distributed Learning",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2405.09394",
      "abstract": "Federated Learning (FL) is a collaborative machine learning paradigm for training models on local sensitive data with privacy protection. Pre-trained transformer-based models have emerged as useful foundation models (FMs) to be fine-tuned for a wide range of downstream tasks. However, large-scale pre-trained models make it challenging for traditional FL due to high communication overhead in the resource-constrained IoT. This has inspired the field of parameter-efficient fine-tuning (PEFT) research. Existing PEFT methods attempt to optimize model performance at the given dropout level. Such an approach places the burden on human users to find a dropout rate that provides a satisfactory level of performance through trial-and-error, which is time consuming and resource intensive. To address this limitation, we propose the Step-wise Parameter Dropout for Continual Federated Learning (SPD-CFL) approach. Instead of pre-defining a desired dropout rate, it allows users to specify the target level of performance and then attempts to find the most suitable dropout rate for the given FL model. Specifically, on the server side, SPD-CFL drops trainable parameters in a stepwise manner to improve communication efficiency by reducing the rank of low-rank adaptation (LoRA). The sensitivity-based gradient consistency (SGC) measure is designed to facilitate the adaptive adjustment of parameter dropout. In addition, SPD-CFL introduces continual learning (CL) on the client side to mitigate performance degradation due to the inconsistent optima with distinct parameter dropout rates under heterogeneous FL. Extensive experiments on the public benchmark dataset CIFAR-10 and a real-world medical Face dataset demonstrate significant superiority of SPD-CFL over state-of-the-art methods. Compared to the best-performing baseline, it achieves a 2.07% higher test AUC while reducing communication overhead by 29.53%.",
      "authors": [
        "Yang, Yuning",
        "Yu, Han",
        "Sun, Chuan",
        "Gao, Tianrun",
        "Liu, Xiaohong",
        "Xu, Xiaodong",
        "Zhang, Ping",
        "Wang, Guangyu"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2405.09394v2",
        "Other Formats": "https://arxiv.org/format/2405.09394",
        "TeX Source": "https://arxiv.org/src/2405.09394",
        "View PDF": "https://arxiv.org/pdf/2405.09394"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Distributed, Parallel, and Cluster Computing (cs.DC)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 15 May 2024 14:50:46 UTC (260 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 03:39:44 UTC (4,086 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/05/15",
      "title": "SPD-CFL: Stepwise Parameter Dropout for Efficient Continual Federated Learning",
      "tasks": [
        "Federated Learning",
        "parameter-efficient fine-tuning",
        "Transfer Learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18091",
      "abstract": "Anaphora resolution plays a critical role in natural language understanding, especially in morphologically rich languages like Czech. This paper presents a comparative evaluation of two modern approaches to anaphora resolution on Czech text: prompt engineering with large language models (LLMs) and fine-tuning compact generative models. Using a dataset derived from the Prague Dependency Treebank, we evaluate several instruction-tuned LLMs, including Mistral Large 2 and Llama 3, using a series of prompt templates. We compare them against fine-tuned variants of the mT5 and Mistral models that we trained specifically for Czech anaphora resolution. Our experiments demonstrate that while prompting yields promising few-shot results (up to 74.5% accuracy), the fine-tuned models, particularly mT5-large, outperform them significantly, achieving up to 88% accuracy while requiring fewer computational resources. We analyze performance across different anaphora types, antecedent distances, and source corpora, highlighting key strengths and trade-offs of each approach.",
      "authors": [
        "Stano, Patrik",
        "Hor\u00e1k, Ale\u0161"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18091v1",
        "Other Formats": "https://arxiv.org/format/2506.18091",
        "TeX Source": "https://arxiv.org/src/2506.18091",
        "View PDF": "https://arxiv.org/pdf/2506.18091"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 16:32:57 UTC (48 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2401.04585",
      "abstract": "Diffusion models have achieved great success in image generation tasks. However, the lengthy denoising process and complex neural networks hinder their low-latency applications in real-world scenarios. Quantization can effectively reduce model complexity, and post-training quantization (PTQ), which does not require fine-tuning, is highly promising for compressing and accelerating diffusion models. Unfortunately, we find that due to the highly dynamic activations, existing PTQ methods suffer from distribution mismatch issues at both calibration sample level and reconstruction output level, which makes the performance far from satisfactory. In this paper, we propose EDA-DM, a standardized PTQ method that efficiently addresses the above issues. Specifically, at the calibration sample level, we extract information from the density and diversity of latent space feature maps, which guides the selection of calibration samples to align with the overall sample distribution; and at the reconstruction output level, we theoretically analyze the reasons for previous reconstruction failures and, based on this insight, optimize block reconstruction using the Hessian loss of layers, aligning the outputs of quantized model and full-precision model at different network granularity. Extensive experiments demonstrate that EDA-DM significantly outperforms the existing PTQ methods across various models and datasets. Our method achieves a 1.83 times speedup and 4 times compression for the popular Stable-Diffusion on MS-COCO, with only a 0.05 loss in CLIP score. Code is available at http://github.com/BienLuky/EDA-DM .",
      "authors": [
        "Liu, Xuewen",
        "Li, Zhikai",
        "Xiao, Junrui",
        "Chen, Mengjuan",
        "Li, Jianquan",
        "Gu, Qingyi"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2401.04585v3",
        "Other Formats": "https://arxiv.org/format/2401.04585",
        "TeX Source": "https://arxiv.org/src/2401.04585",
        "View PDF": "https://arxiv.org/pdf/2401.04585"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Tue, 9 Jan 2024 14:42:49 UTC (19,884 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Thu, 26 Sep 2024 02:53:15 UTC (16,797 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Sun, 22 Jun 2025 12:05:22 UTC (11,111 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/01/09",
      "title": "EDA-DM: Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models",
      "repo_urls": [
        "https://github.com/BienLuky/EDA-DM"
      ],
      "tasks": [
        "Denoising",
        "Image Generation",
        "Noise Estimation",
        "Quantization"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17975",
      "abstract": "Synthetic data has recently reached a level of visual fidelity that makes it nearly indistinguishable from real data, offering great promise for privacy-preserving data sharing in medical imaging. However, fully synthetic datasets still suffer from significant limitations: First and foremost, the legal aspect of sharing synthetic data is often neglected and data regulations, such as the GDPR, are largley ignored. Secondly, synthetic models fall short of matching the performance of real data, even for in-domain downstream applications. Recent methods for image generation have focused on maximising image diversity instead of fidelity solely to improve the mode coverage and therefore the downstream performance of synthetic data. In this work, we shift perspective and highlight how maximizing diversity can also be interpreted as protecting natural persons from being singled out, which leads to predicate singling-out (PSO) secure synthetic datasets. Specifically, we propose a generalisable framework for training diffusion models on personal data which leads to unpersonal synthetic datasets achieving performance within one percentage point of real-data models while significantly outperforming state-of-the-art methods that do not ensure privacy. Our code is available at https://github.com/MischaD/Trichotomy.",
      "authors": [
        "Dombrowski, Mischa",
        "Kainz, Bernhard"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17975v1",
        "Other Formats": "https://arxiv.org/format/2506.17975",
        "TeX Source": "https://arxiv.org/src/2506.17975",
        "View PDF": "https://arxiv.org/pdf/2506.17975"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 10:26:35 UTC (3,422 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Enabling PSO-Secure Synthetic Data Sharing Using Diversity-Aware Diffusion Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18070",
      "abstract": "Deep learning-based medical image classification techniques are rapidly advancing in medical image analysis, making it crucial to develop accurate and trustworthy models that can be efficiently deployed across diverse clinical scenarios. Concept Bottleneck Models (CBMs), which first predict a set of explainable concepts from images and then perform classification based on these concepts, are increasingly being adopted for explainable medical image classification. However, the inherent explainability of CBMs introduces new challenges when deploying trained models to new environments. Variations in imaging protocols and staining methods may induce concept-level shifts, such as alterations in color distribution and scale. Furthermore, since CBM training requires explicit concept annotations, fine-tuning models solely with image-level labels could compromise concept prediction accuracy and faithfulness - a critical limitation given the high cost of acquiring expert-annotated concept labels in medical domains. To address these challenges, we propose a training-free confusion concept identification strategy. By leveraging minimal new data (e.g., 4 images per class) with only image-level labels, our approach enhances out-of-domain performance without sacrificing source domain accuracy through two key operations: masking misactivated confounding concepts and amplifying under-activated discriminative concepts. The efficacy of our method is validated on both skin and white blood cell images. Our code is available at: https://github.com/riverback/TF-TTI-XMed.",
      "authors": [
        "He, Hangzhou",
        "Tang, Jiachen",
        "Zhu, Lei",
        "Li, Kaiwen",
        "Lu, Yanye"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18070v1",
        "Other Formats": "https://arxiv.org/format/2506.18070",
        "TeX Source": "https://arxiv.org/src/2506.18070",
        "View PDF": "https://arxiv.org/pdf/2506.18070"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 15:37:13 UTC (496 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Training-free Test-time Improvement for Explainable Medical Image Classification",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18048",
      "abstract": "Small Vision Language Models (SVLMs) generally refer to models with parameter sizes less than or equal to 2B. Their low cost and power consumption characteristics confer high commercial value. However, their reasoning abilities are limited by the number of parameters. To address this issue, this paper proposes a post-training optimization paradigm called the Incremental Training Strategy to enhance the reasoning ability of SVLMs. Firstly, we constructed a Self-Supervised Chain-of-Thought (COT) Data Construction System, which leverages multiple LVLMs with 7B parameters or more to transform original data into COT data in a self-supervised manner. Our proposed Incremental Training Strategy consists of four stages. Stage 1 injects domain knowledge by performing Supervised Fine-Tuning (SFT) to the pretrained model on the COT data. Stage 2 aligns the COT data format by conducting a small amount of Group Relative Policy Optimization (GRPO) training constrained only by format rewards on the COT data. Stage 3 enhances reasoning ability by applying GRPO training on the COT data with constraints on both format and accuracy rewards. The resulting model shows significant improvement compared to the baseline. Stage 4 addresses the limited capacity of the SVLMs and the weak ability to capture complex patterns by proposing ClipLow GRPO (CLGRPO) to constrain the capture space of the training process. We conducted extensive comparative and ablation experiments on the abstract semantic recognition dataset EMOSet-118K. Experimental results demonstrate that our method significantly improves the reasoning ability of 1B SVLM. Compared to the baseline model fine-tuned on the original data, accuracy increased by 2.77 and recall by 0.69, achieving performance comparable to that of 8B models.",
      "authors": [
        "Wang, Fanyi",
        "Dong, Binzhi",
        "Hu, Haotian",
        "Xu, Jinjin",
        "Zhang, Zhiwang"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18048v1",
        "Other Formats": "https://arxiv.org/format/2506.18048",
        "TeX Source": "https://arxiv.org/src/2506.18048",
        "View PDF": "https://arxiv.org/pdf/2506.18048"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 14:32:15 UTC (803 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "CLGRPO: Reasoning Ability Enhancement for Small VLMs",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18003",
      "abstract": "Cyclostationary analysis is widely used in signal processing, particularly in the analysis of human-made signals, and spectral correlation density (SCD) is often used to characterise cyclostationarity. Unfortunately, for real-time applications, even utilising the fast Fourier transform (FFT), the high computational complexity associated with estimating the SCD limits its applicability. In this work, we present optimised, high-speed field-programmable gate array (FPGA) implementations of two SCD estimation techniques. Specifically, we present an implementation of the FFT accumulation method (FAM) running entirely on the AMD Versal AI engine (AIE) array. We also introduce an efficient implementation of the strip spectral correlation analyser (SSCA) that can be used for window sizes up to $2^{20}$. For both techniques, a generalised methodology is presented to parallelise the computation while respecting memory size and data bandwidth constraints. Compared to an NVIDIA GeForce RTX 3090 graphics processing unit (GPU) which uses a similar 7nm technology to our FPGA, for the same accuracy, our FAM/SSCA implementations achieve speedups of 4.43x/1.90x and a 30.5x/24.5x improvement in energy efficiency.",
      "authors": [
        "Li, Carol Jingyi",
        "Wu, Ruilin",
        "Leong, Philip H. W."
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18003v1",
        "Other Formats": "https://arxiv.org/format/2506.18003",
        "TeX Source": "https://arxiv.org/src/2506.18003",
        "View PDF": "https://arxiv.org/pdf/2506.18003"
      },
      "subjects": [
        "Hardware Architecture (cs.AR)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 11:46:12 UTC (5,100 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "AMD Versal Implementations of FAM and SSCA Estimators",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18087",
      "abstract": "With the widespread application of edge computing and cloud systems in AI-driven applications, how to maintain efficient performance while ensuring data privacy has become an urgent security issue. This paper proposes a federated learning-based data collaboration method to improve the security of edge cloud AI systems, and use large-scale language models (LLMs) to enhance data privacy protection and system robustness. Based on the existing federated learning framework, this method introduces a secure multi-party computation protocol, which optimizes the data aggregation and encryption process between distributed nodes by using LLM to ensure data privacy and improve system efficiency. By combining advanced adversarial training techniques, the model enhances the resistance of edge cloud AI systems to security threats such as data leakage and model poisoning. Experimental results show that the proposed method is 15% better than the traditional federated learning method in terms of data protection and model robustness.",
      "authors": [
        "Luo, Huaiying",
        "Ji, Cheng"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18087v1",
        "Other Formats": "https://arxiv.org/format/2506.18087",
        "TeX Source": "https://arxiv.org/src/2506.18087",
        "View PDF": "https://arxiv.org/pdf/2506.18087"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 16:23:45 UTC (236 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Federated Learning-Based Data Collaboration Method for Enhancing Edge Cloud AI System Security Using Large Language Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2410.13864",
      "abstract": "Vision-centric autonomous driving has demonstrated excellent performance with economical sensors. As the fundamental step, 3D perception aims to infer 3D information from 2D images based on 3D-2D projection. This makes driving perception models susceptible to sensor configuration (e.g., camera intrinsics and extrinsics) variations. However, generalizing across camera configurations is important for deploying autonomous driving models on different car models. In this paper, we present UniDrive, a novel framework for vision-centric autonomous driving to achieve universal perception across camera configurations. We deploy a set of unified virtual cameras and propose a ground-aware projection method to effectively transform the original images into these unified virtual views. We further propose a virtual configuration optimization method by minimizing the expected projection error between original and virtual cameras. The proposed virtual camera projection can be applied to existing 3D perception methods as a plug-and-play module to mitigate the challenges posed by camera parameter variability, resulting in more adaptable and reliable driving perception models. To evaluate the effectiveness of our framework, we collect a dataset on CARLA by driving the same routes while only modifying the camera configurations. Experimental results demonstrate that our method trained on one specific camera configuration can generalize to varying configurations with minor performance degradation.",
      "authors": [
        "Li, Ye",
        "Zheng, Wenzhao",
        "Huang, Xiaonan",
        "Keutzer, Kurt"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2410.13864v2",
        "Other Formats": "https://arxiv.org/format/2410.13864",
        "TeX Source": "https://arxiv.org/src/2410.13864",
        "View PDF": "https://arxiv.org/pdf/2410.13864"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 17 Oct 2024 17:59:59 UTC (16,032 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 17:30:37 UTC (11,526 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/10/17",
      "title": "UniDrive: Towards Universal Driving Perception Across Camera Configurations",
      "repo_urls": [
        "https://github.com/ywyeli/unidrive"
      ],
      "tasks": [
        "Autonomous Driving"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17900",
      "abstract": "With the increasing complexity and rapid expansion of the scale of AI systems in cloud platforms, the log data generated during system operation is massive, unstructured, and semantically ambiguous, which brings great challenges to fault location and system self-repair. In order to solve this problem, this paper proposes an intelligent log processing and automatic debugging framework based on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). This method is extended on the basis of the existing pre-trained Transformer model, and integrates a multi-stage semantic inference mechanism to realize the context understanding of system logs and the automatic reconstruction of fault chains. Firstly, the system log is dynamically structured, and the unsupervised clustering and embedding mechanism is used to extract the event template and semantic schema. Subsequently, the fine-tuned LLM combined with the multi-round attention mechanism to perform contextual reasoning on the log sequence to generate potential fault assumptions and root cause paths. Furthermore, this paper introduces a reinforcement learning-based policy-guided recovery planner, which is driven by the remediation strategy generated by LLM to support dynamic decision-making and adaptive debugging in the cloud environment. Compared with the existing rule engine or traditional log analysis system, the proposed model has stronger semantic understanding ability, continuous learning ability and heterogeneous environment adaptability. Experiments on the cloud platform log dataset show that LLM-ID improves the fault location accuracy by 16.2%, which is significantly better than the current mainstream methods",
      "authors": [
        "Ji, Cheng",
        "Luo, Huaiying"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17900v1",
        "Other Formats": "https://arxiv.org/format/2506.17900",
        "TeX Source": "https://arxiv.org/src/2506.17900",
        "View PDF": "https://arxiv.org/pdf/2506.17900"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Distributed, Parallel, and Cluster Computing (cs.DC)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 04:58:37 UTC (415 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Leveraging Large Language Model for Intelligent Log Processing and Autonomous Debugging in Cloud AI Platforms",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18069",
      "abstract": "We developed a proof-of-concept method for the automatic analysis of the structure and content of incunabula pages. A custom dataset comprising 500 annotated pages from five different incunabula was created using resources from the Jagiellonian Digital Library. Each page was manually labeled with five predefined classes: Text, Title, Picture, Table, and Handwriting. Additionally, the publicly available DocLayNet dataset was utilized as supplementary training data. To perform object detection, YOLO11n and YOLO11s models were employed and trained using two strategies: a combined dataset (DocLayNet and the custom dataset) and the custom dataset alone. The highest performance (F1 = 0.94) was achieved by the YOLO11n model trained exclusively on the custom data. Optical character recognition was then conducted on regions classified as Text, using both Tesseract and Kraken OCR, with Tesseract demonstrating superior results. Subsequently, image classification was applied to the Picture class using a ResNet18 model, achieving an accuracy of 98.7% across five subclasses: Decorative_letter, Illustration, Other, Stamp, and Wrong_detection. Furthermore, the CLIP model was utilized to generate semantic descriptions of illustrations. The results confirm the potential of machine learning in the analysis of early printed books, while emphasizing the need for further advancements in OCR performance and visual content interpretation.",
      "authors": [
        "Ropel, Klaudia",
        "Kutt, Krzysztof",
        "Miranda, Luiz do Valle",
        "Nalepa, Grzegorz J."
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18069v1",
        "Other Formats": "https://arxiv.org/format/2506.18069",
        "TeX Source": "https://arxiv.org/src/2506.18069",
        "View PDF": "https://arxiv.org/pdf/2506.18069"
      },
      "subjects": [
        "Digital Libraries (cs.DL)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 15:33:20 UTC (2,170 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Unfolding the Past: A Comprehensive Deep Learning Approach to Analyzing Incunabula Pages",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17945",
      "abstract": "Multiple unmanned aerial vehicles (UAVs) play a vital role in monitoring and data collection in wide area environments with harsh conditions. In most scenarios, issues such as real-time data retrieval and real-time UAV positioning are often disregarded, essentially neglecting the communication constraints. In this paper, we comprehensively address both the coverage of the target area and the data transmission capabilities of the flying ad hoc network (FANET). The data throughput of the network is therefore maximized by optimizing the network topology and the UAV trajectories. The resultant optimization problem is effectively solved by the proposed reinforcement learning-based trajectory planning (RL-TP) algorithm and the convex-based topology optimization (C-TOP) algorithm sequentially. The RL-TP optimizes the UAV paths while considering the constraints of FANET. The C-TOP maximizes the data throughput of the network while simultaneously constraining the neighbors and transmit powers of the UAVs, which is shown to be a convex problem that can be efficiently solved in polynomial time. Simulations and field experimental results show that the proposed optimization strategy can effectively plan the UAV trajectories and significantly improve the data throughput of the FANET over the adaptive local minimum spanning tree (A-LMST) and cyclic pruning-assisted power optimization (CPAPO) methods.",
      "authors": [
        "He, Ming",
        "Wang, Peizhao",
        "Chen, Haihua",
        "Sun, Bin",
        "Wang, Hongpeng"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17945v1",
        "Other Formats": "https://arxiv.org/format/2506.17945",
        "TeX Source": "https://arxiv.org/src/2506.17945",
        "View PDF": "https://arxiv.org/pdf/2506.17945"
      },
      "subjects": [
        "Multiagent Systems (cs.MA)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 08:41:27 UTC (18,912 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Optimization of Flying Ad Hoc Network Topology and Collaborative Path Planning for Multiple UAVs",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18100",
      "abstract": "Address Resolution Protocol (ARP) spoofing attacks severely threaten Internet of Things (IoT) networks by allowing attackers to intercept, modify, or block communications. Traditional detection methods are insufficient due to high false positives and poor adaptability. This research proposes a multi-layered machine learning-based framework for intelligently detecting ARP spoofing in IoT networks. Our approach utilizes an ensemble of classifiers organized into multiple layers, each layer optimizing detection accuracy and reducing false alarms. Experimental evaluations demonstrate significant improvements in detection accuracy (up to 97.5\\%), reduced false positive rates (less than 2\\%), and faster detection time compared to existing methods. Our key contributions include introducing multi-layer ensemble classifiers specifically tuned for IoT networks, systematically addressing dataset imbalance problems, introducing a dynamic feedback mechanism for classifier retraining, and validating practical applicability through extensive simulations. This research enhances security management in IoT deployments, providing robust defenses against ARP spoofing attacks and improving reliability and trust in IoT environments.",
      "authors": [
        "Ahmad, Taimoor",
        "Ali, Anas"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18100v1",
        "Other Formats": "https://arxiv.org/format/2506.18100",
        "TeX Source": "https://arxiv.org/src/2506.18100",
        "View PDF": "https://arxiv.org/pdf/2506.18100"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 17:10:32 UTC (589 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Optimizing Resource Allocation and Energy Efficiency in Federated Fog Computing for IoT",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17929",
      "abstract": "Supporting decision-making has long been a central vision in the field of spatio-temporal intelligence. While prior work has improved the timeliness and accuracy of spatio-temporal forecasting, converting these forecasts into actionable strategies remains a key challenge. A main limitation is the decoupling of the prediction and the downstream decision phases, which can significantly degrade the downstream efficiency. For example, in emergency response, the priority is successful resource allocation and intervention, not just incident prediction. To this end, it is essential to propose an Adaptive Spatio-Temporal Early Decision model (ASTER) that reforms the forecasting paradigm from event anticipation to actionable decision support. This framework ensures that information is directly used for decision-making, thereby maximizing overall effectiveness. Specifically, ASTER introduces a new Resource-aware Spatio-Temporal interaction module (RaST) that adaptively captures long- and short-term dependencies under dynamic resource conditions, producing context-aware spatiotemporal representations. To directly generate actionable decisions, we further design a Preference-oriented decision agent (Poda) based on multi-objective reinforcement learning, which transforms predictive signals into resource-efficient intervention strategies by deriving optimal actions under specific preferences and dynamic constraints. Experimental results on four benchmark datasets demonstrate the state-of-the-art performance of ASTER in improving both early prediction accuracy and resource allocation outcomes across six downstream metrics.",
      "authors": [
        "Chen, Shulun",
        "Shao, Wei",
        "Salim, Flora D.",
        "Xue, Hao"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17929v1",
        "Other Formats": "https://arxiv.org/format/2506.17929",
        "TeX Source": "https://arxiv.org/src/2506.17929",
        "View PDF": "https://arxiv.org/pdf/2506.17929"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 07:49:37 UTC (542 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "ASTER: Adaptive Spatio-Temporal Early Decision Model for Dynamic Resource Allocation",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18124",
      "abstract": "Multiobject tracking (MOT) is an important task in applications including autonomous driving, ocean sciences, and aerospace surveillance. Traditional MOT methods are model-based and combine sequential Bayesian estimation with data association and an object birth model. More recent methods are fully data-driven and rely on the training of neural networks. Both approaches offer distinct advantages in specific settings. In particular, model-based methods are generally applicable across a wide range of scenarios, whereas data-driven MOT achieves superior performance in scenarios where abundant labeled data for training is available. A natural thought is whether a general framework can integrate the two approaches. This paper introduces a hybrid method that utilizes neural networks to enhance specific aspects of the statistical model in Bayesian MOT that have been identified as overly simplistic. By doing so, the performance of the prediction and update steps of Bayesian MOT is improved. To ensure tractable computation, our framework uses belief propagation to avoid high-dimensional operations combined with sequential Monte Carlo methods to perform low-dimensional operations efficiently. The resulting method combines the flexibility and robustness of model-based approaches with the capability to learn complex information from data of neural networks. We evaluate the performance of the proposed method based on the nuScenes autonomous driving dataset and demonstrate that it has state-of-the-art performance",
      "authors": [
        "Wei, Shaoxiu",
        "Liang, Mingchao",
        "Meyer, Florian"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18124v1",
        "Other Formats": "https://arxiv.org/format/2506.18124",
        "TeX Source": "https://arxiv.org/src/2506.18124",
        "View PDF": "https://arxiv.org/pdf/2506.18124"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Signal Processing (eess.SP)",
        "Machine Learning (stat.ML)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 18:15:08 UTC (11,006 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Bayesian Multiobject Tracking With Neural-Enhanced Motion and Measurement Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17874",
      "abstract": "In many real-world applications, ensuring the robustness and stability of deep neural networks (DNNs) is crucial, particularly for image classification tasks that encounter various input perturbations. While data augmentation techniques have been widely adopted to enhance the resilience of a trained model against such perturbations, there remains significant room for improvement in robustness against corrupted data and adversarial attacks simultaneously. To address this challenge, we introduce DRO-Augment, a novel framework that integrates Wasserstein Distributionally Robust Optimization (W-DRO) with various data augmentation strategies to improve the robustness of the models significantly across a broad spectrum of corruptions. Our method outperforms existing augmentation methods under severe data perturbations and adversarial attack scenarios while maintaining the accuracy on the clean datasets on a range of benchmark datasets, including but not limited to CIFAR-10-C, CIFAR-100-C, MNIST, and Fashion-MNIST. On the theoretical side, we establish novel generalization error bounds for neural networks trained using a computationally efficient, variation-regularized loss function closely related to the W-DRO problem.",
      "authors": [
        "Hu, Jiaming",
        "Mukherjee, Debarghya",
        "Paschalidis, Ioannis Ch."
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17874v1",
        "Other Formats": "https://arxiv.org/format/2506.17874",
        "TeX Source": "https://arxiv.org/src/2506.17874",
        "View PDF": "https://arxiv.org/pdf/2506.17874"
      },
      "subjects": [
        "Machine Learning (stat.ML)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 02:18:03 UTC (741 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "DRO-Augment Framework: Robustness by Synergizing Wasserstein Distributionally Robust Optimization and Data Augmentation",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17911",
      "abstract": "Routing Protocol for Low-Power and Lossy Networks (RPL) is an energy-efficient routing solution for IPv6 over Low-Power Wireless Personal Area Networks (6LoWPAN), recommended for resource-constrained devices. While RPL offers significant benefits, its security vulnerabilities pose challenges, particularly due to unauthenticated control messages used to establish and maintain routing information. These messages are susceptible to manipulation, enabling malicious nodes to inject false routing data. A notable security concern is the Routing Table Falsification (RTF) attack, where attackers forge Destination Advertisement Object (DAO) messages to promote fake routes via a parent nodes routing table. Experimental results indicate that RTF attacks significantly reduce packet delivery ratio, increase end-to-end delay, and leverage power consumption. Currently, no effective countermeasures exist in the literature, reinforcing the need for a security solution to prevent network disruption and protect user applications. This paper introduces a Lightweight Security Solution against Routing Table Falsification Attack (LiSec-RTF), leveraging Physical Unclonable Functions (PUFs) to generate unique authentication codes, termed Licenses. LiSec-RTF mitigates RTF attack impact while considering the resource limitations of 6LoWPAN devices in both static and mobile scenarios. Our testbed experiments indicate that LiSec-RTF significantly improves network performance compared to standard RPL under RTF attacks, thereby ensuring reliable and efficient operation.",
      "authors": [
        "Goel, Shefali",
        "Verma, Vinod Kumar",
        "Verma, Abhishek"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17911v1",
        "Other Formats": "https://arxiv.org/format/2506.17911",
        "TeX Source": "https://arxiv.org/src/2506.17911",
        "View PDF": "https://arxiv.org/pdf/2506.17911"
      },
      "subjects": [
        "Networking and Internet Architecture (cs.NI)",
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 06:23:06 UTC (1,025 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "LiSec-RTF: Reinforcing RPL Resilience Against Routing Table Falsification Attack in 6LoWPAN",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2408.15802",
      "abstract": "Medical image classification plays a crucial role in clinical decision-making, yet most models are constrained to a fixed set of predefined classes, limiting their adaptability to new conditions. Contrastive Language-Image Pretraining (CLIP) offers a promising solution by enabling zero-shot classification through multimodal large-scale pretraining. However, while CLIP effectively captures global image content, radiology requires a more localized focus on specific pathology regions to enhance both interpretability and diagnostic accuracy. To address this, we explore the potential of incorporating visual cues into zero-shot classification, embedding visual markers, such as arrows, bounding boxes, and circles, directly into radiological images to guide model attention. Evaluating across four public chest X-ray datasets, we demonstrate that visual markers improve AUROC by up to 0.185, highlighting their effectiveness in enhancing classification performance. Furthermore, attention map analysis confirms that visual cues help models focus on clinically relevant areas, leading to more interpretable predictions.To support further research, we use public datasets and provide our codebase and preprocessing pipeline under https://github.com/MIC-DKFZ/VPE-in-Radiology, serving as a reference point for future work on localized classification in medical imaging.",
      "authors": [
        "Denner, Stefan",
        "Bujotzek, Markus",
        "Bounias, Dimitrios",
        "Zimmerer, David",
        "Stock, Raphael",
        "Maier-Hein, Klaus"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2408.15802v3",
        "Other Formats": "https://arxiv.org/format/2408.15802",
        "TeX Source": "https://arxiv.org/src/2408.15802",
        "View PDF": "https://arxiv.org/pdf/2408.15802"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 28 Aug 2024 13:53:27 UTC (33,326 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 10 Feb 2025 15:12:04 UTC (12,279 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Sun, 22 Jun 2025 14:06:41 UTC (13,332 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/08/28",
      "title": "Visual Prompt Engineering for Vision Language Models in Radiology",
      "tasks": [
        "Classification",
        "image-classification",
        "Image Classification",
        "Medical Image Analysis",
        "Medical Image Classification",
        "Prompt Engineering",
        "Zero-Shot Learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.22518",
      "abstract": "We introduce IGNIS, a deep-learning framework for constrained parameter estimation in Archimedean copulas with natural domain $\\theta \\geq 1$. While illustrated here on four families (Gumbel, Joe and the novel A1/A2 copulas), IGNIS is readily applicable to any one-parameter Archimedean model with $\\theta \\geq 1$. Classical estimators (Method of Moments (MoM), Maximum Likelihood Estimation (MLE), Maximum Pseudo-Likelihood (MPL)) break down on A1/A2 due to non-monotonic dependence mappings, steep likelihood gradients and the need for custom constraint handling. IGNIS sidesteps these issues by learning a direct mapping from four summary statistics (Kendall's $\\tau$, Spearman's $\\rho$, empirical 0.95 tail-dependence and Pearson $r$) plus a one-hot family indicator to $\\theta$, ending in a softplus + 1 output layer that automatically enforces $\\hat{\\theta} \\geq 1$. Trained on 500 simulated $\\theta$ values per family (10000 observations each), IGNIS outperforms the Method of Moments in extensive simulations and delivers accurate, stable estimates on real-world AAPL-MSFT returns and CDC diabetes data. Our results demonstrate a unified, constraint-aware neural estimator for modern copula-based dependence modeling, easily extendable to any copula family respecting $\\theta \\geq 1$.",
      "authors": [
        "Aich, Agnideep",
        "Aich, Ashit Baran",
        "Wade, Bruce"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2505.22518v2",
        "Other Formats": "https://arxiv.org/format/2505.22518",
        "TeX Source": "https://arxiv.org/src/2505.22518",
        "View PDF": "https://arxiv.org/pdf/2505.22518"
      },
      "subjects": [
        "Machine Learning (stat.ML)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 28 May 2025 16:04:17 UTC (66 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 22 Jun 2025 00:56:05 UTC (993 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/28",
      "title": "IGNIS: A Robust Neural Network Framework for Constrained Parameter Estimation in Archimedean Copulas",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17877",
      "abstract": "Networked mission-critical applications (e.g., avionic control and industrial automation systems) require deterministic packet transmissions to support a range of sensing and control tasks with stringent timing constraints. While specialized network infrastructure (e.g., time-sensitive networking (TSN) switches) provides deterministic data transport across the network, achieving strict end-to-end timing guarantees requires equally capable end devices to support deterministic traffic. These end devices, however, often employ general-purpose computing platforms like standard PCs, which lack native support for deterministic traffic and suffer from unpredictable delays introduced by their software stack and system architecture. Although specialized NICs with hardware scheduling offload can mitigate this problem, the limited compatibility hinders their widespread adoption, particularly for cost-sensitive applications or in legacy devices. To fill this gap, this paper proposes a novel software-based driver model, namely KeepON, to enable the support of deterministic packet transmissions on end devices equipped with standard NICs. The key idea of KeepON is to have the NIC keep on transmitting fixed-size data chunks as placeholders, thereby maintaining a predictable temporal transmission pattern. The real-time packets generated by the mission-critical application(s) will then be precisely inserted into this stream by replacing placeholders at the designated position to ensure their accurate transmission time. We implement and evaluate KeepON by modifying the network driver on a Raspberry Pi using its standard NIC. Our experiments demonstrate that KeepON can achieve x162 times scheduling accuracy comparable to its default driver, and x2.6 times compared to hardware-based solution, thus enabling precise timing control on standard commodity hardware.",
      "authors": [
        "Xue, Chuanyu",
        "Zhang, Tianyu",
        "Loveless, Andrew",
        "Han, Song"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17877v1",
        "Other Formats": "https://arxiv.org/format/2506.17877",
        "TeX Source": "https://arxiv.org/src/2506.17877",
        "View PDF": "https://arxiv.org/pdf/2506.17877"
      },
      "subjects": [
        "Networking and Internet Architecture (cs.NI)",
        "Operating Systems (cs.OS)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 02:37:39 UTC (4,769 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Supporting Deterministic Traffic on Standard NICs",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2406.00958",
      "abstract": "Resolving conflicts is critical for improving the reliability of multi-view classification. While prior work focuses on learning consistent and informative representations across views, it often assumes perfect alignment and equal importance of all views, an assumption rarely met in real-world scenarios, as some views may express distinct information. To address this, we develop a computational trust-based discounting method that enhances the Evidential Multi-view framework by accounting for the instance-wise reliability of each view through a probability-sensitive trust mechanism. We evaluate our method on six real-world datasets using Top-1 Accuracy, Fleiss' Kappa, and a new metric, Multi-View Agreement with Ground Truth, to assess prediction reliability. We also assess the effectiveness of uncertainty in indicating prediction correctness via AUROC. Additionally, we test the scalability of our method through end-to-end training on a large-scale dataset. The experimental results show that computational trust can effectively resolve conflicts, paving the way for more reliable multi-view classification models in real-world applications. Codes available at: https://github.com/OverfitFlow/Trust4Conflict",
      "authors": [
        "Lu, Jueqing",
        "Buntine, Wray",
        "Qi, Yuanyuan",
        "Dipnall, Joanna",
        "Gabbe, Belinda",
        "Du, Lan"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2406.00958v4",
        "Other Formats": "https://arxiv.org/format/2406.00958",
        "TeX Source": "https://arxiv.org/src/2406.00958",
        "View PDF": "https://arxiv.org/pdf/2406.00958"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 3 Jun 2024 03:22:18 UTC (5,050 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 9 Mar 2025 12:32:00 UTC (15,418 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Mon, 26 May 2025 07:53:55 UTC (15,427 KB)",
          "link": "/",
          "version": "[v3]"
        },
        {
          "details": "Sun, 22 Jun 2025 03:01:58 UTC (7,542 KB)",
          "version": "[v4]"
        }
      ],
      "submitted_date": "2024/06/03",
      "title": "Navigating Conflicting Views: Harnessing Trust for Learning",
      "tasks": [],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18156",
      "abstract": "We investigate whether Large Language Models (LLMs) exhibit human-like cognitive patterns under four established frameworks from psychology: Thematic Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and Cognitive Dissonance. We evaluated several proprietary and open-source models using structured prompts and automated scoring. Our findings reveal that these models often produce coherent narratives, show susceptibility to positive framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and demonstrate self-contradictions tempered by extensive rationalization. Such behaviors mirror human cognitive tendencies yet are shaped by their training data and alignment methods. We discuss the implications for AI transparency, ethical deployment, and future work that bridges cognitive psychology and AI safety",
      "authors": [
        "Kundu, Akash",
        "Goswami, Rishika"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18156v1",
        "Other Formats": "https://arxiv.org/format/2506.18156",
        "TeX Source": "https://arxiv.org/src/2506.18156",
        "View PDF": "https://arxiv.org/pdf/2506.18156"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 19:58:19 UTC (313 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18104",
      "abstract": "In this paper, we argue that viewing VICReg-a popular self-supervised learning (SSL) method--through the lens of spectral embedding reveals a potential source of sub-optimality: it may struggle to generalize robustly to unseen data due to overreliance on the training data. This observation invites a closer look at how well this method achieves its goal of producing meaningful representations of images outside of the training set as well. Here, we investigate this issue and introduce SAG-VICReg (Stable and Generalizable VICReg), a method that builds on VICReg by incorporating new training techniques. These enhancements improve the model's ability to capture global semantics within the data and strengthen the generalization capabilities. Experiments demonstrate that SAG-VICReg effectively addresses the generalization challenge while matching or surpassing diverse state-of-the-art SSL baselines. Notably, our method exhibits superior performance on metrics designed to evaluate global semantic understanding, while simultaneously maintaining competitive results on local evaluation metrics. Furthermore, we propose a new standalone evaluation metric for embeddings that complements the standard evaluation methods and accounts for the global data structure without requiring labels--a key issue when tagged data is scarce or not available.",
      "authors": [
        "Simai, Idan",
        "Talmon, Ronen",
        "Shaham, Uri"
      ],
      "last_revised_date": "2025/06/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18104v1",
        "Other Formats": "https://arxiv.org/format/2506.18104",
        "TeX Source": "https://arxiv.org/src/2506.18104",
        "View PDF": "https://arxiv.org/pdf/2506.18104"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 22 Jun 2025 17:17:02 UTC (1,889 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/22",
      "title": "Enhancing VICReg: Random-Walk Pairing for Improved Generalization and Better Global Semantics Capturing",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17673",
      "abstract": "Sparse Autoencoders (SAEs) have emerged as a promising solution for decomposing large language model representations into interpretable features. However, Paulo and Belrose (2025) have highlighted instability across different initialization seeds, and Heap et al. (2025) have pointed out that SAEs may not capture model-internal features. These problems likely stem from training SAEs on external datasets - either collected from the Web or generated by another model - which may contain out-of-distribution (OOD) data beyond the model's generalisation capabilities. This can result in hallucinated SAE features, which we term \"Fake Features\", that misrepresent the model's internal activations. To address these issues, we propose FaithfulSAE, a method that trains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we demonstrate that training SAEs on less-OOD instruction datasets results in SAEs being more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained on web-based datasets in the SAE probing task and exhibit a lower Fake Feature Ratio in 5 out of 7 models. Overall, our approach eliminates the dependency on external datasets, advancing interpretability by better capturing model-internal features while highlighting the often neglected importance of SAE training datasets.",
      "authors": [
        "Cho, Seonglae",
        "Oh, Harryn",
        "Lee, Donghyun",
        "Vieira, Luis Eduardo Rodrigues",
        "Bermingham, Andrew",
        "Sayed, Ziad El"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17673v1",
        "Other Formats": "https://arxiv.org/format/2506.17673",
        "TeX Source": "https://arxiv.org/src/2506.17673",
        "View PDF": "https://arxiv.org/pdf/2506.17673"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 10:18:25 UTC (3,910 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17525",
      "abstract": "Our quality audit for three widely used public multilingual speech datasets - Mozilla Common Voice 17.0, FLEURS, and VoxPopuli - shows that in some languages, these datasets suffer from significant quality issues. We believe addressing these issues will make these datasets more useful as training and evaluation sets, and improve downstream models. We divide these quality issues into two categories: micro-level and macro-level. We find that macro-level issues are more prevalent in less institutionalized, often under-resourced languages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that highlights the need for proactive language planning (e.g. orthography prescriptions, dialect boundary definition) and enhanced data quality control in the process of Automatic Speech Recognition (ASR) dataset creation. We conclude by proposing guidelines and recommendations to mitigate these issues in future dataset development, emphasizing the importance of sociolinguistic awareness in creating robust and reliable speech data resources.",
      "authors": [
        "Lau, Mingfei",
        "Chen, Qian",
        "Fang, Yeming",
        "Xu, Tingting",
        "Chen, Tongzhou",
        "Golik, Pavel"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17525v1",
        "Other Formats": "https://arxiv.org/format/2506.17525",
        "TeX Source": "https://arxiv.org/src/2506.17525",
        "View PDF": "https://arxiv.org/pdf/2506.17525"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 00:34:18 UTC (557 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17807",
      "abstract": "Adapting neural networks to new tasks typically requires task-specific fine-tuning, which is time-consuming and reliant on labeled data. We explore a generative alternative that produces task-specific parameters directly from task identity, eliminating the need for task-specific training. To this end, we propose using diffusion models to learn the underlying structure of effective task-specific parameter space and synthesize parameters on demand. Once trained, the task-conditioned diffusion model can generate specialized weights directly from task identifiers. We evaluate this approach across three scenarios: generating parameters for a single seen task, for multiple seen tasks, and for entirely unseen tasks. Experiments show that diffusion models can generate accurate task-specific parameters and support multi-task interpolation when parameter subspaces are well-structured, but fail to generalize to unseen tasks, highlighting both the potential and limitations of this generative solution.",
      "authors": [
        "Zhang, Lijun",
        "Liu, Xiao",
        "Guan, Hui"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17807v1",
        "Other Formats": "https://arxiv.org/format/2506.17807",
        "TeX Source": "https://arxiv.org/src/2506.17807",
        "View PDF": "https://arxiv.org/pdf/2506.17807"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 20:30:17 UTC (387 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Reimagining Parameter Space Exploration with Diffusion Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17747",
      "abstract": "Accurate geological modeling is critical for reservoir characterization, yet traditional methods struggle with complex subsurface heterogeneity, and they have problems with conditioning to observed data. This study introduces Pix2Geomodel, a novel conditional generative adversarial network (cGAN) framework based on Pix2Pix, designed to predict reservoir properties (facies, porosity, permeability, and water saturation) from the Rotliegend reservoir of the Groningen gas field. Utilizing a 7.6 million-cell dataset from the Nederlandse Aardolie Maatschappij, accessed via EPOS-NL, the methodology included data preprocessing, augmentation to generate 2,350 images per property, and training with a U-Net generator and PatchGAN discriminator over 19,000 steps. Evaluation metrics include pixel accuracy (PA), mean intersection over union (mIoU), frequency weighted intersection over union (FWIoU), and visualizations assessed performance in masked property prediction and property-to-property translation tasks. Results demonstrated high accuracy for facies (PA 0.88, FWIoU 0.85) and water saturation (PA 0.96, FWIoU 0.95), with moderate success for porosity (PA 0.70, FWIoU 0.55) and permeability (PA 0.74, FWIoU 0.60), and robust translation performance (e.g., facies-to-facies PA 0.98, FWIoU 0.97). The framework captured spatial variability and geological realism, as validated by variogram analysis, and calculated the training loss curves for the generator and discriminator for each property. Compared to traditional methods, Pix2Geomodel offers enhanced fidelity in direct property mapping. Limitations include challenges with microstructural variability and 2D constraints, suggesting future integration of multi-modal data and 3D modeling (Pix2Geomodel v2.0). This study advances the application of generative AI in geoscience, supporting improved reservoir management and open science initiatives.",
      "authors": [
        "Al-Fakih, Abdulrahman",
        "Koeshidayatullah, Ardiansyah",
        "Saraih, Nabil A.",
        "Mukerji, Tapan",
        "Kanfar, Rayan",
        "Alali, Abdulmohsen",
        "Kaka, SanLinn I."
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17747v1",
        "Other Formats": "https://arxiv.org/format/2506.17747",
        "TeX Source": "https://arxiv.org/src/2506.17747",
        "View PDF": "https://arxiv.org/pdf/2506.17747"
      },
      "subjects": [
        "Geophysics (physics.geo-ph)",
        "Computational Engineering, Finance, and Science (cs.CE)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)",
        "Neural and Evolutionary Computing (cs.NE)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 15:58:27 UTC (10,646 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Pix2Geomodel: A Next-Generation Reservoir Geomodeling with Property-to-Property Translation",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17693",
      "abstract": "Stance detection, which aims to identify public opinion towards specific targets using social media data, is an important yet challenging task. With the increasing number of online debates among social media users, conversational stance detection has become a crucial research area. However, existing conversational stance detection datasets are restricted to a limited set of specific targets, which constrains the effectiveness of stance detection models when encountering a large number of unseen targets in real-world applications. To bridge this gap, we manually curate a large-scale, high-quality zero-shot conversational stance detection dataset, named ZS-CSD, comprising 280 targets across two distinct target types. Leveraging the ZS-CSD dataset, we propose SITPCL, a speaker interaction and target-aware prototypical contrastive learning model, and establish the benchmark performance in the zero-shot setting. Experimental results demonstrate that our proposed SITPCL model achieves state-of-the-art performance in zero-shot conversational stance detection. Notably, the SITPCL model attains only an F1-macro score of 43.81%, highlighting the persistent challenges in zero-shot conversational stance detection.",
      "authors": [
        "Ding, Yuzhe",
        "He, Kang",
        "Li, Bobo",
        "Zheng, Li",
        "He, Haijun",
        "Li, Fei",
        "Teng, Chong",
        "Ji, Donghong"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17693v1",
        "Other Formats": "https://arxiv.org/format/2506.17693",
        "TeX Source": "https://arxiv.org/src/2506.17693",
        "View PDF": "https://arxiv.org/pdf/2506.17693"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 12:02:06 UTC (1,250 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Zero-Shot Conversational Stance Detection: Dataset and Approaches",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.12524",
      "abstract": "Event-based eye tracking holds significant promise for fine-grained cognitive state inference, offering high temporal resolution and robustness to motion artifacts, critical features for decoding subtle mental states such as attention, confusion, or fatigue. In this work, we introduce a model-agnostic, inference-time refinement framework designed to enhance the output of existing event-based gaze estimation models without modifying their architecture or requiring retraining. Our method comprises two key post-processing modules: (i) Motion-Aware Median Filtering, which suppresses blink-induced spikes while preserving natural gaze dynamics, and (ii) Optical Flow-Based Local Refinement, which aligns gaze predictions with cumulative event motion to reduce spatial jitter and temporal discontinuities. To complement traditional spatial accuracy metrics, we propose a novel Jitter Metric that captures the temporal smoothness of predicted gaze trajectories based on velocity regularity and local signal complexity. Together, these contributions significantly improve the consistency of event-based gaze signals, making them better suited for downstream tasks such as micro-expression analysis and mind-state decoding. Our results demonstrate consistent improvements across multiple baseline models on controlled datasets, laying the groundwork for future integration with multimodal affect recognition systems in real-world environments.",
      "authors": [
        "Bandara, Nuwan",
        "Kandappu, Thivya",
        "Misra, Archan"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.12524v2",
        "Other Formats": "https://arxiv.org/format/2506.12524",
        "TeX Source": "https://arxiv.org/src/2506.12524",
        "View PDF": "https://arxiv.org/pdf/2506.12524"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Human-Computer Interaction (cs.HC)",
        "Machine Learning (cs.LG)",
        "Image and Video Processing (eess.IV)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 14 Jun 2025 14:48:11 UTC (1,604 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 11:04:09 UTC (1,604 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/06/14",
      "title": "Inference-Time Gaze Refinement for Micro-Expression Recognition: Enhancing Event-Based Eye Tracking with Motion-Aware Post-Processing",
      "repo_urls": [
        "https://github.com/eye-tracking-for-physiological-sensing/eyelorin"
      ],
      "tasks": [
        "Gaze Estimation",
        "Micro Expression Recognition",
        "Micro-Expression Recognition",
        "Optical Flow Estimation"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2504.06327",
      "abstract": "Kolmogorov-Arnold Networks (KANs) have gained attention as an alternative to traditional multilayer perceptrons (MLPs) for deep learning applications in computational physics, particularly for solving inverse problems with sparse data, as exemplified by the physics-informed Kolmogorov-Arnold network (PIKAN). However, the capability of KANs to simultaneously solve inverse problems over multiple irregular geometries within a single training run remains unexplored. To address this gap, we introduce the physics-informed Kolmogorov-Arnold PointNet (PI-KAN-PointNet), in which shared KANs are integrated into the PointNet architecture to capture the geometric features of computational domains. The loss function comprises the squared residuals of the governing equations, computed via automatic differentiation, along with sparse observations and partially known boundary conditions. We construct shared KANs using Jacobi polynomials and investigate their performance by considering Jacobi polynomials of different degrees and types in terms of both computational cost and prediction accuracy. As a benchmark test case, we consider natural convection in a square enclosure with a cylinder, where the cylinder's shape varies across a dataset of 135 geometries. PI-KAN-PointNet offers two main advantages. First, it overcomes the limitation of current PIKANs, which are restricted to solving only a single computational domain per training run, thereby reducing computational costs. Second, when comparing the performance of PI-KAN-PointNet with that of the physics-informed PointNet using MLPs, we observe that, with approximately the same number of trainable parameters and comparable computational cost in terms of the number of epochs, training time per epoch, and memory usage, PI-KAN-PointNet yields more accurate predictions, particularly for values on unknown boundary conditions involving nonsmooth geometries.",
      "authors": [
        "Kashefi, Ali",
        "Mukerji, Tapan"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "Other Formats": "https://arxiv.org/format/2504.06327",
        "TeX Source": "https://arxiv.org/src/2504.06327",
        "View PDF": "https://arxiv.org/pdf/2504.06327"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Fluid Dynamics (physics.flu-dyn)"
      ],
      "submission_historys": [
        {
          "details": "Tue, 8 Apr 2025 12:31:57 UTC (5,923 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 19:18:19 UTC (5,793 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/04/08",
      "title": "Physics-informed KAN PointNet: Deep learning for simultaneous solutions to inverse problems in incompressible flow on numerous irregular geometries",
      "tasks": [
        "Kolmogorov-Arnold Networks"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17561",
      "abstract": "Recent studies on Vision-Language-Action (VLA) models have shifted from the end-to-end action-generation paradigm toward a pipeline involving task planning followed by action generation, demonstrating improved performance on various complex, long-horizon manipulation tasks. However, existing approaches vary significantly in terms of network architectures, planning paradigms, representations, and training data sources, making it challenging for researchers to identify the precise sources of performance gains and components to be further improved. To systematically investigate the impacts of different planning paradigms and representations isolating from network architectures and training data, in this paper, we introduce VLA-OS, a unified VLA architecture series capable of various task planning paradigms, and design a comprehensive suite of controlled experiments across diverse object categories (rigid and deformable), visual modalities (2D and 3D), environments (simulation and real-world), and end-effectors (grippers and dexterous hands). Our results demonstrate that: 1) visually grounded planning representations are generally better than language planning representations; 2) the Hierarchical-VLA paradigm generally achieves superior or comparable performance than other paradigms on task performance, pretraining, generalization ability, scalability, and continual learning ability, albeit at the cost of slower training and inference speeds.",
      "authors": [
        "Gao, Chongkai",
        "Liu, Zixuan",
        "Chi, Zhenghao",
        "Huang, Junshan",
        "Fei, Xin",
        "Hou, Yiwen",
        "Zhang, Yuxuan",
        "Lin, Yudi",
        "Fang, Zhirui",
        "Jiang, Zeyu",
        "Shao, Lin"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17561v1",
        "Other Formats": "https://arxiv.org/format/2506.17561",
        "TeX Source": "https://arxiv.org/src/2506.17561",
        "View PDF": "https://arxiv.org/pdf/2506.17561"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 03:07:48 UTC (18,403 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models",
      "datasets": [
        {
          "dataset_name": "Linslab/VLA-OS-Dataset",
          "downloads": "315",
          "likes": "0",
          "link": "https://huggingface.co/datasets/Linslab/VLA-OS-Dataset"
        }
      ],
      "models": [
        {
          "model_path": "Linslab/VLA-OS",
          "downloads": "0",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/Linslab/VLA-OS"
        }
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17811",
      "abstract": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in visuomotor control, yet ensuring their robustness in unstructured real-world environments remains a persistent challenge. In this paper, we investigate test-time scaling through the lens of sampling and verification as means to enhance the robustness and generalization of VLAs. We first demonstrate that the relationship between action error and the number of generated samples follows an exponentiated power law across a range of VLAs, indicating the existence of inference-time scaling laws. Building on these insights, we introduce RoboMonkey, a test-time scaling framework for VLAs. At deployment, RoboMonkey samples a small set of actions from a VLA, applies Gaussian perturbation and majority voting to construct an action proposal distribution, and then uses a Vision Language Model (VLM)-based verifier to select the optimal action. We propose a synthetic data generation pipeline for training such VLM-based action verifiers, and demonstrate that scaling the synthetic dataset consistently improves verification and downstream accuracy. Through extensive simulated and hardware experiments, we show that pairing existing VLAs with RoboMonkey yields significant performance gains, achieving a 25% absolute improvement on out-of-distribution tasks and 8% on in-distribution tasks. Additionally, when adapting to new robot setups, we show that fine-tuning both VLAs and action verifiers yields a 7% performance increase compared to fine-tuning VLAs alone.",
      "authors": [
        "Kwok, Jacky",
        "Agia, Christopher",
        "Sinha, Rohan",
        "Foutter, Matt",
        "Li, Shulu",
        "Stoica, Ion",
        "Mirhoseini, Azalia",
        "Pavone, Marco"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17811v1",
        "Other Formats": "https://arxiv.org/format/2506.17811",
        "TeX Source": "https://arxiv.org/src/2506.17811",
        "View PDF": "https://arxiv.org/pdf/2506.17811"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Artificial Intelligence (cs.AI)",
        "Systems and Control (eess.SY)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 20:56:17 UTC (8,093 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17551",
      "abstract": "With the rapid adoption of large language models (LLMs) in recommendation systems, the computational and communication bottlenecks caused by their massive parameter sizes and large data volumes have become increasingly prominent. This paper systematically investigates two classes of optimization methods-model parallelism and data parallelism-for distributed training of LLMs in recommendation scenarios. For model parallelism, we implement both tensor parallelism and pipeline parallelism, and introduce an adaptive load-balancing mechanism to reduce cross-device communication overhead. For data parallelism, we compare synchronous and asynchronous modes, combining gradient compression and sparsification techniques with an efficient aggregation communication framework to significantly improve bandwidth utilization. Experiments conducted on a real-world recommendation dataset in a simulated service environment demonstrate that our proposed hybrid parallelism scheme increases training throughput by over 30% and improves resource utilization by approximately 20% compared to traditional single-mode parallelism, while maintaining strong scalability and robustness. Finally, we discuss trade-offs among different parallel strategies in online deployment and outline future directions involving heterogeneous hardware integration and automated scheduling technologies.",
      "authors": [
        "Yang, Haowei",
        "Tian, Yu",
        "Yang, Zhongheng",
        "Wang, Zhao",
        "Zhou, Chengrui",
        "Li, Dannier"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17551",
        "View PDF": "https://arxiv.org/pdf/2506.17551"
      },
      "subjects": [
        "Distributed, Parallel, and Cluster Computing (cs.DC)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 02:37:25 UTC (1,385 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Research on Model Parallelism and Data Parallelism Optimization Methods in Large Language Model-Based Recommendation Systems",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17692",
      "abstract": "Knowledge-intensive multi-hop question answering (QA) tasks, which require integrating evidence from multiple sources to address complex queries, often necessitate multiple rounds of retrieval and iterative generation by large language models (LLMs). However, incorporating many documents and extended contexts poses challenges -such as hallucinations and semantic drift-for lightweight LLMs with fewer parameters. This work proposes a novel framework called DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions into logically coherent subquestions to form a hallucination-free reasoning chain. It then iteratively refines these subquestions through context-aware rewriting to generate effective query formulations. For retrieval, we introduce a lightweight discriminative keyword extraction module that leverages extracted keywords to achieve targeted, precise document recall with relatively low computational overhead. Extensive experiments on three multi-hop QA datasets demonstrate that DEC performs on par with or surpasses state-of-the-art benchmarks while significantly reducing token consumption. Notably, our approach attains state-of-the-art results on models with 8B parameters, showcasing its effectiveness in various scenarios, particularly in resource-constrained environments.",
      "authors": [
        "Ji, Binquan",
        "Luo, Haibo",
        "Lu, Yifei",
        "Hei, Lei",
        "Wang, Jiaqi",
        "Liao, Tingjing",
        "Wang, Lingyu",
        "Wang, Shichao",
        "Ren, Feiliang"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17692v1",
        "Other Formats": "https://arxiv.org/format/2506.17692",
        "TeX Source": "https://arxiv.org/src/2506.17692",
        "View PDF": "https://arxiv.org/pdf/2506.17692"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 11:55:27 UTC (552 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2308.14555",
      "abstract": "Mathematical methods are developed to characterize the asymptotics of recurrent neural networks (RNN) as the number of hidden units, data samples in the sequence, hidden state updates, and training steps simultaneously grow to infinity. In the case of an RNN with a simplified weight matrix, we prove the convergence of the RNN to the solution of an infinite-dimensional ODE coupled with the fixed point of a random algebraic equation. The analysis requires addressing several challenges which are unique to RNNs. In typical mean-field applications (e.g., feedforward neural networks), discrete updates are of magnitude $\\mathcal{O}(\\frac{1}{N})$ and the number of updates is $\\mathcal{O}(N)$. Therefore, the system can be represented as an Euler approximation of an appropriate ODE/PDE, which it will converge to as $N \\rightarrow \\infty$. However, the RNN hidden layer updates are $\\mathcal{O}(1)$. Therefore, RNNs cannot be represented as a discretization of an ODE/PDE and standard mean-field techniques cannot be applied. Instead, we develop a fixed point analysis for the evolution of the RNN memory states, with convergence estimates in terms of the number of update steps and the number of hidden units. The RNN hidden layer is studied as a function in a Sobolev space, whose evolution is governed by the data sequence (a Markov chain), the parameter updates, and its dependence on the RNN hidden layer at the previous time step. Due to the strong correlation between updates, a Poisson equation must be used to bound the fluctuations of the RNN around its limit equation. These mathematical methods give rise to the neural tangent kernel (NTK) limits for RNNs trained on data sequences as the number of data samples and size of the neural network grow to infinity.",
      "authors": [
        "Lam, Samuel Chun-Hei",
        "Sirignano, Justin",
        "Spiliopoulos, Konstantinos"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2308.14555v3",
        "Other Formats": "https://arxiv.org/format/2308.14555",
        "TeX Source": "https://arxiv.org/src/2308.14555",
        "View PDF": "https://arxiv.org/pdf/2308.14555"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Probability (math.PR)",
        "Machine Learning (stat.ML)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 28 Aug 2023 13:17:39 UTC (386 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Wed, 15 May 2024 15:21:17 UTC (460 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Sat, 21 Jun 2025 16:08:09 UTC (415 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2023/08/28",
      "title": "Kernel Limit of Recurrent Neural Networks Trained on Ergodic Data Sequences",
      "repo_urls": [
        "https://github.com/samuel-chlam/rnn_ergodicity"
      ],
      "tasks": [],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2411.16298",
      "abstract": "This document is an evaluation of the original \"Rank-N-Contrast\" (arXiv:2210.01189v2) paper published in 2023. This evaluation is done for academic purposes. Deep regression models often fail to capture the continuous nature of sample orders, creating fragmented representations and suboptimal performance. To address this, we reproduced the Rank-N-Contrast (RNC) framework, which learns continuous representations by contrasting samples by their rankings in the target space. Our study validates RNC's theoretical and empirical benefits, including improved performance and robustness. We extended the evaluation to an additional regression dataset and conducted robustness tests using a holdout method, where a specific range of continuous data was excluded from the training set. This approach assessed the model's ability to generalize to unseen data and achieve state-of-the-art performance. This replication study validates the original findings and broadens the understanding of RNC's applicability and robustness.",
      "authors": [
        "Six, Valentin",
        "Chidiac, Alexandre",
        "Worlikar, Arkin"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2411.16298v3",
        "Other Formats": "https://arxiv.org/format/2411.16298",
        "TeX Source": "https://arxiv.org/src/2411.16298",
        "View PDF": "https://arxiv.org/pdf/2411.16298"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Machine Learning (stat.ML)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 25 Nov 2024 11:31:53 UTC (1,363 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Tue, 17 Jun 2025 08:42:46 UTC (1,363 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Sat, 21 Jun 2025 22:04:40 UTC (1,363 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/11/25",
      "title": "Evaluating Rank-N-Contrast: Continuous and Robust Representations for Regression",
      "tasks": [
        "regression"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.00305",
      "abstract": "Robots with multi-modal locomotion are an active research field due to their versatility in diverse environments. In this context, additional actuation can provide humanoid robots with aerial capabilities. Flying humanoid robots face challenges in modeling and control, particularly with aerodynamic forces. This paper addresses these challenges from a technological and scientific standpoint. The technological contribution includes the mechanical design of iRonCub-Mk1, a jet-powered humanoid robot, optimized for jet engine integration, and hardware modifications for wind tunnel experiments on humanoid robots for precise aerodynamic forces and surface pressure measurements. The scientific contribution offers a comprehensive approach to model and control aerodynamic forces using classical and learning techniques. Computational Fluid Dynamics (CFD) simulations calculate aerodynamic forces, validated through wind tunnel experiments on iRonCub-Mk1. An automated CFD framework expands the aerodynamic dataset, enabling the training of a Deep Neural Network and a linear regression model. These models are integrated into a simulator for designing aerodynamic-aware controllers, validated through flight simulations and balancing experiments on the iRonCub-Mk1 physical prototype.",
      "authors": [
        "Paolino, Antonello",
        "Nava, Gabriele",
        "Di Natale, Fabio",
        "Bergonti, Fabio",
        "Vanteddu, Punith Reddy",
        "Grassi, Donato",
        "Riccobene, Luca",
        "Zanotti, Alex",
        "Tognaccini, Renato",
        "Iaccarino, Gianluca",
        "Pucci, Daniele"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.00305v2",
        "Other Formats": "https://arxiv.org/format/2506.00305",
        "TeX Source": "https://arxiv.org/src/2506.00305",
        "View PDF": "https://arxiv.org/pdf/2506.00305"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 30 May 2025 23:27:44 UTC (29,479 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 15:50:05 UTC (29,480 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/30",
      "title": "Learning Aerodynamics for the Control of Flying Humanoid Robots",
      "tasks": [],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2408.08590",
      "abstract": "Recent studies on reasoning in language models (LMs) have sparked a debate on whether they can learn systematic inferential principles or merely exploit superficial patterns in the training data. To understand and uncover the mechanisms adopted for formal reasoning in LMs, this paper presents a mechanistic interpretation of syllogistic inference. Specifically, we present a methodology for circuit discovery aimed at interpreting content-independent and formal reasoning mechanisms. Through two distinct intervention methods, we uncover a sufficient and necessary circuit involving middle-term suppression that elucidates how LMs transfer information to derive valid conclusions from premises. Furthermore, we investigate how belief biases manifest in syllogistic inference, finding evidence of partial contamination from additional attention heads responsible for encoding commonsense and contextualized knowledge. Finally, we explore the generalization of the discovered mechanisms across various syllogistic schemes, model sizes and architectures. The identified circuit is sufficient and necessary for syllogistic schemes on which the models achieve high accuracy (>60%), with compatible activation patterns across models of different families. Overall, our findings suggest that LMs learn transferable content-independent reasoning mechanisms, but that, at the same time, such mechanisms do not involve generalizable and abstract logical primitives, being susceptible to contamination by the same world knowledge acquired during pre-training.",
      "authors": [
        "Kim, Geonhee",
        "Valentino, Marco",
        "Freitas, Andr\u00e9"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2408.08590v3",
        "Other Formats": "https://arxiv.org/format/2408.08590",
        "TeX Source": "https://arxiv.org/src/2408.08590",
        "View PDF": "https://arxiv.org/pdf/2408.08590"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 16 Aug 2024 07:47:39 UTC (2,686 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 17 Feb 2025 12:09:50 UTC (3,758 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Sat, 21 Jun 2025 10:50:24 UTC (2,898 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/08/16",
      "title": "Reasoning Circuits in Language Models: A Mechanistic Interpretation of Syllogistic Inference",
      "tasks": [
        "Logical Reasoning",
        "valid",
        "World Knowledge"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2410.18352",
      "abstract": "Foundation models are now a major focus of leading technology organizations due to their ability to generalize across diverse tasks. Existing approaches for adapting foundation models to new applications often rely on Federated Learning (FL) and disclose the foundation model weights to clients when using it to initialize the global model. While these methods ensure client data privacy, they compromise model and information security. In this paper, we introduce Federated Learning Aggregation Biased by a Foundation Model (FedBaF), a novel method for dynamically integrating pre-trained foundation model weights during the FL aggregation phase. Unlike conventional methods, FedBaF preserves the confidentiality of the foundation model while still leveraging its power to train more accurate models, especially in non-IID and adversarial scenarios. Our comprehensive experiments use Pre-ResNet and foundation models like Vision Transformer to demonstrate that FedBaF not only matches, but often surpasses the test accuracy of traditional weight initialization methods by up to 11.4% in IID and up to 15.8% in non-IID settings. Additionally, FedBaF applied to a Transformer-based language model significantly reduced perplexity by up to 39.2%.",
      "authors": [
        "Park, Jong-Ik",
        "Pranav, Srinivasa",
        "Moura, Jos\u00e9 M. F.",
        "Joe-Wong, Carlee"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2410.18352v3",
        "Other Formats": "https://arxiv.org/format/2410.18352",
        "TeX Source": "https://arxiv.org/src/2410.18352",
        "View PDF": "https://arxiv.org/pdf/2410.18352"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Cryptography and Security (cs.CR)",
        "Distributed, Parallel, and Cluster Computing (cs.DC)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 24 Oct 2024 01:14:23 UTC (1,334 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 27 Apr 2025 21:03:12 UTC (1,595 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Sat, 21 Jun 2025 23:50:22 UTC (1,585 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/10/24",
      "title": "FedBaF: Federated Learning Aggregation Biased by a Foundation Model",
      "tasks": [
        "Federated Learning",
        "Language Modeling",
        "Language Modelling"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17632",
      "abstract": "Stereo Depth Estimation (SDE) is essential for scene understanding in vision-based systems like autonomous driving. However, recent studies show that SDE models are vulnerable to adversarial attacks, which are often limited to unrealistic settings, e.g., digital perturbations on separate stereo views in static scenes, restricting their real-world applicability. This raises a critical question: how can we design physically realizable, scene-adaptive, and transferable attacks against SDE under realistic constraints? To answer this, we make two key contributions. First, we propose a unified attack framework that extends optimization-based techniques to four core stages of stereo matching: feature extraction, cost-volume construction, cost aggregation, and disparity regression. A comprehensive stage-wise evaluation across 9 mainstream SDE models, under constraints like photometric consistency, reveals that optimization-based patches suffer from poor transferability. Interestingly, partially transferable patches suggest that patterns, rather than pixel-level perturbations, may be key to generalizable attacks. Motivated by this, we present PatchHunter, the first optimization-free adversarial patch attack against SDE. PatchHunter formulates patch generation as a reinforcement learning-driven search over a structured space of visual patterns crafted to disrupt SDE assumptions. We validate PatchHunter across three levels: the KITTI dataset, the CARLA simulator, and real-world vehicle deployment. PatchHunter not only surpasses optimization-based methods in effectiveness but also achieves significantly better black-box transferability. Even under challenging physical conditions like low light, PatchHunter maintains high attack success (e.g., D1-all > 0.4), whereas optimization-based methods fail.",
      "authors": [
        "Liu, Hangcheng",
        "Kuang, Xu",
        "Han, Xingshuo",
        "Wu, Xingwan",
        "Ou, Haoran",
        "Guo, Shangwei",
        "Huang, Xingyi",
        "Xiang, Tao",
        "Zhang, Tianwei"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17632v1",
        "Other Formats": "https://arxiv.org/format/2506.17632",
        "TeX Source": "https://arxiv.org/src/2506.17632",
        "View PDF": "https://arxiv.org/pdf/2506.17632"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 08:23:02 UTC (11,384 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Optimization-Free Patch Attack on Stereo Depth Estimation",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.20010",
      "abstract": "This paper initiates the study of data-dependent regret bounds in constrained MAB settings. These bounds depend on the sequence of losses that characterize the problem instance. Thus, they can be much smaller than classical $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret bounds, while being equivalent to them in the worst case. Despite this, data-dependent regret bounds have been completely overlooked in constrained MAB settings. The goal of this paper is to answer the following question: Can data-dependent regret bounds be derived in the presence of constraints? We answer this question affirmatively in constrained MABs with adversarial losses and stochastic constraints. Specifically, our main focus is on the most challenging and natural settings with hard constraints, where the learner must ensure that the constraints are always satisfied with high probability. We design an algorithm with a regret bound consisting of two data-dependent terms. The first term captures the difficulty of satisfying the constraints, while the second one encodes the complexity of learning independently of the presence of constraints. We also prove a lower bound showing that these two terms are not artifacts of our specific approach and analysis, but rather the fundamental components that inherently characterize the complexities of the problem. Finally, in designing our algorithm, we also derive some novel results in the related (and easier) soft constraints settings, which may be of independent interest.",
      "authors": [
        "Genalti, Gianmarco",
        "Stradi, Francesco Emanuele",
        "Castiglioni, Matteo",
        "Marchesi, Alberto",
        "Gatti, Nicola"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2505.20010v2",
        "Other Formats": "https://arxiv.org/format/2505.20010",
        "TeX Source": "https://arxiv.org/src/2505.20010",
        "View PDF": "https://arxiv.org/pdf/2505.20010"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 26 May 2025 14:00:36 UTC (286 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 12:44:40 UTC (52 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/26",
      "title": "Data-Dependent Regret Bounds for Constrained MABs",
      "tasks": [],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17545",
      "abstract": "Currently, utilizing large language models to understand the 3D world is becoming popular. Yet existing 3D-aware LLMs act as black boxes: they output bounding boxes or textual answers without revealing how those decisions are made, and they still rely on pre-trained 3D detectors to supply object proposals. We introduce Scene-R1, a video-grounded framework that learns to reason about 3D scenes without any point-wise 3D instance supervision by pairing reinforcement-learning-driven reasoning with a two-stage grounding pipeline. In the temporal grounding stage, we explicitly reason about the video and select the video snippets most relevant to an open-ended query. In the subsequent image grounding stage, we analyze the image and predict the 2D bounding box. After that, we track the object using SAM2 to produce pixel-accurate masks in RGB frames, and project them back into 3D, thereby eliminating the need for 3D detector-based proposals while capturing fine geometry and material cues. Scene-R1 can also adapt to the 3D visual question answering task to answer free-form questions directly from video. Our training pipeline only needs task-level 2D boxes or textual labels without dense 3D point-wise labels. Scene-R1 surpasses existing open-vocabulary baselines on multiple datasets, while delivering transparent, step-by-step rationales. These results show that reinforcement-learning-based reasoning combined with RGB-D video alone offers a practical, annotation-efficient route to trustworthy 3D scene understanding.",
      "authors": [
        "Yuan, Zhihao",
        "Jiang, Shuyi",
        "Feng, Chun-Mei",
        "Zhang, Yaolun",
        "Cui, Shuguang",
        "Li, Zhen",
        "Zhao, Na"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17545v1",
        "Other Formats": "https://arxiv.org/format/2506.17545",
        "TeX Source": "https://arxiv.org/src/2506.17545",
        "View PDF": "https://arxiv.org/pdf/2506.17545"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 02:29:10 UTC (1,108 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Scene-R1: Video-Grounded Large Language Models for 3D Scene Reasoning without 3D Annotations",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17596",
      "abstract": "Parkinson's disease (PD), characterized by its incurable nature, rapid progression, and severe disability, poses significant challenges to the lives of patients and their families. Given the aging population, the need for early detection of PD is increasing. In vitro diagnosis has garnered attention due to its non-invasive nature and low cost. However, existing methods present several challenges: 1) limited training data for facial expression diagnosis; 2) specialized equipment and acquisition environments required for gait diagnosis, resulting in poor generalizability; 3) the risk of misdiagnosis or missed diagnosis when relying on a single modality. To address these issues, we propose a novel multimodal in vitro diagnostic method for PD, leveraging facial expressions and behavioral gait. Our method employs a lightweight deep learning model for feature extraction and fusion, aimed at improving diagnostic accuracy and facilitating deployment on mobile devices. Furthermore, we have established the largest multimodal PD dataset in collaboration with a hospital and conducted extensive experiments to validate the effectiveness of our proposed method.",
      "authors": [
        "Huang, Wei",
        "Xu, Yinxuan",
        "Zhou, Yintao",
        "Li, Zhengyu",
        "Huang, Jing",
        "Pang, Meng"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17596v1",
        "Other Formats": "https://arxiv.org/format/2506.17596",
        "TeX Source": "https://arxiv.org/src/2506.17596",
        "View PDF": "https://arxiv.org/pdf/2506.17596"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 05:20:46 UTC (5,685 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "A Multimodal In Vitro Diagnostic Method for Parkinson's Disease Combining Facial Expressions and Behavioral Gait Data",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17686",
      "abstract": "Keyword Spotting plays a critical role in enabling hands-free interaction for battery-powered edge devices. Few-Shot Keyword Spotting (FS-KWS) addresses the scalability and adaptability challenges of traditional systems by enabling recognition of custom keywords with only a few examples. However, existing FS-KWS systems achieve subpar accuracy at desirable false acceptance rates, particularly in resource-constrained edge environments. To address these issues, we propose a training scheme that leverages self-supervised learning models for robust feature extraction, dimensionality reduction, and knowledge distillation. The teacher model, based on Wav2Vec 2.0 is trained using Sub-center ArcFace loss, which enhances inter-class separability and intra-class compactness. To enable efficient deployment on edge devices, we introduce attention-based dimensionality reduction and train a standard lightweight ResNet15 student model. We evaluate the proposed approach on the English portion of the Multilingual Spoken Words Corpus (MSWC) and the Google Speech Commands (GSC) datasets. Notably, the proposed training method improves the 10-shot classification accuracy from 33.4% to 74.1% on 11 classes at 1% false alarm accuracy on the GSC dataset, thus making it significantly better-suited for a real use case scenario.",
      "authors": [
        "Gok, Alican",
        "Buyuksolak, Oguzhan",
        "Okman, Osman Erman",
        "Saraclar, Murat"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17686v1",
        "Other Formats": "https://arxiv.org/format/2506.17686",
        "TeX Source": "https://arxiv.org/src/2506.17686",
        "View PDF": "https://arxiv.org/pdf/2506.17686"
      },
      "subjects": [
        "Audio and Speech Processing (eess.AS)",
        "Computation and Language (cs.CL)",
        "Sound (cs.SD)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 11:39:11 UTC (233 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Enhancing Few-shot Keyword Spotting Performance through Pre-Trained Self-supervised Speech Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.05112",
      "abstract": "Acquiring high-quality Positron Emission Tomography (PET) images requires administering high-dose radiotracers, which increases radiation exposure risks. Generating standard-dose PET (SPET) from low-dose PET (LPET) has become a potential solution. However, previous studies have primarily focused on single low-dose PET denoising, neglecting two critical factors: discrepancies in dose response caused by inter-patient variability, and complementary anatomical constraints derived from CT images. In this work, we propose a novel CT-Guided Multi-dose Adaptive Attention Denoising Diffusion Model (MDAA-Diff) for multi-dose PET denoising. Our approach integrates anatomical guidance and dose-level adaptation to achieve superior denoising performance under low-dose conditions. Specifically, this approach incorporates a CT-Guided High-frequency Wavelet Attention (HWA) module, which uses wavelet transforms to separate high-frequency anatomical boundary features from CT images. These extracted features are then incorporated into PET imaging through an adaptive weighted fusion mechanism to enhance edge details. Additionally, we propose the Dose-Adaptive Attention (DAA) module, a dose-conditioned enhancement mechanism that dynamically integrates dose levels into channel-spatial attention weight calculation. Extensive experiments on 18F-FDG and 68Ga-FAPI datasets demonstrate that MDAA-Diff outperforms state-of-the-art approaches in preserving diagnostic quality under reduced-dose conditions. Our code is publicly available.",
      "authors": [
        "Niu, Xiaolong",
        "Ye, Zanting",
        "Han, Xu",
        "Huang, Yanchao",
        "Sun, Hao",
        "Wu, Hubing",
        "Lu, Lijun"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2505.05112v2",
        "Other Formats": "https://arxiv.org/format/2505.05112",
        "TeX Source": "https://arxiv.org/src/2505.05112",
        "View PDF": "https://arxiv.org/pdf/2505.05112"
      },
      "subjects": [
        "Image and Video Processing (eess.IV)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 8 May 2025 10:27:12 UTC (3,155 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 16:17:06 UTC (3,153 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/08",
      "title": "MDAA-Diff: CT-Guided Multi-Dose Adaptive Attention Diffusion Model for PET Denoising",
      "tasks": [
        "Denoising",
        "Diagnostic"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2306.06514",
      "abstract": "Cycle-consistent generative adversarial networks have been widely used in non-parallel voice conversion (VC). Their ability to learn mappings between source and target features without relying on parallel training data eliminates the need for temporal alignments. However, most methods decouple the conversion of acoustic features from synthesizing the audio signal by using separate models for conversion and waveform synthesis. This work unifies conversion and synthesis into a single model, thereby eliminating the need for a separate vocoder. By leveraging cycle-consistent training and a self-supervised auxiliary training task, our model is able to efficiently generate converted high-quality raw audio waveforms. Subjective listening tests showed that our unified approach achieved improvements of up to 6.7% relative to the baseline in whispered VC. Mean opinion score predictions also yielded stable results in conventional VC (between 0.5% and 2.4% relative improvement).",
      "authors": [
        "Wagner, Dominik",
        "Baumann, Ilja",
        "Bocklet, Tobias"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2306.06514v2",
        "Other Formats": "https://arxiv.org/format/2306.06514",
        "TeX Source": "https://arxiv.org/src/2306.06514",
        "View PDF": "https://arxiv.org/pdf/2306.06514"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 10 Jun 2023 19:33:05 UTC (955 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 15:17:28 UTC (968 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2023/06/10",
      "title": "Vocoder-Free Non-Parallel Conversion of Whispered Speech With Masked Cycle-Consistent Generative Adversarial Networks",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2212.10306",
      "abstract": "Accurate time series forecasting is crucial for optimizing resource allocation, industrial production, and urban management, particularly with the growth of cyber-physical and IoT systems. However, limited training sample availability in fields like physics and biology poses significant challenges. Existing models struggle to capture long-term dependencies and to model diverse meta-knowledge explicitly in few-shot scenarios. To address these issues, we propose MetaGP, a meta-learning-based Gaussian process latent variable model that uses a Gaussian process kernel function to capture long-term dependencies and to maintain strong correlations in time series. We also introduce Kernel Association Search (KAS) as a novel meta-learning component to explicitly model meta-knowledge, thereby enhancing both interpretability and prediction accuracy. We study MetaGP on simulated and real-world few-shot datasets, showing that it is capable of state-of-the-art prediction accuracy. We also find that MetaGP can capture long-term dependencies and can model meta-knowledge, thereby providing valuable insights into complex time series patterns.",
      "authors": [
        "Cheng, Yunyao",
        "Guo, Chenjuan",
        "Chen, Kaixuan",
        "Zhao, Kai",
        "Yang, Bin",
        "Xie, Jiandong",
        "Jensen, Christian S.",
        "Huang, Feiteng",
        "Zheng, Kai"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2212.10306v2",
        "Other Formats": "https://arxiv.org/format/2212.10306",
        "TeX Source": "https://arxiv.org/src/2212.10306",
        "View PDF": "https://arxiv.org/pdf/2212.10306"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Tue, 20 Dec 2022 14:54:04 UTC (1,497 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 10:39:07 UTC (1,205 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2022/12/20",
      "title": "Gaussian Process Latent Variable Modeling for Few-shot Time Series Forecasting",
      "tasks": [
        "Diversity",
        "Multivariate Time Series Forecasting",
        "Time Series",
        "Time Series Analysis",
        "Time Series Forecasting"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17685",
      "abstract": "Recognizing human activities from visual inputs, particularly through a first-person viewpoint, is essential for enabling robots to replicate human behavior. Egocentric vision, characterized by cameras worn by observers, captures diverse changes in illumination, viewpoint, and environment. This variability leads to a notable drop in the performance of Egocentric Action Recognition models when tested in environments not seen during training. In this paper, we tackle these challenges by proposing a domain generalization approach for Egocentric Action Recognition. Our insight is that action sequences often reflect consistent user intent across visual domains. By leveraging action sequences, we aim to enhance the model's generalization ability across unseen environments. Our proposed method, named SeqDG, introduces a visual-text sequence reconstruction objective (SeqRec) that uses contextual cues from both text and visual inputs to reconstruct the central action of the sequence. Additionally, we enhance the model's robustness by training it on mixed sequences of actions from different domains (SeqMix). We validate SeqDG on the EGTEA and EPIC-KITCHENS-100 datasets. Results on EPIC-KITCHENS-100, show that SeqDG leads to +2.4% relative average improvement in cross-domain action recognition in unseen environments, and on EGTEA the model achieved +0.6% Top-1 accuracy over SOTA in intra-domain action recognition.",
      "authors": [
        "Nasirimajd, Amirshayan",
        "Plizzari, Chiara",
        "Peirone, Simone Alberto",
        "Ciccone, Marco",
        "Averta, Giuseppe",
        "Caputo, Barbara"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17685v1",
        "Other Formats": "https://arxiv.org/format/2506.17685",
        "TeX Source": "https://arxiv.org/src/2506.17685",
        "View PDF": "https://arxiv.org/pdf/2506.17685"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 11:33:08 UTC (2,916 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Domain Generalization using Action Sequences for Egocentric Action Recognition",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17838",
      "abstract": "This paper proposes a foreground-background separation (FBS) method with a novel foreground model based on convolutional sparse representation (CSR). In order to analyze the dynamic and static components of videos acquired under undesirable conditions, such as hardware, environmental, and power limitations, it is essential to establish an FBS method that can handle videos with low frame rates and various types of noise. Existing FBS methods have two limitations that prevent us from accurately separating foreground and background components from such degraded videos. First, they only capture either data-specific or general features of the components. Second, they do not include explicit models for various types of noise to remove them in the FBS process. To this end, we propose a robust FBS method with a CSR-based foreground model. This model can adaptively capture specific spatial structures scattered in imaging data. Then, we formulate FBS as a constrained multiconvex optimization problem that incorporates CSR, functions that capture general features, and explicit noise characterization functions for multiple types of noise. Thanks to these functions, our method captures both data-specific and general features to accurately separate the components from various types of noise even under low frame rates. To obtain a solution of the optimization problem, we develop an algorithm that alternately solves its two convex subproblems by newly established algorithms. Experiments demonstrate the superiority of our method over existing methods using two types of degraded videos: infrared and microscope videos.",
      "authors": [
        "Naganuma, Kazuki",
        "Ono, Shunsuke"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17838",
        "TeX Source": "https://arxiv.org/src/2506.17838",
        "View PDF": "https://arxiv.org/pdf/2506.17838"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Image and Video Processing (eess.IV)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 22:28:04 UTC (2,989 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Robust Foreground-Background Separation for Severely-Degraded Videos Using Convolutional Sparse Representation Modeling",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17680",
      "abstract": "This paper introduces a novel deep-learning approach to predict true stress-strain curves of high-strength steels from small punch test (SPT) load-displacement data. The proposed approach uses Gramian Angular Field (GAF) to transform load-displacement sequences into images, capturing spatial-temporal features and employs a Sequence-to-Sequence (Seq2Seq) model with an LSTM-based encoder-decoder architecture, enhanced by multi-head cross-attention to improved accuracy. Experimental results demonstrate that the proposed approach achieves superior prediction accuracy, with minimum and maximum mean absolute errors of 0.15 MPa and 5.58 MPa, respectively. The proposed method offers a promising alternative to traditional experimental techniques in materials science, enhancing the accuracy and efficiency of true stress-strain relationship predictions.",
      "authors": [
        "Yang, Zhengni",
        "Yang, Rui",
        "Han, Weijian",
        "Liu, Qixin"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17680v1",
        "Other Formats": "https://arxiv.org/format/2506.17680",
        "TeX Source": "https://arxiv.org/src/2506.17680",
        "View PDF": "https://arxiv.org/pdf/2506.17680"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Materials Science (cond-mat.mtrl-sci)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 11:14:54 UTC (1,887 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Enhancing Stress-Strain Predictions with Seq2Seq and Cross-Attention based on Small Punch Test",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17743",
      "abstract": "In inland waterways, the efficient management of water lock operations impacts the level of congestion and the resulting uncertainty in inland waterway transportation. To achieve reliable and efficient traffic, schedules should be easy to understand and implement, reducing the likelihood of errors. The simplest schedules follow periodic patterns, reducing complexity and facilitating predictable management. Since vessels do not arrive in perfectly regular intervals, periodic schedules may lead to more wait time. The aim of this research is to estimate this cost by evaluating how effective these periodic schedules manage vessel traffic at water locks. The first objective is to estimate a periodic arrival pattern that closely matches a dataset of irregular vessel arrivals at a specific lock. We develop an algorithm that, given a fixed number of vessel streams, solves the problem in polynomial time. The solution then serves as input for the subsequent part, where we consider algorithms that compute operational schedules by formulating an optimisation problem with periodic arrival patterns as input, and the goal is to determine a periodic schedule that minimises the long-run average waiting time of vessels. We present a polynomial-time algorithm for the two-stream case and a pseudo-polynomial-time algorithm for the general case, along with incremental polynomial-time approximation schemes. In our numerical experiments, use AIS data to construct a periodic arrival pattern closely matching the observed data. Our experiments demonstrate that when evaluated against actual data, intuitive and straightforward policies often outperform optimal policies specifically trained on the periodic arrival pattern.",
      "authors": [
        "Golak, Julian",
        "Grigoriev, Alexander",
        "van Lent, Freija",
        "van der Zanden, Tom"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17743v1",
        "Other Formats": "https://arxiv.org/format/2506.17743",
        "TeX Source": "https://arxiv.org/src/2506.17743",
        "View PDF": "https://arxiv.org/pdf/2506.17743"
      },
      "subjects": [
        "Data Structures and Algorithms (cs.DS)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 15:46:20 UTC (24 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Optimizing Periodic Operations for Efficient Inland Waterway Lock Management",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17624",
      "abstract": "Most prior research in deep imitation learning has predominantly utilized fixed cameras for image input, which constrains task performance to the predefined field of view. However, enabling a robot to actively maneuver its neck can significantly expand the scope of imitation learning to encompass a wider variety of tasks and expressive actions such as neck gestures. To facilitate imitation learning in robots capable of neck movement while simultaneously performing object manipulation, we propose a teaching system that systematically collects datasets incorporating neck movements while minimizing discomfort caused by dynamic viewpoints during teleoperation. In addition, we present a novel network model for learning manipulation tasks including active neck motion. Experimental results showed that our model can achieve a high success rate of around 90\\%, regardless of the distraction from the viewpoint variations by active neck motion. Moreover, the proposed model proved particularly effective in challenging scenarios, such as when objects were situated at the periphery or beyond the standard field of view, where traditional models struggled. The proposed approach contributes to the efficiency of dataset collection and extends the applicability of imitation learning to more complex and dynamic scenarios.",
      "authors": [
        "Nakagawa, Koki",
        "Ohmura, Yoshiyuki",
        "Kuniyoshi, Yasuo"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17624v1",
        "Other Formats": "https://arxiv.org/format/2506.17624",
        "TeX Source": "https://arxiv.org/src/2506.17624",
        "View PDF": "https://arxiv.org/pdf/2506.17624"
      },
      "subjects": [
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 07:37:49 UTC (3,294 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Imitation Learning for Active Neck Motion Enabling Robot Manipulation beyond the Field of View",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2408.12936",
      "abstract": "We introduce Smooth InfoMax (SIM), a self-supervised representation learning method that incorporates interpretability constraints into the latent representations at different depths of the network. Based on $\\beta$-VAEs, SIM's architecture consists of probabilistic modules optimized locally with the InfoNCE loss to produce Gaussian-distributed representations regularized toward the standard normal distribution. This creates smooth, well-defined, and better-disentangled latent spaces, enabling easier post-hoc analysis. Evaluated on speech data, SIM preserves the large-scale training benefits of Greedy InfoMax while improving the effectiveness of post-hoc interpretability methods across layers.",
      "authors": [
        "Denoodt, Fabian",
        "de Boer, Bart",
        "Oramas, Jos\u00e9"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "Other Formats": "https://arxiv.org/format/2408.12936",
        "TeX Source": "https://arxiv.org/src/2408.12936",
        "View PDF": "https://arxiv.org/pdf/2408.12936"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 23 Aug 2024 09:36:09 UTC (198 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Wed, 19 Mar 2025 16:58:12 UTC (198 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Sat, 21 Jun 2025 20:22:56 UTC (363 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/08/23",
      "title": "Smooth InfoMax -- Towards Easier Post-Hoc Interpretability",
      "repo_urls": [
        "https://github.com/oboii/smooth-infomax"
      ],
      "tasks": [
        "Decoder",
        "Representation Learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17694",
      "abstract": "Conventional audio-visual methods for speaker verification rely on large amounts of labeled data and separate modality-specific architectures, which is computationally expensive, limiting their scalability. To address these problems, we propose a self-supervised learning framework based on contrastive learning with asymmetric masking and masked data modeling to obtain robust audiovisual feature representations. In particular, we employ a unified framework for self-supervised audiovisual speaker verification using a single shared backbone for audio and visual inputs, leveraging the versatility of vision transformers. The proposed unified framework can handle audio, visual, or audiovisual inputs using a single shared vision transformer backbone during training and testing while being computationally efficient and robust to missing modalities. Extensive experiments demonstrate that our method achieves competitive performance without labeled data while reducing computational costs compared to traditional approaches.",
      "authors": [
        "Rajasekhar, Gnana Praveen",
        "Alam, Jahangir"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17694v1",
        "Other Formats": "https://arxiv.org/format/2506.17694",
        "TeX Source": "https://arxiv.org/src/2506.17694",
        "View PDF": "https://arxiv.org/pdf/2506.17694"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 12:02:53 UTC (2,360 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "SSAVSV: Towards Unified Model for Self-Supervised Audio-Visual Speaker Verification",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17533",
      "abstract": "In this paper, we propose DuaShepherd, a novel reward modeling framework that integrates two complementary reward signals, correctness and potential, to enhance the mathematical reasoning capabilities of Large Language Models (LLMs). While correctness-based signals emphasize identification of stepwise errors, potential-based signals focus on the likelihood of reaching the correct final answer. We developed an automated pipeline for constructing large-scale reward modeling dataset with both signals. A unified, multi-head architecture was explored to train the two reward models in a multi-task setup, demonstrating benefits from learning both correctness and potential in parallel. By combining these two signals into a compound probability, our model achieves consistent performance improvements across multiple benchmarks. Empirical evaluations on MATH500 and ProcessBench confirm that this combined reward significantly outperforms models trained on either reward type alone, achieving state-of-the-art performance under comparable resource constraints.",
      "authors": [
        "Wu, Yuanhao",
        "Song, Juntong",
        "Zhang, Hanning",
        "Zhang, Tong",
        "Niu, Cheng"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17533v1",
        "Other Formats": "https://arxiv.org/format/2506.17533",
        "TeX Source": "https://arxiv.org/src/2506.17533",
        "View PDF": "https://arxiv.org/pdf/2506.17533"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 01:11:01 UTC (7,818 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17700",
      "abstract": "Large Language Models (LLMs) have revolutionized the field of Natural Language Processing (NLP) by automating traditional labor-intensive tasks and consequently accelerated the development of computer-aided applications. As researchers continue to advance this field with the introduction of novel language models and more efficient training/finetuning methodologies, the idea of prompt engineering and subsequent optimization strategies with LLMs has emerged as a particularly impactful trend to yield a substantial performance boost across diverse NLP tasks. To best of our knowledge numerous review articles have explored prompt engineering, however, a critical gap exists in comprehensive analyses of prompt optimization strategies. To bridge this gap this paper provides unique and comprehensive insights about the potential of diverse prompt optimization strategies. It analyzes their underlying working paradigms and based on these principles, categorizes them into 11 distinct classes. Moreover, the paper provides details about various NLP tasks where these prompt optimization strategies have been employed, along with details of different LLMs and benchmark datasets used for evaluation. This comprehensive compilation lays a robust foundation for future comparative studies and enables rigorous assessment of prompt optimization and LLM-based predictive pipelines under consistent experimental settings: a critical need in the current landscape. Ultimately, this research will centralize diverse strategic knowledge to facilitate the adaptation of existing prompt optimization strategies for development of innovative predictors across unexplored tasks.",
      "authors": [
        "Saleem, Summra",
        "Asim, Muhammad Nabeel",
        "Zulfiqar, Shaista",
        "Dengel, Andreas"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17700v1",
        "Other Formats": "https://arxiv.org/format/2506.17700",
        "TeX Source": "https://arxiv.org/src/2506.17700",
        "View PDF": "https://arxiv.org/pdf/2506.17700"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 12:25:37 UTC (1,733 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2304.10805",
      "abstract": "Large-scale pre-trained models surpass in transferability and robust generalization across diverse datasets. The emergence of multimodal pre-trained models like CLIP has significantly boosted performance in various experiments. However, generalizing to new datasets or domains remains challenging, especially with limited labeled data. Also, existing methods often lack interpretability and impose high computational costs. To address this, we propose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the knowledge graph to curate diverse, interpretable prompt sets automatically. Our method autonomously selects the optimal interpretable prompt based on dataset characteristics, achieving performance improvements over zero-shot learning and competitive performance compared to various prompt learning methods. Also, RPLKG efficiently reuses cached prompt embeddings from a single model pass and optimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast training. Moreover, RPLKG advances few-shot learning effectiveness while enhancing interpretability and efficiency in model adaptation. Our",
      "authors": [
        "Lim, YongTaek",
        "Kim, Yewon",
        "Kang, Suho",
        "Yoon, Dokyung",
        "Song, KyungWoo"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2304.10805v2",
        "Other Formats": "https://arxiv.org/format/2304.10805",
        "TeX Source": "https://arxiv.org/src/2304.10805",
        "View PDF": "https://arxiv.org/pdf/2304.10805"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 21 Apr 2023 08:22:58 UTC (22,020 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 08:27:10 UTC (12,780 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2023/04/21",
      "title": "RPLKG: Robust Prompt Learning with Knowledge Graph",
      "tasks": [
        "Domain Generalization",
        "Few-Shot Learning",
        "Prompt Learning",
        "Zero-Shot Learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2504.08217",
      "abstract": "At the current stage, deep learning-based methods have demonstrated excellent capabilities in evaluating aerodynamic performance, significantly reducing the time and cost required for traditional computational fluid dynamics (CFD) simulations. However, when faced with the task of processing extremely complex three-dimensional (3D) vehicle models, the lack of large-scale datasets and training resources, coupled with the inherent diversity and complexity of the geometry of different vehicle models, means that the prediction accuracy and versatility of these networks are still not up to the level required for current production. In view of the remarkable success of Transformer models in the field of natural language processing and their strong potential in the field of image processing, this study innovatively proposes a point cloud learning framework called DrivAer Transformer (DAT). The DAT structure uses the DrivAerNet++ dataset, which contains high-fidelity CFD data of industrial-standard 3D vehicle shapes. enabling accurate estimation of air drag directly from 3D meshes, thus avoiding the limitations of traditional methods such as 2D image rendering or signed distance fields (SDF). DAT enables fast and accurate drag prediction, driving the evolution of the aerodynamic evaluation process and laying the critical foundation for introducing a data-driven approach to automotive design. The framework is expected to accelerate the vehicle design process and improve development efficiency.",
      "authors": [
        "He, Jiaqi",
        "Luo, Xiangwen",
        "Wang, Yiping"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2504.08217v5",
        "Other Formats": "https://arxiv.org/format/2504.08217",
        "TeX Source": "https://arxiv.org/src/2504.08217",
        "View PDF": "https://arxiv.org/pdf/2504.08217"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 11 Apr 2025 02:50:38 UTC (9,808 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Tue, 15 Apr 2025 07:45:49 UTC (9,809 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Fri, 18 Apr 2025 04:24:54 UTC (9,808 KB)",
          "link": "/",
          "version": "[v3]"
        },
        {
          "details": "Tue, 13 May 2025 14:43:14 UTC (19,626 KB)",
          "link": "/",
          "version": "[v4]"
        },
        {
          "details": "Sat, 21 Jun 2025 07:51:51 UTC (19,480 KB)",
          "version": "[v5]"
        }
      ],
      "submitted_date": "2025/04/11",
      "title": "DrivAer Transformer: A high-precision and fast prediction method for vehicle aerodynamic drag coefficient based on the DrivAerNet++ dataset",
      "tasks": [],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17859",
      "abstract": "Recent work analyzing in-context learning (ICL) has identified a broad set of strategies that describe model behavior in different experimental conditions. We aim to unify these findings by asking why a model learns these disparate strategies in the first place. Specifically, we start with the observation that when trained to learn a mixture of tasks, as is popular in the literature, the strategies learned by a model for performing ICL can be captured by a family of Bayesian predictors: a memorizing predictor, which assumes a discrete prior on the set of seen tasks, and a generalizing predictor, wherein the prior matches the underlying task distribution. Adopting the lens of rational analysis from cognitive science, where a learner's behavior is explained as an optimal adaptation to data given computational constraints, we develop a hierarchical Bayesian framework that almost perfectly predicts Transformer next token predictions throughout training without assuming access to its weights. Under this framework, pretraining is viewed as a process of updating the posterior probability of different strategies, and its inference-time behavior as a posterior-weighted average over these strategies' predictions. Our framework draws on common assumptions about neural network learning dynamics, which make explicit a tradeoff between loss and complexity among candidate strategies: beyond how well it explains the data, a model's preference towards implementing a strategy is dictated by its complexity. This helps explain well-known ICL phenomena, while offering novel predictions: e.g., we show a superlinear trend in the timescale for transition to memorization as task diversity is increased. Overall, our work advances an explanatory and predictive account of ICL grounded in tradeoffs between strategy loss and complexity.",
      "authors": [
        "Wurgaft, Daniel",
        "Lubana, Ekdeep Singh",
        "Park, Core Francisco",
        "Tanaka, Hidenori",
        "Reddy, Gautam",
        "Goodman, Noah D."
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17859v1",
        "Other Formats": "https://arxiv.org/format/2506.17859",
        "TeX Source": "https://arxiv.org/src/2506.17859",
        "View PDF": "https://arxiv.org/pdf/2506.17859"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 23:49:08 UTC (9,770 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "In-Context Learning Strategies Emerge Rationally",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17672",
      "abstract": "In ride-hailing systems, drivers decide whether to accept or reject ride requests based on factors such as order characteristics, traffic conditions, and personal preferences. Accurately predicting these decisions is essential for improving the efficiency and reliability of these systems. Traditional models, such as the Random Utility Maximization (RUM) approach, typically predict drivers' decisions by assuming linear correlations among attributes. However, these models often fall short because they fail to account for non-linear interactions between attributes and do not cater to the unique, personalized preferences of individual drivers. In this paper, we develop a method for learning personalized utility functions using hypernetwork and ensemble learning. Hypernetworks dynamically generate weights for a linear utility function based on trip request data and driver profiles, capturing the non-linear relationships. An ensemble of hypernetworks trained on different data segments further improve model adaptability and generalization by introducing controlled randomness, thereby reducing over-fitting. We validate the performance of our ensemble hypernetworks model in terms of prediction accuracy and uncertainty estimation in a real-world dataset. The results demonstrate that our approach not only accurately predicts each driver's utility but also effectively balances the needs for explainability and uncertainty quantification. Additionally, our model serves as a powerful tool for revealing the personalized preferences of different drivers, clearly illustrating which attributes largely impact their rider acceptance decisions.",
      "authors": [
        "Mai, Weiming",
        "Gao, Jie",
        "Cats, Oded"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17672v1",
        "Other Formats": "https://arxiv.org/format/2506.17672",
        "TeX Source": "https://arxiv.org/src/2506.17672",
        "View PDF": "https://arxiv.org/pdf/2506.17672"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Emerging Technologies (cs.ET)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 10:16:34 UTC (183 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Learning Personalized Utility Functions for Drivers in Ride-hailing Systems Using Ensemble Hypernetworks",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2408.08190",
      "abstract": "High-frequency features are critical in multiscale phenomena such as turbulent flows and phase transitions, since they encode essential physical information. The recently proposed Wavelet Neural Operator (WNO) utilizes wavelets' time-frequency localization to capture spatial manifolds effectively. While its factorization strategy improves noise robustness, it suffers from high-frequency information loss caused by finite-scale wavelet decomposition. In this study, a new U-WNO network architecture is proposed. It incorporates the U-Net path and residual shortcut into the wavelet layer to enhance the extraction of high-frequency features and improve the learning of spatial manifolds. Furthermore, we introduce an adaptive activation mechanism to mitigate spectral bias through trainable slope parameters. Extensive benchmarks across seven PDE families (Burgers, Darcy flow, Navier-Stokes, etc.) show that U-WNO achieves 45--83% error reduction compared to baseline WNO, with mean $L_2$ relative errors ranging from 0.043% to 1.56%. This architecture establishes a framework combining multiresolution analysis with deep feature learning, addressing the spectral-spatial tradeoff in operator learning. Code and data used are available on https://github.com/WeiminLei/U-WNO.git.",
      "authors": [
        "Lei, Wei-Min",
        "Li, Hou-Biao"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2408.08190v2",
        "Other Formats": "https://arxiv.org/format/2408.08190",
        "TeX Source": "https://arxiv.org/src/2408.08190",
        "View PDF": "https://arxiv.org/pdf/2408.08190"
      },
      "subjects": [
        "Numerical Analysis (math.NA)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 15 Aug 2024 14:47:54 UTC (4,693 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 14:17:20 UTC (2,163 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/08/15",
      "title": "U-WNO: U-Net Enhanced Wavelet Neural Operator for Solving Parametric Partial Differential Equations",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17633",
      "abstract": "Out-of-distribution (OOD) detection attempts to distinguish outlier samples to prevent models trained on the in-distribution (ID) dataset from producing unavailable outputs. Most OOD detection methods require many IID samples for training, which seriously limits their real-world applications. To this end, we target a challenging setting: few-shot OOD detection, where {Only a few {\\em labeled ID} samples are available.} Therefore, few-shot OOD detection is much more challenging than the traditional OOD detection setting. Previous few-shot OOD detection works ignore the distinct diversity between different classes. In this paper, we propose a novel network: Adaptive Multi-prompt Contrastive Network (AMCN), which adapts the ID-OOD separation boundary by learning inter- and intra-class distribution. To compensate for the absence of OOD and scarcity of ID {\\em image samples}, we leverage CLIP, connecting text with images, engineering learnable ID and OOD {\\em textual prompts}. Specifically, we first generate adaptive prompts (learnable ID prompts, label-fixed OOD prompts and label-adaptive OOD prompts). Then, we generate an adaptive class boundary for each class by introducing a class-wise threshold. Finally, we propose a prompt-guided ID-OOD separation module to control the margin between ID and OOD prompts. Experimental results show that AMCN outperforms other state-of-the-art works.",
      "authors": [
        "Fang, Xiang",
        "Easwaran, Arvind",
        "Genest, Blaise"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17633v1",
        "Other Formats": "https://arxiv.org/format/2506.17633",
        "TeX Source": "https://arxiv.org/src/2506.17633",
        "View PDF": "https://arxiv.org/pdf/2506.17633"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 08:31:29 UTC (683 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Adaptive Multi-prompt Contrastive Network for Few-shot Out-of-distribution Detection",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.12493",
      "abstract": "Training effective Vision Language Models (VLMs) for GUI agents typically relies on supervised fine-tuning (SFT) over large-scale annotated datasets, where the collection process is labor-intensive and error-prone. In this work, we propose a self-supervised inverse dynamics task to enable VLMs to learn from GUI transition pairs by inferring the action that caused that transition. This training task offers two advantages: (1) It enables VLMs to ignore variations unrelated to user actions (e.g., background refreshes, ads) and to focus on true affordances such as buttons and input fields within complex GUIs. (2) The training data can be easily obtained from existing GUI trajectories without requiring human annotation, and it can be easily scaled through automatic offline exploration. Using this training task, we propose UI-shift, a framework for enhancing VLM-based GUI agents through self-supervised reinforcement learning (RL). With only 2K training samples sourced from existing datasets, two VLMs -- Qwen2.5-VL-3B and Qwen2.5-VL-7B -- trained with UI-Shift achieve competitive or superior performance on grounding tasks (ScreenSpot-series benchmarks) and GUI automation tasks (AndroidControl), compared to SFT baselines and GUI-specific models that explicitly elicit reasoning abilities during RL. Our findings suggest a potential direction for enhancing VLMs for GUI agents by leveraging more self-supervised training data in the future. Code, model, and data are available at: https://github.com/UbiquitousLearning/UIShift",
      "authors": [
        "Gao, Longxi",
        "Zhang, Li",
        "Xu, Mengwei"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2505.12493v2",
        "Other Formats": "https://arxiv.org/format/2505.12493",
        "TeX Source": "https://arxiv.org/src/2505.12493",
        "View PDF": "https://arxiv.org/pdf/2505.12493"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 18 May 2025 16:34:30 UTC (172 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 08:05:57 UTC (174 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/18",
      "title": "UIShift: Enhancing VLM-based GUI Agents through Self-supervised Reinforcement Learning",
      "tasks": [
        "2k",
        "Reinforcement Learning (RL)"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17611",
      "abstract": "This paper presents Open Unified Speech Language Models (OpusLMs), a family of open foundational speech language models (SpeechLMs) up to 7B. Initialized from decoder-only text language models, the OpusLMs are continuously pre-trained on 213K hours of speech-text pairs and 292B text-only tokens. We demonstrate our OpusLMs achieve comparable (or even superior) performance with existing SpeechLMs in speech recognition, speech synthesis, and text-only capabilities. Technically, this paper articulates our SpeechLM designs on tokenization, multi-stream language models, and multi-stage training strategies. We experimentally demonstrate the importance of model size scaling and the effect of annealing data selection. The OpusLMs are all built from publicly available materials and are fully transparent models. We release our code, data, checkpoints, and training logs to facilitate open SpeechLM research",
      "authors": [
        "Tian, Jinchuan",
        "Chen, William",
        "Peng, Yifan",
        "Shi, Jiatong",
        "Arora, Siddhant",
        "Bharadwaj, Shikhar",
        "Maekaku, Takashi",
        "Shinohara, Yusuke",
        "Goto, Keita",
        "Yue, Xiang",
        "Yang, Huck",
        "Watanabe, Shinji"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17611v1",
        "Other Formats": "https://arxiv.org/format/2506.17611",
        "TeX Source": "https://arxiv.org/src/2506.17611",
        "View PDF": "https://arxiv.org/pdf/2506.17611"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 06:30:59 UTC (104 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "OpusLM: A Family of Open Unified Speech Language Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.12885",
      "abstract": "Conventional benchmarks for crop type classification from optical satellite time series typically assume access to labeled data from the same year and rely on fixed calendar-day sampling. This limits generalization across seasons, where crop phenology shifts due to interannual climate variability, and precludes real-time application when current-year labels are unavailable. Furthermore, uncertainty quantification is often neglected, making such approaches unreliable for crop monitoring applications. Inspired by ecophysiological principles of plant growth, we propose a simple, model-agnostic sampling strategy that leverages growing degree days (GDD), based on daily average temperature, to replace calendar time with thermal time. By uniformly subsampling time series in this biologically meaningful domain, the method emphasizes phenologically active growth stages while reducing temporal redundancy and noise. We evaluate the method on a multi-year Sentinel-2 dataset spanning all of Switzerland, training on one growing season and testing on other seasons. Compared to state-of-the-art baselines, our method delivers substantial gains in classification accuracy and, critically, produces more calibrated uncertainty estimates. Notably, our method excels in low-data regimes and enables significantly more accurate early-season classification. With only 10 percent of the training data, our method surpasses the state-of-the-art baseline in both predictive accuracy and uncertainty estimation, and by the end of June, it achieves performance similar to a baseline trained on the full season. These results demonstrate that leveraging temperature data not only improves predictive performance across seasons but also enhances the robustness and trustworthiness of crop-type mapping in real-world applications.",
      "authors": [
        "Turkoglu, Mehmet Ozgur",
        "Ledain, Selene",
        "Aasen, Helge"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.12885v2",
        "Other Formats": "https://arxiv.org/format/2506.12885",
        "TeX Source": "https://arxiv.org/src/2506.12885",
        "View PDF": "https://arxiv.org/pdf/2506.12885"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Image and Video Processing (eess.IV)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 15 Jun 2025 15:30:08 UTC (2,986 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 13:06:54 UTC (2,986 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/06/15",
      "title": "Model-Agnostic, Temperature-Informed Sampling Enhances Cross-Year Crop Mapping with Deep Learning",
      "tasks": [
        "Crop Type Mapping",
        "Time Series",
        "Uncertainty Quantification"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2401.00691",
      "abstract": "This paper introduces an iterative algorithm for training nonparametric additive models that enjoys favorable memory storage and computational requirements. The algorithm can be viewed as the functional counterpart of stochastic gradient descent, applied to the coefficients of a truncated basis expansion of the component functions. We show that the resulting estimator satisfies an oracle inequality that allows for model mis-specification. In the well-specified setting, by choosing the learning rate carefully across three distinct stages of training, we demonstrate that its risk is minimax optimal in terms of the dependence on the dimensionality of the data and the size of the training sample. We also provide polynomial convergence rates even when the covariates do not have full support on their domain.",
      "authors": [
        "Chen, Xin",
        "Klusowski, Jason M."
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2401.00691v4",
        "Other Formats": "https://arxiv.org/format/2401.00691",
        "TeX Source": "https://arxiv.org/src/2401.00691",
        "View PDF": "https://arxiv.org/pdf/2401.00691"
      },
      "subjects": [
        "Machine Learning (stat.ML)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 1 Jan 2024 08:03:52 UTC (448 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Tue, 13 Feb 2024 13:40:53 UTC (873 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Tue, 22 Oct 2024 15:06:05 UTC (877 KB)",
          "link": "/",
          "version": "[v3]"
        },
        {
          "details": "Sat, 21 Jun 2025 03:14:14 UTC (331 KB)",
          "version": "[v4]"
        }
      ],
      "submitted_date": "2024/01/01",
      "title": "Stochastic Gradient Descent for Nonparametric Regression",
      "tasks": [
        "Additive models",
        "regression"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2409.05401",
      "abstract": "Given the large number of Hindi speakers worldwide, there is a pressing need for robust and efficient information retrieval systems for Hindi. Despite ongoing research, comprehensive benchmarks for evaluating retrieval models in Hindi are lacking. To address this gap, we introduce the Hindi-BEIR benchmark, comprising 15 datasets across seven distinct tasks. We evaluate state-of-the-art multilingual retrieval models on the Hindi-BEIR benchmark, identifying task and domain-specific challenges that impact Hindi retrieval performance. Building on the insights from these results, we introduce NLLB-E5, a multilingual retrieval model that leverages a zero-shot approach to support Hindi without the need for Hindi training data. We believe our contributions, which include the release of the Hindi-BEIR benchmark and the NLLB-E5 model, will prove to be a valuable resource for researchers and promote advancements in multilingual retrieval models.",
      "authors": [
        "Acharya, Arkadeep",
        "Murthy, Rudra",
        "Kumar, Vishwajeet",
        "Sen, Jaydeep"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2409.05401v3",
        "Other Formats": "https://arxiv.org/format/2409.05401",
        "TeX Source": "https://arxiv.org/src/2409.05401",
        "View PDF": "https://arxiv.org/pdf/2409.05401"
      },
      "subjects": [
        "Information Retrieval (cs.IR)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 9 Sep 2024 07:57:43 UTC (191 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Fri, 25 Oct 2024 08:41:17 UTC (3,200 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Sat, 21 Jun 2025 17:29:02 UTC (3,492 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/09/09",
      "title": "Benchmarking and Building Zero-Shot Hindi Retrieval Model with Hindi-BEIR and NLLB-E5",
      "tasks": [
        "Benchmarking",
        "Information Retrieval",
        "Retrieval",
        "Text Retrieval"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17670",
      "abstract": "Large language models (LLMs) exhibit diverse response behaviors, costs, and strengths, making it challenging to select the most suitable LLM for a given user query. We study the problem of adaptive multi-LLM selection in an online setting, where the learner interacts with users through multi-step query refinement and must choose LLMs sequentially without access to offline datasets or model internals. A key challenge arises from unstructured context evolution: the prompt dynamically changes in response to previous model outputs via a black-box process, which cannot be simulated, modeled, or learned. To address this, we propose the first contextual bandit framework for sequential LLM selection under unstructured prompt dynamics. We formalize a notion of myopic regret and develop a LinUCB-based algorithm that provably achieves sublinear regret without relying on future context prediction. We further introduce budget-aware and positionally-aware (favoring early-stage satisfaction) extensions to accommodate variable query costs and user preferences for early high-quality responses. Our algorithms are theoretically grounded and require no offline fine-tuning or dataset-specific training. Experiments on diverse benchmarks demonstrate that our methods outperform existing LLM routing strategies in both accuracy and cost-efficiency, validating the power of contextual bandits for real-time, adaptive LLM selection.",
      "authors": [
        "Poon, Manhin",
        "Dai, XiangXiang",
        "Liu, Xutong",
        "Kong, Fang",
        "Lui, John C. S.",
        "Zuo, Jinhang"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17670",
        "TeX Source": "https://arxiv.org/src/2506.17670",
        "View PDF": "https://arxiv.org/pdf/2506.17670"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 10:01:46 UTC (642 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Online Multi-LLM Selection via Contextual Bandits under Unstructured Context Evolution",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17585",
      "abstract": "Trustworthy language models should provide both correct and verifiable answers. While language models can sometimes attribute their outputs to pretraining data, their citations are often unreliable due to hallucination. As a result, current systems insert citations by querying an external retriever at inference time, introducing latency, infrastructure dependence, and vulnerability to retrieval noise. We explore whether LLMs can be made to reliably attribute to the documents seen during (continual) pretraining--without test-time retrieval--by revising the training process. To evaluate this, we release CitePretrainBench, a benchmark that mixes real-world corpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and probes both short-form (single fact) and long-form (multi-fact) citation tasks. Our approach follows a two-stage process: (1) continual pretraining to bind facts to persistent document identifiers, and (2) instruction tuning to elicit citation behavior. We find that simple Passive Indexing, which appends an identifier to each document, helps memorize verbatim text but fails on paraphrased or compositional facts. Instead, we propose Active Indexing, which continually pretrains on synthetic QA pairs that (1) restate each fact in diverse compositional forms, and (2) require bidirectional source-to-fact and fact-to-source generation, jointly teaching the model to generate content from a cited source and to attribute its own answers. Experiments with Qwen2.5-7B and 3B show that Active Indexing consistently outperforms Passive Indexing across all tasks and models, with citation precision gains up to 30.2 percent. Our ablation studies reveal that performance continues to improve as we scale the amount of augmented data, showing a clear upward trend even at 16 times the original token count.",
      "authors": [
        "Huang, Yukun",
        "Chen, Sanxing",
        "Pei, Jian",
        "Zaheer, Manzil",
        "Dhingra, Bhuwan"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17585v1",
        "Other Formats": "https://arxiv.org/format/2506.17585",
        "TeX Source": "https://arxiv.org/src/2506.17585",
        "View PDF": "https://arxiv.org/pdf/2506.17585"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 04:48:05 UTC (614 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17818",
      "abstract": "Recent advances in music foundation models have improved audio representation learning, yet their effectiveness across diverse musical traditions remains limited. We introduce CultureMERT-95M, a multi-culturally adapted foundation model developed to enhance cross-cultural music representation learning and understanding. To achieve this, we propose a two-stage continual pre-training strategy that integrates learning rate re-warming and re-decaying, enabling stable adaptation even with limited computational resources. Training on a 650-hour multi-cultural data mix, comprising Greek, Turkish, and Indian music traditions, results in an average improvement of 4.9% in ROC-AUC and AP across diverse non-Western music auto-tagging tasks, surpassing prior state-of-the-art, with minimal forgetting on Western-centric benchmarks. We further investigate task arithmetic, an alternative approach to multi-cultural adaptation that merges single-culture adapted models in the weight space. Task arithmetic performs on par with our multi-culturally trained model on non-Western auto-tagging tasks and shows no regression on Western datasets. Cross-cultural evaluation reveals that single-culture models transfer with varying effectiveness across musical traditions, whereas the multi-culturally adapted model achieves the best overall performance. To support research on world music representation learning, we publicly release CultureMERT-95M and CultureMERT-TA-95M, fostering the development of more culturally aware music foundation models.",
      "authors": [
        "Kanatas, Angelos-Nikolaos",
        "Papaioannou, Charilaos",
        "Potamianos, Alexandros"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17818v1",
        "Other Formats": "https://arxiv.org/format/2506.17818",
        "TeX Source": "https://arxiv.org/src/2506.17818",
        "View PDF": "https://arxiv.org/pdf/2506.17818"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 21:16:39 UTC (4,269 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation Learning",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2503.01103",
      "abstract": "While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective, which minimizes the forward KL divergence, inherently suffers from a mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as a unified framework that integrates likelihood-based generative training and GAN-type discrimination to bypass this fundamental constraint by exploiting reverse KL and self-generated negative signals. Our key insight is to parameterize a discriminator implicitly using the likelihood ratio between a learnable target model and a fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of a well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in a self-play manner for progressive model refinement, with each round requiring less than 1% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79/1.58/1.96 to new records of 1.30/0.97/1.26 on CIFAR-10/ImageNet-64/ImageNet 512x512 datasets without any guidance mechanisms, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256x256.",
      "authors": [
        "Zheng, Kaiwen",
        "Chen, Yongxin",
        "Chen, Huayu",
        "He, Guande",
        "Liu, Ming-Yu",
        "Zhu, Jun",
        "Zhang, Qinsheng"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "Other Formats": "https://arxiv.org/format/2503.01103",
        "TeX Source": "https://arxiv.org/src/2503.01103",
        "View PDF": "https://arxiv.org/pdf/2503.01103"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 3 Mar 2025 02:06:22 UTC (13,422 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 12 May 2025 08:12:46 UTC (16,619 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Sat, 21 Jun 2025 15:54:27 UTC (16,132 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2025/03/03",
      "title": "Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator",
      "repo_urls": [
        "https://github.com/nvlabs/ddo"
      ],
      "conference": "direct-discriminative-optimization-your",
      "conference_url_abs": "https://arxiv.org/abs/2503.01103",
      "tasks": [
        "Image Generation"
      ],
      "models": [
        {
          "model_path": "nvidia/DirectDiscriminativeOptimization",
          "downloads": "0",
          "likes": "4",
          "trending_score": "1.0",
          "link": "https://huggingface.co/nvidia/DirectDiscriminativeOptimization"
        }
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17562",
      "abstract": "LLMs have demonstrated significant potential in Medical Report Generation (MRG), yet their development requires large amounts of medical image-report pairs, which are commonly scattered across multiple centers. Centralizing these data is exceptionally challenging due to privacy regulations, thereby impeding model development and broader adoption of LLM-driven MRG models. To address this challenge, we present FedMRG, the first framework that leverages Federated Learning (FL) to enable privacy-preserving, multi-center development of LLM-driven MRG models, specifically designed to overcome the critical challenge of communication-efficient LLM training under multi-modal data heterogeneity. To start with, our framework tackles the fundamental challenge of communication overhead in FL-LLM tuning by employing low-rank factorization to efficiently decompose parameter updates, significantly reducing gradient transmission costs and making LLM-driven MRG feasible in bandwidth-constrained FL settings. Furthermore, we observed the dual heterogeneity in MRG under the FL scenario: varying image characteristics across medical centers, as well as diverse reporting styles and terminology preferences. To address this, we further enhance FedMRG with (1) client-aware contrastive learning in the MRG encoder, coupled with diagnosis-driven prompts, which capture both globally generalizable and locally distinctive features while maintaining diagnostic accuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder that harmonizes generic and specialized adapters to address variations in reporting styles and terminology. Through extensive evaluation of our established FL-MRG benchmark, we demonstrate the generalizability and adaptability of FedMRG, underscoring its potential in harnessing multi-center data and generating clinically accurate reports while maintaining communication efficiency.",
      "authors": [
        "Che, Haoxuan",
        "Jin, Haibo",
        "Guo, Zhengrui",
        "Lin, Yi",
        "Jin, Cheng",
        "Chen, Hao"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17562v1",
        "Other Formats": "https://arxiv.org/format/2506.17562",
        "TeX Source": "https://arxiv.org/src/2506.17562",
        "View PDF": "https://arxiv.org/pdf/2506.17562"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 03:13:08 UTC (2,172 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2205.07348",
      "abstract": "Automatic food detection is an emerging topic of interest due to its wide array of applications ranging from detecting food images on social media platforms to filtering non-food photos from the users in dietary assessment apps. Recently, during the COVID-19 pandemic, it has facilitated enforcing an eating ban by automatically detecting eating activities from cameras in public places. Therefore, to tackle the challenge of recognizing food images with high accuracy, we proposed the idea of a hybrid framework for extracting and selecting optimal features from an efficient neural network. There on, a nonlinear classifier is employed to discriminate between linearly inseparable feature vectors with great precision. In line with this idea, our method extracts features from MobileNetV3, selects an optimal subset of attributes by using Shapley Additive exPlanations (SHAP) values, and exploits kernel extreme learning machine (KELM) due to its nonlinear decision boundary and good generalization ability. However, KELM suffers from the 'curse of dimensionality problem' for large datasets due to the complex computation of kernel matrix with large numbers of hidden nodes. We solved this problem by proposing a novel multicolumn kernel extreme learning machine (MCKELM) which exploited the k-d tree algorithm to divide data into N subsets and trains separate KELM on each subset of data. Then, the method incorporates KELM classifiers into parallel structures and selects the top k nearest subsets during testing by using the k-d tree search for classifying input instead of the whole network. For evaluating a proposed framework large food/non-food dataset is prepared using nine publically available datasets. Experimental results showed the superiority of our method on an integrated set of measures while solving the problem of 'curse of dimensionality in KELM for large datasets.",
      "authors": [
        "Tahir, Ghalib Ahmed",
        "Loo, Chu Kiong"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2205.07348v2",
        "Other Formats": "https://arxiv.org/format/2205.07348",
        "TeX Source": "https://arxiv.org/src/2205.07348",
        "View PDF": "https://arxiv.org/pdf/2205.07348"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)",
        "Image and Video Processing (eess.IV)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 15 May 2022 18:07:43 UTC (10,021 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 04:54:04 UTC (9,872 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2022/05/15",
      "title": "Novel Multicolumn Kernel Extreme Learning Machine for Food Detection via Optimal Features from CNN",
      "tasks": [
        "Efficient Neural Network"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2503.21411",
      "abstract": "Modern transportation systems face pressing challenges due to increasing demand, dynamic environments, and heterogeneous information integration. The rapid evolution of Large Language Models (LLMs) offers transformative potential to address these challenges. Extensive knowledge and high-level capabilities derived from pretraining evolve the default role of LLMs as text generators to become versatile, knowledge-driven task solvers for intelligent transportation systems. This survey first presents LLM4TR, a novel conceptual framework that systematically categorizes the roles of LLMs in transportation into four synergetic dimensions: information processors, knowledge encoders, component generators, and decision facilitators. Through a unified taxonomy, we systematically elucidate how LLMs bridge fragmented data pipelines, enhance predictive analytics, simulate human-like reasoning, and enable closed-loop interactions across sensing, learning, modeling, and managing tasks in transportation systems. For each role, our review spans diverse applications, from traffic prediction and autonomous driving to safety analytics and urban mobility optimization, highlighting how emergent capabilities of LLMs such as in-context learning and step-by-step reasoning can enhance the operation and management of transportation systems. We further curate practical guidance, including available resources and computational guidelines, to support real-world deployment. By identifying challenges in existing LLM-based solutions, this survey charts a roadmap for advancing LLM-driven transportation research, positioning LLMs as central actors in the next generation of cyber-physical-social mobility ecosystems. Online resources can be found in the project page: https://github.com/tongnie/awesome-llm4tr.",
      "authors": [
        "Nie, Tong",
        "Sun, Jian",
        "Ma, Wei"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2503.21411v2",
        "Other Formats": "https://arxiv.org/format/2503.21411",
        "TeX Source": "https://arxiv.org/src/2503.21411",
        "View PDF": "https://arxiv.org/pdf/2503.21411"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 27 Mar 2025 11:56:27 UTC (1,527 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 02:19:45 UTC (1,535 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/03/27",
      "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
      "repo_urls": [
        "https://github.com/tongnie/awesome-llm4tr"
      ],
      "tasks": [
        "Autonomous Driving",
        "In-Context Learning",
        "Traffic Prediction"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17645",
      "abstract": "Automating medical report generation from histopathology images is a critical challenge requiring effective visual representations and domain-specific knowledge. Inspired by the common practices of human experts, we propose an in-context learning framework called PathGenIC that integrates context derived from the training set with a multimodal in-context learning (ICL) mechanism. Our method dynamically retrieves semantically similar whole slide image (WSI)-report pairs and incorporates adaptive feedback to enhance contextual relevance and generation quality. Evaluated on the HistGen benchmark, the framework achieves state-of-the-art results, with significant improvements across BLEU, METEOR, and ROUGE-L metrics, and demonstrates robustness across diverse report lengths and disease categories. By maximizing training data utility and bridging vision and language with ICL, our work offers a solution for AI-driven histopathology reporting, setting a strong foundation for future advancements in multimodal clinical applications.",
      "authors": [
        "Liu, Shih-Wen",
        "Fan, Hsuan-Yu",
        "Chu, Wei-Ta",
        "Yang, Fu-En",
        "Wang, Yu-Chiang Frank"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17645v1",
        "Other Formats": "https://arxiv.org/format/2506.17645",
        "TeX Source": "https://arxiv.org/src/2506.17645",
        "View PDF": "https://arxiv.org/pdf/2506.17645"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 08:56:45 UTC (3,234 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Histopathology Image Report Generation by Vision Language Model with Multimodal In-Context Learning",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17627",
      "abstract": "Concerns about benchmark leakage in large language models for code (Code LLMs) have raised issues of data contamination and inflated evaluation metrics. The diversity and inaccessibility of many training datasets make it difficult to prevent data leakage entirely, even with time lag strategies. Consequently, generating new datasets through code perturbation has become essential. However, existing methods often fail to produce complex and diverse variations, struggle with complex cross-file dependencies, and lack support for multiple programming languages, which limits their effectiveness in enhancing LLM evaluations for coding tasks. To fill this gap, we propose CodeMorph, an approach designed to support multiple programming languages while preserving cross-file dependencies to mitigate data leakage. CodeMorph consists of two main components that work together to enhance the perturbation process. The first component employs 26 semantic-preserving transformation methods to iteratively perturb code, generating diverse variations while ensuring that the modified code remains compilable. The second component introduces a genetic algorithm-based selection algorithm, PESO, to identify the more effective perturbation method for each iteration by targeting lower similarity scores between the perturbed and original code, thereby enhancing overall perturbation effectiveness. Experimental results demonstrate that after applying CodeMorph, the accuracy of the LLM on code completion tasks across five programming languages decreased by an average of 24.67%, with Python showing the most significant reduction at 45%. The similarity score of code optimized by PESO is, on average, 7.01% lower than that of randomly perturbed code, peaking at a reduction of 42.86%.",
      "authors": [
        "Rao, Hongzhou",
        "Zhao, Yanjie",
        "Zhu, Wenjie",
        "Xiao, Ling",
        "Wang, Meizhen",
        "Wang, Haoyu"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17627v1",
        "Other Formats": "https://arxiv.org/format/2506.17627",
        "TeX Source": "https://arxiv.org/src/2506.17627",
        "View PDF": "https://arxiv.org/pdf/2506.17627"
      },
      "subjects": [
        "Software Engineering (cs.SE)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 08:04:12 UTC (1,836 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "CodeMorph: Mitigating Data Leakage in Large Language Model Assessment",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17774",
      "abstract": "Foundation models have achieved remarkable success across video, image, and language domains. By scaling up the number of parameters and training datasets, these models acquire generalizable world knowledge and often surpass task-specific approaches. However, such progress has yet to extend to the domain of physics simulation. A primary bottleneck is data scarcity: while millions of images, videos, and textual resources are readily available on the internet, the largest physics simulation datasets contain only tens of thousands of samples. This data limitation hinders the use of large models, as overfitting becomes a major concern. As a result, physics applications typically rely on small models, which struggle with long-range prediction due to limited context understanding. Additionally, unlike images, videos, or text-which typically exhibit fixed granularity-physics datasets often vary drastically in scale, amplifying the challenges of scaling up multitask training. We introduce PhysiX, the first large-scale foundation model for physics simulation. PhysiX is a 4.5B parameter autoregressive generative model. It uses a discrete tokenizer to encode physical processes at different scales into a sequence of discrete tokens, and employs an autoregressive next-token prediction objective to model such processes in the token space. To mitigate the rounding error in the discretization process, PhysiX incorporates a specialized refinement module. Through extensive experiments, we show that PhysiX effectively addresses the data bottleneck, outperforming task-specific baselines under comparable settings as well as the previous absolute state-of-the-art approaches on The Well benchmark. Our results indicate that knowledge learned from natural videos can be successfully transferred to physics simulation, and that joint training across diverse simulation tasks enables synergistic learning.",
      "authors": [
        "Nguyen, Tung",
        "Koneru, Arsh",
        "Li, Shufan",
        "grover, Aditya"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17774v1",
        "Other Formats": "https://arxiv.org/format/2506.17774",
        "TeX Source": "https://arxiv.org/src/2506.17774",
        "View PDF": "https://arxiv.org/pdf/2506.17774"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 18:10:12 UTC (7,961 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "PhysiX: A Foundation Model for Physics Simulations",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2411.01418",
      "abstract": "Accurately predicting blood glucose (BG) levels of ICU patients is critical, as both hypoglycemia (BG < 70 mg/dL) and hyperglycemia (BG > 180 mg/dL) are associated with increased morbidity and mortality. This study presents a proof-of-concept machine learning framework, the Multi-source Irregular Time-Series Transformer (MITST), designed to predict BG levels in ICU patients. In contrast to existing methods that rely heavily on manual feature engineering or utilize limited Electronic Health Record (EHR) data sources, MITST integrates diverse clinical data--including laboratory results, medications, and vital signs without predefined aggregation. The model leverages a hierarchical Transformer architecture, designed to capture interactions among features within individual timestamps, temporal dependencies across different timestamps, and semantic relationships across multiple data sources. Evaluated using the extensive eICU database (200,859 ICU stays across 208 hospitals), MITST achieves a statistically significant ( p < 0.001 ) average improvement of 1.7 percentage points (pp) in AUROC and 1.8 pp in AUPRC over a state-of-the-art random forest baseline. Crucially, for hypoglycemia--a rare but life-threatening condition--MITST increases sensitivity by 7.2 pp, potentially enabling hundreds of earlier interventions across ICU populations. The flexible architecture of MITST allows seamless integration of new data sources without retraining the entire model, enhancing its adaptability for clinical decision support. While this study focuses on predicting BG levels, we also demonstrate MITST's ability to generalize to a distinct clinical task (in-hospital mortality prediction), highlighting its potential for broader applicability in ICU settings. MITST thus offers a robust and extensible solution for analyzing complex, multi-source, irregular time-series data.",
      "authors": [
        "Mehdizavareh, Hadi",
        "Khan, Arijit",
        "Cichosz, Simon Lebech"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2411.01418v3",
        "Other Formats": "https://arxiv.org/format/2411.01418",
        "TeX Source": "https://arxiv.org/src/2411.01418",
        "View PDF": "https://arxiv.org/pdf/2411.01418"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Signal Processing (eess.SP)",
        "Quantitative Methods (q-bio.QM)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 3 Nov 2024 03:03:11 UTC (798 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sun, 26 Jan 2025 17:40:38 UTC (670 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Sat, 21 Jun 2025 18:18:52 UTC (827 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/11/03",
      "title": "Enhancing Glucose Level Prediction of ICU Patients through Hierarchical Modeling of Irregular Time-Series",
      "repo_urls": [
        "https://github.com/zavareh89/MITST"
      ],
      "tasks": [
        "Data Integration",
        "Feature Engineering",
        "Irregular Time Series",
        "Time Series",
        "Time Series Analysis"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17765",
      "abstract": "Current recommendation systems often require some form of textual data summarization, such as generating concise and coherent titles for product carousels or other grouped item displays. While large language models have shown promise in NLP domains for textual summarization, these approaches do not directly apply to recommendation systems, where explanations must be highly relevant to the core features of item sets, adhere to strict word limit constraints. In this paper, we propose CARTS (Collaborative Agents for Recommendation Textual Summarization), a multi-agent LLM framework designed for structured summarization in recommendation systems. CARTS decomposes the task into three stages-Generation Augmented Generation (GAG), refinement circle, and arbitration, where successive agent roles are responsible for extracting salient item features, iteratively refining candidate titles based on relevance and length feedback, and selecting the final title through a collaborative arbitration process. Experiments on large-scale e-commerce data and live A/B testing show that CARTS significantly outperforms single-pass and chain-of-thought LLM baselines, delivering higher title relevance and improved user engagement metrics.",
      "authors": [
        "Chen, Jiao",
        "Yao, Kehui",
        "Maragheh, Reza Yousefi",
        "Zhao, Kai",
        "Xu, Jianpeng",
        "Cho, Jason",
        "Korpeoglu, Evren",
        "Kumar, Sushant",
        "Achan, Kannan"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17765v1",
        "Other Formats": "https://arxiv.org/format/2506.17765",
        "TeX Source": "https://arxiv.org/src/2506.17765",
        "View PDF": "https://arxiv.org/pdf/2506.17765"
      },
      "subjects": [
        "Information Retrieval (cs.IR)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 17:18:35 UTC (2,228 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "CARTS: Collaborative Agents for Recommendation Textual Summarization",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17675",
      "abstract": "In this paper, we introduce the notion of neural simulation gap functions, which formally quantifies the gap between the mathematical model and the model in the high-fidelity simulator, which closely resembles reality. Many times, a controller designed for a mathematical model does not work in reality because of the unmodelled gap between the two systems. With the help of this simulation gap function, one can use existing model-based tools to design controllers for the mathematical system and formally guarantee a decent transition from the simulation to the real world. Although in this work, we have quantified this gap using a neural network, which is trained using a finite number of data points, we give formal guarantees on the simulation gap function for the entire state space including the unseen data points. We collect data from high-fidelity simulators leveraging recent advancements in Real-to-Sim transfer to ensure close alignment with reality. We demonstrate our results through two case studies - a Mecanum bot and a Pendulum.",
      "authors": [
        "Sangeerth, P",
        "Jagtap, Pushpak"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17675v1",
        "Other Formats": "https://arxiv.org/format/2506.17675",
        "TeX Source": "https://arxiv.org/src/2506.17675",
        "View PDF": "https://arxiv.org/pdf/2506.17675"
      },
      "subjects": [
        "Systems and Control (eess.SY)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 10:53:09 UTC (771 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Quantification of Sim2Real Gap via Neural Simulation Gap Function",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17597",
      "abstract": "Purpose: To develop an age prediction model which is interpretable and robust to demographic and technological variances in brain MRI scans. Materials and Methods: We propose a transformer-based architecture that leverages self-supervised pre-training on large-scale datasets. Our model processes pseudo-3D T1-weighted MRI scans from three anatomical views and incorporates brain volumetric information. By introducing a stem architecture, we reduce the conventional quadratic complexity of transformer models to linear complexity, enabling scalability for high-dimensional MRI data. We trained our model on ADNI2 $\\&$ 3 (N=1348) and OASIS3 (N=716) datasets (age range: 42 - 95) from the North America, with an 8:1:1 split for train, validation and test. Then, we validated it on the AIBL dataset (N=768, age range: 60 - 92) from Australia. Results: We achieved an MAE of 3.65 years on ADNI2 $\\&$ 3 and OASIS3 test set and a high generalizability of MAE of 3.54 years on AIBL. There was a notable increase in brain age gap (BAG) across cognitive groups, with mean of 0.15 years (95% CI: [-0.22, 0.51]) in CN, 2.55 years ([2.40, 2.70]) in MCI, 6.12 years ([5.82, 6.43]) in AD. Additionally, significant negative correlation between BAG and cognitive scores was observed, with correlation coefficient of -0.185 (p < 0.001) for MoCA and -0.231 (p < 0.001) for MMSE. Gradient-based feature attribution highlighted ventricles and white matter structures as key regions influenced by brain aging. Conclusion: Our model effectively fused information from different views and volumetric information to achieve state-of-the-art brain age prediction accuracy, improved generalizability and interpretability with association to neurodegenerative disorders.",
      "authors": [
        "Kan, Pengyu",
        "Jones, Craig",
        "Oishi, Kenichi"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17597v1",
        "Other Formats": "https://arxiv.org/format/2506.17597",
        "TeX Source": "https://arxiv.org/src/2506.17597",
        "View PDF": "https://arxiv.org/pdf/2506.17597"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 05:24:42 UTC (3,894 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "OpenMAP-BrainAge: Generalizable and Interpretable Brain Age Predictor",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17576",
      "abstract": "Graph Convolutional Networks (GCNs) suffer from severe performance degradation in deep architectures due to over-smoothing. While existing studies primarily attribute the over-smoothing to repeated applications of graph Laplacian operators, our empirical analysis reveals a critical yet overlooked factor: trainable linear transformations in GCNs significantly exacerbate feature collapse, even at moderate depths (e.g., 8 layers). In contrast, Simplified Graph Convolution (SGC), which removes these transformations, maintains stable feature diversity up to 32 layers, highlighting linear transformations' dual role in facilitating expressive power and inducing over-smoothing. However, completely removing linear transformations weakens the model's expressive capacity. To address this trade-off, we propose Layer-wise Gradual Training (LGT), a novel training strategy that progressively builds deep GCNs while preserving their expressiveness. LGT integrates three complementary components: (1) layer-wise training to stabilize optimization from shallow to deep layers, (2) low-rank adaptation to fine-tune shallow layers and accelerate training, and (3) identity initialization to ensure smooth integration of new layers and accelerate convergence. Extensive experiments on benchmark datasets demonstrate that LGT achieves state-of-the-art performance on vanilla GCN, significantly improving accuracy even in 32-layer settings. Moreover, as a training method, LGT can be seamlessly combined with existing methods such as PairNorm and ContraNorm, further enhancing their performance in deeper networks. LGT offers a general, architecture-agnostic training framework for scalable deep GCNs. The code is available at [https://github.com/jfklasdfj/LGT_GCN].",
      "authors": [
        "Peng, Furong",
        "Gao, Jinzhen",
        "Lu, Xuan",
        "Liu, Kang",
        "Huo, Yifan",
        "Wang, Sheng"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17576v1",
        "Other Formats": "https://arxiv.org/format/2506.17576",
        "TeX Source": "https://arxiv.org/src/2506.17576",
        "View PDF": "https://arxiv.org/pdf/2506.17576"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 04:09:02 UTC (1,713 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Towards Deeper GCNs: Alleviating Over-smoothing via Iterative Training and Fine-tuning",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17748",
      "abstract": "Contemporary Language Models (LMs), while impressively fluent, often generate content that is factually incorrect or unfaithful to the input context - a critical issue commonly referred to as 'hallucination'. This tendency of LMs to generate hallucinated content undermines their reliability, especially because these fabrications are often highly convincing and therefore difficult to detect. While several existing methods attempt to detect hallucinations, most rely on analyzing multiple generations per input, leading to increased computational cost and latency. To address this, we propose a single-pass, training-free approach for effective Hallucination detectIon via Decoupled rEpresentations (HIDE). Our approach leverages the hypothesis that hallucinations result from a statistical decoupling between an LM's internal representations of input context and its generated output. We quantify this decoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to hidden-state representations extracted while generating the output sequence. We conduct extensive experiments on four diverse question answering datasets, evaluating both faithfulness and factuality hallucinations across six open-source LMs of varying scales and properties. Our results demonstrate that HIDE outperforms other single-pass methods in almost all settings, achieving an average relative improvement of ~29% in AUC-ROC over the best-performing single-pass strategy across various models and datasets. Additionally, HIDE shows competitive and often superior performance with multi-pass state-of-the-art methods, obtaining an average relative improvement of ~3% in AUC-ROC while consuming ~51% less computation time. Our findings highlight the effectiveness of exploiting internal representation decoupling in LMs for efficient and practical hallucination detection.",
      "authors": [
        "Chatterjee, Anwoy",
        "Goel, Yash",
        "Chakraborty, Tanmoy"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17748v1",
        "Other Formats": "https://arxiv.org/format/2506.17748",
        "TeX Source": "https://arxiv.org/src/2506.17748",
        "View PDF": "https://arxiv.org/pdf/2506.17748"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 16:02:49 UTC (335 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.09075",
      "abstract": "This work examines risk bounds for nonparametric distributional regression estimators. For convex-constrained distributional regression, general upper bounds are established for the continuous ranked probability score (CRPS) and the worst-case mean squared error (MSE) across the domain. These theoretical results are applied to isotonic and trend filtering distributional regression, yielding convergence rates consistent with those for mean estimation. Furthermore, a general upper bound is derived for distributional regression under non-convex constraints, with a specific application to neural network-based estimators. Comprehensive experiments on both simulated and real data validate the theoretical contributions, demonstrating their practical effectiveness.",
      "authors": [
        "Padilla, Carlos Misael Madrid",
        "Padilla, Oscar Hernan Madrid",
        "Chatterjee, Sabyasachi"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2505.09075v2",
        "Other Formats": "https://arxiv.org/format/2505.09075",
        "TeX Source": "https://arxiv.org/src/2505.09075",
        "View PDF": "https://arxiv.org/pdf/2505.09075"
      },
      "subjects": [
        "Machine Learning (stat.ML)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 14 May 2025 02:22:12 UTC (5,592 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 05:03:34 UTC (5,593 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/14",
      "title": "Risk Bounds For Distributional Regression",
      "tasks": [
        "regression"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17636",
      "abstract": "Recent developments in 3D Gaussian Splatting have made significant advances in surface reconstruction. However, scaling these methods to large-scale scenes remains challenging due to high computational demands and the complex dynamic appearances typical of outdoor environments. These challenges hinder the application in aerial surveying and autonomous driving. This paper proposes a novel solution to reconstruct large-scale surfaces with fine details, supervised by full-sized images. Firstly, we introduce a coarse-to-fine strategy to reconstruct a coarse model efficiently, followed by adaptive scene partitioning and sub-scene refining from image segments. Additionally, we integrate a decoupling appearance model to capture global appearance variations and a transient mask model to mitigate interference from moving objects. Finally, we expand the multi-view constraint and introduce a single-view regularization for texture-less areas. Our experiments were conducted on the publicly available dataset GauU-Scene V2, which was captured using unmanned aerial vehicles. To the best of our knowledge, our method outperforms existing NeRF-based and Gaussian-based methods, achieving high-fidelity visual results and accurate surface from full-size image optimization. Open-source code will be available on GitHub.",
      "authors": [
        "Chen, Shihan",
        "Li, Zhaojin",
        "Chen, Zeyu",
        "Yan, Qingsong",
        "Shen, Gaoyang",
        "Duan, Ran"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17636v1",
        "Other Formats": "https://arxiv.org/format/2506.17636",
        "TeX Source": "https://arxiv.org/src/2506.17636",
        "View PDF": "https://arxiv.org/pdf/2506.17636"
      },
      "subjects": [
        "Graphics (cs.GR)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Image and Video Processing (eess.IV)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 08:41:28 UTC (1,309 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in Large-Scale Scene",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17781",
      "abstract": "Dense embeddings are fundamental to modern machine learning systems, powering Retrieval-Augmented Generation (RAG), information retrieval, and representation learning. While instruction-conditioning has become the dominant approach for embedding specialization, its direct application to low-capacity models imposes fundamental representational constraints that limit the performance gains derived from specialization. In this paper, we analyze these limitations and introduce the Mixture of Task Experts (MoTE) transformer block, which leverages task-specialized parameters trained with Task-Aware Contrastive Learning (\\tacl) to enhance the model ability to generate specialized embeddings. Empirical results show that MoTE achieves $64\\%$ higher performance gains in retrieval datasets ($+3.27 \\rightarrow +5.21$) and $43\\%$ higher performance gains across all datasets ($+1.81 \\rightarrow +2.60$). Critically, these gains are achieved without altering instructions, training data, inference time, or number of active parameters.",
      "authors": [
        "Romero, Miguel",
        "Ding, Shuoyang",
        "Barret, Corey D.",
        "Dinu, Georgiana",
        "Karypis, George"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17781v1",
        "Other Formats": "https://arxiv.org/format/2506.17781",
        "TeX Source": "https://arxiv.org/src/2506.17781",
        "View PDF": "https://arxiv.org/pdf/2506.17781"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 18:28:25 UTC (3,427 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Beyond instruction-conditioning, MoTE: Mixture of Task Experts for Multi-task Embedding Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2504.17028",
      "abstract": "This paper demonstrates the feasibility of democratizing AI-driven global weather forecasting models among university research groups by leveraging Graphics Processing Units (GPUs) and freely available AI models, such as NVIDIA's FourCastNetv2. FourCastNetv2 is an NVIDIA's advanced neural network for weather prediction and is trained on a 73-channel subset of the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset at single levels and different pressure levels. Although the training specifications for FourCastNetv2 are not released to the public, the training documentation of the model's first generation, FourCastNet, is available to all users. The training had 64 A100 GPUs and took 16 hours to complete. Although NVIDIA's models offer significant reductions in both time and cost compared to traditional Numerical Weather Prediction (NWP), reproducing published forecasting results presents ongoing challenges for resource-constrained university research groups with limited GPU availability. We demonstrate both (i) leveraging FourCastNetv2 to create predictions through the designated application programming interface (API) and (ii) utilizing NVIDIA hardware to train the original FourCastNet model. Further, this paper demonstrates the capabilities and limitations of NVIDIA A100's for resource-limited research groups in universities. We also explore data management, training efficiency, and model validation, highlighting the advantages and challenges of using limited high-performance computing resources. Consequently, this paper and its corresponding GitHub materials may serve as an initial guide for other university research groups and courses related to machine learning, climate science, and data science to develop research and education programs on AI weather forecasting, and hence help democratize the AI NWP in the digital economy.",
      "authors": [
        "Khadir, Iman",
        "Stevenson, Shane",
        "Li, Henry",
        "Krick, Kyle",
        "Burrows, Abram",
        "Hall, David",
        "Posey, Stan",
        "Shen, Samuel S. P."
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2504.17028v2",
        "Other Formats": "https://arxiv.org/format/2504.17028",
        "TeX Source": "https://arxiv.org/src/2504.17028",
        "View PDF": "https://arxiv.org/pdf/2504.17028"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Atmospheric and Oceanic Physics (physics.ao-ph)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 23 Apr 2025 18:15:31 UTC (3,681 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 00:51:40 UTC (5,749 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/04/23",
      "title": "Democracy of AI Numerical Weather Models: An Example of Global Forecasting with FourCastNetv2 Made by a University Research Lab Using GPU",
      "tasks": [
        "Weather Forecasting"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.14436",
      "abstract": "Adapting large-scale foundation models in multi-task scenarios often suffers from task conflict and oblivion. To mitigate such issues, we propose a novel ''model MoE-ization'' strategy that leads to a conflict- and oblivion-resistant multi-task adaptation method. Given a weight matrix of a pre-trained model, our method applies SVD to it and introduces a learnable router to adjust its singular values based on tasks and samples. Accordingly, the weight matrix becomes a Mixture of Orthogonal Rank-one Experts (MoORE), in which each expert corresponds to the outer product of a left singular vector and the corresponding right one. We can improve the model capacity by imposing a learnable orthogonal transform on the right singular vectors. Unlike low-rank adaptation (LoRA) and its MoE-driven variants, MoORE guarantees the experts' orthogonality and maintains the column space of the original weight matrix. These two properties make the adapted model resistant to the conflicts among the new tasks and the oblivion of its original tasks, respectively. Experiments on various datasets demonstrate that MoORE outperforms existing multi-task adaptation methods consistently, showing its superiority in terms of conflict- and oblivion-resistance. The code of the experiments is available at https://github.com/DaShenZi721/MoORE.",
      "authors": [
        "Yuan, Shen",
        "Zheng, Yin",
        "Wang, Taifeng",
        "Liu, Binbin",
        "Xu, Hongteng"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.14436v2",
        "Other Formats": "https://arxiv.org/format/2506.14436",
        "TeX Source": "https://arxiv.org/src/2506.14436",
        "View PDF": "https://arxiv.org/pdf/2506.14436"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Tue, 17 Jun 2025 11:55:08 UTC (2,346 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 15:53:00 UTC (2,351 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/06/17",
      "title": "MoORE: SVD-based Model MoE-ization for Conflict- and Oblivion-Resistant Multi-Task Adaptation",
      "tasks": [],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17740",
      "abstract": "Multi-condition fault diagnosis is prevalent in industrial systems and presents substantial challenges for conventional diagnostic approaches. The discrepancy in data distributions across different operating conditions degrades model performance when a model trained under one condition is applied to others. With the recent advancements in deep learning, transfer learning has been introduced to the fault diagnosis field as a paradigm for addressing multi-condition fault diagnosis. Among these methods, domain generalization approaches can handle complex scenarios by extracting condition-invariant fault features. Although many studies have considered fault diagnosis in specific multi-condition scenarios, the extent to which operating conditions affect fault information has been scarcely studied, which is crucial. However, the extent to which operating conditions affect fault information has been scarcely studied, which is crucial. When operating conditions have a significant impact on fault features, directly applying domain generalization methods may lead the model to learn condition-specific information, thereby reducing its overall generalization ability. This paper investigates the performance of existing end-to-end domain generalization methods under varying conditions, specifically in variable-speed and variable-load scenarios, using multiple experiments on a real-world gearbox. Additionally, a two-stage diagnostic framework is proposed, aiming to improve fault diagnosis performance under scenarios with significant operating condition impacts. By incorporating a domain-generalized encoder with a retraining strategy, the framework is able to extract condition-invariant fault features while simultaneously alleviating potential overfitting to the source domain. Several experiments on a real-world gearbox dataset are conducted to validate the effectiveness of the proposed approach.",
      "authors": [
        "Han, Pengyu",
        "Liu, Zeyi",
        "Chen, Shijin",
        "Zou, Dongliang",
        "He, Xiao"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17740v1",
        "Other Formats": "https://arxiv.org/format/2506.17740",
        "TeX Source": "https://arxiv.org/src/2506.17740",
        "View PDF": "https://arxiv.org/pdf/2506.17740"
      },
      "subjects": [
        "Signal Processing (eess.SP)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 15:34:51 UTC (2,083 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Rethinking the Role of Operating Conditions for Learning-based Multi-condition Fault Diagnosis",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2405.10513",
      "abstract": "Federated learning (FL) necessitates that edge devices conduct local training and communicate with a parameter server, resulting in significant energy consumption. A key challenge in practical FL systems is the rapid depletion of battery-limited edge devices, which limits their operational lifespan and impacts learning performance. To tackle this issue, we implement energy harvesting techniques in FL systems to capture ambient energy, thereby providing continuous power to edge devices. We first establish the convergence bound for the wireless FL system with energy harvesting devices, illustrating that the convergence is affected by partial device participation and packet drops, both of which depend on the energy supply. To accelerate the convergence, we formulate a joint device scheduling and power control problem and model it as a Markov decision process (MDP). By solving this MDP, we derive the optimal transmission policy and demonstrate that it possesses a monotone structure with respect to the battery and channel states. To overcome the curse of dimensionality caused by the exponential complexity of computing the optimal policy, we propose a low-complexity algorithm, which is asymptotically optimal as the number of devices increases. Furthermore, for unknown channels and harvested energy statistics, we develop a structure-enhanced deep reinforcement learning algorithm that leverages the monotone structure of the optimal policy to improve the training performance. Finally, extensive numerical experiments on real-world datasets are presented to validate the theoretical results and corroborate the effectiveness of the proposed algorithms.",
      "authors": [
        "Zhang, Kai",
        "Cao, Xuanyu",
        "Letaief, Khaled B."
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2405.10513v2",
        "Other Formats": "https://arxiv.org/format/2405.10513",
        "TeX Source": "https://arxiv.org/src/2405.10513",
        "View PDF": "https://arxiv.org/pdf/2405.10513"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Signal Processing (eess.SP)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 17 May 2024 03:41:40 UTC (3,986 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 06:59:52 UTC (440 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/05/17",
      "title": "Federated Learning With Energy Harvesting Devices: An MDP Framework",
      "tasks": [
        "Deep Reinforcement Learning",
        "Federated Learning",
        "Scheduling"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17755",
      "abstract": "Retired electric vehicle batteries offer immense potential to support low-carbon energy systems, but uncertainties in their degradation behavior and data inaccessibilities under second-life use pose major barriers to safe and scalable deployment. This work proposes a Physics-Informed Mixture of Experts (PIMOE) network that computes battery degradation trajectories using partial, field-accessible signals in a single cycle. PIMOE leverages an adaptive multi-degradation prediction module to classify degradation modes using expert weight synthesis underpinned by capacity-voltage and relaxation data, producing latent degradation trend embeddings. These are input to a use-dependent recurrent network for long-term trajectory prediction. Validated on 207 batteries across 77 use conditions and 67,902 cycles, PIMOE achieves an average mean absolute percentage (MAPE) errors of 0.88% with a 0.43 ms inference time. Compared to the state-of-the-art Informer and PatchTST, it reduces computational time and MAPE by 50%, respectively. Compatible with random state of charge region sampling, PIMOE supports 150-cycle forecasts with 1.50% average and 6.26% maximum MAPE, and operates effectively even with pruned 5MB training data. Broadly, PIMOE framework offers a deployable, history-free solution for battery degradation trajectory computation, redefining how second-life energy storage systems are assessed, optimized, and integrated into the sustainable energy landscape.",
      "authors": [
        "Huang, Xinghao",
        "Tao, Shengyu",
        "Liang, Chen",
        "Chen, Jiawei",
        "Shi, Junzhe",
        "Li, Yuqi",
        "Xia, Bizhong",
        "Zhou, Guangmin",
        "Zhang, Xuan"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17755",
        "View PDF": "https://arxiv.org/pdf/2506.17755"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 16:26:59 UTC (11,789 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Physics-informed mixture of experts network for interpretable battery degradation trajectory computation amid second-life complexities",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17709",
      "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable utility across diverse applications, and their growing complexity has made Machine Learning as a Service (MLaaS) a viable platform for scalable deployment. However, this accessibility also exposes GNN to serious security threats, most notably model extraction attacks (MEAs), in which adversaries strategically query a deployed model to construct a high-fidelity replica. In this work, we evaluate the vulnerability of GNNs to MEAs and explore their potential for cost-effective model acquisition in non-adversarial research settings. Importantly, adaptive node querying strategies can also serve a critical role in research, particularly when labeling data is expensive or time-consuming. By selectively sampling informative nodes, researchers can train high-performing GNNs with minimal supervision, which is particularly valuable in domains such as biomedicine, where annotations often require expert input. To address this, we propose a node querying strategy tailored to a highly practical yet underexplored scenario, where bulk queries are prohibited, and only a limited set of initial nodes is available. Our approach iteratively refines the node selection mechanism over multiple learning cycles, leveraging historical feedback to improve extraction efficiency. Extensive experiments on benchmark graph datasets demonstrate our superiority over comparable baselines on accuracy, fidelity, and F1 score under strict query-size constraints. These results highlight both the susceptibility of deployed GNNs to extraction attacks and the promise of ethical, efficient GNN acquisition methods to support low-resource research environments.",
      "authors": [
        "Wang, Zebin",
        "Lin, Menghan",
        "Shen, Bolin",
        "Anderson, Ken",
        "Liu, Molei",
        "Cai, Tianxi",
        "Dong, Yushun"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17709v1",
        "Other Formats": "https://arxiv.org/format/2506.17709",
        "TeX Source": "https://arxiv.org/src/2506.17709",
        "View PDF": "https://arxiv.org/pdf/2506.17709"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Cryptography and Security (cs.CR)",
        "Machine Learning (stat.ML)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 13:11:42 UTC (336 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "CEGA: A Cost-Effective Approach for Graph-Based Model Extraction and Acquisition",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17779",
      "abstract": "Autonomous agents must know how to explore user interfaces (UIs) for reliable task solving, yet systematic evaluation of this crucial phase is lacking. We introduce UIExplore-Bench, the first benchmark explicitly dedicated to UI exploration. The benchmark evaluates agents with either Structured mode (granting access to layout information like DOM trees) or Screen mode (relying on GUI-only observations such as screenshots and human-like mouse/keyboard interactions) across three levels in a standardized GitLab sandbox environment. We formalize exploration as the process of maximizing the set of actionable UI components discovered and propose a metric, human-normalized UI-Functionalities Observed (hUFO), to quantify the effectiveness of exploration. Our results show that UIExplore-AlGo achieves the leading mean hUFO scores, reaching up to 77.2% of human performance in Structured mode and 59.0% in Screen mode at 2,000 steps, particularly excelling at the Sparse level. The results highlight the relevance of our benchmark, as current agents show a substantial performance gap compared to one hour of human expert exploration, indicating ample room for future advancements. We publicly release the benchmark environment, an exploration dataset, and an evaluation suite to catalyze research into efficient UI exploration strategies and their downstream applications, such as experience-driven task completion and automated training data generation.",
      "authors": [
        "Nica, Andrei Cristian",
        "Shanbhogue, Akshaya Vishnu Kudlu",
        "Shah, Harshil",
        "Cambray, Aleix",
        "Berariu, Tudor",
        "Maystre, Lucas",
        "Barber, David"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17779",
        "TeX Source": "https://arxiv.org/src/2506.17779",
        "View PDF": "https://arxiv.org/pdf/2506.17779"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 18:16:27 UTC (4,770 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Toward Autonomous UI Exploration: The UIExplorer Benchmark",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2406.09496",
      "abstract": "We introduce the World Wide recipe, which sets forth a framework for culturally aware and participatory data collection, and the resultant regionally diverse World Wide Dishes evaluation dataset. We also analyse bias operationalisation to highlight how current systems underperform across several dimensions: (in-)accuracy, (mis-)representation, and cultural (in-)sensitivity, with evidence from qualitative community-based observations and quantitative automated tools. We find that these T2I models generally do not produce quality outputs of dishes specific to various regions. This is true even for the US, which is typically considered more well-resourced in training data -- although the generation of US dishes does outperform that of the investigated African countries. The models demonstrate the propensity to produce inaccurate and culturally misrepresentative, flattening, and insensitive outputs. These representational biases have the potential to further reinforce stereotypes and disproportionately contribute to erasure based on region. The dataset and code are available at https://github.com/oxai/world-wide-dishes.",
      "authors": [
        "Magomere, Jabez",
        "Ishida, Shu",
        "Afonja, Tejumade",
        "Salama, Aya",
        "Kochin, Daniel",
        "Yuehgoh, Foutse",
        "Hamzaoui, Imane",
        "Sefala, Raesetje",
        "Alaagib, Aisha",
        "Dalal, Samantha",
        "Marchegiani, Beatrice",
        "Semenova, Elizaveta",
        "Crais, Lauren",
        "Hall, Siobhan Mackenzie"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2406.09496v4",
        "Other Formats": "https://arxiv.org/format/2406.09496",
        "TeX Source": "https://arxiv.org/src/2406.09496",
        "View PDF": "https://arxiv.org/pdf/2406.09496"
      },
      "subjects": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 13 Jun 2024 18:00:00 UTC (1,717 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Tue, 1 Oct 2024 23:11:00 UTC (6,063 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Sun, 9 Feb 2025 17:13:58 UTC (6,057 KB)",
          "link": "/",
          "version": "[v3]"
        },
        {
          "details": "Sat, 21 Jun 2025 16:40:59 UTC (5,119 KB)",
          "version": "[v4]"
        }
      ],
      "submitted_date": "2024/06/13",
      "title": "The World Wide recipe: A community-centred framework for fine-grained data collection and regional bias operationalisation",
      "repo_urls": [
        "https://github.com/oxai/world-wide-dishes"
      ],
      "tasks": [
        "Fairness"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2411.13181",
      "abstract": "The classification of distracted drivers is pivotal for ensuring safe driving. Previous studies demonstrated the effectiveness of neural networks in automatically predicting driver distraction, fatigue, and potential hazards. However, recent research has uncovered a significant loss of accuracy in these models when applied to samples acquired under conditions that differ from the training data. In this paper, we introduce a robust model designed to withstand changes in camera position within the vehicle. Our Driver Behavior Monitoring Network (DBMNet) relies on a lightweight backbone and integrates a disentanglement module to discard camera view information from features, coupled with contrastive learning to enhance the encoding of various driver actions. Experiments conducted using a leave-one-camera-out protocol on the daytime and nighttime subsets of the 100-Driver dataset validate the effectiveness of our approach. Cross-dataset and cross-camera experiments conducted on three benchmark datasets, namely AUCDD-V1, EZZ2021 and SFD, demonstrate the superior generalization capabilities of the proposed method. Overall DBMNet achieves an improvement of 7% in Top-1 accuracy compared to existing approaches. Moreover, a quantized version of the DBMNet and all considered methods has been deployed on a Coral Dev Board board. In this deployment scenario, DBMNet outperforms alternatives, achieving the lowest average error while maintaining a compact model size, low memory footprint, fast inference time, and minimal power consumption.",
      "authors": [
        "Bianco, Simone",
        "Celona, Luigi",
        "Napoletano, Paolo"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2411.13181v2",
        "Other Formats": "https://arxiv.org/format/2411.13181",
        "TeX Source": "https://arxiv.org/src/2411.13181",
        "View PDF": "https://arxiv.org/pdf/2411.13181"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 20 Nov 2024 10:27:12 UTC (6,009 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 16:43:18 UTC (4,786 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/11/20",
      "title": "Cross-Camera Distracted Driver Classification through Feature Disentanglement and Contrastive Learning",
      "tasks": [
        "Contrastive Learning",
        "Disentanglement"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17590",
      "abstract": "Understanding the short-term motion of vulnerable road users (VRUs) like pedestrians and cyclists is critical for safe autonomous driving, especially in urban scenarios with ambiguous or high-risk behaviors. While vision-language models (VLMs) have enabled open-vocabulary perception, their utility for fine-grained intent reasoning remains underexplored. Notably, no existing benchmark evaluates multi-class intent prediction in safety-critical situations, To address this gap, we introduce DRAMA-X, a fine-grained benchmark constructed from the DRAMA dataset via an automated annotation pipeline. DRAMA-X contains 5,686 accident-prone frames labeled with object bounding boxes, a nine-class directional intent taxonomy, binary risk scores, expert-generated action suggestions for the ego vehicle, and descriptive motion summaries. These annotations enable a structured evaluation of four interrelated tasks central to autonomous decision-making: object detection, intent prediction, risk assessment, and action suggestion. As a reference baseline, we propose SGG-Intent, a lightweight, training-free framework that mirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene graph from visual input using VLM-backed detectors, infers intent, assesses risk, and recommends an action using a compositional reasoning stage powered by a large language model. We evaluate a range of recent VLMs, comparing performance across all four DRAMA-X tasks. Our experiments demonstrate that scene-graph-based reasoning enhances intent prediction and risk assessment, especially when contextual cues are explicitly modeled.",
      "authors": [
        "Godbole, Mihir",
        "Gao, Xiangbo",
        "Tu, Zhengzhong"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17590v1",
        "Other Formats": "https://arxiv.org/format/2506.17590",
        "TeX Source": "https://arxiv.org/src/2506.17590",
        "View PDF": "https://arxiv.org/pdf/2506.17590"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 05:01:42 UTC (3,811 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.01713",
      "abstract": "Multimodal large language models (MLLMs) have shown promising capabilities in reasoning tasks, yet still struggle with complex problems requiring explicit self-reflection and self-correction, especially compared to their unimodal text-based counterparts. Existing reflection methods are simplistic and struggle to generate meaningful and instructive feedback, as the reasoning ability and knowledge limits of pre-trained models are largely fixed during initial training. To overcome these challenges, we propose Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO), a two-stage reflection-aware reinforcement learning (RL) framework explicitly designed to enhance multimodal LLM reasoning. In the first stage, we construct a high-quality, reflection-focused dataset under the guidance of an advanced MLLM, which generates reflections based on initial responses to help the policy model learn both reasoning and self-reflection. In the second stage, we introduce a novel reward mechanism within the GRPO framework that encourages concise and cognitively meaningful reflection while avoiding redundancy. Extensive experiments across multiple multimodal reasoning benchmarks, including MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms state-of-the-art models, achieving notable improvements in both reasoning accuracy and reflection quality.",
      "authors": [
        "Wan, Zhongwei",
        "Dou, Zhihao",
        "Liu, Che",
        "Zhang, Yu",
        "Cui, Dongfei",
        "Zhao, Qinjian",
        "Shen, Hui",
        "Xiong, Jing",
        "Xin, Yi",
        "Jiang, Yifan",
        "Tao, Chaofan",
        "He, Yangfan",
        "Zhang, Mi",
        "Yan, Shen"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.01713v2",
        "Other Formats": "https://arxiv.org/format/2506.01713",
        "TeX Source": "https://arxiv.org/src/2506.01713",
        "View PDF": "https://arxiv.org/pdf/2506.01713"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 2 Jun 2025 14:21:44 UTC (4,157 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 03:17:00 UTC (4,157 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/06/02",
      "title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning",
      "tasks": [
        "Multimodal Reasoning",
        "Reinforcement Learning (RL)"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17837",
      "abstract": "In-context learning (ICL) enables generalization to new tasks with minimal labeled data. However, mainstream ICL approaches rely on a gridding strategy, which lacks the flexibility required for vision applications. We introduce Temporal, a time-contrastive self-supervised objective that pretrains a prompt retriever for visual ICL, and formulate ICL as a video object segmentation (VOS) task. Temporal addresses key limitations of grid-based methods that restrict the number and resolution of context images. By reframing ICL as a VOS problem, our approach supports a variable number of context images while preserving their full resolution. To address the challenge of selecting optimal context sets for queries, we pretrain a prompt retriever on videos via self-supervised learning, where adjacent frames serve as positives and distant frames as negatives. For image segmentation, the prompt retriever selects relevant sequences that, when combined with the query, form coherent videos for VOS processing. For video segmentation, it identifies keyframes, predicts their masks using our ICL pipeline, and propagates them throughout the sequence. When evaluated on MICCAI FLARE 2022, our method achieves substantial improvements over baselines: 90.95% Dice score for image segmentation (10.64% improvement) and 92.45% Dice for video segmentation (14.88% improvement).",
      "authors": [
        "Wahd, Assefa",
        "Jaremko, Jacob",
        "Hareendranathan, Abhilash"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17837v1",
        "Other Formats": "https://arxiv.org/format/2506.17837",
        "TeX Source": "https://arxiv.org/src/2506.17837",
        "View PDF": "https://arxiv.org/pdf/2506.17837"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 22:26:20 UTC (15,181 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Time-Contrastive Pretraining for In-Context Image and Video Segmentation",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17759",
      "abstract": "Hyperspectral image classification remains a challenging task due to the high dimensionality of spectral data, significant inter-band redundancy, and the limited availability of annotated samples. While recent transformer-based models have improved the global modeling of spectral-spatial dependencies, their scalability and adaptability under label-scarce conditions remain limited. In this work, we propose \\textbf{LoLA-SpecViT}(Low-rank adaptation Local Attention Spectral Vision Transformer), a lightweight spectral vision transformer that addresses these limitations through a parameter-efficient architecture tailored to the unique characteristics of hyperspectral imagery. Our model combines a 3D convolutional spectral front-end with local window-based self-attention, enhancing both spectral feature extraction and spatial consistency while reducing computational complexity. To further improve adaptability, we integrate low-rank adaptation (LoRA) into attention and projection layers, enabling fine-tuning with over 80\\% fewer trainable parameters. A novel cyclical learning rate scheduler modulates LoRA adaptation strength during training, improving convergence and generalisation. Extensive experiments on three benchmark datasets WHU-Hi LongKou, WHU-Hi HongHu, and Salinas demonstrate that LoLA-SpecViT consistently outperforms state-of-the-art baselines, achieving up to 99.91\\% accuracy with substantially fewer parameters and enhanced robustness under low-label regimes. The proposed framework provides a scalable and generalizable solution for real-world HSI applications in agriculture, environmental monitoring, and remote sensing analytics. Our code is available in the following \\href{https://github.com/FadiZidiDz/LoLA-SpecViT}{GitHub Repository}.",
      "authors": [
        "Zidi, Fadi Abdeladhim",
        "Boukhari, Djamel Eddine",
        "Sellam, Abdellah Zakaria",
        "Ouafi, Abdelkrim",
        "Distante, Cosimo",
        "Bekhouche, Salah Eddine",
        "Taleb-Ahmed, Abdelmalik"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17759v1",
        "Other Formats": "https://arxiv.org/format/2506.17759",
        "TeX Source": "https://arxiv.org/src/2506.17759",
        "View PDF": "https://arxiv.org/pdf/2506.17759"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 16:46:00 UTC (2,254 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "LoLA-SpecViT: Local Attention SwiGLU Vision Transformer with LoRA for Hyperspectral Imaging",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17658",
      "abstract": "The last decade has witnessed the proliferation of network function virtualization (NFV) in the telco industry, thanks to its unparalleled flexibility, scalability, and cost-effectiveness. However, as the NFV infrastructure is shared by virtual network functions (VNFs), sporadic resource contentions are inevitable. Such contention makes it extremely challenging to guarantee the performance of the provisioned network services, especially in high-speed regimes (e.g., Gigabit Ethernet). Existing solutions typically rely on direct traffic analysis (e.g., packet- or flow-level measurements) to detect performance degradation and identify bottlenecks, which is not always applicable due to significant integration overhead and system-level constraints. This paper complements existing solutions with a lightweight, non-intrusive framework for online performance inference and adaptation. Instead of direct data-plane collection, we reuse hardware features in the underlying NFV infrastructure, introducing negligible interference in the data plane. This framework can be integrated into existing NFV systems with minimal engineering effort and operates without the need for predefined traffic models or VNF-specific customization. Through comprehensive evaluation across diverse NFV scenarios, our Drift-Resilient and Self-Tuning (DRST) framework delivers accurate performance inference, runtime bottleneck diagnose, and automated adaptation under runtime drift, via a lightweight MLOps pipeline.",
      "authors": [
        "Liu, Qiong",
        "Lin, Jianke",
        "Zhang, Tianzhu",
        "Linguaglossa, Leonardo"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17658v1",
        "Other Formats": "https://arxiv.org/format/2506.17658",
        "TeX Source": "https://arxiv.org/src/2506.17658",
        "View PDF": "https://arxiv.org/pdf/2506.17658"
      },
      "subjects": [
        "Networking and Internet Architecture (cs.NI)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 09:33:56 UTC (1,421 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Non-Intrusive MLOps-Driven Performance Intelligence in Software Data Planes",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17552",
      "abstract": "A reliable evaluation of surgical difficulty can improve the success of the treatment for rectal cancer and the current evaluation method is based on clinical data. However, more data about rectal cancer can be collected with the development of technology. Meanwhile, with the development of artificial intelligence, its application in rectal cancer treatment is becoming possible. In this paper, a multi-view rectal cancer dataset is first constructed to give a more comprehensive view of patients, including the high-resolution MRI image view, pressed-fat MRI image view, and clinical data view. Then, an interpretable incomplete multi-view surgical evaluation model is proposed, considering that it is hard to obtain extensive and complete patient data in real application scenarios. Specifically, a dual representation incomplete multi-view learning model is first proposed to extract the common information between views and specific information in each view. In this model, the missing view imputation is integrated into representation learning, and second-order similarity constraint is also introduced to improve the cooperative learning between these two parts. Then, based on the imputed multi-view data and the learned dual representation, a multi-view surgical evaluation model with the TSK fuzzy system is proposed. In the proposed model, a cooperative learning mechanism is constructed to explore the consistent information between views, and Shannon entropy is also introduced to adapt the view weight. On the MVRC dataset, we compared it with several advanced algorithms and DRIMV_TSK obtained the best results.",
      "authors": [
        "Zhang, Wei",
        "Wang, Zi",
        "Zhou, Hanwen",
        "Deng, Zhaohong",
        "Ding, Weiping",
        "Ge, Yuxi",
        "Zhang, Te",
        "Zhang, Yuanpeng",
        "Choi, Kup-Sze",
        "Wang, Shitong",
        "Hu, Shudong"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17552",
        "View PDF": "https://arxiv.org/pdf/2506.17552"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 02:38:45 UTC (1,659 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "DRIMV_TSK: An Interpretable Surgical Evaluation Model for Incomplete Multi-View Rectal Cancer Data",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17558",
      "abstract": "Learning to infer object representations, and in particular part-whole hierarchies, has been the focus of extensive research in computer vision, in pursuit of improving data efficiency, systematic generalisation, and robustness. Models which are \\emph{designed} to infer part-whole hierarchies, often referred to as capsule networks, are typically trained end-to-end on supervised tasks such as object classification, in which case it is difficult to evaluate whether such a model \\emph{actually} learns to infer part-whole hierarchies, as claimed. To address this difficulty, we present a SYNthetic DAtaset for CApsule Testing and Evaluation, abbreviated as SynDaCaTE, and establish its utility by (1) demonstrating the precise bottleneck in a prominent existing capsule model, and (2) demonstrating that permutation-equivariant self-attention is highly effective for parts-to-wholes inference, which motivates future directions for designing effective inductive biases for computer vision.",
      "authors": [
        "Levi, Jake",
        "van der Wilk, Mark"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17558v1",
        "Other Formats": "https://arxiv.org/format/2506.17558",
        "TeX Source": "https://arxiv.org/src/2506.17558",
        "View PDF": "https://arxiv.org/pdf/2506.17558"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 03:01:16 UTC (516 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "SynDaCaTE: A Synthetic Dataset For Evaluating Part-Whole Hierarchical Inference",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17644",
      "abstract": "Capture-the-Flag (CTF) competitions are crucial for cybersecurity education and training. As large language models (LLMs) evolve, there is increasing interest in their ability to automate CTF challenge solving. For example, DARPA has organized the AIxCC competition since 2023 to advance AI-powered automated offense and defense. However, this demands a combination of multiple abilities, from knowledge to reasoning and further to actions. In this paper, we highlight the importance of technical knowledge in solving CTF problems and deliberately construct a focused benchmark, CTFKnow, with 3,992 questions to measure LLMs' performance in this core aspect. Our study offers a focused and innovative measurement of LLMs' capability in understanding CTF knowledge and applying it to solve CTF challenges. Our key findings reveal that while LLMs possess substantial technical knowledge, they falter in accurately applying this knowledge to specific scenarios and adapting their strategies based on feedback from the CTF environment. Based on insights derived from this measurement study, we propose CTFAgent, a novel LLM-driven framework for advancing CTF problem-solving. CTFAgent introduces two new modules: two-stage Retrieval Augmented Generation (RAG) and interactive Environmental Augmentation, which enhance LLMs' technical knowledge and vulnerability exploitation on CTF, respectively. Our experimental results show that, on two popular CTF datasets, CTFAgent both achieves over 80% performance improvement. Moreover, in the recent picoCTF2024 hosted by CMU, CTFAgent ranked in the top 23.6% of nearly 7,000 participating teams. This reflects the benefit of our measurement study and the potential of our framework in advancing LLMs' capabilities in CTF problem-solving.",
      "authors": [
        "Ji, Zimo",
        "Wu, Daoyuan",
        "Jiang, Wenyuan",
        "Ma, Pingchuan",
        "Li, Zongjie",
        "Wang, Shuai"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17644",
        "TeX Source": "https://arxiv.org/src/2506.17644",
        "View PDF": "https://arxiv.org/pdf/2506.17644"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 08:56:20 UTC (859 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Measuring and Augmenting Large Language Models for Solving Capture-the-Flag Challenges",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17847",
      "abstract": "High-quality training data is critical to the performance of machine learning models, particularly Large Language Models (LLMs). However, obtaining real, high-quality data can be challenging, especially for smaller organizations and early-stage startups. Synthetic data generators provide a promising solution by replicating the statistical and structural properties of real data while preserving privacy and scalability. This study evaluates the performance of six tabular synthetic data generators from two widely used open-source libraries: SDV (Gaussian Copula, CTGAN, TVAE) and Synthicity (Bayesian Network, CTGAN, TVAE). Using a real-world dataset from the UCI Machine Learning Repository, comprising energy consumption and environmental variables from Belgium, we simulate a low-data regime by training models on only 1,000 rows. Each generator is then tasked with producing synthetic datasets under two conditions: a 1:1 (1,000 rows) and a 1:10 (10,000 rows) input-output ratio. Evaluation is conducted using two criteria: statistical similarity, measured via classical statistics and distributional metrics; and predictive utility, assessed using a \"Train on Synthetic, Test on Real\" approach with four regression models. While statistical similarity remained consistent across models in both scenarios, predictive utility declined notably in the 1:10 case. The Bayesian Network from Synthicity achieved the highest fidelity in both scenarios, while TVAE from SDV performed best in predictive tasks under the 1:10 setting. Although no significant performance gap was found between the two libraries, SDV stands out for its superior documentation and ease of use, making it more accessible for practitioners.",
      "authors": [
        "Del Gobbo, Cristian"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17847v1",
        "Other Formats": "https://arxiv.org/format/2506.17847",
        "TeX Source": "https://arxiv.org/src/2506.17847",
        "View PDF": "https://arxiv.org/pdf/2506.17847"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 22:45:40 UTC (72 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "A Comparative Study of Open-Source Libraries for Synthetic Tabular Data Generation: SDV vs. SynthCity",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.13138",
      "abstract": "The generation of temporally consistent, high-fidelity driving videos over extended horizons presents a fundamental challenge in autonomous driving world modeling. Existing approaches often suffer from error accumulation and feature misalignment due to inadequate decoupling of spatio-temporal dynamics and limited cross-frame feature propagation mechanisms. To address these limitations, we present STAGE (Streaming Temporal Attention Generative Engine), a novel auto-regressive framework that pioneers hierarchical feature coordination and multi-phase optimization for sustainable video synthesis. To achieve high-quality long-horizon driving video generation, we introduce Hierarchical Temporal Feature Transfer (HTFT) and a novel multi-stage training strategy. HTFT enhances temporal consistency between video frames throughout the video generation process by modeling the temporal and denoising process separately and transferring denoising features between frames. The multi-stage training strategy is to divide the training into three stages, through model decoupling and auto-regressive inference process simulation, thereby accelerating model convergence and reducing error accumulation. Experiments on the Nuscenes dataset show that STAGE has significantly surpassed existing methods in the long-horizon driving video generation task. In addition, we also explored STAGE's ability to generate unlimited-length driving videos. We generated 600 frames of high-quality driving videos on the Nuscenes dataset, which far exceeds the maximum length achievable by existing methods.",
      "authors": [
        "Wang, Jiamin",
        "Yao, Yichen",
        "Feng, Xiang",
        "Wu, Hang",
        "Wang, Yaming",
        "Huang, Qingqiu",
        "Ma, Yuexin",
        "Zhu, Xinge"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.13138v2",
        "Other Formats": "https://arxiv.org/format/2506.13138",
        "TeX Source": "https://arxiv.org/src/2506.13138",
        "View PDF": "https://arxiv.org/pdf/2506.13138"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 16 Jun 2025 06:53:05 UTC (4,165 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 07:27:24 UTC (4,165 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/06/16",
      "title": "STAGE: A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17828",
      "abstract": "Aligning large language models (LLMs) with human preferences usually requires fine-tuning methods such as RLHF and DPO. These methods directly optimize the model parameters, so they cannot be used in test-time to improve model performance, nor are they applicable when the model weights are not accessible. In contrast, test-time methods sidestep weight updates by leveraging reward functions to guide and improve output quality. However, they incur high inference costs, and their one-shot guidance is often based on imperfect reward or value functions, leading to suboptimal outputs. In this work, we present a method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning (RL) framework that performs RL-style alignment of the (frozen) base model without touching its parameters. During training, each iteration (i) samples candidates from the base model, (ii) resamples using current value functions, and (iii) trains a new lightweight value function that guides the next decoding pass. At test time, the value functions are used to guide the base model generation via a search-based optimization process. Notably, users can apply IRO to align a model on their own dataset, similar to OpenAI's reinforcement fine-tuning (RFT), but without requiring access to the model weights.",
      "authors": [
        "Zhang, Xinnan",
        "Li, Chenliang",
        "Zeng, Siliang",
        "Li, Jiaxiang",
        "Wang, Zhongruo",
        "Lin, Kaixiang",
        "Lu, Songtao",
        "Garcia, Alfredo",
        "Hong, Mingyi"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17828",
        "TeX Source": "https://arxiv.org/src/2506.17828",
        "View PDF": "https://arxiv.org/pdf/2506.17828"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 21:49:02 UTC (1,939 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17857",
      "abstract": "Accurate prediction of antibody-antigen (Ab-Ag) binding affinity is essential for therapeutic design and vaccine development, yet the performance of current models is limited by noisy experimental labels, heterogeneous assay conditions, and poor generalization across the vast antibody and antigen sequence space. We introduce AbRank, a large-scale benchmark and evaluation framework that reframes affinity prediction as a pairwise ranking problem. AbRank aggregates over 380,000 binding assays from nine heterogeneous sources, spanning diverse antibodies, antigens, and experimental conditions, and introduces standardized data splits that systematically increase distribution shift, from local perturbations such as point mutations to broad generalization across novel antigens and antibodies. To ensure robust supervision, AbRank defines an m-confident ranking framework by filtering out comparisons with marginal affinity differences, focusing training on pairs with at least an m-fold difference in measured binding strength. As a baseline for the benchmark, we introduce WALLE-Affinity, a graph-based approach that integrates protein language model embeddings with structural information to predict pairwise binding preferences. Our benchmarks reveal significant limitations in current methods under realistic generalization settings and demonstrate that ranking-based training improves robustness and transferability. In summary, AbRank offers a robust foundation for machine learning models to generalize across the antibody-antigen space, with direct relevance for scalable, structure-aware antibody therapeutic design.",
      "authors": [
        "Liu, Chunan",
        "Pelissier, Aurelien",
        "Shao, Yanjun",
        "Denzler, Lilian",
        "Martin, Andrew C. R.",
        "Paige, Brooks",
        "Martinez, Mariia Rodriguez"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17857v1",
        "Other Formats": "https://arxiv.org/format/2506.17857",
        "TeX Source": "https://arxiv.org/src/2506.17857",
        "View PDF": "https://arxiv.org/pdf/2506.17857"
      },
      "subjects": [
        "Biomolecules (q-bio.BM)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 23:34:46 UTC (916 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "AbRank: A Benchmark Dataset and Metric-Learning Framework for Antibody-Antigen Affinity Ranking",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17601",
      "abstract": "Safe, reliable navigation in extreme, unfamiliar terrain is required for future robotic space exploration missions. Recent generative-AI methods learn semantically aware navigation policies from large, cross-embodiment datasets, but offer limited safety guarantees. Inspired by human cognitive science, we propose a risk-guided diffusion framework that fuses a fast, learned \"System-1\" with a slow, physics-based \"System-2\", sharing computation at both training and inference to couple adaptability with formal safety. Hardware experiments conducted at the NASA JPL's Mars-analog facility, Mars Yard, show that our approach reduces failure rates by up to $4\\times$ while matching the goal-reaching performance of learning-based robotic models by leveraging inference-time compute without any additional training.",
      "authors": [
        "Thakker, Rohan",
        "Patnaik, Adarsh",
        "Kurtz, Vince",
        "Frey, Jonas",
        "Becktor, Jonathan",
        "Moon, Sangwoo",
        "Royce, Rob",
        "Kaufmann, Marcel",
        "Georgakis, Georgios",
        "Roth, Pascal",
        "Burdick, Joel",
        "Hutter, Marco",
        "Khattak, Shehryar"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17601v1",
        "Other Formats": "https://arxiv.org/format/2506.17601",
        "TeX Source": "https://arxiv.org/src/2506.17601",
        "View PDF": "https://arxiv.org/pdf/2506.17601"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 21 Jun 2025 05:39:04 UTC (24,220 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/21",
      "title": "Risk-Guided Diffusion: Toward Deploying Robot Foundation Models in Space, Where Failure Is Not An Option",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2407.14320",
      "abstract": "Early exits enable the network's forward pass to terminate early by attaching trainable internal classifiers to the backbone network. Existing early-exit methods typically adopt either a joint training approach, where the backbone and exit heads are trained simultaneously, or a disjoint approach, where the heads are trained separately. However, the implications of this choice are often overlooked, with studies typically adopting one approach without adequate justification. This choice influences training dynamics and its impact remains largely unexplored. In this paper, we introduce a set of metrics to analyze early-exit training dynamics and guide the choice of training strategy. We demonstrate that conventionally used joint and disjoint regimes yield suboptimal performance. To address these limitations, we propose a mixed training strategy: the backbone is trained first, followed by the training of the entire multi-exit network. Through comprehensive evaluations of training strategies across various architectures, datasets, and early-exit methods, we present the strengths and weaknesses of the early exit training strategies. In particular, we show consistent improvements in performance and efficiency using the proposed mixed strategy.",
      "authors": [
        "Kubaty, Piotr",
        "W\u00c3\u00b3jcik, Bartosz",
        "Krzepkowski, Bart\u00c5\u0082omiej",
        "Michaluk, Monika",
        "Trzci\u00c5\u0084ski, Tomasz",
        "Pomponi, Jary",
        "Adamczewski, Kamil"
      ],
      "last_revised_date": "2025/06/21",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2407.14320v2",
        "Other Formats": "https://arxiv.org/format/2407.14320",
        "TeX Source": "https://arxiv.org/src/2407.14320",
        "View PDF": "https://arxiv.org/pdf/2407.14320"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 19 Jul 2024 13:56:57 UTC (5,879 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Sat, 21 Jun 2025 10:00:51 UTC (2,326 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/07/19",
      "title": "How to Train Your Multi-Exit Model? Analyzing the Impact of Training Strategies",
      "repo_urls": [
        "https://github.com/kamadforge/early-exit-benchmark"
      ],
      "tasks": [],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17220",
      "abstract": "Recent advancements in video diffusion models based on Diffusion Transformers (DiTs) have achieved remarkable success in generating temporally coherent videos. Yet, a fundamental question persists: how do these models internally establish and represent temporal correspondences across frames? We introduce DiffTrack, the first quantitative analysis framework designed to answer this question. DiffTrack constructs a dataset of prompt-generated video with pseudo ground-truth tracking annotations and proposes novel evaluation metrics to systematically analyze how each component within the full 3D attention mechanism of DiTs (e.g., representations, layers, and timesteps) contributes to establishing temporal correspondences. Our analysis reveals that query-key similarities in specific, but not all, layers play a critical role in temporal matching, and that this matching becomes increasingly prominent during the denoising process. We demonstrate practical applications of DiffTrack in zero-shot point tracking, where it achieves state-of-the-art performance compared to existing vision foundation and self-supervised video models. Further, we extend our findings to motion-enhanced video generation with a novel guidance method that improves temporal consistency of generated videos without additional training. We believe our work offers crucial insights into the inner workings of video DiTs and establishes a foundation for further research and applications leveraging their temporal understanding.",
      "authors": [
        "Nam, Jisu",
        "Son, Soowon",
        "Chung, Dahyun",
        "Kim, Jiyoung",
        "Jin, Siyoon",
        "Hur, Junhwa",
        "Kim, Seungryong"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17220",
        "TeX Source": "https://arxiv.org/src/2506.17220",
        "View PDF": "https://arxiv.org/pdf/2506.17220"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 17:59:55 UTC (24,561 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Emergent Temporal Correspondences from Video Diffusion Transformers",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17374",
      "abstract": "Efficient and accurate extraction of key information from 2D engineering drawings is essential for advancing digital manufacturing workflows. Such information includes geometric dimensioning and tolerancing (GD&T), measures, material specifications, and textual annotations. Manual extraction is slow and labor-intensive, while generic OCR models often fail due to complex layouts, engineering symbols, and rotated text, leading to incomplete and unreliable outputs. These limitations result in incomplete and unreliable outputs. To address these challenges, we propose a hybrid vision-language framework that integrates a rotation-aware object detection model (YOLOv11-obb) with a transformer-based vision-language parser. Our structured pipeline applies YOLOv11-OBB to localize annotations and extract oriented bounding box (OBB) patches, which are then parsed into structured outputs using a fine-tuned, lightweight vision-language model (VLM). We curate a dataset of 1,367 2D mechanical drawings annotated across nine key categories. YOLOv11-OBB is trained on this dataset to detect OBBs and extract annotation patches. These are parsed using two open-source VLMs: Donut and Florence-2. Both models are lightweight and well-suited for specialized industrial tasks under limited computational overhead. Following fine-tuning of both models on the curated dataset of image patches paired with structured annotation labels, a comparative experiment is conducted to evaluate parsing performance across four key metrics. Donut outperforms Florence-2, achieving 88.5% precision, 99.2% recall, and a 93.5% F1-score, with a hallucination rate of 11.5%. Finally, a case study demonstrates how the extracted structured information supports downstream manufacturing tasks such as process and tool selection, showcasing the practical utility of the proposed framework in modernizing 2D drawing interpretation.",
      "authors": [
        "Khan, Muhammad Tayyab",
        "Chen, Lequn",
        "Yong, Zane",
        "Tan, Jun Ming",
        "Feng, Wenhe",
        "Moon, Seung Ki"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17374",
        "View PDF": "https://arxiv.org/pdf/2506.17374"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Information Retrieval (cs.IR)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 17:10:01 UTC (4,597 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "From Drawings to Decisions: A Hybrid Vision-Language Framework for Parsing 2D Engineering Drawings into Structured Manufacturing Knowledge",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17409",
      "abstract": "Localizing acoustic sound sources in the ocean is a challenging task due to the complex and dynamic nature of the environment. Factors such as high background noise, irregular underwater geometries, and varying acoustic properties make accurate localization difficult. To address these obstacles, we propose a multi-branch network architecture designed to accurately predict the distance between a moving acoustic source and a receiver, tested on real-world underwater signal arrays. The network leverages Convolutional Neural Networks (CNNs) for robust spatial feature extraction and integrates Conformers with self-attention mechanism to effectively capture temporal dependencies. Log-mel spectrogram and generalized cross-correlation with phase transform (GCC-PHAT) features are employed as input representations. To further enhance the model performance, we introduce an Adaptive Gain Control (AGC) layer, that adaptively adjusts the amplitude of input features, ensuring consistent energy levels across varying ranges, signal strengths, and noise conditions. We assess the model's generalization capability by training it in one domain and testing it in a different domain, using only a limited amount of data from the test domain for fine-tuning. Our proposed method outperforms state-of-the-art (SOTA) approaches in similar settings, establishing new benchmarks for underwater sound localization.",
      "authors": [
        "Vo, Quoc Thinh",
        "Woods, Joe",
        "Chowdhury, Priontu",
        "Han, David K."
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17409v1",
        "Other Formats": "https://arxiv.org/format/2506.17409",
        "TeX Source": "https://arxiv.org/src/2506.17409",
        "View PDF": "https://arxiv.org/pdf/2506.17409"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Machine Learning (cs.LG)",
        "Audio and Speech Processing (eess.AS)",
        "Signal Processing (eess.SP)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 18:13:30 UTC (675 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Adaptive Control Attention Network for Underwater Acoustic Localization and Domain Adaptation",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.21777",
      "abstract": "Hopfield networks are associative memory (AM) systems, designed for storing and retrieving patterns as local minima of an energy landscape. In the classical Hopfield model, an interesting phenomenon occurs when the amount of training data reaches its critical memory load $- spurious\\,\\,states$, or unintended stable points, emerge at the end of the retrieval dynamics, leading to incorrect recall. In this work, we examine diffusion models, commonly used in generative modeling, from the perspective of AMs. The training phase of diffusion model is conceptualized as memory encoding (training data is stored in the memory). The generation phase is viewed as an attempt of memory retrieval. In the small data regime the diffusion model exhibits a strong memorization phase, where the network creates distinct basins of attraction around each sample in the training set, akin to the Hopfield model below the critical memory load. In the large data regime, a different phase appears where an increase in the size of the training set fosters the creation of new attractor states that correspond to manifolds of the generated samples. Spurious states appear at the boundary of this transition and correspond to emergent attractor states, which are absent in the training set, but, at the same time, have distinct basins of attraction around them. Our findings provide: a novel perspective on the memorization-generalization phenomenon in diffusion models via the lens of AMs, theoretical prediction of existence of spurious states, empirical validation of this prediction in commonly-used diffusion models.",
      "authors": [
        "Pham, Bao",
        "Raya, Gabriel",
        "Negri, Matteo",
        "Zaki, Mohammed J.",
        "Ambrogioni, Luca",
        "Krotov, Dmitry"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2505.21777v2",
        "Other Formats": "https://arxiv.org/format/2505.21777",
        "TeX Source": "https://arxiv.org/src/2505.21777",
        "View PDF": "https://arxiv.org/pdf/2505.21777"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Neurons and Cognition (q-bio.NC)",
        "Machine Learning (stat.ML)"
      ],
      "submission_historys": [
        {
          "details": "Tue, 27 May 2025 21:20:57 UTC (17,957 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Fri, 20 Jun 2025 19:20:01 UTC (18,186 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/27",
      "title": "Memorization to Generalization: Emergence of Diffusion Models from Associative Memory",
      "repo_urls": [
        "https://github.com/Lemon-cmd/Diffusion-Models-and-Associative-Memory"
      ],
      "tasks": [
        "Memorization",
        "Retrieval"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17101",
      "abstract": "Driving scene identification, which assigns multiple non-exclusive class labels to a scene, provides the contextual awareness necessary for enhancing autonomous vehicles' ability to understand, reason about, and interact with the complex driving environment. As a multi-label classification problem, it is better tackled via multitasking learning. However, directly training a multi-label classification model for driving scene identification through multitask learning presents two main challenges: acquiring a balanced, comprehensively annotated multi-label dataset and balancing learning across different tasks. This paper introduces a novel learning system that synergizes knowledge acquisition and accumulation (KAA) with consistency-based active learning (CAL) to address those challenges. KAA acquires and accumulates knowledge about scene identification from various single-label datasets via monotask learning. Subsequently, CAL effectively resolves the knowledge gap caused by the discrepancy between the marginal distributions of individual attributes and their joint distribution. An ablation study on our Driving Scene Identification (DSI) dataset demonstrates a 56.1% performance increase over the baseline model pretrained on ImageNet. Of this, KAA accounts for 31.3% of the gain, and CAL contributes 24.8%. Moreover, KAA-CAL stands out as the best performer when compared to state-of-the-art (SOTA) multi-label models on two public datasets, BDD100K and HSD, achieving this while using 85% less data. The DSI dataset and the implementation code for KAA-CAL are available at https://github.com/KELISBU/KAA-CAL .",
      "authors": [
        "Li, Ke",
        "Zhang, Chenyu",
        "Ding, Yuxin",
        "Hu, Xianbiao",
        "Qin, Ruwen"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17101v1",
        "Other Formats": "https://arxiv.org/format/2506.17101",
        "TeX Source": "https://arxiv.org/src/2506.17101",
        "View PDF": "https://arxiv.org/pdf/2506.17101"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 16:06:53 UTC (15,007 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Acquiring and Accumulating Knowledge from Diverse Datasets for Multi-label Driving Scene Classification",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.16784",
      "abstract": "Deep learning has demonstrated remarkable success in medical image segmentation and computer-aided diagnosis. In particular, numerous advanced methods have achieved state-of-the-art performance in brain tumor segmentation from MRI scans. While recent studies in other medical imaging domains have revealed that integrating textual reports with visual data can enhance segmentation accuracy, the field of brain tumor analysis lacks a comprehensive dataset that combines radiological images with corresponding textual annotations. This limitation has hindered the exploration of multimodal approaches that leverage both imaging and textual data. To bridge this critical gap, we introduce the TextBraTS dataset, the first publicly available volume-level multimodal dataset that contains paired MRI volumes and rich textual annotations, derived from the widely adopted BraTS2020 benchmark. Building upon this novel dataset, we propose a novel baseline framework and sequential cross-attention method for text-guided volumetric medical image segmentation. Through extensive experiments with various text-image fusion strategies and templated text formulations, our approach demonstrates significant improvements in brain tumor segmentation accuracy, offering valuable insights into effective multimodal integration techniques. Our dataset, implementation code, and pre-trained models are publicly available at https://github.com/Jupitern52/TextBraTS.",
      "authors": [
        "Shi, Xiaoyu",
        "Jain, Rahul Kumar",
        "Li, Yinhao",
        "Hou, Ruibo",
        "Cheng, Jingliang",
        "Bai, Jie",
        "Zhao, Guohua",
        "Lin, Lanfen",
        "Xu, Rui",
        "Chen, Yen-wei"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.16784v1",
        "Other Formats": "https://arxiv.org/format/2506.16784",
        "TeX Source": "https://arxiv.org/src/2506.16784",
        "View PDF": "https://arxiv.org/pdf/2506.16784"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Multimedia (cs.MM)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 06:57:56 UTC (2,940 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "TextBraTS: Text-Guided Volumetric Brain Tumor Segmentation with Innovative Dataset Development and Fusion Module Exploration",
      "models": [
        {
          "model_path": "Jupitern52/TextBraTS",
          "downloads": "0",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/Jupitern52/TextBraTS"
        }
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17466",
      "abstract": "Federated learning continues to evolve but faces challenges in interpretability and explainability. To address these challenges, we introduce a novel approach that employs Neural Additive Models (NAMs) within a federated learning framework. This new Federated Neural Additive Models (FedNAMs) approach merges the advantages of NAMs, where individual networks concentrate on specific input features, with the decentralized approach of federated learning, ultimately producing interpretable analysis results. This integration enhances privacy by training on local data across multiple devices, thereby minimizing the risks associated with data centralization and improving model robustness and generalizability. FedNAMs maintain detailed, feature-specific learning, making them especially valuable in sectors such as finance and healthcare. They facilitate the training of client-specific models to integrate local updates, preserve privacy, and mitigate concerns related to centralization. Our studies on various text and image classification tasks, using datasets such as OpenFetch ML Wine, UCI Heart Disease, and Iris, show that FedNAMs deliver strong interpretability with minimal accuracy loss compared to traditional Federated Deep Neural Networks (DNNs). The research involves notable findings, including the identification of critical predictive features at both client and global levels. Volatile acidity, sulfates, and chlorides for wine quality. Chest pain type, maximum heart rate, and number of vessels for heart disease. Petal length and width for iris classification. This approach strengthens privacy and model efficiency and improves interpretability and robustness across diverse datasets. Finally, FedNAMs generate insights on causes of highly and low interpretable features.",
      "authors": [
        "Nanda, Amitash",
        "Balija, Sree Bhargavi",
        "Sahoo, Debashis"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17466v1",
        "Other Formats": "https://arxiv.org/format/2506.17466",
        "TeX Source": "https://arxiv.org/src/2506.17466",
        "View PDF": "https://arxiv.org/pdf/2506.17466"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 20:14:13 UTC (2,226 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "FedNAMs: Performing Interpretability Analysis in Federated Learning Context",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17064",
      "abstract": "Generating diverse, all-atom conformational ensembles of dynamic proteins such as G-protein-coupled receptors (GPCRs) is critical for understanding their function, yet most generative models simplify atomic detail or ignore conformational diversity altogether. We present latent diffusion for full protein generation (LD-FPG), a framework that constructs complete all-atom protein structures, including every side-chain heavy atom, directly from molecular dynamics (MD) trajectories. LD-FPG employs a Chebyshev graph neural network (ChebNet) to obtain low-dimensional latent embeddings of protein conformations, which are processed using three pooling strategies: blind, sequential and residue-based. A diffusion model trained on these latent representations generates new samples that a decoder, optionally regularized by dihedral-angle losses, maps back to Cartesian coordinates. Using D2R-MD, a 2-microsecond MD trajectory (12 000 frames) of the human dopamine D2 receptor in a membrane environment, the sequential and residue-based pooling strategy reproduces the reference ensemble with high structural fidelity (all-atom lDDT of approximately 0.7; C-alpha-lDDT of approximately 0.8) and recovers backbone and side-chain dihedral-angle distributions with a Jensen-Shannon divergence of less than 0.03 compared to the MD data. LD-FPG thereby offers a practical route to system-specific, all-atom ensemble generation for large proteins, providing a promising tool for structure-based therapeutic design on complex, dynamic targets. The D2R-MD dataset and our implementation are freely available to facilitate further research.",
      "authors": [
        "Sengar, Aditya",
        "Hariri, Ali",
        "Probst, Daniel",
        "Barth, Patrick",
        "Vandergheynst, Pierre"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17064v1",
        "Other Formats": "https://arxiv.org/format/2506.17064",
        "TeX Source": "https://arxiv.org/src/2506.17064",
        "View PDF": "https://arxiv.org/pdf/2506.17064"
      },
      "subjects": [
        "Biomolecules (q-bio.BM)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 15:12:34 UTC (25,985 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Generative Modeling of Full-Atom Protein Conformations using Latent Diffusion on Graph Embeddings",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17405",
      "abstract": "This paper proposes a provably convergent multiblock ADMM for nonconvex optimization with nonlinear dynamics constraints, overcoming the divergence issue in classical extensions. We consider a class of optimization problems that arise from discretization of dynamics-constrained variational problems that are optimization problems for a functional constrained by time-dependent ODEs or PDEs. This is a family of $n$-sum nonconvex optimization problems with nonlinear constraints. We study the convergence properties of the proximal alternating direction method of multipliers (proximal ADMM) applied to those problems. Taking the advantage of the special problem structure, we show that under local Lipschitz and local $L$-smooth conditions, the sequence generated by the proximal ADMM is bounded and all accumulation points are KKT points. Based on our analysis, we also design a procedure to determine the penalty parameters $\\rho_i$ and the proximal parameters $\\eta_i$. We further prove that among all the subsequences that converge, the fast one converges at the rate of $o(1/k)$. The numerical experiments are performed on 4D variational data assimilation problems and as the solver of implicit schemes for stiff problems. The proposed proximal ADMM has more stable performance than gradient-based methods. We discuss the implementation to solve the subproblems, a new way to solve the implicit schemes, and the advantages of the proposed algorithm.",
      "authors": [
        "Li, Bowen",
        "Yuan, Ya-xiang"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17405v1",
        "Other Formats": "https://arxiv.org/format/2506.17405",
        "TeX Source": "https://arxiv.org/src/2506.17405",
        "View PDF": "https://arxiv.org/pdf/2506.17405"
      },
      "subjects": [
        "Optimization and Control (math.OC)",
        "Numerical Analysis (math.NA)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 18:09:56 UTC (154 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Convergent Proximal Multiblock ADMM for Nonconvex Dynamics-Constrained Optimization",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2412.14312",
      "abstract": "Dyna-style off-policy model-based reinforcement learning (DMBRL) algorithms are a family of techniques for generating synthetic state transition data and thereby enhancing the sample efficiency of off-policy RL algorithms. This paper identifies and investigates a surprising performance gap observed when applying DMBRL algorithms across different benchmark environments with proprioceptive observations. We show that, while DMBRL algorithms perform well in OpenAI Gym, their performance can drop significantly in DeepMind Control Suite (DMC), even though these settings offer similar tasks and identical physics backends. Modern techniques designed to address several key issues that arise in these settings do not provide a consistent improvement across all environments, and overall our results show that adding synthetic rollouts to the training process -- the backbone of Dyna-style algorithms -- significantly degrades performance across most DMC environments. Our findings contribute to a deeper understanding of several fundamental challenges in model-based RL and show that, like many optimization fields, there is no free lunch when evaluating performance across diverse benchmarks in RL.",
      "authors": [
        "Barkley, Brett",
        "Fridovich-Keil, David"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2412.14312v3",
        "Other Formats": "https://arxiv.org/format/2412.14312",
        "TeX Source": "https://arxiv.org/src/2412.14312",
        "View PDF": "https://arxiv.org/pdf/2412.14312"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 18 Dec 2024 20:25:04 UTC (12,964 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Fri, 20 Dec 2024 16:43:29 UTC (12,963 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Fri, 20 Jun 2025 18:24:06 UTC (9,266 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/12/18",
      "title": "Stealing That Free Lunch: Exposing the Limits of Dyna-Style Reinforcement Learning",
      "tasks": [
        "Model-based Reinforcement Learning",
        "OpenAI Gym"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2501.06686",
      "abstract": "In this work, we study the feasibility of using neural ordinary differential equations (NODEs) to model systems with intrinsic privacy properties. Unlike conventional feedforward neural networks, which have unlimited expressivity and can represent arbitrary mappings between inputs and outputs, NODEs constrain their learning to the solution of a system of differential equations. We first examine whether this constraint reduces memorization and, consequently, the membership inference risks associated with NODEs. We conduct a comprehensive evaluation of NODEs under membership inference attacks and show that they exhibit twice the resistance compared to conventional models such as ResNets. By analyzing the variance in membership risks across different NODE models, we find that their limited expressivity leads to reduced overfitting to the training data. We then demonstrate, both theoretically and empirically, that membership inference risks can be further mitigated by utilizing a stochastic variant of NODEs: neural stochastic differential equations (NSDEs). We show that NSDEs are differentially-private (DP) learners that provide the same provable privacy guarantees as DPSGD, the de-facto mechanism for training private models. NSDEs are also effective in mitigating membership inference attacks, achieving risk levels comparable to private models trained with DP-SGD while offering an improved privacyutility trade-off. Moreover, we propose a drop-in-replacement strategy that efficiently integrates NSDEs into conventional feedforward architectures to enhance their privacy.",
      "authors": [
        "Hong, Sanghyun",
        "Wu, Fan",
        "Gruber, Anthony",
        "Lee, Kookjin"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2501.06686v2",
        "Other Formats": "https://arxiv.org/format/2501.06686",
        "TeX Source": "https://arxiv.org/src/2501.06686",
        "View PDF": "https://arxiv.org/pdf/2501.06686"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 12 Jan 2025 02:26:21 UTC (4,004 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Fri, 20 Jun 2025 22:28:35 UTC (4,498 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/01/12",
      "title": "Modeling Neural Networks with Privacy Using Neural Stochastic Differential Equations",
      "tasks": [
        "Language Modeling",
        "Language Modelling"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.12965",
      "abstract": "Randomness is an unavoidable part of training deep learning models, yet something that traditional training data attribution algorithms fail to rigorously account for. They ignore the fact that, due to stochasticity in the initialisation and batching, training on the same dataset can yield different models. In this paper, we address this shortcoming through introducing distributional training data attribution (d-TDA), the goal of which is to predict how the distribution of model outputs (over training runs) depends upon the dataset. We demonstrate the practical significance of d-TDA in experiments, e.g. by identifying training examples that drastically change the distribution of some target measurement without necessarily changing the mean. Intriguingly, we also find that influence functions (IFs), a popular but poorly-understood data attribution tool, emerge naturally from our distributional framework as the limit to unrolled differentiation; without requiring restrictive convexity assumptions. This provides a new mathematical motivation for their efficacy in deep learning, and helps to characterise their limitations.",
      "authors": [
        "Mlodozeniec, Bruno",
        "Reid, Isaac",
        "Power, Sam",
        "Krueger, David",
        "Erdogdu, Murat",
        "Turner, Richard E.",
        "Grosse, Roger"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.12965",
        "View PDF": "https://arxiv.org/pdf/2506.12965"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (stat.ML)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 15 Jun 2025 21:02:36 UTC (3,302 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Fri, 20 Jun 2025 20:38:39 UTC (3,303 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/06/15",
      "title": "Distributional Training Data Attribution",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.06718",
      "abstract": "Foundational models have shown remarkable potential in natural language processing and computer vision, yet remain in their infancy in wireless communications. While a few efforts have explored image-based modalities such as channel state information (CSI) and frequency spectrograms, foundational models that operate directly on raw IQ data remain largely unexplored. This paper presents, IQFM, the first I/Q signal foundational model for wireless communications. IQFM supporting diverse tasks: modulation classification, angle-of-arrival (AoA), beam prediction, and RF fingerprinting, without heavy preprocessing or handcrafted features. We also introduce a task-aware augmentation strategy that categorizes transformations into core augmentations, such as cyclic time shifting, and task-specific augmentations. This strategy forms the basis for structured, task-dependent representation learning within a contrastive self-supervised learning (SSL) framework. Using this strategy, the lightweight encoder, pre-trained via SSL on over-the-air multi-antenna IQ data, achieves up to 99.67% and 65.45% accuracy on modulation and AoA classification, respectively, using only one labeled sample per class, outperforming supervised baselines by up to 7x and 145x. The model also generalizes to out-of-distribution tasks; when adapted to new tasks using only 500 samples per class and minimal parameter updates via LoRA, the same frozen encoder achieves 94.15% on beam prediction (vs. 89.53% supervised), 50.00% on RML2016a modulation classification (vs. 49.30%), and 96.05% on RF fingerprinting (vs. 96.64%). These results demonstrate the potential of raw IQ-based foundational models as efficient, reusable encoders for multi-task learning in AI-native 6G systems.",
      "authors": [
        "Mashaal, Omar",
        "Abou-Zeid, Hatem"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.06718v2",
        "Other Formats": "https://arxiv.org/format/2506.06718",
        "TeX Source": "https://arxiv.org/src/2506.06718",
        "View PDF": "https://arxiv.org/pdf/2506.06718"
      },
      "subjects": [
        "Signal Processing (eess.SP)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 7 Jun 2025 09:01:38 UTC (20,010 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Fri, 20 Jun 2025 23:14:19 UTC (3,259 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/06/07",
      "title": "IQFM A Wireless Foundational Model for I/Q Streams in AI-Native 6G",
      "tasks": [
        "Beam Prediction",
        "Multi-Task Learning",
        "Self-Supervised Learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17403",
      "abstract": "Automating embryo viability prediction for in vitro fertilization (IVF) is important but challenging due to the limited availability of labeled pregnancy outcome data, as only a small fraction of embryos are labeled after transfer. Self-supervised learning (SSL) can leverage both labeled and unlabeled data to improve prediction. However, existing SSL methods for videos are not directly applicable to embryo development videos due to two challenges: (1) embryo time-lapse videos contain hundreds of frames, requiring significant GPU memory for conventional SSL; (2) the dataset contains videos with varying lengths and many outlier frames, causing traditional video alignment methods to struggle with semantic misalignment. We propose Spatial-Temporal Pre-Training (STPT) to address these challenges. STPT includes two stages: spatial and temporal. In each stage, only one encoder is trained while the other is frozen, reducing memory demands. To handle temporal misalignment, STPT avoids frame-by-frame alignment across videos. The spatial stage learns from alignments within each video and its temporally consistent augmentations. The temporal stage then models relationships between video embeddings. Our method efficiently handles long videos and temporal variability. On 23,027 time-lapse videos (3,286 labeled), STPT achieves the highest AUC of 0.635 (95% CI: 0.632-0.638) compared to baselines, with limited computational resources.",
      "authors": [
        "Shi, Zhiyi",
        "Kim, Junsik",
        "Yang, Helen Y.",
        "Song, Yonghyun",
        "Oh, Hyun-Jic",
        "Ben-Yosef, Dalit",
        "Needleman, Daniel",
        "Pfister, Hanspeter"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17403v1",
        "Other Formats": "https://arxiv.org/format/2506.17403",
        "TeX Source": "https://arxiv.org/src/2506.17403",
        "View PDF": "https://arxiv.org/pdf/2506.17403"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 18:08:41 UTC (6,706 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Spatial-Temporal Pre-Training for Embryo Viability Prediction Using Time-Lapse Videos",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.16733",
      "abstract": "Positron emission tomography (PET) is widely used to assess metabolic activity, but its application is limited by the availability of radiotracers. 18F-labeled fluorodeoxyglucose (18F-FDG) is the most commonly used tracer but shows limited effectiveness for certain tumors. In contrast, 6-18F-fluoro-3,4-dihydroxy-L-phenylalanine (18F-DOPA) offers higher specificity for neuroendocrine tumors and neurological disorders. However, its complex synthesis and limitations in transportation and clinical use hinder widespread adoption. During PET imaging, the sinogram represents a form of raw data acquired by the scanner. Therefore, modeling in projection domain enables more direct utilization of the original information, potentially reducing the accumulation of errors introduced during the image reconstruction process. Inspired by these factors, this study proposes a prior-guided joint diffusion model (PJDM) for transforming 18F-FDG PET images into 18F-DOPA PET images in projection domain. Specifically, a coarse estimation model and a prior refinement model are trained independently. During inference, an initial synthetic 18F-DOPA PET sinogram is generated using a higher-order hybrid sampler. This sinogram is then degraded and serves as an additional condition to guide the iterative refinement process using learned prior. Experimental results demonstrated that PJDM effectively improved both sinogram quality and synthetic outcomes. The code is available at: https://github.com/yqx7150/PJDM.",
      "authors": [
        "Chen, Fang",
        "Zhang, Weifeng",
        "Ai, Xingyu",
        "Li, BingXuan",
        "Li, An",
        "Liu, Qiegen"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.16733",
        "View PDF": "https://arxiv.org/pdf/2506.16733"
      },
      "subjects": [
        "Image and Video Processing (eess.IV)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 04:05:34 UTC (5,639 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "A Prior-Guided Joint Diffusion Model in Projection Domain for PET Tracer Conversion",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2502.15895",
      "abstract": "Robust fine-tuning aims to adapt large foundation models to downstream tasks while preserving their robustness to distribution shifts. Existing methods primarily focus on constraining and projecting current model towards the pre-trained initialization based on the magnitudes between fine-tuned and pre-trained weights, which often require extensive hyper-parameter tuning and can sometimes result in underfitting. In this work, we propose Directional Gradient Projection (DiGraP), a novel layer-wise trainable method that incorporates directional information from gradients to bridge regularization and multi-objective optimization. Besides demonstrating our method on image classification, as another contribution we generalize this area to the multi-modal evaluation settings for robust fine-tuning. Specifically, we first bridge the uni-modal and multi-modal gap by performing analysis on Image Classification reformulated Visual Question Answering (VQA) benchmarks and further categorize ten out-of-distribution (OOD) VQA datasets by distribution shift types and degree (i.e. near versus far OOD). Experimental results show that DiGraP consistently outperforms existing baselines across Image Classfication and VQA tasks with discriminative and generative backbones, improving both in-distribution (ID) generalization and OOD robustness.",
      "authors": [
        "Huang, Chengyue",
        "Tian, Junjiao",
        "Maneechotesuwan, Brisa",
        "Chopra, Shivang",
        "Kira, Zsolt"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2502.15895v2",
        "Other Formats": "https://arxiv.org/format/2502.15895",
        "TeX Source": "https://arxiv.org/src/2502.15895",
        "View PDF": "https://arxiv.org/pdf/2502.15895"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 21 Feb 2025 19:31:55 UTC (23,547 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Fri, 20 Jun 2025 19:54:55 UTC (14,580 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/02/21",
      "title": "Directional Gradient Projection for Robust Fine-Tuning of Foundation Models",
      "tasks": [
        "image-classification",
        "Image Classification",
        "Question Answering",
        "Visual Question Answering",
        "Visual Question Answering (VQA)"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17351",
      "abstract": "Cognitive impairment (CI) is of growing public health concern, and early detection is vital for effective intervention. Speech has gained attention as a non-invasive and easily collectible biomarker for assessing cognitive decline. Traditional CI detection methods typically rely on supervised models trained on acoustic and linguistic features extracted from speech, which often require manual annotation and may not generalise well across datasets and languages. In this work, we propose the first zero-shot speech-based CI detection method using the Qwen2- Audio AudioLLM, a model capable of processing both audio and text inputs. By designing prompt-based instructions, we guide the model in classifying speech samples as indicative of normal cognition or cognitive impairment. We evaluate our approach on two datasets: one in English and another multilingual, spanning different cognitive assessment tasks. Our results show that the zero-shot AudioLLM approach achieves performance comparable to supervised methods and exhibits promising generalizability and consistency across languages, tasks, and datasets.",
      "authors": [
        "Shahin, Mostafa",
        "Ahmed, Beena",
        "Epps, Julien"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17351v1",
        "Other Formats": "https://arxiv.org/format/2506.17351",
        "TeX Source": "https://arxiv.org/src/2506.17351",
        "View PDF": "https://arxiv.org/pdf/2506.17351"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Multimedia (cs.MM)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 01:28:43 UTC (361 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Zero-Shot Cognitive Impairment Detection from Speech Using AudioLLM",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17353",
      "abstract": "The increasing demand for domain-specific and human-aligned Large Language Models (LLMs) has led to the widespread adoption of Supervised Fine-Tuning (SFT) techniques. SFT datasets often comprise valuable instruction-response pairs, making them highly valuable targets for potential extraction. This paper studies this critical research problem for the first time. We start by formally defining and formulating the problem, then explore various attack goals, types, and variants based on the unique properties of SFT data in real-world scenarios. Based on our analysis of extraction behaviors of direct extraction, we develop a novel extraction method specifically designed for SFT models, called Differentiated Data Extraction (DDE), which exploits the confidence levels of fine-tuned models and their behavioral differences from pre-trained base models. Through extensive experiments across multiple domains and scenarios, we demonstrate the feasibility of SFT data extraction using DDE. Our results show that DDE consistently outperforms existing extraction baselines in all attack settings. To counter this new attack, we propose a defense mechanism that mitigates DDE attacks with minimal impact on model performance. Overall, our research reveals hidden data leak risks in fine-tuned LLMs and provides insights for developing more secure models.",
      "authors": [
        "Li, Zongjie",
        "Wu, Daoyuan",
        "Wang, Shuai",
        "Su, Zhendong"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17353v1",
        "Other Formats": "https://arxiv.org/format/2506.17353",
        "TeX Source": "https://arxiv.org/src/2506.17353",
        "View PDF": "https://arxiv.org/pdf/2506.17353"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 02:43:36 UTC (1,460 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Differentiation-Based Extraction of Proprietary Data from Fine-Tuned LLMs",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17114",
      "abstract": "Large reasoning models (e.g., R1, o3) have demonstrated remarkable mathematical problem-solving abilities. However, the high reported accuracy of these advanced models on popular datasets, reliance on purely numerical evaluation and potential benchmark leakage, often masks their true reasoning shortcomings. To address this, we propose leveraging the inherent rigor and methodological complexity of mathematical proofs as a diagnostic tool to expose these hidden failures. Specifically, we introduce the RFMDataset (Reveal Failure Modes), a collection of 200 diverse mathematical proof problems, and thoroughly evaluate advanced models' performance on it. Our in-depth analysis of their failures uncovers 10 fine-grained error types, which shows fundamental limitations in current large reasoning models: 1) large reasoning models grapple profoundly with mathematical proofs, with some generating entirely correct proofs for less than 20% of problems and failing even on basic ones; 2) models exhibit a diverse spectrum of reasoning failures, prominently demonstrating the lack of guarantees for the correctness and rigor of single-step reasoning; and 3) models show hallucination and incompleteness during the reasoning process. Our findings reveal that models' self-reflection is insufficient to resolve the current logical dilemmas, necessitating formalized and fine-grained logical training.",
      "authors": [
        "Guo, Dadi",
        "Liu, Jiayu",
        "Fan, Zhiyuan",
        "He, Zhitao",
        "Li, Haoran",
        "Wang, Yumeng",
        "R., Yi",
        "Fung"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17114v1",
        "Other Formats": "https://arxiv.org/format/2506.17114",
        "TeX Source": "https://arxiv.org/src/2506.17114",
        "View PDF": "https://arxiv.org/pdf/2506.17114"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 16:14:18 UTC (1,619 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17499",
      "abstract": "In few-shot classification tasks (so-called episodes), a small set of labeled support samples is provided during inference to aid the classification of unlabeled query samples. Metric-based models typically operate by computing similarities between query and support embeddings within a learned metric space, followed by nearest-neighbor classification. However, these labeled support samples are often underutilized--they are only used for similarity comparison, despite their potential to fine-tune and adapt the metric space itself to the classes in the current episode. To address this, we propose a series of simple yet effective episode-specific, during-inference fine-tuning methods for metric-based models, including Rotational Division Fine-Tuning (RDFT) and its two variants, Iterative Division Fine-Tuning (IDFT) and Augmented Division Fine-Tuning (ADFT). These methods construct pseudo support-query pairs from the given support set to enable fine-tuning even for non-parametric models. Nevertheless, the severely limited amount of data in each task poses a substantial risk of overfitting when applying such fine-tuning strategies. To mitigate this, we further propose to train the metric-based model within an optimization-based meta-learning framework. With the combined efforts of episode-specific fine-tuning and optimization-based meta-training, metric-based models are equipped with the ability to rapidly adapt to the limited support samples during inference while avoiding overfitting. We validate our approach on three audio datasets from diverse domains, namely ESC-50 (environmental sounds), Speech Commands V2 (spoken keywords), and Medley-solos-DB (musical instrument). Experimental results demonstrate that our approach consistently improves performance for all evaluated metric-based models (especially for attention-based models) and generalizes well across different audio domains.",
      "authors": [
        "Zhuang, Xuanyu",
        "Peeters, Geoffroy",
        "Richard, Ga\u00ebl"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17499v1",
        "Other Formats": "https://arxiv.org/format/2506.17499",
        "TeX Source": "https://arxiv.org/src/2506.17499",
        "View PDF": "https://arxiv.org/pdf/2506.17499"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Multimedia (cs.MM)",
        "Sound (cs.SD)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 22:24:38 UTC (1,536 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Episode-specific Fine-tuning for Metric-based Few-shot Learners with Optimization-based Training",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2410.10200",
      "abstract": "Federated Learning enables the fine-tuning of foundation models (FMs) across distributed clients for specific tasks; however, its scalability is limited by the heterogeneity of client memory capacities. In this work, we propose Fed-pilot, a memory-efficient federated fine-tuning framework. It enables memory-constrained clients to participate in Low-Rank Adaptation (LoRA)-based fine-tuning by training only a subset of LoRA modules locally. Fed-pilot identifies the optimal selection of trainable LoRA modules as a knapsack optimization problem, maximizing model performance under memory constraints for each client. To mitigate inconsistencies arising from heterogeneous module allocations and Non-IID data, Fed-pilot employs a novel aggregation rule that dynamically compensates for under-updated layers. Extensive experiments on five diverse datasets across various heterogeneous data settings demonstrate Fed-pilot's effectiveness and efficiency compared to state-of-the-art methods. To the best of our knowledge, this is the first study on federated fine-tuning of FMs that integrates memory-constrained optimization. The code will be publicly available.",
      "authors": [
        "Zhang, Zikai",
        "Hu, Rui",
        "Liu, Ping",
        "Xu, Jiahao"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2410.10200v2",
        "Other Formats": "https://arxiv.org/format/2410.10200",
        "TeX Source": "https://arxiv.org/src/2410.10200",
        "View PDF": "https://arxiv.org/pdf/2410.10200"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Distributed, Parallel, and Cluster Computing (cs.DC)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 14 Oct 2024 06:36:41 UTC (1,709 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Fri, 20 Jun 2025 20:43:04 UTC (1,271 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/10/14",
      "title": "Fed-pilot: Optimizing LoRA Allocation for Efficient Federated Fine-Tuning with Heterogeneous Clients",
      "tasks": [
        "Federated Learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17486",
      "abstract": "Large language models (LLMs) provide robots with powerful contextual reasoning abilities and a natural human interface. Yet, current LLM-enabled robots typically depend on cloud-hosted models, limiting their usability in environments with unreliable communication infrastructure, such as outdoor or industrial settings. We present PRISM, a framework for distilling small language model (SLM)-enabled robot planners that run on-device with minimal human supervision. Starting from an existing LLM-enabled planner, PRISM automatically synthesizes diverse tasks and environments, elicits plans from the LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in replacement of the source model. We apply PRISM to three LLM-enabled planners for mapping and exploration, manipulation, and household assistance, and we demonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20% of GPT-4o's performance to over 93% - using only synthetic data. We further demonstrate that the distilled planners generalize across heterogeneous robotic platforms (ground and aerial) and diverse environments (indoor and outdoor). We release all software, trained models, and datasets at https://zacravichandran.github.io/PRISM.",
      "authors": [
        "Ravichandran, Zachary",
        "Hounie, Ignacio",
        "Cladera, Fernando",
        "Ribeiro, Alejandro",
        "Pappas, George J.",
        "Kumar, Vijay"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17486",
        "TeX Source": "https://arxiv.org/src/2506.17486",
        "View PDF": "https://arxiv.org/pdf/2506.17486"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 21:44:27 UTC (24,724 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Distilling On-device Language Models for Robot Planning with Minimal Human Intervention",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17370",
      "abstract": "As e-commerce rapidly integrates artificial intelligence for content creation and product recommendations, these technologies offer significant benefits in personalization and efficiency. AI-driven systems automate product descriptions, generate dynamic advertisements, and deliver tailored recommendations based on consumer behavior, as seen in major platforms like Amazon and Shopify. However, the widespread use of AI in e-commerce raises crucial ethical challenges, particularly around data privacy, algorithmic bias, and consumer autonomy. Bias -- whether cultural, gender-based, or socioeconomic -- can be inadvertently embedded in AI models, leading to inequitable product recommendations and reinforcing harmful stereotypes. This paper examines the ethical implications of AI-driven content creation and product recommendations, emphasizing the need for frameworks to ensure fairness, transparency, and need for more established and robust ethical standards. We propose actionable best practices to remove bias and ensure inclusivity, such as conducting regular audits of algorithms, diversifying training data, and incorporating fairness metrics into AI models. Additionally, we discuss frameworks for ethical conformance that focus on safeguarding consumer data privacy, promoting transparency in decision-making processes, and enhancing consumer autonomy. By addressing these issues, we provide guidelines for responsibly utilizing AI in e-commerce applications for content creation and product recommendations, ensuring that these technologies are both effective and ethically sound.",
      "authors": [
        "Jain, Aditi Madhusudan",
        "Jain, Ayush"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17370",
        "View PDF": "https://arxiv.org/pdf/2506.17370"
      },
      "subjects": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 15:54:25 UTC (401 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "AI based Content Creation and Product Recommendation Applications in E-commerce: An Ethical overview",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17469",
      "abstract": "Traditional particle size distribution (PSD) analyses create significant downtime and are expensive in labor and maintenance. These drawbacks could be alleviated using optical grain size analysis integrated into routine geotechnical laboratory workflow. This paper presents a high-resolution dataset of 12,714 images of 321 different soil samples collected in the Montreal, Quebec region, alongside their PSD analysis. It is designed to provide a robust starting point for training convolutional neural networks (CNN) in geotechnical applications. Soil samples were photographed in a standardized top-view position with a resolution of 45 MP and a minimum scale of 39.4 micrometers per pixel, both in their moist and dry states. A custom test bench employing 13x9 inch white aluminum trays, on which the samples are spread in a thin layer, was used. For samples exceeding a size limit, a coning and quartering method was employed for mass reduction.",
      "authors": [
        "St-Cyr, Thomas Plante",
        "Duhaime, Fran\u00e7ois",
        "Dub\u00e9, Jean-S\u00e9bastien",
        "Grenier, Simon"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17469",
        "View PDF": "https://arxiv.org/pdf/2506.17469"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 20:19:45 UTC (1,231 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Photogranulometry -- Dataset of soil images with corresponding particle size distributions",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17442",
      "abstract": "Artificial intelligence (AI) is increasingly integrated into modern healthcare, offering powerful support for clinical decision-making. However, in real-world settings, AI systems may experience performance degradation over time, due to factors such as shifting data distributions, changes in patient characteristics, evolving clinical protocols, and variations in data quality. These factors can compromise model reliability, posing safety concerns and increasing the likelihood of inaccurate predictions or adverse outcomes. This review presents a forward-looking perspective on monitoring and maintaining the \"health\" of AI systems in healthcare. We highlight the urgent need for continuous performance monitoring, early degradation detection, and effective self-correction mechanisms. The paper begins by reviewing common causes of performance degradation at both data and model levels. We then summarize key techniques for detecting data and model drift, followed by an in-depth look at root cause analysis. Correction strategies are further reviewed, ranging from model retraining to test-time adaptation. Our survey spans both traditional machine learning models and state-of-the-art large language models (LLMs), offering insights into their strengths and limitations. Finally, we discuss ongoing technical challenges and propose future research directions. This work aims to guide the development of reliable, robust medical AI systems capable of sustaining safe, long-term deployment in dynamic clinical settings.",
      "authors": [
        "Guan, Hao",
        "Bates, David",
        "Zhou, Li"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17442v1",
        "Other Formats": "https://arxiv.org/format/2506.17442",
        "TeX Source": "https://arxiv.org/src/2506.17442",
        "View PDF": "https://arxiv.org/pdf/2506.17442"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Emerging Technologies (cs.ET)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 19:22:07 UTC (564 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Keeping Medical AI Healthy: A Review of Detection and Correction Methods for System Degradation",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17497",
      "abstract": "Despite progress in controllable symbolic music generation, data scarcity remains a challenge for certain control modalities. Composer-style music generation is a prime example, as only a few pieces per composer are available, limiting the modeling of both styles and fundamental music elements (e.g., melody, chord, rhythm). In this paper, we investigate how general music knowledge learned from a broad corpus can enhance the mastery of specific composer styles, with a focus on piano piece generation. Our approach follows a two-stage training paradigm. First, we pre-train a REMI-based music generation model on a large corpus of pop, folk, and classical music. Then, we fine-tune it on a small, human-verified dataset from four renowned composers, namely Bach, Mozart, Beethoven, and Chopin, using a lightweight adapter module to condition the model on style indicators. To evaluate the effectiveness of our approach, we conduct both objective and subjective evaluations on style accuracy and musicality. Experimental results demonstrate that our method outperforms ablations and baselines, achieving more precise composer-style modeling and better musical aesthetics. Additionally, we provide observations on how the model builds music concepts from the generality pre-training and refines its stylistic understanding through the mastery fine-tuning.",
      "authors": [
        "Yao, Mingyang",
        "Chen, Ke"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17497v1",
        "Other Formats": "https://arxiv.org/format/2506.17497",
        "TeX Source": "https://arxiv.org/src/2506.17497",
        "View PDF": "https://arxiv.org/pdf/2506.17497"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 22:20:59 UTC (1,229 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "From Generality to Mastery: Composer-Style Symbolic Music Generation via Large-Scale Pre-training",
      "models": [
        {
          "model_path": "Itsuki-music/Generality_to_Mastery",
          "downloads": "0",
          "likes": "1",
          "trending_score": "1.0",
          "link": "https://huggingface.co/Itsuki-music/Generality_to_Mastery"
        }
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2407.21515",
      "abstract": "Representation-based retrieval models, so-called bi-encoders, estimate the relevance of a document to a query by calculating the similarity of their respective embeddings. Current state-of-the-art bi-encoders are trained using an expensive training regime involving knowledge distillation from a teacher model and batch-sampling. Instead of relying on a teacher model, we contribute a novel parameter-free loss function for self-supervision that exploits the pre-trained language modeling capabilities of the encoder model as a training signal, eliminating the need for batch sampling by performing implicit hard negative mining. We investigate the capabilities of our proposed approach through extensive experiments, demonstrating that self-distillation can match the effectiveness of teacher distillation using only 13.5% of the data, while offering a speedup in training time between 3x and 15x compared to parametrized losses. All code and data is made openly available.",
      "authors": [
        "Gienapp, Lukas",
        "Deckers, Niklas",
        "Potthast, Martin",
        "Scells, Harrisen"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2407.21515v2",
        "Other Formats": "https://arxiv.org/format/2407.21515",
        "TeX Source": "https://arxiv.org/src/2407.21515",
        "View PDF": "https://arxiv.org/pdf/2407.21515"
      },
      "subjects": [
        "Information Retrieval (cs.IR)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 31 Jul 2024 10:33:32 UTC (3,739 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Fri, 20 Jun 2025 19:43:59 UTC (2,289 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/07/31",
      "title": "Learning Effective Representations for Retrieval Using Self-Distillation with Adaptive Relevance Margins",
      "tasks": [
        "Knowledge Distillation",
        "Language Modeling",
        "Language Modelling",
        "Retrieval"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2410.14031",
      "abstract": "Over the past decade, predictive modeling of neural responses in the primate visual system has advanced significantly, largely driven by various DNN approaches. These include models optimized directly for visual recognition, cross-modal alignment through contrastive objectives, neural response prediction from scratch, and large language model embeddings.Likewise, different readout mechanisms, ranging from fully linear to spatial-feature factorized methods have been explored for mapping network activations to neural responses. Despite the diversity of these approaches, it remains unclear which method performs best across different visual regions. In this study, we systematically compare these approaches for modeling the human visual system and investigate alternative strategies to improve response predictions. Our findings reveal that for early to mid-level visual areas, response-optimized models with visual inputs offer superior prediction accuracy, while for higher visual regions, embeddings from LLMs based on detailed contextual descriptions of images and task-optimized models pretrained on large vision datasets provide the best fit. Through comparative analysis of these modeling approaches, we identified three distinct regions in the visual cortex: one sensitive primarily to perceptual features of the input that are not captured by linguistic descriptions, another attuned to fine-grained visual details representing semantic information, and a third responsive to abstract, global meanings aligned with linguistic content. We also highlight the critical role of readout mechanisms, proposing a novel scheme that modulates receptive fields and feature maps based on semantic content, resulting in an accuracy boost of 3-23% over existing SOTAs for all models and brain regions. Together, these findings offer key insights into building more precise models of the visual system.",
      "authors": [
        "Saha, Shreya",
        "Chadha, Ishaan",
        "khosla, Meenakshi"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2410.14031v4",
        "Other Formats": "https://arxiv.org/format/2410.14031",
        "TeX Source": "https://arxiv.org/src/2410.14031",
        "View PDF": "https://arxiv.org/pdf/2410.14031"
      },
      "subjects": [
        "Neural and Evolutionary Computing (cs.NE)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 17 Oct 2024 21:11:13 UTC (4,569 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Wed, 11 Dec 2024 19:07:02 UTC (7,729 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Fri, 13 Dec 2024 21:49:39 UTC (7,729 KB)",
          "link": "/",
          "version": "[v3]"
        },
        {
          "details": "Fri, 20 Jun 2025 20:17:21 UTC (11,772 KB)",
          "version": "[v4]"
        }
      ],
      "submitted_date": "2024/10/17",
      "title": "Modeling the Human Visual System: Comparative Insights from Response-Optimized and Task-Optimized Vision Models, Language Models, and different Readout Mechanisms",
      "repo_urls": [
        "https://github.com/NeuroML-Lab/Visual-Stream-Modeling"
      ],
      "tasks": [
        "cross-modal alignment",
        "Large Language Model"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.16846",
      "abstract": "Decision trees are popular in survival analysis for their interpretability and ability to model complex relationships. Survival trees, which predict the timing of singular events using censored historical data, are typically built through heuristic approaches. Recently, there has been growing interest in globally optimized trees, where the overall tree is trained by minimizing the error function over all its parameters. We propose a new soft survival tree model (SST), with a soft splitting rule at each branch node, trained via a nonlinear optimization formulation amenable to decomposition. Since SSTs provide for every input vector a specific survival function associated to a single leaf node, they satisfy the conditional computation property and inherit the related benefits. SST and the training formulation combine flexibility with interpretability: any smooth survival function (parametric, semiparametric, or nonparametric) estimated through maximum likelihood can be used, and each leaf node of an SST yields a cluster of distinct survival functions which are associated to the data points routed to it. Numerical experiments on 15 well-known datasets show that SSTs, with parametric and spline-based semiparametric survival functions, trained using an adaptation of the node-based decomposition algorithm proposed by Consolo et al. (2024) for soft regression trees, outperform three benchmark survival trees in terms of four widely-used discrimination and calibration measures. SSTs can also be extended to consider group fairness.",
      "authors": [
        "Consoloa, Antonio",
        "Amaldi, Edoardo",
        "Carrizosa, Emilio"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.16846v1",
        "Other Formats": "https://arxiv.org/format/2506.16846",
        "TeX Source": "https://arxiv.org/src/2506.16846",
        "View PDF": "https://arxiv.org/pdf/2506.16846"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Optimization and Control (math.OC)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 08:51:33 UTC (24,744 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Soft decision trees for survival analysis",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17487",
      "abstract": "A variational framework for structural topology optimization is developed, integrating quantum and classical latent encoding strategies within a coordinate-based neural decoding architecture. In this approach, a low-dimensional latent vector, generated either by a variational quantum circuit or sampled from a Gaussian distribution, is mapped to a higher-dimensional latent space via a learnable projection layer. This enriched representation is then decoded into a high-resolution material distribution using a neural network that takes both the latent vector and Fourier-mapped spatial coordinates as input. The optimization is performed directly on the latent parameters, guided solely by physics-based objectives such as compliance minimization and volume constraints evaluated through finite element analysis, without requiring any precomputed datasets or supervised training. Quantum latent vectors are constructed from the expectation values of Pauli observables measured on parameterized quantum circuits, providing a structured and entangled encoding of information. The classical baseline uses Gaussian-sampled latent vectors projected in the same manner. The proposed variational formulation enables the generation of diverse and physically valid topologies by exploring the latent space through sampling or perturbation, in contrast to traditional optimization methods that yield a single deterministic solution. Numerical experiments show that both classical and quantum encodings produce high-quality structural designs. However, quantum encodings demonstrate advantages in several benchmark cases in terms of compliance and design diversity. These results highlight the potential of quantum circuits as an effective and scalable tool for physics-constrained topology optimization and suggest promising directions for applying near-term quantum hardware in structural design.",
      "authors": [
        "Tabarraei, Alireza"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17487v1",
        "Other Formats": "https://arxiv.org/format/2506.17487",
        "TeX Source": "https://arxiv.org/src/2506.17487",
        "View PDF": "https://arxiv.org/pdf/2506.17487"
      },
      "subjects": [
        "Computational Engineering, Finance, and Science (cs.CE)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 21:49:06 UTC (4,112 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Variational Quantum Latent Encoding for Topology Optimization",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17459",
      "abstract": "Automatic Speech Recognition (ASR) has reached impressive accuracy for high-resource languages, yet its utility in linguistic fieldwork remains limited. Recordings collected in fieldwork contexts present unique challenges, including spontaneous speech, environmental noise, and severely constrained datasets from under-documented languages. In this paper, we benchmark the performance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five typologically diverse low-resource languages with control of training data duration. Our findings show that MMS is best suited when extremely small amounts of training data are available, whereas XLS-R shows parity performance once training data exceed one hour. We provide linguistically grounded analysis for further provide insights towards practical guidelines for field linguists, highlighting reproducible ASR adaptation approaches to mitigate the transcription bottleneck in language documentation.",
      "authors": [
        "Liang, Siyu",
        "Levow, Gina-Anne"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17459v1",
        "Other Formats": "https://arxiv.org/format/2506.17459",
        "TeX Source": "https://arxiv.org/src/2506.17459",
        "View PDF": "https://arxiv.org/pdf/2506.17459"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 19:59:49 UTC (749 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Breaking the Transcription Bottleneck: Fine-tuning ASR Models for Extremely Low-Resource Fieldwork Languages",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17503",
      "abstract": "Medical vision-language models (VLMs) have demonstrated unprecedented transfer capabilities and are being increasingly adopted for data-efficient image classification. Despite its growing popularity, its reliability aspect remains largely unexplored. This work explores the split conformal prediction (SCP) framework to provide trustworthiness guarantees when transferring such models based on a small labeled calibration set. Despite its potential, the generalist nature of the VLMs' pre-training could negatively affect the properties of the predicted conformal sets for specific tasks. While common practice in transfer learning for discriminative purposes involves an adaptation stage, we observe that deploying such a solution for conformal purposes is suboptimal since adapting the model using the available calibration data breaks the rigid exchangeability assumptions for test data in SCP. To address this issue, we propose transductive split conformal adaptation (SCA-T), a novel pipeline for transfer learning on conformal scenarios, which performs an unsupervised transductive adaptation jointly on calibration and test data. We present comprehensive experiments utilizing medical VLMs across various image modalities, transfer tasks, and non-conformity scores. Our framework offers consistent gains in efficiency and conditional coverage compared to SCP, maintaining the same empirical guarantees.",
      "authors": [
        "Silva-Rodr\u00edguez, Julio",
        "Ayed, Ismail Ben",
        "Dolz, Jose"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17503v1",
        "Other Formats": "https://arxiv.org/format/2506.17503",
        "TeX Source": "https://arxiv.org/src/2506.17503",
        "View PDF": "https://arxiv.org/pdf/2506.17503"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 22:48:07 UTC (108 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Trustworthy Few-Shot Transfer of Medical VLMs through Split Conformal Prediction",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17350",
      "abstract": "Backdoor attacks have emerged as a critical security threat against deep neural networks in recent years. The majority of existing backdoor attacks focus on targeted backdoor attacks, where trigger is strongly associated to specific malicious behavior. Various backdoor detection methods depend on this inherent property and shows effective results in identifying and mitigating such targeted attacks. However, a purely untargeted attack in backdoor scenarios is, in some sense, self-weakening, since the target nature is what makes backdoor attacks so powerful. In light of this, we introduce a novel Constrained Untargeted Backdoor Attack (CUBA), which combines the flexibility of untargeted attacks with the intentionality of targeted attacks. The compromised model, when presented with backdoor images, will classify them into random classes within a constrained range of target classes selected by the attacker. This combination of randomness and determinedness enables the proposed untargeted backdoor attack to natively circumvent existing backdoor defense methods. To implement the untargeted backdoor attack under controlled flexibility, we propose to apply logit normalization on cross-entropy loss with flipped one-hot labels. By constraining the logit during training, the compromised model will show a uniform distribution across selected target classes, resulting in controlled untargeted attack. Extensive experiments demonstrate the effectiveness of the proposed CUBA on different datasets.",
      "authors": [
        "Wu, Yinghao",
        "Zhang, Liyan"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17350v1",
        "Other Formats": "https://arxiv.org/format/2506.17350",
        "TeX Source": "https://arxiv.org/src/2506.17350",
        "View PDF": "https://arxiv.org/pdf/2506.17350"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 00:47:30 UTC (1,141 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "CUBA: Controlled Untargeted Backdoor Attack against Deep Neural Networks",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17458",
      "abstract": "Robotic manipulation in space is essential for emerging applications such as debris removal and in-space servicing, assembly, and manufacturing (ISAM). A key requirement for these tasks is the ability to perform precise, contact-rich manipulation under significant uncertainty. In particular, thermal-induced deformation of manipulator links and temperature-dependent encoder bias introduce kinematic parameter errors that significantly degrade end-effector accuracy. Traditional calibration techniques rely on external sensors or dedicated calibration procedures, which can be infeasible or risky in dynamic, space-based operational scenarios. This paper proposes a novel method for kinematic parameter estimation that only requires encoder measurements and binary contact detection. The approach focuses on estimating link thermal deformation strain and joint encoder biases by leveraging information of the contact manifold - the set of relative SE(3) poses at which contact between the manipulator and environment occurs. We present two core contributions: (1) a differentiable, learning-based model of the contact manifold, and (2) an optimization-based algorithm for estimating kinematic parameters from encoder measurements at contact instances. By enabling parameter estimation using only encoder measurements and contact detection, this method provides a robust, interpretable, and data-efficient solution for safe and accurate manipulation in the challenging conditions of space.",
      "authors": [
        "Negi, Abhay",
        "Manyar, Omey M.",
        "Gupta, Satyandra K."
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17458v1",
        "Other Formats": "https://arxiv.org/format/2506.17458",
        "TeX Source": "https://arxiv.org/src/2506.17458",
        "View PDF": "https://arxiv.org/pdf/2506.17458"
      },
      "subjects": [
        "Robotics (cs.RO)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 19:59:31 UTC (1,665 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Kinematic Model Optimization via Differentiable Contact Manifold for In-Space Manipulation",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17505",
      "abstract": "Despite its importance for performance and injury prevention, golf swing analysis is limited by isolated metrics, underrepresentation of professional athletes, and a lack of rich, interpretable movement representations. We address these gaps with a holistic, data-driven framework for personalized golf swing analysis from a single wrist-worn sensor. We build a large dataset of professional swings from publicly available videos, reconstruct full-body 3D kinematics using biologically accurate human mesh recovery, and generate synthetic inertial data to train neural networks that infer motion and segment swing phases from wrist-based input. We learn a compositional, discrete vocabulary of motion primitives that facilitates the detection and visualization of technical flaws, and is expressive enough to predict player identity, club type, sex, and age. Our system accurately estimates full-body kinematics and swing events from wrist data, delivering lab-grade motion analysis on-course and supporting early detection of anomalous movement patterns. Explainability methods reveal subtle, individualized movement signatures, reinforcing the view that variability is a hallmark of skilled performance. Longitudinal tracking demonstrates practical value: as one player's handicap improved from 50 to 2.2 over 1.5 years, our system captured measurable technical progress and provided targeted, actionable feedback. Our findings challenge common assumptions, such as swing consistency across clubs and the existence of a single \"ideal\" swing, and uncover latent biomarkers shaped by both intrinsic traits and task-specific constraints. This work bridges lab and field-based biomechanics, offering scalable, accessible, high-fidelity motion analysis for research, coaching, and injury prevention, while opening new directions in movement-based phenotyping, personalized equipment design, and motor skill development.",
      "authors": [
        "Lauer, Jessy"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17505v1",
        "Other Formats": "https://arxiv.org/format/2506.17505",
        "TeX Source": "https://arxiv.org/src/2506.17505",
        "View PDF": "https://arxiv.org/pdf/2506.17505"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 22:57:59 UTC (4,247 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Learning golf swing signatures from a single wrist-worn inertial sensor",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2504.06610",
      "abstract": "In this work, we propose DARSLP, a simple gloss-free, transformer-based sign language production (SLP) framework that directly maps spoken-language text to sign pose sequences. We first train a pose autoencoder that encodes sign poses into a compact latent space using an articulator-based disentanglement strategy, where features corresponding to the face, right hand, left hand, and body are modeled separately to promote structured and interpretable representation learning. Next, a non-autoregressive transformer decoder is trained to predict these latent representations from sentence-level text embeddings. To guide this process, we apply channel-aware regularization by aligning predicted latent distributions with priors extracted from the ground-truth encodings using a KL-divergence loss. The contribution of each channel to the loss is weighted according to its associated articulator region, enabling the model to account for the relative importance of different articulators during training. Our approach does not rely on gloss supervision or pretrained models, and achieves state-of-the-art results on the PHOENIX14T and CSL-Daily datasets.",
      "authors": [
        "Tasyurek, Sumeyye Meryem",
        "Kiziltepe, Tugce",
        "Keles, Hacer Yalim"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2504.06610v2",
        "Other Formats": "https://arxiv.org/format/2504.06610",
        "TeX Source": "https://arxiv.org/src/2504.06610",
        "View PDF": "https://arxiv.org/pdf/2504.06610"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 9 Apr 2025 06:14:19 UTC (4,575 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Fri, 20 Jun 2025 21:17:30 UTC (4,750 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/04/09",
      "title": "Disentangle and Regularize: Sign Language Production with Articulator-Based Disentanglement and Channel-Aware Regularization",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17500",
      "abstract": "Vision-language models (VLMs) are gaining attention in medical image analysis. These are pre-trained on large, heterogeneous data sources, yielding rich and transferable representations. Notably, the combination of modality-specialized VLMs with few-shot adaptation has provided fruitful results, enabling the efficient deployment of high-performing solutions. However, previous works on this topic make strong assumptions about the distribution of adaptation data, which are unrealistic in the medical domain. First, prior art assumes access to a balanced support set, a condition that breaks the natural imbalance in disease prevalence found in real-world scenarios. Second, these works typically assume the presence of an additional validation set to fix critical hyper-parameters, which is highly data-inefficient. This work challenges these favorable deployment scenarios and introduces a realistic, imbalanced, validation-free adaptation setting. Our extensive benchmark across various modalities and downstream tasks demonstrates that current methods systematically compromise their performance when operating under realistic conditions, occasionally even performing worse than zero-shot inference. Also, we introduce a training-free linear probe that adaptively blends visual and textual supervision. Detailed studies demonstrate that the proposed solver is a strong, efficient baseline, enabling robust adaptation in challenging scenarios.",
      "authors": [
        "Silva-Rodr\u00edguez, Julio",
        "Shakeri, Fereshteh",
        "Bahig, Houda",
        "Dolz, Jose",
        "Ayed, Ismail Ben"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17500v1",
        "Other Formats": "https://arxiv.org/format/2506.17500",
        "TeX Source": "https://arxiv.org/src/2506.17500",
        "View PDF": "https://arxiv.org/pdf/2506.17500"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 22:35:00 UTC (162 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or Validation",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2411.06236",
      "abstract": "Architecture performance evaluation is the most time-consuming part of neural architecture search (NAS). Zero-Shot NAS accelerates the evaluation by utilizing zero-cost proxies instead of training. Though effective, existing zero-cost proxies require invoking backpropagations or running networks on input data, making it difficult to further accelerate the computation of proxies. To alleviate this issue, architecture topologies are used to evaluate the performance of networks in this study. We prove that particular architectural topologies decrease the local entropy of feature maps, which degrades specific features to a bias, thereby reducing network performance. Based on this proof, architectural topologies are utilized to quantify the suppression of local entropy decrease (SED) as a data-free and running-free proxy. Experimental results show that SED outperforms most state-of-the-art proxies in terms of architecture selection on five benchmarks, with computation time reduced by three orders of magnitude. We further compare the SED-based NAS with state-of-the-art proxies. SED-based NAS selects the architecture with higher accuracy and fewer parameters in only one second. The theoretical analyses of local entropy and experimental results demonstrate that the suppression of local entropy decrease facilitates selecting optimal architectures in Zero-Shot NAS.",
      "authors": [
        "Wu, Ning",
        "Huang, Han",
        "Xu, Yueting",
        "Hao, Zhifeng"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2411.06236v3",
        "Other Formats": "https://arxiv.org/format/2411.06236",
        "TeX Source": "https://arxiv.org/src/2411.06236",
        "View PDF": "https://arxiv.org/pdf/2411.06236"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Neural and Evolutionary Computing (cs.NE)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 9 Nov 2024 17:36:53 UTC (16,939 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Tue, 12 Nov 2024 08:51:40 UTC (16,939 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Fri, 20 Jun 2025 18:01:50 UTC (312 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2024/11/09",
      "title": "Zero-Shot NAS via the Suppression of Local Entropy Decrease",
      "tasks": [
        "Neural Architecture Search"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17453",
      "abstract": "The UT GraphCast Hindcast Dataset from 1979 to 2024 is a comprehensive global weather forecast archive generated using the Google DeepMind GraphCast Operational model. Developed by researchers at The University of Texas at Austin under the WCRP umbrella, this dataset provides daily 15 day deterministic forecasts at 00UTC on an approximately 25 km global grid for a 45 year period. GraphCast is a physics informed graph neural network that was trained on ECMWF ERA5 reanalysis. It predicts more than a dozen key atmospheric and surface variables on 37 vertical levels, delivering a full medium range forecast in under one minute on modern hardware.",
      "authors": [
        "Sudharsan, Naveen",
        "Singh, Manmeet",
        "Kamath, Harsh",
        "Dashtian, Hassan",
        "Dawson, Clint",
        "Yang, Zong-Liang",
        "Niyogi, Dev"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17453",
        "View PDF": "https://arxiv.org/pdf/2506.17453"
      },
      "subjects": [
        "Geophysics (physics.geo-ph)",
        "Machine Learning (cs.LG)",
        "Atmospheric and Oceanic Physics (physics.ao-ph)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 20 Jun 2025 19:42:36 UTC (2,837 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/20",
      "title": "UT-GraphCast Hindcast Dataset: A Global AI Forecast Archive from UT Austin for Weather and Climate Applications",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2302.13231",
      "abstract": "We developed a synthetic Texas 123-bus backbone transmission system (TX-123BT) with spatio-temporally correlated grid profiles of solar power, wind power, dynamic line ratings and loads at one-hour resolution for five continuous years, which demonstrates unique advantages compared to conventional test cases that offer single static system profile snapshots. Three weather-dependent models are used to create the hourly wind power productions, solar power productions, and dynamic line ratings respectively. The actual historical weather information is also provided along with this dataset, which is suitable for machine learning models. Security-constrained unit commitment is conducted on TX-123BT daily grid profiles and numerical results are compared with the actual Texas system for validation. The created hourly DLR profiles can cut operating cost from USD 8.09 M to USD 7.95 M (-1.7 %), raises renewable dispatch by 1.3 %, and lowers average LMPs from USD 18.66 to USD 17.98 /MWh (-3.6 %). Two hydrogen options -- a 200 MW dual hub and a 500 MW hydrogen-energy transmission and conversion system -- reduce high-load Q3 daily costs by 13.9 % and 14.1 %, respectively. Sensitivity tests show that suppressing the high-resolution weather-driven profiles can push system cost up by as much as 15 %, demonstrating the economic weight of temporal detail.",
      "authors": [
        "Lu, Jin",
        "Li, Xingpeng",
        "Li, Hongyi",
        "Chegini, Taher",
        "Gamarra, Carlos",
        "Yang, Y. C. Ethan",
        "Cook, Margaret",
        "Dillingham, Gavin"
      ],
      "last_revised_date": "2025/06/20",
      "links": {
        "Other Formats": "https://arxiv.org/format/2302.13231",
        "View PDF": "https://arxiv.org/pdf/2302.13231"
      },
      "subjects": [
        "Systems and Control (eess.SY)",
        "Applications (stat.AP)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 26 Feb 2023 04:09:47 UTC (707 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Fri, 16 Aug 2024 14:45:24 UTC (673 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Fri, 20 Jun 2025 22:57:12 UTC (842 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2023/02/26",
      "title": "A Synthetic Texas Power System with Time-Series Weather-Dependent Spatiotemporal Profiles",
      "repo_urls": [
        "https://github.com/xuwkk/task_aware_machine_unlearning",
        "https://github.com/xuwkk/lapso_exp",
        "https://github.com/xuwkk/e2e-at"
      ],
      "tasks": [
        "Time Series",
        "Time Series Analysis"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17337",
      "abstract": "Medical vision-language models (VLMs) leverage large-scale pretraining for diverse imaging tasks but require substantial computational and data resources. Meanwhile, common or general-purpose VLMs (e.g., CLIP, LLaVA), though not trained for medical use, show promise with fine-tuning. This raises a key question: Can efficient fine-tuned common VLMs rival generalist medical VLMs for solving specific medical imaging tasks? This study systematically evaluates common and medical VLMs across disease diagnosis and visual question answering (VQA). Using CLIP-based and LLaVA-based models, we examine (1) off-the-shelf performance gaps in in-domain (ID) settings, (2) whether fine-tuning bridges these gaps, and (3) generalization to out-of-domain (OOD) tasks on unseen medical modalities. While medical-specific pretraining provides advantages in ID settings, common VLMs match or surpass medical-specific models after lightweight fine-tuning, with LoRA-based adaptation proving highly effective among different tasks. In OOD tasks, common VLMs demonstrate strong adaptability in some tasks, challenging the assumption that medical-specific pre-training is essential. These findings suggest that leveraging common VLMs with fine-tuning offers a scalable and cost-effective alternative to developing large-scale medical VLMs, providing crucial insights for future research in the medical imaging field.",
      "authors": [
        "Zhong, Yuan",
        "Jin, Ruinan",
        "Li, Xiaoxiao",
        "Dou, Qi"
      ],
      "last_revised_date": "2025/06/19",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17337v1",
        "Other Formats": "https://arxiv.org/format/2506.17337",
        "TeX Source": "https://arxiv.org/src/2506.17337",
        "View PDF": "https://arxiv.org/pdf/2506.17337"
      },
      "subjects": [
        "Image and Video Processing (eess.IV)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 19 Jun 2025 07:59:00 UTC (1,075 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/19",
      "title": "Can Common VLMs Rival Medical VLMs? Evaluation and Strategic Insights",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17342",
      "abstract": "The social metaverse is a growing digital ecosystem that blends virtual and physical worlds. It allows users to interact socially, work, shop, and enjoy entertainment. However, privacy remains a major challenge, as immersive interactions require continuous collection of biometric and behavioral data. At the same time, ensuring high-quality, low-latency streaming is difficult due to the demands of real-time interaction, immersive rendering, and bandwidth optimization. To address these issues, we propose ASMS (Adaptive Social Metaverse Streaming), a novel streaming system based on Federated Multi-Agent Proximal Policy Optimization (F-MAPPO). ASMS leverages F-MAPPO, which integrates federated learning (FL) and deep reinforcement learning (DRL) to dynamically adjust streaming bit rates while preserving user privacy. Experimental results show that ASMS improves user experience by at least 14% compared to existing streaming methods across various network conditions. Therefore, ASMS enhances the social metaverse experience by providing seamless and immersive streaming, even in dynamic and resource-constrained networks, while ensuring that sensitive user data remains on local devices.",
      "authors": [
        "Long, Zijian",
        "Wang, Haopeng",
        "Dong, Haiwei",
        "Saddik, Abdulmotaleb El"
      ],
      "last_revised_date": "2025/06/19",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17342v1",
        "Other Formats": "https://arxiv.org/format/2506.17342",
        "TeX Source": "https://arxiv.org/src/2506.17342",
        "View PDF": "https://arxiv.org/pdf/2506.17342"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Multimedia (cs.MM)",
        "Networking and Internet Architecture (cs.NI)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 19 Jun 2025 13:33:43 UTC (4,897 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/19",
      "title": "Adaptive Social Metaverse Streaming based on Federated Multi-Agent Deep Reinforcement Learning",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.16388",
      "abstract": "This paper presents our approach to multi-label emotion detection in Hausa, a low-resource African language, as part of SemEval Track A. We fine-tuned AfriBERTa, a transformer-based model pre-trained on African languages, to classify Hausa text into six emotions: anger, disgust, fear, joy, sadness, and surprise. Our methodology involved data preprocessing, tokenization, and model fine-tuning using the Hugging Face Trainer API. The system achieved a validation accuracy of 74.00%, with an F1-score of 73.50%, demonstrating the effectiveness of transformer-based models for emotion detection in low-resource languages.",
      "authors": [
        "Sani, Sani Abdullahi",
        "Abubakar, Salim",
        "Lawan, Falalu Ibrahim",
        "Abubakar, Abdulhamid",
        "Bala, Maryam"
      ],
      "last_revised_date": "2025/06/19",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.16388v1",
        "Other Formats": "https://arxiv.org/format/2506.16388",
        "TeX Source": "https://arxiv.org/src/2506.16388",
        "View PDF": "https://arxiv.org/pdf/2506.16388"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 19 Jun 2025 15:19:35 UTC (388 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/19",
      "title": "HausaNLP at SemEval-2025 Task 11: Advancing Hausa Text-based Emotion Detection",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17345",
      "abstract": "The prediction of crystal properties is essential for understanding structure-property relationships and accelerating the discovery of functional materials. However, conventional approaches relying on experimental measurements or density functional theory (DFT) calculations are often resource-intensive, limiting their scalability. Machine learning (ML) models offer a promising alternative by learning complex structure-property relationships from data, enabling faster predictions. Yet, existing ML models often rely on labeled data, adopt representations that poorly capture essential structural characteristics, and lack integration with physical principles--factors that limit their generalizability and interpretability. Here, we introduce CLOUD (Crystal Language mOdel for Unified and Differentiable materials modeling), a transformer-based framework trained on a novel Symmetry-Consistent Ordered Parameter Encoding (SCOPE) that encodes crystal symmetry, Wyckoff positions, and composition in a compact, coordinate-free string representation. Pre-trained on over six million crystal structures, CLOUD is fine-tuned on multiple downstream tasks and achieves competitive performance in predicting a wide range of material properties, demonstrating strong scaling performance. Furthermore, as proof of concept of differentiable materials modeling, CLOUD is applied to predict the phonon internal energy and heat capacity, which integrates the Debye model to preserve thermodynamic consistency. The CLOUD-DEBYE framework enforces thermodynamic consistency and enables temperature-dependent property prediction without requiring additional data. These results demonstrate the potential of CLOUD as a scalable and physics-informed foundation model for crystalline materials, unifying symmetry-consistent representations with physically grounded learning for property prediction and materials discovery.",
      "authors": [
        "Xu, Changwen",
        "Zhu, Shang",
        "Viswanathan, Venkatasubramanian"
      ],
      "last_revised_date": "2025/06/19",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17345v1",
        "Other Formats": "https://arxiv.org/format/2506.17345",
        "TeX Source": "https://arxiv.org/src/2506.17345",
        "View PDF": "https://arxiv.org/pdf/2506.17345"
      },
      "subjects": [
        "Materials Science (cond-mat.mtrl-sci)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 19 Jun 2025 15:45:24 UTC (1,891 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/19",
      "title": "CLOUD: A Scalable and Physics-Informed Foundation Model for Crystal Representation Learning",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.16254",
      "abstract": "Enhancing the sustainability and efficiency of wireless sensor networks (WSN) in dynamic and unpredictable environments requires adaptive communication and energy harvesting strategies. We propose a novel adaptive control strategy for WSNs that optimizes data transmission and EH to minimize overall energy consumption while ensuring queue stability and energy storing constraints under dynamic environmental conditions. The notion of adaptability therein is achieved by transferring the known environment-specific knowledge to new conditions resorting to the lifelong reinforcement learning concepts. We evaluate our proposed method against two baseline frameworks: Lyapunov-based optimization, and policy-gradient reinforcement learning (RL). Simulation results demonstrate that our approach rapidly adapts to changing environmental conditions by leveraging transferable knowledge, achieving near-optimal performance approximately $30\\%$ faster than the RL method and $60\\%$ faster than the Lyapunov-based approach.",
      "authors": [
        "Firouzjaei, Hossein Mohammadi",
        "Scaciota, Rafaela",
        "Samarakoon, Sumudu"
      ],
      "last_revised_date": "2025/06/19",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.16254v1",
        "Other Formats": "https://arxiv.org/format/2506.16254",
        "TeX Source": "https://arxiv.org/src/2506.16254",
        "View PDF": "https://arxiv.org/pdf/2506.16254"
      },
      "subjects": [
        "Systems and Control (eess.SY)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 19 Jun 2025 12:13:30 UTC (293 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/19",
      "title": "Multi-Task Lifelong Reinforcement Learning for Wireless Sensor Networks",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17333",
      "abstract": "Cellular automata (CA) provide a minimal formalism for investigating how simple local interactions generate rich spatiotemporal behavior in domains as diverse as traffic flow, ecology, tissue morphogenesis and crystal growth. However, automatically discovering the local update rules for a given phenomenon and using them for quantitative prediction remains challenging. Here we present AutomataGPT, a decoder-only transformer pretrained on around 1 million simulated trajectories that span 100 distinct two-dimensional binary deterministic CA rules on toroidal grids. When evaluated on previously unseen rules drawn from the same CA family, AutomataGPT attains 98.5% perfect one-step forecasts and reconstructs the governing update rule with up to 96% functional (application) accuracy and 82% exact rule-matrix match. These results demonstrate that large-scale pretraining over wider regions of rule space yields substantial generalization in both the forward (state forecasting) and inverse (rule inference) problems, without hand-crafted priors. By showing that transformer models can faithfully infer and execute CA dynamics from data alone, our work lays the groundwork for abstracting real-world dynamical phenomena into data-efficient CA surrogates, opening avenues in biology, tissue engineering, physics and AI-driven scientific discovery.",
      "authors": [
        "Berkovich, Jaime A.",
        "David, Noah S.",
        "Buehler, Markus J."
      ],
      "last_revised_date": "2025/06/19",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17333v1",
        "Other Formats": "https://arxiv.org/format/2506.17333",
        "TeX Source": "https://arxiv.org/src/2506.17333",
        "View PDF": "https://arxiv.org/pdf/2506.17333"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
        "Materials Science (cond-mat.mtrl-sci)",
        "Quantitative Methods (q-bio.QM)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 19 Jun 2025 05:54:08 UTC (3,107 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/19",
      "title": "AutomataGPT: Forecasting and Ruleset Inference for Two-Dimensional Cellular Automata",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17332",
      "abstract": "By 2050, people aged 65 and over are projected to make up 16 percent of the global population. As aging is closely associated with increased fall risk, particularly in wet and confined environments such as bathrooms where over 80 percent of falls occur. Although recent research has increasingly focused on non-intrusive, privacy-preserving approaches that do not rely on wearable devices or video-based monitoring, these efforts have not fully overcome the limitations of existing unimodal systems (e.g., WiFi-, infrared-, or mmWave-based), which are prone to reduced accuracy in complex environments. These limitations stem from fundamental constraints in unimodal sensing, including system bias and environmental interference, such as multipath fading in WiFi-based systems and drastic temperature changes in infrared-based methods. To address these challenges, we propose a Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments. First, we develop a sensor evaluation framework to select and fuse millimeter-wave radar with 3D vibration sensing, and use it to construct and preprocess a large-scale, privacy-preserving multimodal dataset in real bathroom settings, which will be released upon publication. Second, we introduce P2MFDS, a dual-stream network combining a CNN-BiLSTM-Attention branch for radar motion dynamics with a multi-scale CNN-SEBlock-Self-Attention branch for vibration impact detection. By uniting macro- and micro-scale features, P2MFDS delivers significant gains in accuracy and recall over state-of-the-art approaches. Code and pretrained models will be made available at: https://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom.",
      "authors": [
        "Wang, Haitian",
        "Wang, Yiren",
        "Wang, Xinyu",
        "Miao, Yumeng",
        "Zhang, Yuliang",
        "Zhang, Yu",
        "Mansoor, Atif"
      ],
      "last_revised_date": "2025/06/19",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17332v1",
        "Other Formats": "https://arxiv.org/format/2506.17332",
        "TeX Source": "https://arxiv.org/src/2506.17332",
        "View PDF": "https://arxiv.org/pdf/2506.17332"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 19 Jun 2025 05:22:14 UTC (5,518 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/19",
      "title": "P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.16039",
      "abstract": "The brain is a highly complex organ that manages many important tasks, including movement, memory and thinking. Brain-related conditions, like tumors and degenerative disorders, can be hard to diagnose and treat. Magnetic Resonance Imaging (MRI) serves as a key tool for identifying these conditions, offering high-resolution images of brain structures. Despite this, interpreting MRI scans can be complicated. This study tackles this challenge by conducting a comparative analysis of Vision Transformer (ViT) and Transfer Learning (TL) models such as VGG16, VGG19, Resnet50V2, MobilenetV2 for classifying brain diseases using MRI data from Bangladesh based dataset. ViT, known for their ability to capture global relationships in images, are particularly effective for medical imaging tasks. Transfer learning helps to mitigate data constraints by fine-tuning pre-trained models. Furthermore, Explainable AI (XAI) methods such as GradCAM, GradCAM++, LayerCAM, ScoreCAM, and Faster-ScoreCAM are employed to interpret model predictions. The results demonstrate that ViT surpasses transfer learning models, achieving a classification accuracy of 94.39%. The integration of XAI methods enhances model transparency, offering crucial insights to aid medical professionals in diagnosing brain diseases with greater precision.",
      "authors": [
        "Sarker, Shuvashis",
        "Refat, Shamim Rahim",
        "Preotee, Faika Fairuj",
        "Islam, Shifat",
        "Muhammad, Tashreef",
        "Hoque, Mohammad Ashraful"
      ],
      "last_revised_date": "2025/06/19",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2505.16039v2",
        "Other Formats": "https://arxiv.org/format/2505.16039",
        "TeX Source": "https://arxiv.org/src/2505.16039",
        "View PDF": "https://arxiv.org/pdf/2505.16039"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 21 May 2025 21:38:22 UTC (1,865 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Thu, 19 Jun 2025 19:22:10 UTC (1,755 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/05/21",
      "title": "An Exploratory Approach Towards Investigating and Explaining Vision Transformer and Transfer Learning for Brain Disease Detection",
      "tasks": [
        "Transfer Learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.15787",
      "abstract": "We introduce SLR, an end-to-end framework for systematic evaluation and training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given a user's task specification, SLR enables scalable, automated synthesis of inductive reasoning tasks with precisely controlled difficulty. For each task, SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation program used by a symbolic judge to deterministically verify model outputs, and (iii) an instruction prompt for the reasoning task. Using SLR, we create SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum levels that progressively increase in relational, arithmetic, and recursive complexity. Large-scale evaluation reveals that contemporary LLMs readily produce syntactically valid rules, yet often fail at correct logical inference. Recent reasoning LLMs do somewhat better, but incur substantial increases in test-time compute, sometimes exceeding 15k completion tokens. Finally, logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully automated, requires no human annotation, ensures dataset novelty, and offers a scalable environment for probing and advancing LLMs' reasoning capabilities.",
      "authors": [
        "Helff, Lukas",
        "Omar, Ahmad",
        "Friedrich, Felix",
        "Stammer, Wolfgang",
        "W\u00fcst, Antonia",
        "Woydt, Tim",
        "Mitchell, Rupert",
        "Schramowski, Patrick",
        "Kersting, Kristian"
      ],
      "last_revised_date": "2025/06/18",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.15787v1",
        "Other Formats": "https://arxiv.org/format/2506.15787",
        "TeX Source": "https://arxiv.org/src/2506.15787",
        "View PDF": "https://arxiv.org/pdf/2506.15787"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 18 Jun 2025 18:10:30 UTC (1,012 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/18",
      "title": "SLR: An Automated Synthesis Framework for Scalable Logical Reasoning",
      "datasets": [
        {
          "dataset_name": "AIML-TUDA/SLR-Bench",
          "downloads": "95",
          "likes": "2",
          "link": "https://huggingface.co/datasets/AIML-TUDA/SLR-Bench"
        }
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17311",
      "abstract": "Academic paper review typically requires substantial time, expertise, and human resources. Large Language Models (LLMs) present a promising method for automating the review process due to their extensive training data, broad knowledge base, and relatively low usage cost. This work explores the feasibility of using LLMs for academic paper review by proposing an automated review system. The system integrates Retrieval Augmented Generation (RAG), the AutoGen multi-agent system, and Chain-of-Thought prompting to support tasks such as format checking, standardized evaluation, comment generation, and scoring. Experiments conducted on 290 submissions from the WASA 2024 conference using GPT-4o show that LLM-based review significantly reduces review time (average 2.48 hours) and cost (average \\$104.28 USD). However, the similarity between LLM-selected papers and actual accepted papers remains low (average 38.6\\%), indicating issues such as hallucination, lack of independent judgment, and retrieval preferences. Therefore, it is recommended to use LLMs as assistive tools to support human reviewers, rather than to replace them.",
      "authors": [
        "Li, Chuanlei",
        "Hu, Xu",
        "Xu, Minghui",
        "Li, Kun",
        "Zhang, Yue",
        "Cheng, Xiuzhen"
      ],
      "last_revised_date": "2025/06/18",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17311",
        "TeX Source": "https://arxiv.org/src/2506.17311",
        "View PDF": "https://arxiv.org/pdf/2506.17311"
      },
      "subjects": [
        "Computers and Society (cs.CY)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 18 Jun 2025 10:19:18 UTC (183 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/18",
      "title": "Can Large Language Models Be Trusted Paper Reviewers? A Feasibility Study",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.15872",
      "abstract": "Loss curves are smooth during most of model training, so visible discontinuities stand out as possible conceptual breakthroughs. Studying these breakthroughs enables a deeper understanding of learning dynamics, but only when they are properly identified. This paper argues that similar breakthroughs occur frequently throughout training but they are obscured by a loss metric that collapses all variation into a single scalar. To find these hidden transitions, we introduce POLCA, a method for decomposing changes in loss along arbitrary bases of the low-rank training subspace. We use our method to identify clusters of samples that share similar changes in loss during training, disaggregating the overall loss into that of smaller groups of conceptually similar data. We validate our method on synthetic arithmetic and natural language tasks, showing that POLCA recovers clusters that represent interpretable breakthroughs in the model's capabilities. We demonstrate the promise of these hidden phase transitions as a tool for unsupervised interpretability.",
      "authors": [
        "Kangaslahti, Sara",
        "Rosenfeld, Elan",
        "Saphra, Naomi"
      ],
      "last_revised_date": "2025/06/18",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.15872v1",
        "Other Formats": "https://arxiv.org/format/2506.15872",
        "TeX Source": "https://arxiv.org/src/2506.15872",
        "View PDF": "https://arxiv.org/pdf/2506.15872"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 18 Jun 2025 20:40:16 UTC (3,106 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/18",
      "title": "Hidden Breakthroughs in Language Model Training",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2411.19479",
      "abstract": "Deep neural networks (DNNs) are susceptible to backdoor attacks, where adversaries poison datasets with adversary-specified triggers to implant hidden backdoors, enabling malicious manipulation of model predictions. Dataset purification serves as a proactive defense by removing malicious training samples to prevent backdoor injection at its source. We first reveal that the current advanced purification methods rely on a latent assumption that the backdoor connections between triggers and target labels in backdoor attacks are simpler to learn than the benign features. We demonstrate that this assumption, however, does not always hold, especially in all-to-all (A2A) and untargeted (UT) attacks. As a result, purification methods that analyze the separation between the poisoned and benign samples in the input-output space or the final hidden layer space are less effective. We observe that this separability is not confined to a single layer but varies across different hidden layers. Motivated by this understanding, we propose FLARE, a universal purification method to counter various backdoor attacks. FLARE aggregates abnormal activations from all hidden layers to construct representations for clustering. To enhance separation, FLARE develops an adaptive subspace selection algorithm to isolate the optimal space for dividing an entire dataset into two clusters. FLARE assesses the stability of each cluster and identifies the cluster with higher stability as poisoned. Extensive evaluations on benchmark datasets demonstrate the effectiveness of FLARE against 22 representative backdoor attacks, including all-to-one (A2O), all-to-all (A2A), and untargeted (UT) attacks, and its robustness to adaptive attacks. Codes are available at \\href{https://github.com/THUYimingLi/BackdoorBox}{BackdoorBox} and \\href{https://github.com/vtu81/backdoor-toolbox}{backdoor-toolbox}.",
      "authors": [
        "Hou, Linshan",
        "Luo, Wei",
        "Hua, Zhongyun",
        "Chen, Songhua",
        "Zhang, Leo Yu",
        "Li, Yiming"
      ],
      "last_revised_date": "2025/06/18",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2411.19479v2",
        "Other Formats": "https://arxiv.org/format/2411.19479",
        "TeX Source": "https://arxiv.org/src/2411.19479",
        "View PDF": "https://arxiv.org/pdf/2411.19479"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 29 Nov 2024 05:34:21 UTC (2,783 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Wed, 18 Jun 2025 08:32:27 UTC (3,172 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/11/29",
      "title": "FLARE: Towards Universal Dataset Purification against Backdoor Attacks",
      "repo_urls": [
        "https://github.com/vtu81/backdoor-toolbox",
        "https://github.com/thuyimingli/backdoorbox"
      ],
      "tasks": [
        "All"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17325",
      "abstract": "Predicting user churn in non-subscription gig platforms, where disengagement is implicit, poses unique challenges due to the absence of explicit labels and the dynamic nature of user behavior. Existing methods often rely on aggregated snapshots or static visual representations, which obscure temporal cues critical for early detection. In this work, we propose a temporally-aware computer vision framework that models user behavioral patterns as a sequence of radar chart images, each encoding day-level behavioral features. By integrating a pretrained CNN encoder with a bidirectional LSTM, our architecture captures both spatial and temporal patterns underlying churn behavior. Extensive experiments on a large real-world dataset demonstrate that our method outperforms classical models and ViT-based radar chart baselines, yielding gains of 17.7 in F1 score, 29.4 in precision, and 16.1 in AUC, along with improved interpretability. The framework's modular design, explainability tools, and efficient deployment characteristics make it suitable for large-scale churn modeling in dynamic gig-economy platforms.",
      "authors": [
        "Najafi, Sina",
        "Sepanj, M. Hadi",
        "Jafari, Fahimeh"
      ],
      "last_revised_date": "2025/06/18",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17325v1",
        "Other Formats": "https://arxiv.org/format/2506.17325",
        "TeX Source": "https://arxiv.org/src/2506.17325",
        "View PDF": "https://arxiv.org/pdf/2506.17325"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 18 Jun 2025 22:20:49 UTC (5,984 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/18",
      "title": "RadarSeq: A Temporal Vision Framework for User Churn Prediction via Radar Chart Sequences",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17307",
      "abstract": "Few-shot Test-Time Domain Adaptation focuses on adapting a model at test time to a specific domain using only a few unlabeled examples, addressing domain shift. Prior methods leverage CLIP's strong out-of-distribution (OOD) abilities by generating domain-specific prompts to guide its generalized, frozen features. However, since downstream datasets are not explicitly seen by CLIP, solely depending on the feature space knowledge is constrained by CLIP's prior knowledge. Notably, when using a less robust backbone like ViT-B/16, performance significantly drops on challenging real-world benchmarks. Departing from the state-of-the-art of inheriting the intrinsic OOD capability of CLIP, this work introduces learning directly on the input space to complement the dataset-specific knowledge for frozen CLIP. Specifically, an independent side branch is attached in parallel with CLIP and enforced to learn exclusive knowledge via revert attention. To better capture the dataset-specific label semantics for downstream adaptation, we propose to enhance the inter-dispersion among text features via greedy text ensemble and refinement. The text and visual features are then progressively fused in a domain-aware manner by a generated domain prompt to adapt toward a specific domain. Extensive experiments show our method's superiority on 5 large-scale benchmarks (WILDS and DomainNet), notably improving over smaller networks like ViT-B/16 with gains of \\textbf{+5.1} in F1 for iWildCam and \\textbf{+3.1\\%} in WC Acc for FMoW.",
      "authors": [
        "Chi, Zhixiang",
        "Gu, Li",
        "Liu, Huan",
        "Wang, Ziqiang",
        "Wu, Yanan",
        "Wang, Yang",
        "Plataniotis, Konstantinos N"
      ],
      "last_revised_date": "2025/06/18",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17307v1",
        "Other Formats": "https://arxiv.org/format/2506.17307",
        "TeX Source": "https://arxiv.org/src/2506.17307",
        "View PDF": "https://arxiv.org/pdf/2506.17307"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 18 Jun 2025 03:49:22 UTC (932 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/18",
      "title": "Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.15121",
      "abstract": "We introduce a generative modeling framework for thermodynamic computing, in which structured data is synthesized from noise by the natural time evolution of a physical system governed by Langevin dynamics. While conventional diffusion models use neural networks to perform denoising, here the information needed to generate structure from noise is encoded by the dynamics of a thermodynamic system. Training proceeds by maximizing the probability with which the computer generates the reverse of a noising trajectory, which ensures that the computer generates data with minimal heat emission. We demonstrate this framework within a digital simulation of a thermodynamic computer. If realized in analog hardware, such a system would function as a generative model that produces structured samples without the need for artificially-injected noise or active control of denoising.",
      "authors": [
        "Whitelam, Stephen"
      ],
      "last_revised_date": "2025/06/18",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.15121v1",
        "Other Formats": "https://arxiv.org/format/2506.15121",
        "TeX Source": "https://arxiv.org/src/2506.15121",
        "View PDF": "https://arxiv.org/pdf/2506.15121"
      },
      "subjects": [
        "Statistical Mechanics (cond-mat.stat-mech)",
        "Neural and Evolutionary Computing (cs.NE)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 18 Jun 2025 03:41:00 UTC (4,405 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/18",
      "title": "Generative thermodynamic computing",
      "tasks": [
        "Denoising"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.14665",
      "abstract": "Density Functional Theory (DFT) is the most widely used electronic structure method for predicting the properties of molecules and materials. Although DFT is, in principle, an exact reformulation of the Schr\\\"odinger equation, practical applications rely on approximations to the unknown exchange-correlation (XC) functional. Most existing XC functionals are constructed using a limited set of increasingly complex, hand-crafted features that improve accuracy at the expense of computational efficiency. Yet, no current approximation achieves the accuracy and generality for predictive modeling of laboratory experiments at chemical accuracy -- typically defined as errors below 1 kcal/mol. In this work, we present Skala, a modern deep learning-based XC functional that bypasses expensive hand-designed features by learning representations directly from data. Skala achieves chemical accuracy for atomization energies of small molecules while retaining the computational efficiency typical of semi-local DFT. This performance is enabled by training on an unprecedented volume of high-accuracy reference data generated using computationally intensive wavefunction-based methods. Notably, Skala systematically improves with additional training data covering diverse chemistry. By incorporating a modest amount of additional high-accuracy data tailored to chemistry beyond atomization energies, Skala achieves accuracy competitive with the best-performing hybrid functionals across general main group chemistry, at the cost of semi-local DFT. As the training dataset continues to expand, Skala is poised to further enhance the predictive power of first-principles simulations.",
      "authors": [
        "Luise, Giulia",
        "Huang, Chin-Wei",
        "Vogels, Thijs",
        "Kooi, Derk P.",
        "Ehlert, Sebastian",
        "Lanius, Stephanie",
        "Giesbertz, Klaas J. H.",
        "Karton, Amir",
        "Gunceler, Deniz",
        "Stanley, Megan",
        "Bruinsma, Wessel P.",
        "Huang, Lin",
        "Wei, Xinran",
        "Torres, Jos\u00e9 Garrido",
        "Katbashev, Abylay",
        "M\u00e1t\u00e9, B\u00e1lint",
        "Kaba, S\u00e9kou-Oumar",
        "Sordillo, Roberto",
        "Chen, Yingrong",
        "Williams-Young, David B.",
        "Bishop, Christopher M.",
        "Hermann, Jan",
        "Berg, Rianne van den",
        "Gori-Giorgi, Paola"
      ],
      "last_revised_date": "2025/06/18",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.14665",
        "TeX Source": "https://arxiv.org/src/2506.14665",
        "View PDF": "https://arxiv.org/pdf/2506.14665"
      },
      "subjects": [
        "Chemical Physics (physics.chem-ph)",
        "Artificial Intelligence (cs.AI)",
        "Computational Engineering, Finance, and Science (cs.CE)",
        "Machine Learning (cs.LG)",
        "Computational Physics (physics.comp-ph)"
      ],
      "submission_historys": [
        {
          "details": "Tue, 17 Jun 2025 15:56:56 UTC (3,972 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Wed, 18 Jun 2025 08:39:15 UTC (4,700 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/06/17",
      "title": "Accurate and scalable exchange-correlation with deep learning",
      "tasks": [
        "Computational Efficiency",
        "Deep Learning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17309",
      "abstract": "Malware detection using machine learning requires feature extraction from binary files, as models cannot process raw binaries directly. A common approach involves using LIEF for raw feature extraction and the EMBER vectorizer to generate 2381-dimensional feature vectors. However, the high dimensionality of these features introduces significant computational challenges. This study addresses these challenges by applying two dimensionality reduction techniques: XGBoost-based feature selection and Principal Component Analysis (PCA). We evaluate three reduced feature dimensions (128, 256, and 384), which correspond to approximately 5.4%, 10.8%, and 16.1% of the original 2381 features, across four models-XGBoost, LightGBM, Extra Trees, and Random Forest-using a unified training, validation, and testing split formed from the EMBER-2018, ERMDS, and BODMAS datasets. This approach ensures generalization and avoids dataset bias. Experimental results show that LightGBM trained on the 384-dimensional feature set after XGBoost feature selection achieves the highest accuracy of 97.52% on the unified dataset, providing an optimal balance between computational efficiency and detection performance. The best model, trained in 61 minutes using 30 GB of RAM and 19.5 GB of disk space, generalizes effectively to completely unseen datasets, maintaining 95.31% accuracy on TRITIUM and 93.98% accuracy on INFERNO. These findings present a scalable, compute-efficient approach for malware detection without compromising accuracy.",
      "authors": [
        "Choudhary, Aditya",
        "Pawar, Sarthak",
        "Haribhakta, Yashodhara"
      ],
      "last_revised_date": "2025/06/18",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17309v1",
        "Other Formats": "https://arxiv.org/format/2506.17309",
        "TeX Source": "https://arxiv.org/src/2506.17309",
        "View PDF": "https://arxiv.org/pdf/2506.17309"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 18 Jun 2025 06:56:59 UTC (484 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/18",
      "title": "Efficient Malware Detection with Optimized Learning on High-Dimensional Features",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17324",
      "abstract": "As diffusion models have become the tool of choice for image generation and as the quality of the images continues to improve, the question of how `creativity' originates in diffusion has become increasingly important. The score matching perspective on diffusion has proven particularly fruitful for understanding how and why diffusion models generate images that remain plausible while differing significantly from their training images. In particular, as explained in (Kamb \\& Ganguli, 2024) and others, e.g., (Ambrogioni, 2023), theory suggests that if our score matching were optimal, we would only be able to recover training samples through our diffusion process. However, as shown by Kamb \\& Ganguli, (2024), in diffusion models where the score is parametrized by a simple CNN, the inductive biases of the CNN itself (translation equivariance and locality) allow the model to generate samples that globally do not match any training samples, but are rather patch-wise `mosaics'. Notably, however, this theory does not extend to describe the role of self-attention in this process. In this work, we take a preliminary step in this direction to extend this theory to the case of diffusion models whose score is parametrized by a CNN with a final self-attention layer. We show that our theory suggests that self-attention will induce a globally image-consistent arrangement of local features beyond the patch-level in generated samples, and we verify this behavior empirically on a carefully crafted dataset.",
      "authors": [
        "Finn, Emma",
        "Keller, T. Anderson",
        "Theodosis, Manos",
        "Ba, Demba E."
      ],
      "last_revised_date": "2025/06/18",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17324",
        "TeX Source": "https://arxiv.org/src/2506.17324",
        "View PDF": "https://arxiv.org/pdf/2506.17324"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 18 Jun 2025 21:14:56 UTC (237 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/18",
      "title": "Origins of Creativity in Attention-Based Diffusion Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.14003",
      "abstract": "Machine unlearning (MU) for large language models (LLMs), commonly referred to as LLM unlearning, seeks to remove specific undesirable data or knowledge from a trained model, while maintaining its performance on standard tasks. While unlearning plays a vital role in protecting data privacy, enforcing copyright, and mitigating sociotechnical harms in LLMs, we identify a new vulnerability post-unlearning: unlearning trace detection. We discover that unlearning leaves behind persistent ''fingerprints'' in LLMs, detectable traces in both model behavior and internal representations. These traces can be identified from output responses, even when prompted with forget-irrelevant inputs. Specifically, a simple supervised classifier can reliably determine whether a model has undergone unlearning based solely on its textual outputs. Further analysis shows that these traces are embedded in intermediate activations and propagate nonlinearly to the final layer, forming low-dimensional, learnable manifolds in activation space. Through extensive experiments, we show that forget-relevant prompts enable over 90% accuracy in detecting unlearning traces across all model sizes. Even with forget-irrelevant inputs, large LLMs maintain high detectability, demonstrating the broad applicability of unlearning trace detection. These findings reveal that unlearning leaves measurable signatures, introducing a new risk of reverse-engineering forgotten information when a model is identified as unlearned given an input query. Codes are available at [this URL](https://github.com/OPTML-Group/Unlearn-Trace).",
      "authors": [
        "Chen, Yiwei",
        "Pal, Soumyadeep",
        "Zhang, Yimeng",
        "Qu, Qing",
        "Liu, Sijia"
      ],
      "last_revised_date": "2025/06/16",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.14003",
        "TeX Source": "https://arxiv.org/src/2506.14003",
        "View PDF": "https://arxiv.org/pdf/2506.14003"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 16 Jun 2025 21:03:51 UTC (3,683 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/16",
      "title": "Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs",
      "repo_urls": [
        "https://github.com/optml-group/unlearn-trace"
      ],
      "tasks": [
        "Machine Unlearning"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17292",
      "abstract": "Federated Learning enables collaborative learning among clients via a coordinating server while avoiding direct data sharing, offering a perceived solution to preserve privacy. However, recent studies on Membership Inference Attacks (MIAs) have challenged this notion, showing high success rates against unprotected training data. While local differential privacy (LDP) is widely regarded as a gold standard for privacy protection in data analysis, most studies on MIAs either neglect LDP or fail to provide theoretical guarantees for attack success rates against LDP-protected data. To address this gap, we derive theoretical lower bounds for the success rates of low-polynomial time MIAs that exploit vulnerabilities in fully connected or self-attention layers. We establish that even when data are protected by LDP, privacy risks persist, depending on the privacy budget. Practical evaluations on federated vision models confirm considerable privacy risks, revealing that the noise required to mitigate these attacks significantly degrades models' utility.",
      "authors": [
        "Nguyen, Quan",
        "Vu, Minh N.",
        "Nguyen, Truc",
        "Thai, My T."
      ],
      "last_revised_date": "2025/06/16",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17292v1",
        "Other Formats": "https://arxiv.org/format/2506.17292",
        "TeX Source": "https://arxiv.org/src/2506.17292",
        "View PDF": "https://arxiv.org/pdf/2506.17292"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 16 Jun 2025 21:48:11 UTC (12,123 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/16",
      "title": "Theoretically Unmasking Inference Attacks Against LDP-Protected Clients in Federated Vision Models",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17285",
      "abstract": "Modern recommendation systems typically follow two complementary paradigms: collaborative filtering, which models long-term user preferences from historical interactions, and conversational recommendation systems (CRS), which interact with users in natural language to uncover immediate needs. Each captures a different dimension of user intent. While CRS models lack collaborative signals, leading to generic or poorly personalized suggestions, traditional recommenders lack mechanisms to interactively elicit immediate needs. Unifying these paradigms promises richer personalization but remains challenging due to the lack of large-scale conversational datasets grounded in real user behavior. We present ConvRecStudio, a framework that uses large language models (LLMs) to simulate realistic, multi-turn dialogs grounded in timestamped user-item interactions and reviews. ConvRecStudio follows a three-stage pipeline: (1) Temporal Profiling, which constructs user profiles and community-level item sentiment trajectories over fine-grained aspects; (2) Semantic Dialog Planning, which generates a structured plan using a DAG of flexible super-nodes; and (3) Multi-Turn Simulation, which instantiates the plan using paired LLM agents for the user and system, constrained by executional and behavioral fidelity checks. We apply ConvRecStudio to three domains -- MobileRec, Yelp, and Amazon Electronics -- producing over 12K multi-turn dialogs per dataset. Human and automatic evaluations confirm the naturalness, coherence, and behavioral grounding of the generated conversations. To demonstrate utility, we build a cross-attention transformer model that jointly encodes user history and dialog context, achieving gains in Hit@K and NDCG@K over baselines using either signal alone or naive fusion. Notably, our model achieves a 10.9% improvement in Hit@1 on Yelp over the strongest baseline.",
      "authors": [
        "Chhetri, Vinaik",
        "Reza, Yousaf",
        "Fereidouni, Moghis",
        "Maji, Srijata",
        "Farooq, Umar",
        "Siddique, AB"
      ],
      "last_revised_date": "2025/06/14",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17285v1",
        "Other Formats": "https://arxiv.org/format/2506.17285",
        "TeX Source": "https://arxiv.org/src/2506.17285",
        "View PDF": "https://arxiv.org/pdf/2506.17285"
      },
      "subjects": [
        "Information Retrieval (cs.IR)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 14 Jun 2025 22:58:48 UTC (1,428 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/14",
      "title": "A Framework for Generating Conversational Recommendation Datasets from Behavioral Interactions",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.11976",
      "abstract": "Effective multimodal reasoning depends on the alignment of visual and linguistic representations, yet the mechanisms by which vision-language models (VLMs) achieve this alignment remain poorly understood. We introduce a methodological framework that deliberately maintains a frozen large language model (LLM) and a frozen vision transformer (ViT), connected solely by training a linear adapter during visual instruction tuning. This design is fundamental to our approach: by keeping the language model frozen, we ensure it maintains its original language representations without adaptation to visual data. Consequently, the linear adapter must map visual features directly into the LLM's existing representational space rather than allowing the language model to develop specialized visual understanding through fine-tuning. Our experimental design uniquely enables the use of pre-trained sparse autoencoders (SAEs) of the LLM as analytical probes. These SAEs remain perfectly aligned with the unchanged language model and serve as a snapshot of the learned language feature-representations. Through systematic analysis of SAE reconstruction error, sparsity patterns, and feature SAE descriptions, we reveal the layer-wise progression through which visual representations gradually align with language feature representations, converging in middle-to-later layers. This suggests a fundamental misalignment between ViT outputs and early LLM layers, raising important questions about whether current adapter-based architectures optimally facilitate cross-modal representation learning.",
      "authors": [
        "Venhoff, Constantin",
        "Khakzar, Ashkan",
        "Joseph, Sonia",
        "Torr, Philip",
        "Nanda, Neel"
      ],
      "last_revised_date": "2025/06/13",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.11976v1",
        "Other Formats": "https://arxiv.org/format/2506.11976",
        "TeX Source": "https://arxiv.org/src/2506.11976",
        "View PDF": "https://arxiv.org/pdf/2506.11976"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 13 Jun 2025 17:34:05 UTC (64 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/13",
      "title": "How Visual Representations Map to Language Feature Space in Multimodal LLMs",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.11425",
      "abstract": "Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted as the de facto method for enhancing the reasoning capabilities of large language models and has demonstrated notable success in verifiable domains like math and competitive programming tasks. However, the efficacy of RLVR diminishes significantly when applied to agentic environments. These settings, characterized by multi-step, complex problem solving, lead to high failure rates even for frontier LLMs, as the reward landscape is too sparse for effective model training via conventional RLVR. In this work, we introduce Agent-RLVR, a framework that makes RLVR effective in challenging agentic settings, with an initial focus on software engineering tasks. Inspired by human pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively steers the agent towards successful trajectories by leveraging diverse informational cues. These cues, ranging from high-level strategic plans to dynamic feedback on the agent's errors and environmental interactions, emulate a teacher's guidance, enabling the agent to navigate difficult solution spaces and promotes active self-improvement via additional environment exploration. In the Agent-RLVR training loop, agents first attempt to solve tasks to produce initial trajectories, which are then validated by unit tests and supplemented with agent guidance. Agents then reattempt with guidance, and the agent policy is updated with RLVR based on the rewards of these guided trajectories. Agent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4% to 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data is additionally useful for test-time reward model training, shown by further boosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents with RLVR in complex, real-world environments where conventional RL methods struggle.",
      "authors": [
        "Da, Jeff",
        "Wang, Clinton",
        "Deng, Xiang",
        "Ma, Yuntao",
        "Barhate, Nikhil",
        "Hendryx, Sean"
      ],
      "last_revised_date": "2025/06/13",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.11425v1",
        "Other Formats": "https://arxiv.org/format/2506.11425",
        "TeX Source": "https://arxiv.org/src/2506.11425",
        "View PDF": "https://arxiv.org/pdf/2506.11425"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 13 Jun 2025 02:46:53 UTC (781 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/13",
      "title": "Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards",
      "tasks": [
        "Math",
        "Navigate"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.12158",
      "abstract": "Large Language Models (LLMs) are increasingly used to generate synthetic textual data for training smaller specialized models. However, a comparison of various generation strategies for low-resource language settings is lacking. While various prompting strategies have been proposed, such as demonstrations, label-based summaries, and self-revision, their comparative effectiveness remains unclear, especially for low-resource languages. In this paper, we systematically evaluate the performance of these generation strategies and their combinations across 11 typologically diverse languages, including several extremely low-resource ones. Using three NLP tasks and four open-source LLMs, we assess downstream model performance on generated versus gold-standard data. Our results show that strategic combinations of generation methods, particularly target-language demonstrations with LLM-based revisions, yield strong performance, narrowing the gap with real data to as little as 5% in some settings. We also find that smart prompting techniques can reduce the advantage of larger LLMs, highlighting efficient generation strategies for synthetic data generation in low-resource scenarios with smaller models.",
      "authors": [
        "Ankinina, Tatiana",
        "Cegin, Jan",
        "Simko, Jakub",
        "Ostermann, Simon"
      ],
      "last_revised_date": "2025/06/13",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.12158v1",
        "Other Formats": "https://arxiv.org/format/2506.12158",
        "TeX Source": "https://arxiv.org/src/2506.12158",
        "View PDF": "https://arxiv.org/pdf/2506.12158"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 13 Jun 2025 18:24:25 UTC (2,048 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/13",
      "title": "A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.09655",
      "abstract": "Diplomacy is a complex multiplayer game that requires both cooperation and competition, posing significant challenges for AI systems. Traditional methods rely on equilibrium search to generate extensive game data for training, which demands substantial computational resources. Large Language Models (LLMs) offer a promising alternative, leveraging pre-trained knowledge to achieve strong performance with relatively small-scale fine-tuning. However, applying LLMs to Diplomacy remains challenging due to the exponential growth of possible action combinations and the intricate strategic interactions among players. To address this challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns equilibrium policies for Diplomacy. DipLLM employs an autoregressive factorization framework to simplify the complex task of multi-unit action assignment into a sequence of unit-level decisions. By defining an equilibrium policy within this framework as the learning objective, we fine-tune the model using only 1.5% of the data required by the state-of-the-art Cicero model, surpassing its performance. Our results demonstrate the potential of fine-tuned LLMs for tackling complex strategic decision-making in multiplayer games.",
      "authors": [
        "Xu, Kaixuan",
        "Chai, Jiajun",
        "Li, Sicheng",
        "Fu, Yuqian",
        "Zhu, Yuanheng",
        "Zhao, Dongbin"
      ],
      "last_revised_date": "2025/06/11",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.09655v1",
        "Other Formats": "https://arxiv.org/format/2506.09655",
        "TeX Source": "https://arxiv.org/src/2506.09655",
        "View PDF": "https://arxiv.org/pdf/2506.09655"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 11 Jun 2025 12:25:32 UTC (3,815 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/11",
      "title": "DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17264",
      "abstract": "Fine-tuning large language models (LLMs) using zeroth-order optimization (ZO) offers a memory-efficient alternative to gradient-based methods but suffers from slower convergence and unstable optimization due to noisy gradient estimates. This paper introduces OAT-Rephrase, an Optimization-Aware Training data rephrasing strategy that leverages an LLM to rephrase training instances based on its understanding of the ZO dynamics, specifically MeZO, derived directly from its paper. The approach incorporates a dual-stage pipeline featuring a rewriter LLM and a semantic judge, ensuring all rephrasings retain task relevance and logical consistency. Evaluations across five classification tasks and three LLM architectures demonstrate that OAT-Rephrase consistently improves MeZO fine-tuning performance, often narrowing or eliminating the gap with first-order methods. Our findings suggest that optimization-aware rephrasing serves as a reusable and low-overhead enhancement for zeroth-order tuning regimes.",
      "authors": [
        "Long, Jikai",
        "Hu, Zijian",
        "Yu, Xiaodong",
        "Xie, Jianwen",
        "Xu, Zhaozhuo"
      ],
      "last_revised_date": "2025/06/10",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17264v1",
        "Other Formats": "https://arxiv.org/format/2506.17264",
        "TeX Source": "https://arxiv.org/src/2506.17264",
        "View PDF": "https://arxiv.org/pdf/2506.17264"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Tue, 10 Jun 2025 02:53:04 UTC (5,646 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/10",
      "title": "OAT-Rephrase: Optimization-Aware Training Data Rephrasing for Zeroth-Order LLM Fine-Tuning",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17265",
      "abstract": "Multimodal Large Language Models (MLLMs) trained on massive data may memorize sensitive personal information and photos, posing serious privacy risks. To mitigate this, MLLM unlearning methods are proposed, which fine-tune MLLMs to reduce the ``forget'' sensitive information. However, it remains unclear whether the knowledge has been truly forgotten or just hidden in the model. Therefore, we propose to study a novel problem of LLM unlearning attack, which aims to recover the unlearned knowledge of an unlearned LLM. To achieve the goal, we propose a novel framework Stealthy Unlearning Attack (SUA) framework that learns a universal noise pattern. When applied to input images, this noise can trigger the model to reveal unlearned content. While pixel-level perturbations may be visually subtle, they can be detected in the semantic embedding space, making such attacks vulnerable to potential defenses. To improve stealthiness, we introduce an embedding alignment loss that minimizes the difference between the perturbed and denoised image embeddings, ensuring the attack is semantically unnoticeable. Experimental results show that SUA can effectively recover unlearned information from MLLMs. Furthermore, the learned noise generalizes well: a single perturbation trained on a subset of samples can reveal forgotten content in unseen images. This indicates that knowledge reappearance is not an occasional failure, but a consistent behavior.",
      "authors": [
        "Zhang, Xianren",
        "Liu, Hui",
        "Zhang, Delvin Ce",
        "Tang, Xianfeng",
        "He, Qi",
        "Lee, Dongwon",
        "Wang, Suhang"
      ],
      "last_revised_date": "2025/06/10",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17265",
        "TeX Source": "https://arxiv.org/src/2506.17265",
        "View PDF": "https://arxiv.org/pdf/2506.17265"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Tue, 10 Jun 2025 04:52:03 UTC (1,760 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/10",
      "title": "Does Multimodal Large Language Model Truly Unlearn? Stealthy MLLM Unlearning Attack",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.07636",
      "abstract": "Large language models (LLMs) have advanced rapidly from conversational problem solving to addressing real-world tasks involving tool use, such as software engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex and Cursor, have offered end-to-end automation of the software development process. However, building effective SWE agents remains challenging due to the lack of high-quality training data and effective test cases. To address this issue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we develop a robust pipeline to synthesize test cases for patch evaluation. Second, we scale up agent trajectories to construct the training data for building SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the SWE-Dev models can achieve top performance among all open SWE agents. Specifically, the success rates of the SWE-Dev 7B and 32B parameter models reach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source models. All code, models, and datasets are publicly available at https://github.com/THUDM/SWE-Dev.",
      "authors": [
        "Wang, Haoran",
        "Hou, Zhenyu",
        "Wei, Yao",
        "Tang, Jie",
        "Dong, Yuxiao"
      ],
      "last_revised_date": "2025/06/09",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.07636",
        "TeX Source": "https://arxiv.org/src/2506.07636",
        "View PDF": "https://arxiv.org/pdf/2506.07636"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 9 Jun 2025 11:03:16 UTC (291 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/09",
      "title": "SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling",
      "models": [
        {
          "model_path": "THUDM/SWE-Dev-32B",
          "downloads": "36",
          "likes": "23",
          "trending_score": "1.0",
          "link": "https://huggingface.co/THUDM/SWE-Dev-32B"
        }
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17261",
      "abstract": "The proliferation of deep learning applications has intensified the demand for electronic hardware with low energy consumption and fast computing speed. Neuromorphic photonics have emerged as a viable alternative to directly process high-throughput information at the physical space. However, the simultaneous attainment of high linear and nonlinear expressivity posse a considerable challenge due to the power efficiency and impaired manipulability in conventional nonlinear materials and optoelectronic conversion. Here we introduce a parallel nonlinear neuromorphic processor that enables arbitrary superposition of information states in multi-dimensional channels, only by leveraging the temporal encoding of spatiotemporal metasurfaces to map the input data and trainable weights. The proposed temporal encoding nonlinearity is theoretically proved to flexibly customize the nonlinearity, while preserving quasi-static linear transformation capability within each time partition. We experimentally demonstrated the concept based on distributed spatiotemporal metasurfaces, showcasing robust performance in multi-label recognition and multi-task parallelism with asynchronous modulation. Remarkably, our nonlinear processor demonstrates dynamic memory capability in autonomous planning tasks and real-time responsiveness to canonical maze-solving problem. Our work opens up a flexible avenue for a variety of temporally-modulated neuromorphic processors tailored for complex scenarios.",
      "authors": [
        "You, Guangfeng",
        "Qian, Chao",
        "Chen, Hongsheng"
      ],
      "last_revised_date": "2025/06/09",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17261",
        "View PDF": "https://arxiv.org/pdf/2506.17261"
      },
      "subjects": [
        "Applied Physics (physics.app-ph)",
        "Neural and Evolutionary Computing (cs.NE)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 9 Jun 2025 14:55:05 UTC (7,380 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/09",
      "title": "Parallel nonlinear neuromorphic computing with temporal encoding",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17258",
      "abstract": "Generation IV (Gen-IV) nuclear power plants are envisioned to replace the current reactor fleet, bringing improvements in performance, safety, reliability, and sustainability. However, large cost investments currently inhibit the deployment of these advanced reactor concepts. Digital twins bridge real-world systems with digital tools to reduce costs, enhance decision-making, and boost operational efficiency. In this work, a digital twin framework is designed to operate the Gen-IV Fluoride-salt-cooled High-temperature Reactor, utilizing data-enhanced methods to optimize operational and maintenance policies while adhering to system constraints. The closed-loop framework integrates surrogate modeling, reinforcement learning, and Bayesian inference to streamline end-to-end communication for online regulation and self-adjustment. Reinforcement learning is used to consider component health and degradation to drive the target power generations, with constraints enforced through a Reference Governor control algorithm that ensures compliance with pump flow rate and temperature limits. These input driving modules benefit from detailed online simulations that are assimilated to measurement data with Bayesian filtering. The digital twin is demonstrated in three case studies: a one-year long-term operational period showcasing maintenance planning capabilities, short-term accuracy refinement with high-frequency measurements, and system shock capturing that demonstrates real-time recalibration capabilities when change in boundary conditions. These demonstrations validate robustness for health-aware and constraint-informed nuclear plant operation, with general applicability to other advanced reactor concepts and complex engineering systems.",
      "authors": [
        "Lim, Jasmin Y.",
        "Pylorof, Dimitrios",
        "Garcia, Humberto E.",
        "Duraisamy, Karthik"
      ],
      "last_revised_date": "2025/06/09",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17258v1",
        "Other Formats": "https://arxiv.org/format/2506.17258",
        "TeX Source": "https://arxiv.org/src/2506.17258",
        "View PDF": "https://arxiv.org/pdf/2506.17258"
      },
      "subjects": [
        "Systems and Control (eess.SY)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 9 Jun 2025 02:23:34 UTC (9,761 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/09",
      "title": "A Digital Twin Framework for Generation-IV Reactors with Reinforcement Learning-Enabled Health-Aware Supervisory Control",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17259",
      "abstract": "As artificial intelligence capabilities rapidly advance, Telco operators face a growing need to unify fragmented AI efforts across customer experience, network operations, and service orchestration. This paper proposes the design and deployment of a horizontal federated AI operating system tailored for the telecommunications domain. Unlike vertical vendor-driven platforms, this system acts as a common execution and coordination layer, enabling Telcos to deploy AI agents at scale while preserving data locality, regulatory compliance, and architectural heterogeneity. We argue that such an operating system must expose tightly scoped abstractions for telemetry ingestion, agent execution, and model lifecycle management. It should support federated training across sovereign operators, offer integration hooks into existing OSS and BSS systems, and comply with TM Forum and O-RAN standards. Importantly, the platform must be governed through a neutral foundation model to ensure portability, compatibility, and multi-vendor extensibility. This architecture offers a path to break the current silos, unlock ecosystem-level intelligence, and provide a foundation for agent-based automation across the Telco stack. The case for this horizontal layer is not only technical but structural, redefining how intelligence is deployed and composed in a distributed network environment.",
      "authors": [
        "Barros, Sebastian"
      ],
      "last_revised_date": "2025/06/09",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17259v1",
        "Other Formats": "https://arxiv.org/format/2506.17259",
        "TeX Source": "https://arxiv.org/src/2506.17259",
        "View PDF": "https://arxiv.org/pdf/2506.17259"
      },
      "subjects": [
        "Networking and Internet Architecture (cs.NI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 9 Jun 2025 03:40:03 UTC (16 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/09",
      "title": "The Case for a Horizontal Federated AI operating System for Telcos",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17262",
      "abstract": "Objective: (1) To assess whether ONH biomechanics improves prediction of three progressive visual field loss patterns in glaucoma; (2) to use explainable AI to identify strain-sensitive ONH regions contributing to these predictions. Methods: We recruited 237 glaucoma subjects. The ONH of one eye was imaged under two conditions: (1) primary gaze and (2) primary gaze with IOP elevated to ~35 mmHg via ophthalmo-dynamometry. Glaucoma experts classified the subjects into four categories based on the presence of specific visual field defects: (1) superior nasal step (N=26), (2) superior partial arcuate (N=62), (3) full superior hemifield defect (N=25), and (4) other/non-specific defects (N=124). Automatic ONH tissue segmentation and digital volume correlation were used to compute IOP-induced neural tissue and lamina cribrosa (LC) strains. Biomechanical and structural features were input to a Geometric Deep Learning model. Three classification tasks were performed to detect: (1) superior nasal step, (2) superior partial arcuate, (3) full superior hemifield defect. For each task, the data were split into 80% training and 20% testing sets. Area under the curve (AUC) was used to assess performance. Explainable AI techniques were employed to highlight the ONH regions most critical to each classification. Results: Models achieved high AUCs of 0.77-0.88, showing that ONH strain improved VF loss prediction beyond morphology alone. The inferior and inferotemporal rim were identified as key strain-sensitive regions, contributing most to visual field loss prediction and showing progressive expansion with increasing disease severity. Conclusion and Relevance: ONH strain enhances prediction of glaucomatous VF loss patterns. Neuroretinal rim, rather than the LC, was the most critical region contributing to model predictions.",
      "authors": [
        "Chuangsuwanich, Thanadet",
        "Nongpiur, Monisha E.",
        "Braeu, Fabian A.",
        "Tun, Tin A.",
        "Thiery, Alexandre",
        "Perera, Shamira",
        "Ho, Ching Lin",
        "Buist, Martin",
        "Barbastathis, George",
        "Aung, Tin",
        "Girard, Micha\u00ebl J. A."
      ],
      "last_revised_date": "2025/06/09",
      "links": {
        "Other Formats": "https://arxiv.org/format/2506.17262",
        "View PDF": "https://arxiv.org/pdf/2506.17262"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 9 Jun 2025 16:00:01 UTC (661 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/09",
      "title": "AI to Identify Strain-sensitive Regions of the Optic Nerve Head Linked to Functional Loss in Glaucoma",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17255",
      "abstract": "The rapid growth of large language models (LLMs) has outpaced the memory constraints of edge devices, necessitating extreme weight compression beyond the 1-bit limit. While quantization reduces model size, it is fundamentally limited to 1 bit per weight. Existing multiple-to-one compression methods either rely on mapping tables (inducing memory overhead) or incur severe accuracy degradation due to random weight grouping. We introduce UltraSketchLLM, an index-free, sketch-based framework that achieves ultra-low bit compression (down to 0.5 bits per weight) while preserving model performance. UltraSketchLLM leverages data sketching, a sub-linear representation technique from streaming applications, to map multiple weights to single values with bounded error. Our approach integrates an underestimate AbsMaxMin sketch to minimize relative errors for small weights, importance-aware space allocation to prioritize salient weights, and a straight-through estimator for compression-aware finetuning. Experiments on Llama-3.2-1B demonstrate up to 0.5-bit compression with competitive perplexity, alongside tolerable latency overhead. UltraSketchLLM offers a practical solution for deploying LLMs in resource-constrained environments.",
      "authors": [
        "Zou, Sunan",
        "Zhang, Ziyun",
        "Sun, Xueting",
        "Luo, Guojie"
      ],
      "last_revised_date": "2025/06/08",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17255v1",
        "Other Formats": "https://arxiv.org/format/2506.17255",
        "TeX Source": "https://arxiv.org/src/2506.17255",
        "View PDF": "https://arxiv.org/pdf/2506.17255"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 8 Jun 2025 16:55:42 UTC (960 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/08",
      "title": "UltraSketchLLM: Saliency-Driven Sketching for Ultra-Low Bit LLM Compression",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17252",
      "abstract": "Direct Preference Optimization (DPO) has emerged as an effective approach for aligning large language models (LLMs) with human preferences. However, its performance is highly dependent on the quality of the underlying human preference data. To address this bottleneck, prior work has explored various data selection strategies, but these methods often overlook the impact of the evolving states of the language model during the DPO process. %including active querying, response pair selection, and data pre-selection. In this paper, we introduce a novel problem: Sample Scheduling for DPO, which aims to dynamically and adaptively schedule training samples based on the model's evolving states throughout preference optimization. To solve this problem, we propose SamS, an efficient and effective algorithm that adaptively selects samples in each training batch based on the LLM's learning feedback to maximize the potential generalization performance. Notably, without modifying the core DPO algorithm, simply integrating SamS significantly improves performance across tasks, with minimal additional computational overhead. This work points to a promising new direction for improving LLM alignment through more effective utilization of fixed preference datasets.",
      "authors": [
        "Huang, Zixuan",
        "Ban, Yikun",
        "Fu, Lean",
        "Li, Xiaojie",
        "Dai, Zhongxiang",
        "Li, Jianxin",
        "Wang, Deqing"
      ],
      "last_revised_date": "2025/06/08",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17252v1",
        "Other Formats": "https://arxiv.org/format/2506.17252",
        "TeX Source": "https://arxiv.org/src/2506.17252",
        "View PDF": "https://arxiv.org/pdf/2506.17252"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 8 Jun 2025 10:26:09 UTC (555 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/08",
      "title": "Adaptive Sample Scheduling for Direct Preference Optimization",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17244",
      "abstract": "Short-term sentiment forecasting in financial markets (e.g., stocks, indices) is challenging due to volatility, non-linearity, and noise in OHLC (Open, High, Low, Close) data. This paper introduces a novel CMG (Chaos-Markov-Gaussian) framework that integrates chaos theory, Markov property, and Gaussian processes to improve prediction accuracy. Chaos theory captures nonlinear dynamics; the Markov chain models regime shifts; Gaussian processes add probabilistic reasoning. We enhance the framework with transformer-based deep learning models to capture temporal patterns efficiently. The CMG Framework is designed for fast, resource-efficient, and accurate forecasting of any financial instrument's OHLC time series. Unlike traditional models that require heavy infrastructure and instrument-specific tuning, CMG reduces overhead and generalizes well. We evaluate the framework on market indices, forecasting sentiment for the next trading day's first quarter. A comparative study against statistical, ML, and DL baselines trained on the same dataset with no feature engineering shows CMG consistently outperforms in accuracy and efficiency, making it valuable for analysts and financial institutions.",
      "authors": [
        "Pathan, Arif"
      ],
      "last_revised_date": "2025/06/06",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17244v1",
        "Other Formats": "https://arxiv.org/format/2506.17244",
        "TeX Source": "https://arxiv.org/src/2506.17244",
        "View PDF": "https://arxiv.org/pdf/2506.17244"
      },
      "subjects": [
        "Statistical Finance (q-fin.ST)",
        "Computational Engineering, Finance, and Science (cs.CE)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 6 Jun 2025 09:53:13 UTC (338 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/06",
      "title": "Transformers Beyond Order: A Chaos-Markov-Gaussian Framework for Short-Term Sentiment Forecasting of Any Financial OHLC timeseries Data",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.17135",
      "abstract": "Recent studies have shown that vector representations of contextual embeddings learned by pre-trained large language models (LLMs) are effective in various downstream tasks in numerical domains. Despite their significant benefits, the tendency of LLMs to hallucinate in such domains can have severe consequences in applications such as energy, nature, finance, healthcare, retail and transportation, among others. To guarantee prediction reliability and accuracy in numerical domains, it is necessary to open the black-box and provide performance guarantees through explanation. However, there is little theoretical understanding of when pre-trained language models help solve numeric downstream tasks. This paper seeks to bridge this gap by understanding when the next-word prediction capability of LLMs can be adapted to numerical domains through a novel analysis based on the concept of isotropy in the contextual embedding space. Specifically, we consider a log-linear model for LLMs in which numeric data can be predicted from its context through a network with softmax in the output layer of LLMs (i.e., language model head in self-attention). We demonstrate that, in order to achieve state-of-the-art performance in numerical domains, the hidden representations of the LLM embeddings must possess a structure that accounts for the shift-invariance of the softmax function. By formulating a gradient structure of self-attention in pre-trained models, we show how the isotropic property of LLM embeddings in contextual embedding space preserves the underlying structure of representations, thereby resolving the shift-invariance problem and providing a performance guarantee. Experiments show that different characteristics of numeric data and model architecture could have different impacts on isotropy.",
      "authors": [
        "Shelim, Rashed",
        "Xu, Shengzhe",
        "Saad, Walid",
        "Ramakrishnan, Naren"
      ],
      "last_revised_date": "2025/06/04",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2505.17135v3",
        "Other Formats": "https://arxiv.org/format/2505.17135",
        "TeX Source": "https://arxiv.org/src/2505.17135",
        "View PDF": "https://arxiv.org/pdf/2505.17135"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 22 May 2025 05:10:34 UTC (27,842 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 26 May 2025 03:55:16 UTC (27,841 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Wed, 4 Jun 2025 19:58:08 UTC (5,501 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2025/05/22",
      "title": "When can isotropy help adapt LLMs' next word prediction to numerical domains?",
      "tasks": [],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17238",
      "abstract": "Reasoning models are large language models that emit a long chain-of-thought before answering, providing both higher accuracy and explicit reasoning for their response. A major question has been whether language model reasoning generalizes beyond mathematics, programming, and logic, where most previous work has focused. We demonstrate that reasoning models can be post-trained for chemistry without additional domain pretraining, and require substantially less data compared to contemporary domain-specific models. We report ether0, a 24B parameter LLM (based on Mistral-Small-24B) that can reason in natural language and respond with chemical structures. This reasoning model was trained with reinforcement learning on 640,730 experimentally-grounded chemistry problems across 375 tasks ranging from synthesizability, to blood-brain barrier permeability, to human receptor activity, to scent. Our model exceeds general-purpose chemistry models, frontier models, and human experts on molecular design tasks. It is also more data efficient relative to specialized models. We anticipate that this method can be applied to train data-efficient language models specialized for tasks across a wide variety of scientific domains.",
      "authors": [
        "Narayanan, Siddharth M.",
        "Braza, James D.",
        "Griffiths, Ryan-Rhys",
        "Bou, Albert",
        "Wellawatte, Geemi",
        "Ramos, Mayk Caldas",
        "Mitchener, Ludovico",
        "Rodriques, Samuel G.",
        "White, Andrew D."
      ],
      "last_revised_date": "2025/06/04",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17238v1",
        "Other Formats": "https://arxiv.org/format/2506.17238",
        "TeX Source": "https://arxiv.org/src/2506.17238",
        "View PDF": "https://arxiv.org/pdf/2506.17238"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 4 Jun 2025 17:57:18 UTC (12,228 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/06/04",
      "title": "Training a Scientific Reasoning Model for Chemistry",
      "datasets": [
        {
          "dataset_name": "futurehouse/ether0-benchmark",
          "downloads": "1471",
          "likes": "8",
          "link": "https://huggingface.co/datasets/futurehouse/ether0-benchmark"
        }
      ],
      "models": [
        {
          "model_path": "futurehouse/ether0",
          "downloads": "999",
          "likes": "48",
          "trending_score": "6.0",
          "link": "https://huggingface.co/futurehouse/ether0"
        }
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.15690",
      "abstract": "The increasing use of synthetic data from the public Internet has enhanced data usage efficiency in large language model (LLM) training. However, the potential threat of model collapse remains insufficiently explored. Existing studies primarily examine model collapse in a single model setting or rely solely on statistical surrogates. In this work, we introduce LLM Web Dynamics (LWD), an efficient framework for investigating model collapse at the network level. By simulating the Internet with a retrieval-augmented generation (RAG) database, we analyze the convergence pattern of model outputs. Furthermore, we provide theoretical guarantees for this convergence by drawing an analogy to interacting Gaussian Mixture Models.",
      "authors": [
        "Wang, Tianyu",
        "Pang, Lingyou",
        "Horiguchi, Akira",
        "Priebe, Carey E."
      ],
      "last_revised_date": "2025/05/26",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.15690v1",
        "Other Formats": "https://arxiv.org/format/2506.15690",
        "TeX Source": "https://arxiv.org/src/2506.15690",
        "View PDF": "https://arxiv.org/pdf/2506.15690"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Social and Information Networks (cs.SI)",
        "Methodology (stat.ME)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 26 May 2025 22:10:52 UTC (373 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/05/26",
      "title": "LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2503.13794",
      "abstract": "Large foundation models trained on large-scale vision-language data can boost Open-Vocabulary Object Detection (OVD) via synthetic training data, yet the hand-crafted pipelines often introduce bias and overfit to specific prompts. We sidestep this issue by directly fusing hidden states from Large Language Models (LLMs) into detectors-an avenue surprisingly under-explored. This paper presents a systematic method to enhance visual grounding by utilizing decoder layers of the LLM of an MLLM. We introduce a zero-initialized cross-attention adapter to enable efficient knowledge fusion from LLMs to object detectors, a new approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We find that intermediate LLM layers already encode rich spatial semantics; adapting only the early layers yields most of the gain. With Swin-T as the vision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at just 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to 6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths further corroborate our design.",
      "authors": [
        "Zhou, Yang",
        "Zhao, Shiyu",
        "Chen, Yuxiao",
        "Wang, Zhenting",
        "Jin, Can",
        "Metaxas, Dimitris N."
      ],
      "last_revised_date": "2025/05/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2503.13794v3",
        "Other Formats": "https://arxiv.org/format/2503.13794",
        "TeX Source": "https://arxiv.org/src/2503.13794",
        "View PDF": "https://arxiv.org/pdf/2503.13794"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Tue, 18 Mar 2025 00:50:40 UTC (42,633 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Tue, 20 May 2025 14:22:45 UTC (40,446 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Thu, 22 May 2025 18:47:26 UTC (40,438 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2025/03/18",
      "title": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation",
      "repo_urls": [
        "https://github.com/xiaofeng94/gen-enhanced-negs"
      ],
      "tasks": [
        "Decoder",
        "Object",
        "object-detection",
        "Object Detection",
        "Open-vocabulary object detection",
        "Open Vocabulary Object Detection",
        "Transfer Learning",
        "Visual Grounding"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.18770",
      "abstract": "As Artificial Intelligence (AI) becomes increasingly integrated into high-stakes domains like healthcare, effective collaboration between healthcare experts and AI systems is critical. Data-centric steering, which involves fine-tuning prediction models by improving training data quality, plays a key role in this process. However, little research has explored how varying levels of user control affect healthcare experts during data-centric steering. We address this gap by examining manual and automated steering approaches through a between-subjects, mixed-methods user study with 74 healthcare experts. Our findings show that manual steering, which grants direct control over training data, significantly improves model performance while maintaining trust and system understandability. Based on these findings, we propose design implications for a hybrid steering system that combines manual and automated approaches to increase user involvement during human-AI collaboration.",
      "authors": [
        "Bhattacharya, Aditya",
        "Stumpf, Simone",
        "Verbert, Katrien"
      ],
      "last_revised_date": "2025/05/22",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.18770v1",
        "Other Formats": "https://arxiv.org/format/2506.18770",
        "TeX Source": "https://arxiv.org/src/2506.18770",
        "View PDF": "https://arxiv.org/pdf/2506.18770"
      },
      "subjects": [
        "Human-Computer Interaction (cs.HC)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 22 May 2025 09:36:17 UTC (8,574 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/05/22",
      "title": "Importance of User Control in Data-Centric Steering for Healthcare Experts",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2505.07891",
      "abstract": "In the age of social media, the rapid spread of misinformation and rumors has led to the emergence of infodemics, where false information poses a significant threat to society. To combat this issue, we introduce TrumorGPT , a novel generative artificial intelligence solution designed for fact-checking in the health domain. TrumorGPT aims to distinguish \"trumors\", which are health-related rumors that turn out to be true, providing a crucial tool in differentiating between mere speculation and verified facts. This framework leverages a large language model (LLM) with few-shot learning for semantic health knowledge graph construction and semantic reasoning. TrumorGPT incorporates graph-based retrieval-augmented generation (GraphRAG) to address the hallucination issue common in LLMs and the limitations of static training data. GraphRAG involves accessing and utilizing information from regularly updated semantic health knowledge graphs that consist of the latest medical news and health information, ensuring that fact-checking by TrumorGPT is based on the most recent data. Evaluating with extensive healthcare datasets, TrumorGPT demonstrates superior performance in fact-checking for public health claims. Its ability to effectively conduct fact-checking across various platforms marks a critical step forward in the fight against health-related misinformation, enhancing trust and accuracy in the digital information age.",
      "authors": [
        "Hang, Ching Nam",
        "Yu, Pei-Duo",
        "Tan, Chee Wei"
      ],
      "last_revised_date": "2025/05/11",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2505.07891v1",
        "Other Formats": "https://arxiv.org/format/2505.07891",
        "TeX Source": "https://arxiv.org/src/2505.07891",
        "View PDF": "https://arxiv.org/pdf/2505.07891"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 11 May 2025 17:00:21 UTC (898 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/05/11",
      "title": "TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking",
      "tasks": [
        "Fact Checking",
        "Few-Shot Learning",
        "graph construction",
        "Hallucination",
        "Knowledge Graphs",
        "Language Modeling",
        "Language Modelling",
        "Large Language Model",
        "Misinformation",
        "Retrieval",
        "Retrieval-augmented Generation"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2504.17761",
      "abstract": "In recent years, image editing models have witnessed remarkable and rapid development. The recent unveiling of cutting-edge multimodal models such as GPT-4o and Gemini2 Flash has introduced highly promising image editing capabilities. These models demonstrate an impressive aptitude for fulfilling a vast majority of user-driven editing requirements, marking a significant advancement in the field of image manipulation. However, there is still a large gap between the open-source algorithm with these closed-source models. Thus, in this paper, we aim to release a state-of-the-art image editing model, called Step1X-Edit, which can provide comparable performance against the closed-source models like GPT-4o and Gemini2 Flash. More specifically, we adopt the Multimodal LLM to process the reference image and the user's editing instruction. A latent embedding has been extracted and integrated with a diffusion image decoder to obtain the target image. To train the model, we build a data generation pipeline to produce a high-quality dataset. For evaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world user instructions. Experimental results on GEdit-Bench demonstrate that Step1X-Edit outperforms existing open-source baselines by a substantial margin and approaches the performance of leading proprietary models, thereby making significant contributions to the field of image editing.",
      "authors": [
        "Liu, Shiyu",
        "Han, Yucheng",
        "Xing, Peng",
        "Yin, Fukun",
        "Wang, Rui",
        "Cheng, Wei",
        "Liao, Jiaqi",
        "Wang, Yingming",
        "Fu, Honghao",
        "Han, Chunrui",
        "Li, Guopeng",
        "Peng, Yuang",
        "Sun, Quan",
        "Wu, Jingwei",
        "Cai, Yan",
        "Ge, Zheng",
        "Ming, Ranchen",
        "Xia, Lei",
        "Zeng, Xianfang",
        "Zhu, Yibo",
        "Jiao, Binxing",
        "Zhang, Xiangyu",
        "Yu, Gang",
        "Jiang, Daxin"
      ],
      "last_revised_date": "2025/05/06",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2504.17761v3",
        "Other Formats": "https://arxiv.org/format/2504.17761",
        "TeX Source": "https://arxiv.org/src/2504.17761",
        "View PDF": "https://arxiv.org/pdf/2504.17761"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "details": "Thu, 24 Apr 2025 17:25:12 UTC (11,536 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 28 Apr 2025 09:56:08 UTC (11,541 KB)",
          "link": "/",
          "version": "[v2]"
        },
        {
          "details": "Tue, 6 May 2025 15:58:40 UTC (12,347 KB)",
          "version": "[v3]"
        }
      ],
      "submitted_date": "2025/04/24",
      "title": "Step1X-Edit: A Practical Framework for General Image Editing",
      "repo_urls": [
        "https://github.com/stepfun-ai/step1x-edit"
      ],
      "tasks": [
        "",
        "Image Editing",
        "Image Manipulation"
      ],
      "datasets": [
        {
          "dataset_name": "stepfun-ai/GEdit-Bench",
          "downloads": "1745",
          "likes": "13",
          "link": "https://huggingface.co/datasets/stepfun-ai/GEdit-Bench"
        }
      ],
      "models": [
        {
          "model_path": "stepfun-ai/Step1X-Edit",
          "downloads": "722",
          "likes": "294",
          "trending_score": "3.0",
          "link": "https://huggingface.co/stepfun-ai/Step1X-Edit"
        }
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2506.17224",
      "abstract": "Hydrogen's role is growing as an energy carrier, increasing the need for efficient production, with methane steam reforming being the most widely used technique. This process is crucial for applications like fuel cells, where hydrogen is converted into electricity, pushing for reactor miniaturization and optimized process control through numerical simulations. Existing models typically address either kinetic or equilibrium regimes, limiting their applicability. Here we show a surrogate model capable of unifying both regimes. An artificial neural network trained on a comprehensive dataset that includes experimental data from kinetic and equilibrium experiments, interpolated data, and theoretical data derived from theoretical models for each regime. Data augmentation and assigning appropriate weights to each data type enhanced training. After evaluating Bayesian Optimization and Random Sampling, the optimal model demonstrated high predictive accuracy for the composition of the post-reaction mixture under varying operating parameters, indicated by a mean squared error of 0.000498 and strong Pearson correlation coefficients of 0.927. The network's ability to provide continuous derivatives of its predictions makes it particularly useful for process modeling and optimization. The results confirm the surrogate model's robustness for simulating methane steam reforming in both kinetic and equilibrium regimes, making it a valuable tool for design and process optimization.",
      "authors": [
        "Pizo\u0144, Zofia",
        "Kimijima, Shinji",
        "Brus, Grzegorz"
      ],
      "last_revised_date": "2025/04/15",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2506.17224v1",
        "Other Formats": "https://arxiv.org/format/2506.17224",
        "TeX Source": "https://arxiv.org/src/2506.17224",
        "View PDF": "https://arxiv.org/pdf/2506.17224"
      },
      "subjects": [
        "Computational Engineering, Finance, and Science (cs.CE)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Tue, 15 Apr 2025 14:55:06 UTC (456 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/04/15",
      "title": "Bridging Equilibrium and Kinetics Prediction with a Data-Weighted Neural Network Model of Methane Steam Reforming",
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2504.09895",
      "abstract": "Large language models~(LLMs) are expected to be helpful, harmless, and honest. In various alignment scenarios, such as general human preference, safety, and confidence alignment, binary preference data collection and reward modeling are resource-intensive but necessary for human preference transferring. In this work, we explore using the similarity between sampled generations and high-quality reference answers as an alternative reward function for LLM alignment. Using similarity as a reward circumvents training reward models, and collecting a single reference answer potentially costs less time than constructing binary preference pairs when multiple candidates are available. Specifically, we develop \\textit{RefAlign}, a versatile REINFORCE-style alignment algorithm, which is free of reference and reward models. Instead, RefAlign utilizes BERTScore between sampled generations and high-quality reference answers as the surrogate reward. Beyond general human preference optimization, RefAlign can be readily extended to diverse scenarios, such as safety and confidence alignment, by incorporating the similarity reward with task-related objectives. In various scenarios, {RefAlign} demonstrates comparable performance to previous alignment methods while offering high efficiency.",
      "authors": [
        "Zhao, Shuai",
        "Zhu, Linchao",
        "Yang, Yi"
      ],
      "last_revised_date": "2025/04/14",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2504.09895v1",
        "Other Formats": "https://arxiv.org/format/2504.09895",
        "TeX Source": "https://arxiv.org/src/2504.09895",
        "View PDF": "https://arxiv.org/pdf/2504.09895"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 14 Apr 2025 05:43:21 UTC (414 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/04/14",
      "title": "Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data",
      "tasks": [
        "Language Modeling",
        "Language Modelling"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2504.09710",
      "abstract": "Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https://github.com/ZhentingWang/DUMP.",
      "authors": [
        "Wang, Zhenting",
        "Cui, Guofeng",
        "Wan, Kun",
        "Zhao, Wentian"
      ],
      "last_revised_date": "2025/04/13",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2504.09710v1",
        "Other Formats": "https://arxiv.org/format/2504.09710",
        "TeX Source": "https://arxiv.org/src/2504.09710",
        "View PDF": "https://arxiv.org/pdf/2504.09710"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 13 Apr 2025 20:10:27 UTC (8,946 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/04/13",
      "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training",
      "repo_urls": [
        "https://github.com/zhentingwang/dump"
      ],
      "tasks": [
        "Reinforcement Learning (RL)"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2504.01444",
      "abstract": "Multimodal Large Language Models (MLLMs), which integrate vision and other modalities into Large Language Models (LLMs), significantly enhance AI capabilities but also introduce new security vulnerabilities. By exploiting the vulnerabilities of the visual modality and the long-tail distribution characteristic of code training data, we present PiCo, a novel jailbreaking framework designed to progressively bypass multi-tiered defense mechanisms in advanced MLLMs. PiCo employs a tier-by-tier jailbreak strategy, using token-level typographic attacks to evade input filtering and embedding harmful intent within programming context instructions to bypass runtime monitoring. To comprehensively assess the impact of attacks, a new evaluation metric is further proposed to assess both the toxicity and helpfulness of model outputs post-attack. By embedding harmful intent within code-style visual instructions, PiCo achieves an average Attack Success Rate (ASR) of 84.13% on Gemini-Pro Vision and 52.66% on GPT-4, surpassing previous methods. Experimental results highlight the critical gaps in current defenses, underscoring the need for more robust strategies to secure advanced MLLMs.",
      "authors": [
        "Liu, Aofan",
        "Tang, Lulu",
        "Pan, Ting",
        "Yin, Yuguo",
        "Wang, Bin",
        "Yang, Ao"
      ],
      "last_revised_date": "2025/04/07",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2504.01444v2",
        "Other Formats": "https://arxiv.org/format/2504.01444",
        "TeX Source": "https://arxiv.org/src/2504.01444",
        "View PDF": "https://arxiv.org/pdf/2504.01444"
      },
      "subjects": [
        "Cryptography and Security (cs.CR)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 2 Apr 2025 07:54:32 UTC (243 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Mon, 7 Apr 2025 08:05:25 UTC (243 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2025/04/02",
      "title": "PiCo: Jailbreaking Multimodal Large Language Models via $\\textbf{Pi}$ctorial $\\textbf{Co}$de Contextualization",
      "tasks": [
        "input filtering",
        "PICO"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2504.00839",
      "abstract": "Predicting human behavior in shared environments is crucial for safe and efficient human-robot interaction. Traditional data-driven methods to that end are pre-trained on domain-specific datasets, activity types, and prediction horizons. In contrast, the recent breakthroughs in Large Language Models (LLMs) promise open-ended cross-domain generalization to describe various human activities and make predictions in any context. In particular, Multimodal LLMs (MLLMs) are able to integrate information from various sources, achieving more contextual awareness and improved scene understanding. The difficulty in applying general-purpose MLLMs directly for prediction stems from their limited capacity for processing large input sequences, sensitivity to prompt design, and expensive fine-tuning. In this paper, we present a systematic analysis of applying pre-trained MLLMs for context-aware human behavior prediction. To this end, we introduce a modular multimodal human activity prediction framework that allows us to benchmark various MLLMs, input variations, In-Context Learning (ICL), and autoregressive techniques. Our evaluation indicates that the best-performing framework configuration is able to reach 92.8% semantic similarity and 66.1% exact label accuracy in predicting human behaviors in the target frame.",
      "authors": [
        "Liu, Yuchen",
        "Lerch, Lino",
        "Palmieri, Luigi",
        "Rudenko, Andrey",
        "Koch, Sebastian",
        "Ropinski, Timo",
        "Aiello, Marco"
      ],
      "last_revised_date": "2025/04/01",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2504.00839v1",
        "Other Formats": "https://arxiv.org/format/2504.00839",
        "TeX Source": "https://arxiv.org/src/2504.00839",
        "View PDF": "https://arxiv.org/pdf/2504.00839"
      },
      "subjects": [
        "Robotics (cs.RO)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Tue, 1 Apr 2025 14:28:19 UTC (13,942 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/04/01",
      "title": "Context-Aware Human Behavior Prediction Using Multimodal Large Language Models: Challenges and Insights",
      "tasks": [
        "Activity Prediction",
        "Domain Generalization",
        "In-Context Learning",
        "Prediction",
        "Scene Understanding",
        "Semantic Similarity",
        "Semantic Textual Similarity"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2503.17195",
      "abstract": "Model customization requires high-quality and diverse datasets, but acquiring such data remains challenging and costly. Although large language models (LLMs) can synthesize training data, current approaches are constrained by limited seed data, model bias and insufficient control over the generation process, resulting in limited diversity and biased distribution with the increase of data scales. To tackle this challenge, we present TreeSynth, a tree-guided subspace-based data synthesis framework that recursively partitions the entire data space into hierar-chical subspaces, enabling comprehensive and diverse scaling of data synthesis. Briefly, given a task-specific description, we construct a data space partitioning tree by iteratively executing criteria determination and subspace coverage steps. This hierarchically divides the whole space (i.e., root node) into mutually exclusive and complementary atomic subspaces (i.e., leaf nodes). By collecting synthesized data according to the attributes of each leaf node, we obtain a diverse dataset that fully covers the data space. Empirically, our extensive experiments demonstrate that TreeSynth surpasses both human-designed datasets and the state-of-the-art data synthesis baselines, achieving maximum improvements of 45.2% in data diversity and 17.6% in downstream task performance across various models and tasks. Hopefully, TreeSynth provides a scalable solution to synthesize diverse and comprehensive datasets from scratch without human intervention.",
      "authors": [
        "Wang, Sheng",
        "Chen, Pengan",
        "Zhou, Jingqi",
        "Li, Qintong",
        "Dong, Jingwei",
        "Gao, Jiahui",
        "Xue, Boyang",
        "Jiang, Jiyue",
        "Kong, Lingpeng",
        "Wu, Chuan"
      ],
      "last_revised_date": "2025/03/21",
      "links": {
        "Other Formats": "https://arxiv.org/format/2503.17195",
        "TeX Source": "https://arxiv.org/src/2503.17195",
        "View PDF": "https://arxiv.org/pdf/2503.17195"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Fri, 21 Mar 2025 14:43:23 UTC (3,783 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/03/21",
      "title": "TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided Subspace Partitioning",
      "tasks": [
        "Diversity"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2412.18750",
      "abstract": "Large Language Models (LLMs) have shown significant potential in software engineering tasks such as Fault Localization (FL) and Automatic Program Repair (APR). This study investigates how input order and context size influence LLM performance in FL, a crucial step for many downstream software engineering tasks. We evaluate different method orderings using Kendall Tau distances, including \"perfect\" (where ground truths appear first) and \"worst\" (where ground truths appear last), across two benchmarks containing Java and Python projects. Our results reveal a strong order bias: in Java projects, Top-1 FL accuracy drops from 57% to 20% when reversing the order, while in Python projects, it decreases from 38% to approximately 3%. However, segmenting inputs into smaller contexts mitigates this bias, reducing the performance gap in FL from 22% and 6% to just 1% across both benchmarks. We replaced method names with semantically meaningful alternatives to determine whether this bias is due to data leakage. The observed trends remained consistent, suggesting that the bias is not caused by memorization from training data but rather by the inherent effect of input order. Additionally, we explored ordering methods based on traditional FL techniques and metrics, finding that DepGraph's ranking achieves 48% Top-1 accuracy, outperforming simpler approaches such as CallGraph(DFS). These findings highlight the importance of structuring inputs, managing context effectively, and selecting appropriate ordering strategies to enhance LLM performance in FL and other software engineering applications.",
      "authors": [
        "Rafi, Md Nakhla",
        "Kim, Dong Jae",
        "Chen, Tse-Hsun",
        "Wang, Shaowei"
      ],
      "last_revised_date": "2025/03/19",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2412.18750v2",
        "Other Formats": "https://arxiv.org/format/2412.18750",
        "TeX Source": "https://arxiv.org/src/2412.18750",
        "View PDF": "https://arxiv.org/pdf/2412.18750"
      },
      "subjects": [
        "Software Engineering (cs.SE)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 25 Dec 2024 02:48:53 UTC (1,962 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Wed, 19 Mar 2025 16:08:36 UTC (3,200 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/12/25",
      "title": "The Impact of Input Order Bias on Large Language Models for Software Fault Localization",
      "tasks": [
        "Fault localization",
        "Memorization",
        "Program Repair"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2503.03705",
      "abstract": "Large language models (LLMs) are trained on enormous documents that contain extensive world knowledge. However, it is still not well-understood how knowledge is acquired via autoregressive pre-training. This lack of understanding greatly hinders effective knowledge learning, especially for continued pretraining on up-to-date information, as this evolving information often lacks diverse repetitions like foundational knowledge. In this paper, we focus on understanding and improving LLM knowledge learning. We found and verified that knowledge learning for LLMs can be deemed as an implicit supervised task hidden in the autoregressive pre-training objective. Our findings suggest that knowledge learning for LLMs would benefit from methods designed to improve generalization ability for supervised tasks. Based on our analysis, we propose the formatting-based data augmentation to grow in-distribution samples, which does not present the risk of altering the facts embedded in documents as text paraphrasing. We also introduce sharpness-aware minimization as an effective optimization algorithm to better improve generalization. Moreover, our analysis and method can be readily extended to instruction tuning. Extensive experiment results validate our findings and demonstrate our methods' effectiveness in both continued pre-training and instruction tuning. This paper offers new perspectives and insights to interpret and design effective strategies for LLM knowledge learning.",
      "authors": [
        "Zhu, Mingkang",
        "Chen, Xi",
        "Wang, Zhongdao",
        "Yu, Bei",
        "Zhao, Hengshuang",
        "Jia, Jiaya"
      ],
      "last_revised_date": "2025/03/05",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2503.03705v1",
        "Other Formats": "https://arxiv.org/format/2503.03705",
        "TeX Source": "https://arxiv.org/src/2503.03705",
        "View PDF": "https://arxiv.org/pdf/2503.03705"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Wed, 5 Mar 2025 17:56:20 UTC (143 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/03/05",
      "title": "Effective LLM Knowledge Learning via Model Generalization",
      "tasks": [
        "Data Augmentation",
        "model",
        "World Knowledge"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2501.15225",
      "abstract": "In this work, we introduce a novel approach called Scaling to Emphasize Attention for Long-context retrieval (SEAL), which enhances the retrieval performance of large language models (LLMs) over extended contexts. Previous studies have shown that each attention head in LLMs has a unique functionality and collectively contributes to the overall behavior of the model. Similarly, we observe that specific heads are closely tied to long-context retrieval, showing positive or negative correlation with retrieval scores. Built on this insight, we propose a learning-based mechanism using zero-shot generated data to emphasize these heads, improving the model's performance in long-context retrieval tasks. By applying SEAL, we can achieve significant improvements in in-domain retrieval performance, including document QA tasks from LongBench, and considerable improvements in out-of-domain cases. Additionally, when combined with existing training-free context extension techniques, SEAL extends the context limits of LLMs while maintaining highly reliable outputs, opening new avenues for research in this field.",
      "authors": [
        "Lee, Changhun",
        "Jin, Jun-gyu",
        "Cho, Younghyun",
        "Park, Eunhyeok"
      ],
      "last_revised_date": "2025/01/25",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2501.15225v1",
        "Other Formats": "https://arxiv.org/format/2501.15225",
        "TeX Source": "https://arxiv.org/src/2501.15225",
        "View PDF": "https://arxiv.org/pdf/2501.15225"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "details": "Sat, 25 Jan 2025 14:09:39 UTC (3,849 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2025/01/25",
      "title": "SEAL: Scaling to Emphasize Attention for Long-Context Retrieval",
      "tasks": [
        "Retrieval"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2405.17618",
      "abstract": "Reinforcement learning (RL) training is inherently unstable due to factors such as moving targets and high gradient variance. Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF) can introduce additional difficulty. Differing preferences can complicate the alignment process, and prediction errors in a trained reward model can become more severe as the LLM generates unseen outputs. To enhance training robustness, RL has adopted techniques from supervised learning, such as ensembles and layer normalization. In this work, we improve the stability of RL training by adapting the reverse cross entropy (RCE) from supervised learning for noisy data to define a symmetric RL loss. We demonstrate performance improvements across various tasks and scales. We conduct experiments in discrete action tasks (Atari games) and continuous action space tasks (MuJoCo benchmark and Box2D) using Symmetric A2C (SA2C) and Symmetric PPO (SPPO), with and without added noise with especially notable performance in SPPO across different hyperparameters. Furthermore, we validate the benefits of the symmetric RL loss when using SPPO for large language models through improved performance in RLHF tasks, such as IMDB positive sentiment sentiment and TL;DR summarization tasks.",
      "authors": [
        "Byun, Ju-Seung",
        "Perrault, Andrew"
      ],
      "last_revised_date": "2024/05/29",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2405.17618v2",
        "Other Formats": "https://arxiv.org/format/2405.17618",
        "TeX Source": "https://arxiv.org/src/2405.17618",
        "View PDF": "https://arxiv.org/pdf/2405.17618"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "details": "Mon, 27 May 2024 19:28:33 UTC (245 KB)",
          "link": "/",
          "version": "[v1]"
        },
        {
          "details": "Wed, 29 May 2024 04:19:00 UTC (245 KB)",
          "version": "[v2]"
        }
      ],
      "submitted_date": "2024/05/27",
      "title": "Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales",
      "repo_urls": [
        "https://github.com/shashacks/symmetric_rl"
      ],
      "tasks": [
        "Atari Games",
        "MuJoCo",
        "reinforcement-learning",
        "Reinforcement Learning",
        "Reinforcement Learning (RL)"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    },
    {
      "id": "2402.05123",
      "abstract": "Instruction tuning is a vital step of training large language models (LLM), so how to enhance the effect of instruction tuning has received increased attention. Existing works indicate that the quality of the dataset is more crucial than the quantity during instruction tuning of LLM. Therefore, recently a lot of studies focus on exploring the methods of selecting high-quality subset from instruction datasets, aiming to reduce training costs and enhance the instruction-following capabilities of LLMs. This paper presents a comprehensive survey on data selection for LLM instruction tuning. Firstly, we introduce the wildly used instruction datasets. Then, we propose a new taxonomy of the data selection methods and provide a detailed introduction of recent advances,and the evaluation strategies and results of data selection methods are also elaborated in detail. Finally, we emphasize the open challenges and present new frontiers of this task.",
      "authors": [
        "Wang, Jiahao",
        "Zhang, Bolin",
        "Du, Qianlong",
        "Zhang, Jiajun",
        "Chu, Dianhui"
      ],
      "last_revised_date": "2024/02/04",
      "links": {
        "HTML (experimental)": "https://arxiv.org/html/2402.05123v1",
        "Other Formats": "https://arxiv.org/format/2402.05123",
        "TeX Source": "https://arxiv.org/src/2402.05123",
        "View PDF": "https://arxiv.org/pdf/2402.05123"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "details": "Sun, 4 Feb 2024 13:32:01 UTC (41 KB)",
          "version": "[v1]"
        }
      ],
      "submitted_date": "2024/02/04",
      "title": "A Survey on Data Selection for LLM Instruction Tuning",
      "repo_urls": [
        "https://github.com/bolin97/awesome-instruction-selector"
      ],
      "tasks": [
        "Instruction Following",
        "Survey"
      ],
      "type": "paper",
      "source": "arxiv",
      "relevance": {}
    }
  ],
  "subjects": [
    "Geophysics (physics.geo-ph)",
    "Computer Vision and Pattern Recognition (cs.CV)",
    "Networking and Internet Architecture (cs.NI)",
    "Multimedia (cs.MM)",
    "Optimization and Control (math.OC)",
    "Quantum Physics (quant-ph)",
    "Computer Science and Game Theory (cs.GT)",
    "Computational Engineering, Finance, and Science (cs.CE)",
    "Computation and Language (cs.CL)",
    "Databases (cs.DB)",
    "Image and Video Processing (eess.IV)",
    "Sound (cs.SD)",
    "Methodology (stat.ME)",
    "Systems and Control (eess.SY)",
    "Statistical Mechanics (cond-mat.stat-mech)",
    "Cryptography and Security (cs.CR)",
    "Robotics (cs.RO)",
    "Materials Science (cond-mat.mtrl-sci)",
    "Information Retrieval (cs.IR)",
    "Artificial Intelligence (cs.AI)",
    "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
    "Computational Geometry (cs.CG)",
    "Atmospheric and Oceanic Physics (physics.ao-ph)",
    "Neurons and Cognition (q-bio.NC)",
    "Applications (stat.AP)",
    "Fluid Dynamics (physics.flu-dyn)",
    "Applied Physics (physics.app-ph)",
    "Quantitative Methods (q-bio.QM)",
    "Statistical Finance (q-fin.ST)",
    "Biological Physics (physics.bio-ph)",
    "Probability (math.PR)",
    "Emerging Technologies (cs.ET)",
    "Signal Processing (eess.SP)",
    "Computation (stat.CO)",
    "Audio and Speech Processing (eess.AS)",
    "Social and Information Networks (cs.SI)",
    "Differential Geometry (math.DG)",
    "Machine Learning (stat.ML)",
    "Information Theory (cs.IT)",
    "Computers and Society (cs.CY)",
    "Mathematical Physics (math-ph)",
    "Biomolecules (q-bio.BM)",
    "Molecular Networks (q-bio.MN)",
    "Tissues and Organs (q-bio.TO)",
    "Multiagent Systems (cs.MA)",
    "Statistics Theory (math.ST)",
    "Hardware Architecture (cs.AR)",
    "Neural and Evolutionary Computing (cs.NE)",
    "Computational Physics (physics.comp-ph)",
    "Software Engineering (cs.SE)",
    "Distributed, Parallel, and Cluster Computing (cs.DC)",
    "Human-Computer Interaction (cs.HC)",
    "Data Structures and Algorithms (cs.DS)",
    "Graphics (cs.GR)",
    "Digital Libraries (cs.DL)",
    "Operating Systems (cs.OS)",
    "Machine Learning (cs.LG)",
    "Chemical Physics (physics.chem-ph)",
    "Numerical Analysis (math.NA)"
  ],
  "prompt": {
    "train_data": "\nYou are an expert in information retrieval. I will provide you with a list of research papers from arXiv, specifically in the *cs.HC* (Human-Computer Interaction) category.\n\nYour task is to analyze each paper and determine its relevance to the topic of **Creativity**.\n\nClassify each paper into one of the following relevance levels:\n\n* `\"strong\"`: The paper is explicitly focused on creativity.\n* `\"weak\"`: The paper mentions or touches on creativity, but it is not the main focus.\n* `\"none\"`: The paper is not related to creativity.\n\nReturn your results in the following JSON format:\n\n```json\n{\n  \"relevance\": \"strong | weak | none\",\n  \"reason\": \"Brief justification based on the paper content\"\n}\n```\n\nBe concise but specific in your reasoning, referencing key terms or sections when applicable.\n"
  },
  "description": "Data source: https://arxiv.org/list/cs.HC/new"
}