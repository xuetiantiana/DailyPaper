{
  "data": [
    {
      "id": "2506.19952",
      "abstract": "Large language models (LLMs), despite their ability to perform few-shot machine translation (MT), often lag behind dedicated MT systems trained on parallel corpora, which are crucial for high quality machine translation (MT). However, parallel corpora are often scarce or non-existent for low-resource languages. In this paper, we propose CycleDistill, a bootstrapping approach leveraging LLMs and few-shot translation to obtain high-quality MT systems. CycleDistill involves iteratively generating synthetic parallel corpora from monolingual corpora via zero- or few-shot MT, which is then used to fine-tune the model that was used for generating said data for MT. CycleDistill does not need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments focusing on three Indian languages, by relying solely on monolingual corpora, it can achieve high-quality machine translation, improving upon a few-shot baseline model by over 20-30 chrF points on average in the first iteration. We also study the effect of leveraging softmax activations during the distillation process and observe mild improvements in translation quality.",
      "authors": [
        "Deepon Halder",
        "Thanmay Jayakumar",
        "Raj Dabre"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19952",
        "HTML": "https://arxiv.org/html/2506.19952",
        "PDF": "https://arxiv.org/pdf/2506.19952"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 18:56:57 GMT",
          "size": "726kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper proposes CycleDistill, a method to generate synthetic parallel corpora from monolingual corpora using LLMs, specifically addressing the creation and processing of training data for machine translation with LLMs."
      }
    },
    {
      "id": "2506.20057",
      "abstract": "We investigate the use of randomly generated data for the sake of pre-training a model. We justify this approach theoretically from the perspective of algorithmic complexity, building on recent research that shows that sequence models can be trained to approximate Solomonoff induction. We derive similar, but complementary theoretical results. We show empirically that synthetically generated data can be used to pre-train a model before the data is seen. We replicate earlier results that models trained this way show zero-shot in-context learning across a variety of datasets, and that this performance improves with scale. We extend earlier results to real-world data, and show that finetuning a model after pre-training offers faster convergence and better generalization.",
      "authors": [
        "Peter Bloem"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20057",
        "HTML": "https://arxiv.org/html/2506.20057",
        "PDF": "https://arxiv.org/pdf/2506.20057"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 23:36:35 GMT",
          "size": "420kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Universal pre-training by iterated random computation",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper investigates the concept of using synthetically generated data for pre-training models, involving novel data processing strategies that fall under training-stage data processing relevant to LLMs."
      }
    },
    {
      "id": "2506.20061",
      "abstract": "Developing effective instruction-following policies in reinforcement learning remains challenging due to the reliance on extensive human-labeled instruction datasets and the difficulty of learning from sparse rewards. In this paper, we propose a novel approach that leverages the capabilities of large language models (LLMs) to automatically generate open-ended instructions retrospectively from previously collected agent trajectories. Our core idea is to employ LLMs to relabel unsuccessful trajectories by identifying meaningful subtasks the agent has implicitly accomplished, thereby enriching the agent's training data and substantially alleviating reliance on human annotations. Through this open-ended instruction relabeling, we efficiently learn a unified instruction-following policy capable of handling diverse tasks within a single policy. We empirically evaluate our proposed method in the challenging Craftax environment, demonstrating clear improvements in sample efficiency, instruction coverage, and overall policy performance compared to state-of-the-art baselines. Our results highlight the effectiveness of utilizing LLM-guided open-ended instruction relabeling to enhance instruction-following reinforcement learning.",
      "authors": [
        "Zhicheng Zhang",
        "Ziyan Wang",
        "Yali Du",
        "Fei Fang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20061",
        "HTML": "https://arxiv.org/html/2506.20061",
        "PDF": "https://arxiv.org/pdf/2506.20061"
      },
      "subjects": [
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 23:49:28 GMT",
          "size": "554kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper proposes a novel method using LLMs to relabel unsuccessful trajectories in instruction-following policies, effectively augmenting training data and reducing reliance on human annotations, which is a direct contribution to LLM training-stage data processing."
      }
    },
    {
      "id": "2506.20151",
      "abstract": "Autoregressive (AR) models have achieved unified and strong performance across both visual understanding and image generation tasks. However, removing undesired concepts from AR models while maintaining overall generation quality remains an open challenge. In this paper, we propose Erasure Autoregressive Model (EAR), a fine-tuning method for effective and utility-preserving concept erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation (WGA) strategy to align patch-level decoding with erasure objectives, and Thresholded Loss Masking (TLM) strategy to protect content unrelated to the target concept during fine-tuning. Furthermore, we propose a novel benchmark, Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more rigorous and comprehensive foundation for evaluating concept erasure in AR models. Specifically, we first employ structured templates across diverse large language models (LLMs) to pre-generate a large-scale corpus of target-replacement concept prompt pairs. Subsequently, we generate images from these prompts and subject them to rigorous filtering via a visual classifier to ensure concept fidelity and alignment. Extensive experimental results conducted on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR achieves marked improvements in both erasure effectiveness and model utility preservation. Code is available at: https://github.com/immc-lab/ear/",
      "authors": [
        "Haipeng Fan",
        "Shiyuan Zhang",
        "Baohunesitu",
        "Zihang Guo",
        "Huaiwen Zhang"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20151",
        "HTML": "https://arxiv.org/html/2506.20151",
        "PDF": "https://arxiv.org/pdf/2506.20151"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 06:15:07 GMT",
          "size": "10812kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "EAR: Erasing Concepts from Unified Autoregressive Models",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The EAR method involves fine-tuning autoregressive models for concept erasure, using structured templates across diverse LLMs to create a large-scale corpus. It directly relates to LLM training-stage data processing and assessing model behaviors."
      }
    },
    {
      "id": "2506.20168",
      "abstract": "Recent advancements in multimodal large language models have enhanced document understanding by integrating textual and visual information. However, existing models exhibit incompleteness within their paradigm in real-world scenarios, particularly under visual degradation. In such conditions, the current response paradigm often fails to adequately perceive visual degradation and ambiguity, leading to overreliance on linguistic priors or misaligned visual-textual reasoning. This difficulty in recognizing uncertainty frequently results in the generation of hallucinatory content, especially when a precise answer is not feasible. To better demonstrate and analyze this phenomenon and problem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR hallucination in degraded document understanding. This dataset includes test samples spanning identity cards and invoices, with simulated real-world degradations for OCR reliability. This setup allows for evaluating models' capacity, under degraded input, to distinguish reliable visual information and answer accordingly, thereby highlighting the challenge of avoiding hallucination on uncertain data. To achieve vision-faithful reasoning and thereby avoid the aforementioned issues, we further introduce a GRPO-based framework featuring a novel reward mechanism. By incorporating a self-awareness of visual uncertainty and an analysis method that initiates refusal to answer to increase task difficulty within our supervised fine-tuning and reinforcement learning framework, we successfully mitigated hallucinations in ambiguous regions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model achieves a 22\\% absolute improvement in hallucination-free accuracy over GPT-4o on KIE-HVQA and there is no significant performance drop in standard tasks, highlighting both effectiveness and robustness.",
      "authors": [
        "Zhentao He",
        "Can Zhang",
        "Ziheng Wu",
        "Zhenghao Chen",
        "Yufei Zhan",
        "Yifan Li",
        "Zhao Zhang",
        "Xian Wang",
        "Minghui Qiu"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20168",
        "HTML": "https://arxiv.org/html/2506.20168",
        "PDF": "https://arxiv.org/pdf/2506.20168"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 06:44:07 GMT",
          "size": "9379kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper introduces the KIE-HVQA benchmark to evaluate OCR hallucination in LLMs, specifically focusing on degraded document understanding. It details methods for training-stage data processing, such as supervised fine-tuning and reinforcement learning to mitigate hallucinations."
      }
    },
    {
      "id": "2506.20199",
      "abstract": "Large language models (LLMs) have enabled a wide variety of real-world applications in various domains. However, creating a high-performing application with high accuracy remains challenging, particularly for subjective tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this study investigates approaches to improving conversational emotion recognition (CER) by LLMs. Specifically, we explore how to retrieve high-quality examples in in-context learning (ICL) to enhance CER. We propose various strategies based on random and augmented example retrieval and also analyze the impact of conversational context on CER accuracy. Experiments were conducted on the three datasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented example retrieval consistently outperforms other techniques under investigation across all datasets, highlighting the importance of retrieving coherent targeted examples and enhancing them through paraphrasing.",
      "authors": [
        "Mengqi Wang",
        "Tiantian Feng",
        "Shrikanth Narayanan"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20199",
        "HTML": "https://arxiv.org/html/2506.20199",
        "PDF": "https://arxiv.org/pdf/2506.20199"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 07:39:19 GMT",
          "size": "3442kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper investigates methods to enhance conversational emotion recognition through in-context learning by retrieving and processing training examples, directly addressing training-stage data processing for improving LLM applications."
      }
    },
    {
      "id": "2506.20241",
      "abstract": "Recent Large Language Models (LLMs) have significantly advanced natural language processing and automated decision-making. However, these models still encounter difficulties when performing complex reasoning tasks involving logical deduction and systematic planning, primarily due to their reliance on implicit statistical relationships without structured knowledge representation.Inspired by cognitive science and neurosymbolic AI, we introduce a novel approach to enhance LLMs through explicit structured reasoning. First, we convert unstructured data into structured formats by explicitly annotating reasoning steps. We then employ this structured dataset to train LLMs through Supervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning capabilities of LLMs using Group Relative Policy Optimization (GRPO), incorporating two innovative algorithms--MAX-Flow and Longest Common Subsequence (LCS)--which notably improve reasoning effectiveness and reduce computational complexity. Experimental results from fine-tuning a DeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust performance across various scenarios, and improved compatibility with optimization techniques, validating the efficacy of structured reasoning integration in LLMs.",
      "authors": [
        "Yubo Dong",
        "Hehe Fan"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20241",
        "HTML": "https://arxiv.org/html/2506.20241",
        "PDF": "https://arxiv.org/pdf/2506.20241"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 08:36:12 GMT",
          "size": "8851kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "Enhancing Large Language Models through Structured Reasoning",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper focuses on enhancing LLMs through structured reasoning by converting unstructured data into structured formats and training LLMs via Supervised Fine-Tuning (SFT), directly involving data processing for LLM training."
      }
    },
    {
      "id": "2506.20274",
      "abstract": "Large Language Models (LLMs) ) have demonstrated promise in boosting productivity across AI-powered tools, yet existing benchmarks like Massive Multitask Language Understanding (MMLU) inadequately assess enterprise-specific task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy to holistically evaluate LLM capabilities in enterprise contexts. To address challenges of noisy data and costly annotation, we develop a scalable pipeline combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six leading models shows open-source contenders like DeepSeek R1 rival proprietary models in reasoning tasks but lag in judgment-based scenarios, likely due to overthinking. Our benchmark reveals critical enterprise performance gaps and offers actionable insights for model optimization. This work provides enterprises a blueprint for tailored evaluations and advances practical LLM deployment.",
      "authors": [
        "Liya Wang",
        "David Yi",
        "Damien Jose",
        "John Passarelli",
        "James Gao",
        "Jordan Leventis",
        "and Kang Li"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20274",
        "HTML": "https://arxiv.org/html/2506.20274",
        "PDF": "https://arxiv.org/pdf/2506.20274"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 09:34:25 GMT",
          "size": "784kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "Enterprise Large Language Model Evaluation Benchmark",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper describes the development of a scalable pipeline for curating a benchmark for LLM evaluation, emphasizing data quality enhancement and labeling, which is directly related to the processing of training data for LLMs."
      }
    },
    {
      "id": "2506.20331",
      "abstract": "We introduce Biomed-Enriched, a biomedical text dataset constructed from PubMed via a two-stage annotation process. In the first stage, a large language model annotates 400K paragraphs from PubMed scientific articles, assigning scores for their type (review, study, clinical case, other), domain (clinical, biomedical, other), and educational quality. The educational quality score (rated 1 to 5) estimates how useful a paragraph is for college-level learning. These annotations are then used to fine-tune a small language model, which propagates the labels across the full PMC-OA corpus. The resulting metadata allows us to extract refined subsets, including 2M clinical case paragraphs with over 450K high-quality ones from articles with commercial-use licenses, and to construct several variants via quality filtering and domain upsampling. Clinical text is typically difficult to access due to privacy constraints, as hospital records cannot be publicly shared. Hence, our dataset provides an alternative large-scale, openly available collection of clinical cases from PubMed, making it a valuable resource for biomedical and clinical NLP. Preliminary continual-pretraining experiments with OLMo2 suggest these curated subsets enable targeted improvements, with clinical upsampling boosting performance by ~5% on MMLU ProfMed and educational quality filtering improving MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster convergence, reaching same performance with a third of training tokens, indicating potential for more efficient and effective biomedical pretraining strategies.",
      "authors": [
        "Rian Touchent",
        "Nathan Godey",
        "Eric de la Clergerie"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20331",
        "HTML": "https://arxiv.org/html/2506.20331",
        "PDF": "https://arxiv.org/pdf/2506.20331"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 11:30:25 GMT",
          "size": "190kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper introduces a new dataset, Biomed-Enriched, that enriches biomedical text with LLMs for pretraining, involving detailed data collection and annotation processes directly contributing to data engineering and training-stage data processing for LLMs."
      }
    },
    {
      "id": "2506.20481",
      "abstract": "Machine learning models are known to memorize samples from their training data, raising concerns around privacy and generalization. Counterfactual self-influence is a popular metric to study memorization, quantifying how the model's prediction for a sample changes depending on the sample's inclusion in the training dataset. However, recent work has shown memorization to be affected by factors beyond self-influence, with other training samples, in particular (near-)duplicates, having a large impact. We here study memorization treating counterfactual influence as a distributional quantity, taking into account how all training samples influence how a sample is memorized. For a small language model, we compute the full influence distribution of training samples on each other and analyze its properties. We find that solely looking at self-influence can severely underestimate tangible risks associated with memorization: the presence of (near-)duplicates seriously reduces self-influence, while we find these samples to be (near-)extractable. We observe similar patterns for image classification, where simply looking at the influence distributions reveals the presence of near-duplicates in CIFAR-10. Our findings highlight that memorization stems from complex interactions across training data and is better captured by the full influence distribution than by self-influence alone.",
      "authors": [
        "Matthieu Meeus",
        "Igor Shilov",
        "Georgios Kaissis",
        "Yves-Alexandre de Montjoye"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20481",
        "HTML": "https://arxiv.org/html/2506.20481",
        "PDF": "https://arxiv.org/pdf/2506.20481"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Cryptography and Security (cs.CR)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 14:25:11 GMT",
          "size": "1980kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "Counterfactual Influence as a Distributional Quantity",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper explores memorization by quantifying the influence of training data samples, including (near-)duplicates, on language models. This relates to data engineering tasks like deduplication and data quality enhancement."
      }
    },
    {
      "id": "2506.20495",
      "abstract": "Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at https://github.com/zjunlp/ReCode.",
      "authors": [
        "Haoze Wu",
        "Yunzhi Yao",
        "Wenhao Yu",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20495",
        "HTML": "https://arxiv.org/html/2506.20495",
        "PDF": "https://arxiv.org/pdf/2506.20495"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Information Retrieval (cs.IR)",
        "Machine Learning (cs.LG)",
        "Software Engineering (cs.SE)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 14:41:13 GMT",
          "size": "580kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper presents a novel dataset construction for training LLMs to adapt to code API changes, a clear contribution to data processing for LLM training and fine-tuning in dynamic environments."
      }
    },
    {
      "id": "2506.20512",
      "abstract": "Different base language model families, such as Llama and Qwen, exhibit divergent behaviors during post-training with reinforcement learning (RL), especially on reasoning-intensive tasks. What makes a base language model suitable for reinforcement learning? Gaining deeper insight into this question is essential for developing RL-scalable foundation models of the next generation. In this work, we investigate how mid-training strategies shape RL dynamics, focusing on two representative model families: Qwen and Llama. Our study reveals that (1) high-quality mathematical corpora, such as MegaMath-Web-Pro, significantly improve both base model and RL performance, while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further adding QA-style data, particularly long chain-of-thought (CoT) reasoning examples, enhances RL outcomes, and instruction data further unlocks this effect; (3) while long-CoT improves reasoning depth, it can also induce verbosity of model responses and unstability of RL training, underscoring the importance of data formatting; (4) scaling mid-training consistently leads to stronger downstream RL performance. Building on these insights, we introduce a two-stage mid-training strategy, Stable-then-Decay, in which base models are first trained on 200B tokens with a constant learning rate, followed by 20B tokens across three CoT-focused branches with learning rate decay. This yields OctoThinker, a family of models demonstrating strong RL compatibility and closing the performance gap with more RL-friendly model families, i.e., Qwen. We hope our work will help shape pre-training strategies for foundation models in the RL era. To support further research, we release our open-source models along with a curated math reasoning-intensive corpus of over 70 billion tokens (i.e., MegaMath-Web-Pro-Max).",
      "authors": [
        "Zengzhi Wang and Fan Zhou and Xuefeng Li and Pengfei Liu"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20512",
        "HTML": "https://arxiv.org/html/2506.20512",
        "PDF": "https://arxiv.org/pdf/2506.20512"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 14:58:13 GMT",
          "size": "2387kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper involves the design and processing of large-scale training data for LLMs by introducing strategies like adding QA-style data and a large math reasoning-intensive corpus (MegaMath-Web-Pro-Max) for RL purposes, which demonstrates a strong involvement in LLM training data processing."
      }
    },
    {
      "id": "2506.20621",
      "abstract": "[Context] The increasing adoption of machine learning (ML) in software systems demands specialized ideation approaches that address ML-specific challenges, including data dependencies, technical feasibility, and alignment between business objectives and probabilistic system behavior. Traditional ideation methods like Lean Inception lack structured support for these ML considerations, which can result in misaligned product visions and unrealistic expectations. [Goal] This paper presents Define-ML, a framework that extends Lean Inception with tailored activities - Data Source Mapping, Feature-to-Data Source Mapping, and ML Mapping - to systematically integrate data and technical constraints into early-stage ML product ideation. [Method] We developed and validated Define-ML following the Technology Transfer Model, conducting both static validation (with a toy problem) and dynamic validation (in a real-world industrial case study). The analysis combined quantitative surveys with qualitative feedback, assessing utility, ease of use, and intent of adoption. [Results] Participants found Define-ML effective for clarifying data concerns, aligning ML capabilities with business goals, and fostering cross-functional collaboration. The approach's structured activities reduced ideation ambiguity, though some noted a learning curve for ML-specific components, which can be mitigated by expert facilitation. All participants expressed the intention to adopt Define-ML. [Conclusion] Define-ML provides an openly available, validated approach for ML product ideation, building on Lean Inception's agility while aligning features with available data and increasing awareness of technical feasibility.",
      "authors": [
        "Silvio Alonso",
        "Antonio Pedro Santos Alves",
        "Lucas Romao",
        "H\\'elio Lopes",
        "Marcos Kalinowski"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20621",
        "HTML": "https://arxiv.org/html/2506.20621",
        "PDF": "https://arxiv.org/pdf/2506.20621"
      },
      "subjects": [
        "Software Engineering (cs.SE)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 17:11:26 GMT",
          "size": "1062kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "Define-ML: An Approach to Ideate Machine Learning-Enabled Systems",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper presents a framework (Define-ML) for integrating data considerations into early-stage ML product ideation. It emphasizes the systematic integration of data and technical constraints, implicitly involving data engineering aspects for LLM systems."
      }
    },
    {
      "id": "2506.20629",
      "abstract": "Low-Rank Adaptation (LoRA) is a widely used finetuning method for large models. Its small memory footprint allows practitioners to adapt large models to specific tasks at a fraction of the cost of full finetuning. Different modifications have been proposed to enhance its efficiency by, for example, setting the learning rate, the rank, and the initialization. Another improvement axis is adapter placement strategy: when using LoRA, practitioners usually pick module types to adapt with LoRA, such as Query and Key modules. Few works have studied the problem of adapter placement, with nonconclusive results: original LoRA paper suggested placing adapters in attention modules, while other works suggested placing them in the MLP modules. Through an intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a lightweight method that allows automatic identification of module types where LoRA adapters should be placed, given a pretrained model and a finetuning task. We demonstrate that PLoP consistently outperforms, and in the worst case competes, with commonly used placement strategies through comprehensive experiments on supervised finetuning and reinforcement learning for reasoning.",
      "authors": [
        "Soufiane Hayou",
        "Nikhil Ghosh",
        "Bin Yu"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20629",
        "HTML": "https://arxiv.org/html/2506.20629",
        "PDF": "https://arxiv.org/pdf/2506.20629"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Computation and Language (cs.CL)",
        "Machine Learning (stat.ML)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 17:25:02 GMT",
          "size": "3256kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "This paper introduces PLoP, a method for precise LoRA adapter placement, which is a significant contribution to the fine-tuning stage of large models, directly related to data processing for LLM training."
      }
    },
    {
      "id": "2506.20639",
      "abstract": "Diffusion large language models (dLLMs) are compelling alternatives to autoregressive (AR) models because their denoising models operate over the entire sequence. The global planning and iterative refinement features of dLLMs are particularly useful for code generation. However, current training and inference mechanisms for dLLMs in coding are still under-explored. To demystify the decoding behavior of dLLMs and unlock their potential for coding, we systematically investigate their denoising processes and reinforcement learning (RL) methods. We train a 7B dLLM, \\textbf{DiffuCoder}, on 130B tokens of code. Using this model as a testbed, we analyze its decoding behavior, revealing how it differs from that of AR models: (1) dLLMs can decide how causal their generation should be without relying on semi-AR decoding, and (2) increasing the sampling temperature diversifies not only token choices but also their generation order. This diversity creates a rich search space for RL rollouts. For RL training, to reduce the variance of token log-likelihood estimates and maintain training efficiency, we propose \\textbf{coupled-GRPO}, a novel sampling scheme that constructs complementary mask noise for completions used in training. In our experiments, coupled-GRPO significantly improves DiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and reduces reliance on AR causal during decoding. Our work provides deeper insight into the machinery of dLLM generation and offers an effective, diffusion-native RL training framework. https://github.com/apple/ml-diffucoder.",
      "authors": [
        "Shansan Gong and Ruixiang Zhang and Huangjie Zheng and Jiatao Gu and Navdeep Jaitly and Lingpeng Kong and Yizhe Zhang"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20639",
        "HTML": "https://arxiv.org/html/2506.20639",
        "PDF": "https://arxiv.org/pdf/2506.20639"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 17:35:47 GMT",
          "size": "2004kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper involves training a large language model ('DiffuCoder') on 130B tokens of code and proposes new methods for reinforcement learning, focusing on the training process of LLMs for code generation."
      }
    },
    {
      "id": "2506.20573",
      "abstract": "The widespread availability of large public datasets is a key factor behind the recent successes of statistical inference and machine learning methods. However, these datasets often contain some low-quality or contaminated data, to which many learning procedures are sensitive. Therefore, the question of whether and how public datasets should be prefiltered to facilitate accurate downstream learning arises. On a technical level this requires the construction of principled data prefiltering methods which are learner-agnostic robust, in the sense of provably protecting a set of pre-specified downstream learners from corrupted data. In this work, we formalize the problem of Learner-Agnostic Robust data Prefiltering (LARP), which aims at finding prefiltering procedures that minimize a worst-case loss over a pre-specified set of learners. We first instantiate our framework in the context of scalar mean estimation with Huber estimators under the Huber data contamination model. We provide a hardness result on a specific problem instance and analyze several natural prefiltering procedures. Our theoretical results indicate that performing LARP on a heterogeneous set of learners leads to some loss in model performance compared to the alternative of prefiltering data for each learner/use-case individually. We explore the resulting utility loss and its dependence on the problem parameters via extensive experiments on real-world image and tabular data, observing statistically significant reduction in utility. Finally, we model the trade-off between the utility drop and the cost of repeated (learner-specific) prefiltering within a game-theoretic framework and showcase benefits of LARP for large datasets.",
      "authors": [
        "Kristian Minchev",
        "Dimitar Iliev Dimitrov",
        "Nikola Konstantinov"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.20573",
        "HTML": "https://arxiv.org/html/2506.20573",
        "PDF": "https://arxiv.org/pdf/2506.20573"
      },
      "subjects": [
        "Machine Learning (stat.ML)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 25 Jun 2025 16:07:59 GMT",
          "size": "110kb",
          "version": "v1"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "LARP: Learner-Agnostic Robust Data Prefiltering",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper introduces Learner-Agnostic Robust Data Prefiltering (LARP), focusing on constructing methods for robust data prefiltering which can enhance data quality for downstream learning, thus directly related to data processing for machine learning models."
      }
    },
    {
      "id": "2406.11898",
      "abstract": "Knowledge Graph Completion (KGC) attempts to predict missing facts in a Knowledge Graph (KG). Recently, there's been an increased focus on designing KGC methods that can excel in the inductive setting, where a portion or all of the entities and relations seen in inference are unobserved during training. Numerous benchmark datasets have been proposed for inductive KGC, all of which are subsets of existing KGs used for transductive KGC. However, we find that the current procedure for constructing inductive KGC datasets inadvertently creates a shortcut that can be exploited even while disregarding the relational information. Specifically, we observe that the Personalized PageRank (PPR) score can achieve strong or near SOTA performance on most datasets. In this paper, we study the root cause of this problem. Using these insights, we propose an alternative strategy for constructing inductive KGC datasets that helps mitigate the PPR shortcut. We then benchmark multiple popular methods using the newly constructed datasets and analyze their performance. The new benchmark datasets help promote a better understanding of the capabilities and challenges of inductive KGC by removing any shortcuts that obfuscate performance. The code and dataset and can be found at https://github.com/HarryShomer/Better-Inductive-KGC.",
      "authors": [
        "Harry Shomer",
        "Jay Revolinsky",
        "Jiliang Tang"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2406.11898",
        "HTML": "https://arxiv.org/html/2406.11898",
        "PDF": "https://arxiv.org/pdf/2406.11898"
      },
      "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Fri, 14 Jun 2024 21:01:46 GMT",
          "size": "1314kb",
          "version": "v1"
        },
        {
          "date": "Sun, 06 Oct 2024 07:06:34 GMT",
          "size": "2101kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 21:16:19 GMT",
          "size": "1525kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Towards Better Benchmark Datasets for Inductive Knowledge Graph Completion",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper proposes an alternative strategy for constructing inductive Knowledge Graph Completion (KGC) datasets to improve benchmarking and remove shortcuts. This contribution relates to data construction and enhancement of dataset quality, making it relevant to data engineering for LLMs."
      },
      "tasks": [
        "Inductive knowledge graph completion",
        "Knowledge Graph Completion"
      ],
      "repo_urls": [
        "https://github.com/HarryShomer/Better-Inductive-KGC"
      ]
    },
    {
      "id": "2502.11962",
      "abstract": "Instruction fine-tuning (IFT) can increase the informativeness of large language models (LLMs), but may reduce their truthfulness. This trade-off arises because IFT steers LLMs to generate responses containing long-tail knowledge that was not well covered during pre-training. As a result, models become more informative but less accurate when generalizing to unseen tasks. In this paper, we empirically demonstrate how unfamiliar knowledge in IFT datasets can negatively affect the truthfulness of LLMs, and we introduce two new IFT paradigms, $UNIT_{cut}$ and $UNIT_{ref}$, to address this issue. $UNIT_{cut}$ identifies and removes unfamiliar knowledge from IFT datasets to mitigate its impact on model truthfulness, whereas $UNIT_{ref}$ trains LLMs to recognize their uncertainty and explicitly indicate it at the end of their responses. Our experiments show that $UNIT_{cut}$ substantially improves LLM truthfulness, while $UNIT_{ref}$ maintains high informativeness and reduces hallucinations by distinguishing between confident and uncertain statements.",
      "authors": [
        "Tianyi Wu",
        "Jingwei Ni",
        "Bryan Hooi",
        "Jiaheng Zhang",
        "Elliott Ash",
        "See-Kiong Ng",
        "Mrinmaya Sachan",
        "Markus Leippold"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2502.11962",
        "HTML": "https://arxiv.org/html/2502.11962",
        "PDF": "https://arxiv.org/pdf/2502.11962"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 17 Feb 2025 16:10:30 GMT",
          "size": "9302kb",
          "version": "v1"
        },
        {
          "date": "Sun, 25 May 2025 19:39:50 GMT",
          "size": "9951kb",
          "version": "v2"
        },
        {
          "date": "Wed, 25 Jun 2025 09:51:33 GMT",
          "size": "9951kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "Balancing Truthfulness and Informativeness with Uncertainty-Aware Instruction Fine-Tuning",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper proposes new instruction fine-tuning paradigms ($UNIT_{cut}$ and $UNIT_{ref}$) that involve processing IFT datasets to improve LLM truthfulness, directly involving training-stage data processing tasks."
      },
      "tasks": []
    },
    {
      "id": "2503.02502",
      "abstract": "Long-context modeling has drawn more and more attention in the area of Large Language Models (LLMs). Continual training with long-context data becomes the de-facto method to equip LLMs with the ability to process long inputs. However, it still remains an open challenge to measure the quality of long-context training data. To address this issue, we propose a Long-context data selection framework with Attention-based Dependency Measurement (LADM), which can efficiently identify high-quality long-context data from a large-scale, multi-domain pre-training corpus. LADM leverages the retrieval capabilities of the attention mechanism to capture contextual dependencies, ensuring a comprehensive quality measurement of long-context data. Experimental results show that our LADM framework significantly boosts the performance of LLMs on multiple long-context tasks with only 1B tokens for continual training.",
      "authors": [
        "Jianghao Chen",
        "Junhong Wu",
        "Yangyifan Xu",
        "Jiajun Zhang"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.02502",
        "HTML": "https://arxiv.org/html/2503.02502",
        "PDF": "https://arxiv.org/pdf/2503.02502"
      },
      "subjects": [
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 04 Mar 2025 11:10:13 GMT",
          "size": "7065kb",
          "version": "v1"
        },
        {
          "date": "Wed, 25 Jun 2025 09:27:33 GMT",
          "size": "3354kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "LADM: Long-context Training Data Selection with Attention-based Dependency Measurement for LLMs",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper introduces a novel framework, LADM, specifically for selecting long-context training data for LLMs. The framework addresses data quality in a large-scale, multi-domain pre-training corpus, which is directly related to processing LLM training data."
      },
      "models": [
        {
          "model_path": "UltraRonin/Long-Attn-Calculator",
          "downloads": "43",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/UltraRonin/Long-Attn-Calculator"
        }
      ],
      "datasets": [
        {
          "dataset_name": "UltraRonin/pile-LlamaTokenizerFast-32k-truncated-toy",
          "downloads": "102",
          "likes": "0",
          "link": "https://huggingface.co/datasets/UltraRonin/pile-LlamaTokenizerFast-32k-truncated-toy"
        }
      ],
      "tasks": []
    },
    {
      "id": "2503.08727",
      "abstract": "Dynamically integrating new or rapidly evolving information after (Large) Language Model pre-training remains challenging, particularly in low-data scenarios or when dealing with private and specialized documents. In-context learning and retrieval-augmented generation (RAG) face limitations, including their high inference costs and their inability to capture global document information. In this paper, we propose a way of modularizing knowledge by training document-level Knowledge Modules (KMs). KMs are lightweight components implemented as parameter-efficient LoRA modules, which are trained to store information about new documents and can be easily plugged into models on demand. We show that next-token prediction performs poorly as the training objective for KMs. We instead propose Deep Context Distillation: we learn KMs parameters such as to simulate hidden states and logits of a teacher that takes the document in context. Our method outperforms standard next-token prediction and pre-instruction training techniques, across two datasets. Finally, we highlight synergies between KMs and RAG.",
      "authors": [
        "Lucas Caccia",
        "Alan Ansell",
        "Edoardo Ponti",
        "Ivan Vuli\\'c",
        "Alessandro Sordoni"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2503.08727",
        "HTML": "https://arxiv.org/html/2503.08727",
        "PDF": "https://arxiv.org/pdf/2503.08727"
      },
      "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 11 Mar 2025 01:07:57 GMT",
          "size": "68kb",
          "version": "v1"
        },
        {
          "date": "Tue, 29 Apr 2025 17:11:44 GMT",
          "size": "731kb",
          "version": "v2"
        },
        {
          "date": "Wed, 25 Jun 2025 14:45:56 GMT",
          "size": "147kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "Training Plug-n-Play Knowledge Modules with Deep Context Distillation",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper discusses modularizing knowledge and training document-level Knowledge Modules (KMs) using Deep Context Distillation, which directly pertains to the processing and preparation of training data for LLMs post-training, particularly in low-data scenarios."
      },
      "tasks": [
        "In-Context Learning",
        "Language Modeling",
        "Language Modelling",
        "Large Language Model",
        "RAG",
        "Retrieval",
        "Retrieval-augmented Generation"
      ]
    },
    {
      "id": "2505.12434",
      "abstract": "Reinforcement fine-tuning (RFT) has shown great promise in achieving humanlevel reasoning capabilities of Large Language Models (LLMs), and has recently been extended to MLLMs. Nevertheless, reasoning about videos, which is a fundamental aspect of human intelligence, remains a persistent challenge due to the complex logic, temporal and causal structures inherent in video data. To fill this gap, we propose VIDEORFT, a novel approach that extends the RFT paradigm to cultivate human-like video reasoning capabilities in MLLMs. VIDEORFT follows the standard two-stage scheme in RFT: supervised fine-tuning (SFT) with chain-of-thought (CoT) annotations, followed by reinforcement learning (RL) to improve generalization. A central challenge to achieve this in the video domain lies in the scarcity of large-scale, high-quality video CoT datasets. We address this by building a fully automatic CoT curation pipeline. First, we devise a cognitioninspired prompting strategy to elicit a reasoning LLM to generate preliminary CoTs based solely on rich, structured, and literal representations of video content. Subsequently, these CoTs are revised by a visual-language model conditioned on the actual video, ensuring visual consistency and reducing visual hallucinations. This pipeline results in two new datasets - VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To further strengthen the RL phase, we introduce a novel semantic-consistency reward that explicitly promotes the alignment between textual reasoning and visual evidence. This reward encourages the model to produce coherent, context-aware reasoning outputs grounded in visual input. Extensive experiments show that VIDEORFT achieves state-of-the-art performance on six video reasoning benchmarks.",
      "authors": [
        "Qi Wang",
        "Yanrui Yu",
        "Ye Yuan",
        "Rui Mao",
        "Tianfei Zhou"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.12434",
        "HTML": "https://arxiv.org/html/2505.12434",
        "PDF": "https://arxiv.org/pdf/2505.12434"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
      ],
      "submission_historys": [
        {
          "date": "Sun, 18 May 2025 14:14:35 GMT",
          "size": "10463kb",
          "version": "v1"
        },
        {
          "date": "Wed, 25 Jun 2025 07:35:51 GMT",
          "size": "10987kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper's primary contribution is the construction of a novel video CoT curation pipeline to create high-quality datasets (VideoRFT-CoT-102K and VideoRFT-RL-310K) for fine-tuning LLMs, directly addressing the challenges in large-scale data generation and processing."
      },
      "tasks": [
        "Reinforcement Learning (RL)"
      ],
      "repo_urls": [
        "https://github.com/qiwang98/videorft"
      ]
    },
    {
      "id": "2505.16065",
      "abstract": "Embedding-Based Retrieval (EBR) is an important technique in modern search engines, enabling semantic match between search queries and relevant results. However, search logging data on platforms like Facebook Marketplace lacks the diversity and details needed for effective EBR model training, limiting the models' ability to capture nuanced search patterns. To address this challenge, we propose Aug2Search, an EBR-based framework leveraging synthetic data generated by Generative AI (GenAI) models, in a multimodal and multitask approach to optimize query-product relevance. This paper investigates the capabilities of GenAI, particularly Large Language Models (LLMs), in generating high-quality synthetic data, and analyzing its impact on enhancing EBR models. We conducted experiments using eight Llama models and 100 million data points from Facebook Marketplace logs. Our synthetic data generation follows three strategies: (1) generate queries, (2) enhance product listings, and (3) generate queries from enhanced listings. We train EBR models on three different datasets: sampled engagement data or original data ((e.g., \"Click\" and \"Listing Interactions\")), synthetic data, and a mixture of both engagement and synthetic data to assess their performance across various training sets. Our findings underscore the robustness of Llama models in producing synthetic queries and listings with high coherence, relevance, and diversity, while maintaining low levels of hallucination. Aug2Search achieves an improvement of up to 4% in ROC_AUC with 100 million synthetic data samples, demonstrating the effectiveness of our approach. Moreover, our experiments reveal that with the same volume of training data, models trained exclusively on synthetic data often outperform those trained on original data only or a mixture of original and synthetic data.",
      "authors": [
        "Ruijie Xi",
        "He Ba",
        "Hao Yuan",
        "Rishu Agrawal",
        "Yuxin Tian",
        "Ruoyan Kong",
        "Arul Prakash"
      ],
      "last_revised_date": "2025/06/24",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.16065",
        "HTML": "https://arxiv.org/html/2505.16065",
        "PDF": "https://arxiv.org/pdf/2505.16065"
      },
      "subjects": [
        "Information Retrieval (cs.IR)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Wed, 21 May 2025 22:33:40 GMT",
          "size": "558kb",
          "version": "v1"
        },
        {
          "date": "Wed, 18 Jun 2025 17:04:04 GMT",
          "size": "558kb",
          "version": "v2"
        },
        {
          "date": "Tue, 24 Jun 2025 18:46:45 GMT",
          "size": "558kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/24",
      "title": "Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated Synthetic Data Augmentation",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper primarily focuses on generating synthetic training data using LLMs to enhance EBR models, describing processes like query generation, product listing enhancement, and analysis of their impact on model training."
      },
      "tasks": [
        "Data Augmentation",
        "Diversity",
        "Hallucination",
        "Synthetic Data Generation"
      ]
    },
    {
      "id": "2505.23018",
      "abstract": "In recent years, emotion recognition plays a critical role in applications such as human-computer interaction, mental health monitoring, and sentiment analysis. While datasets for emotion analysis in languages such as English have proliferated, there remains a pressing need for high-quality, comprehensive datasets tailored to the unique linguistic, cultural, and multimodal characteristics of Chinese. In this work, we propose \\textbf{EmotionTalk}, an interactive Chinese multimodal emotion dataset with rich annotations. This dataset provides multimodal information from 19 actors participating in dyadic conversational settings, incorporating acoustic, visual, and textual modalities. It includes 23.6 hours of speech (19,250 utterances), annotations for 7 utterance-level emotion categories (happy, surprise, sad, disgust, anger, fear, and neutral), 5-dimensional sentiment labels (negative, weakly negative, neutral, weakly positive, and positive) and 4-dimensional speech captions (speaker, speaking style, emotion and overall). The dataset is well-suited for research on unimodal and multimodal emotion recognition, missing modality challenges, and speech captioning tasks. To our knowledge, it represents the first high-quality and versatile Chinese dialogue multimodal emotion dataset, which is a valuable contribution to research on cross-cultural emotion analysis and recognition. Additionally, we conduct experiments on EmotionTalk to demonstrate the effectiveness and quality of the dataset. It will be open-source and freely available for all academic purposes. The dataset and codes will be made available at: https://github.com/NKU-HLT/EmotionTalk.",
      "authors": [
        "Haoqin Sun",
        "Xuechen Wang",
        "Jinghua Zhao",
        "Shiwan Zhao",
        "Jiaming Zhou",
        "Hui Wang",
        "Jiabei He",
        "Aobo Kong",
        "Xi Yang",
        "Yequan Wang",
        "Yonghua Lin",
        "Yong Qin"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2505.23018",
        "HTML": "https://arxiv.org/html/2505.23018",
        "PDF": "https://arxiv.org/pdf/2505.23018"
      },
      "subjects": [
        "Multimedia (cs.MM)"
      ],
      "submission_historys": [
        {
          "date": "Thu, 29 May 2025 02:56:08 GMT",
          "size": "984kb",
          "version": "v1"
        },
        {
          "date": "Fri, 30 May 2025 05:09:13 GMT",
          "size": "984kb",
          "version": "v2"
        },
        {
          "date": "Wed, 25 Jun 2025 09:38:19 GMT",
          "size": "984kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "EmotionTalk: An Interactive Chinese Multimodal Emotion Dataset With Rich Annotations",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper introduces EmotionTalk, a comprehensive Chinese multimodal emotion dataset with rich annotations, directly relevant to the construction and processing of training data for LLMs, particularly for emotion recognition tasks."
      },
      "repo_urls": [
        "https://github.com/nku-hlt/emotiontalk"
      ]
    },
    {
      "id": "2506.04689",
      "abstract": "Scaling laws predict that the performance of large language models improves with increasing model size and data size. In practice, pre-training has been relying on massive web crawls, using almost all data sources publicly available on the internet so far. However, this pool of natural data does not grow at the same rate as the compute supply. Furthermore, the availability of high-quality texts is even more limited: data filtering pipelines often remove up to 99% of the initial web scrapes to achieve state-of-the-art. To address the \"data wall\" of pre-training scaling, our work explores ways to transform and recycle data discarded in existing filtering processes. We propose REWIRE, REcycling the Web with guIded REwrite, a method to enrich low-quality documents so that they could become useful for training. This in turn allows us to increase the representation of synthetic data in the final pre-training set. Experiments at 1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw texts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points improvement respectively across 22 diverse tasks, compared to training on only filtered web data. Training on the raw-synthetic data mix is also more effective than having access to 2x web data. Through further analysis, we demonstrate that about 82% of the mixed in texts come from transforming lower-quality documents that would otherwise be discarded. REWIRE also outperforms related approaches of generating synthetic data, including Wikipedia-style paraphrasing, question-answer synthesizing and knowledge extraction. These results suggest that recycling web texts holds the potential for being a simple and effective approach for scaling pre-training data.",
      "authors": [
        "Thao Nguyen",
        "Yang Li",
        "Olga Golovneva",
        "Luke Zettlemoyer",
        "Sewoong Oh",
        "Ludwig Schmidt",
        "Xian Li"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.04689",
        "HTML": "https://arxiv.org/html/2506.04689",
        "PDF": "https://arxiv.org/pdf/2506.04689"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Thu, 05 Jun 2025 07:12:12 GMT",
          "size": "1252kb",
          "version": "v1"
        },
        {
          "date": "Wed, 25 Jun 2025 17:12:12 GMT",
          "size": "1252kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper contributes a novel method (REWIRE) for transforming and recycling discarded web data to enhance pre-training data quality and quantity for language models, directly addressing data filtering and enrichment in LLM data engineering."
      },
      "tasks": []
    },
    {
      "id": "2506.14293",
      "abstract": "We present Sleeping-DISCO 9M, a large-scale pre-training dataset for music and song. To the best of our knowledge, there are no open-source high-quality dataset representing popular and well-known songs for generative music modeling tasks such as text-music, music-captioning, singing-voice synthesis, melody reconstruction and cross-model retrieval. Past contributions focused on isolated and constrained factors whose core perspective was to create synthetic or re-recorded music corpus (e.g. GTSinger, M4Singer) and arbitrarily large-scale audio datasets (e.g. DISCO-10M and LAIONDISCO-12M) had been another focus for the community. Unfortunately, adoption of these datasets has been below substantial in the generative music community as these datasets fail to reflect real-world music and its flavour. Our dataset changes this narrative and provides a dataset that is constructed using actual popular music and world-renowned artists.",
      "authors": [
        "Tawsif Ahmed",
        "Andrej Radonjic",
        "Gollam Rabby"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.14293",
        "HTML": "https://arxiv.org/html/2506.14293",
        "PDF": "https://arxiv.org/pdf/2506.14293"
      },
      "subjects": [
        "Sound (cs.SD)",
        "Machine Learning (cs.LG)",
        "Audio and Speech Processing (eess.AS)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 17 Jun 2025 08:08:08 GMT",
          "size": "380kb",
          "version": "v1"
        },
        {
          "date": "Mon, 23 Jun 2025 18:39:59 GMT",
          "size": "380kb",
          "version": "v2"
        },
        {
          "date": "Wed, 25 Jun 2025 08:18:37 GMT",
          "size": "380kb",
          "version": "v3"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "SLEEPING-DISCO 9M: A large-scale pre-training dataset for generative music modeling",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "This paper's primary contribution involves the creation of 'Sleeping-DISCO 9M', a large-scale dataset for pre-training generative music models, which falls within training data processing by constructing a novel dataset for model training."
      },
      "datasets": [
        {
          "dataset_name": "sleeping-ai/Sleeping-DISCO-9M",
          "downloads": "707",
          "likes": "7",
          "link": "https://huggingface.co/datasets/sleeping-ai/Sleeping-DISCO-9M"
        }
      ],
      "tasks": [
        "Music Captioning",
        "Music Modeling",
        "Singing Voice Synthesis"
      ]
    },
    {
      "id": "2506.18023",
      "abstract": "This report introduces PP-DocBee2, an advanced version of the PP-DocBee, designed to enhance multimodal document understanding. Built on a large multimodal model architecture, PP-DocBee2 addresses the limitations of its predecessor through key technological improvements, including enhanced synthetic data quality, improved visual feature fusion strategy, and optimized inference methodologies. These enhancements yield an $11.4\\%$ performance boost on internal benchmarks for Chinese business documents, and reduce inference latency by $73.0\\%$ to the vanilla version. A key innovation of our work is a data quality optimization strategy for multimodal document tasks. By employing a large-scale multimodal pre-trained model to evaluate data, we apply a novel statistical criterion to filter outliers, ensuring high-quality training data. Inspired by insights into underutilized intermediate features in multimodal models, we enhance the ViT representational capacity by decomposing it into layers and applying a novel feature fusion strategy to improve complex reasoning. The source code and pre-trained model are available at \\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.",
      "authors": [
        "Kui Huang",
        "Xinrong Chen",
        "Wenyu Lv",
        "Jincheng Liao",
        "Guanzhong Wang",
        "Yi Liu"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18023",
        "HTML": "https://arxiv.org/html/2506.18023",
        "PDF": "https://arxiv.org/pdf/2506.18023"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Sun, 22 Jun 2025 13:06:13 GMT",
          "size": "132kb",
          "version": "v1"
        },
        {
          "date": "Wed, 25 Jun 2025 02:40:39 GMT",
          "size": "132kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper contributes significantly to data quality optimization for multimodal document understanding by proposing a method for filtering and enhancing synthetic data. It involves data processing strategies relevant to LLM training stages, such as ensuring high-quality training data."
      }
    },
    {
      "id": "2506.18871",
      "abstract": "In this work, we introduce OmniGen2, a versatile and open-source generative model designed to provide a unified solution for diverse generation tasks, including text-to-image, image editing, and in-context generation. Unlike OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer. This design enables OmniGen2 to build upon existing multimodal understanding models without the need to re-adapt VAE inputs, thereby preserving the original text generation capabilities. To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data. Additionally, we introduce a reflection mechanism tailored for image generation tasks and curate a dedicated reflection dataset based on OmniGen2. Despite its relatively modest parameter size, OmniGen2 achieves competitive results on multiple task benchmarks, including text-to-image and image editing. To further evaluate in-context generation, also referred to as subject-driven tasks, we introduce a new benchmark named OmniContext. OmniGen2 achieves state-of-the-art performance among open-source models in terms of consistency. We will release our models, training code, datasets, and data construction pipeline to support future research in this field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link: https://github.com/VectorSpaceLab/OmniGen2",
      "authors": [
        "Chenyuan Wu",
        "Pengfei Zheng",
        "Ruiran Yan",
        "Shitao Xiao",
        "Xin Luo",
        "Yueze Wang",
        "Wanli Li",
        "Xiyan Jiang",
        "Yexin Liu",
        "Junjie Zhou",
        "Ze Liu",
        "Ziyi Xia",
        "Chaofan Li",
        "Haoge Deng",
        "Jiahao Wang",
        "Kun Luo",
        "Bo Zhang",
        "Defu Lian",
        "Xinlong Wang",
        "Zhongyuan Wang",
        "Tiejun Huang",
        "Zheng Liu"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://creativecommons.org/licenses/by/4.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.18871",
        "HTML": "https://arxiv.org/html/2506.18871",
        "PDF": "https://arxiv.org/pdf/2506.18871"
      },
      "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
      ],
      "submission_historys": [
        {
          "date": "Mon, 23 Jun 2025 17:38:54 GMT",
          "size": "13203kb",
          "version": "v1"
        },
        {
          "date": "Wed, 25 Jun 2025 17:54:25 GMT",
          "size": "13203kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "The paper describes the development of comprehensive data construction pipelines, specifically for image editing and in-context generation data, which are crucial for training the OmniGen2 model. This indicates a significant focus on the processing and preparation of training data."
      },
      "models": [
        {
          "model_path": "OmniGen2/OmniGen2",
          "downloads": "1150",
          "likes": "144",
          "trending_score": "137.0",
          "link": "https://huggingface.co/OmniGen2/OmniGen2"
        },
        {
          "model_path": "jobs-git/OmniGen2",
          "downloads": "0",
          "likes": "0",
          "trending_score": "0.0",
          "link": "https://huggingface.co/jobs-git/OmniGen2"
        }
      ]
    },
    {
      "id": "2506.19262",
      "abstract": "With the remarkable generative capabilities of large language models (LLMs), using LLM-generated data to train downstream models has emerged as a promising approach to mitigate data scarcity in specific domains and reduce time-consuming annotations. However, recent studies have highlighted a critical issue: iterative training on self-generated data results in model collapse, where model performance degrades over time. Despite extensive research on the implications of LLM-generated data, these works often neglect the importance of data diversity, a key factor in data quality. In this work, we aim to understand the implications of the diversity of LLM-generated data on downstream model performance. Specifically, we explore how varying levels of diversity in LLM-generated data affect downstream model performance. Additionally, we investigate the performance of models trained on data that mixes different proportions of LLM-generated data, which we refer to as synthetic data. Our experimental results show that, with minimal distribution shift, moderately diverse LLM-generated data can enhance model performance in scenarios with insufficient labeled data, whereas highly diverse generated data has a negative impact. We hope our empirical findings will offer valuable guidance for future studies on LLMs as data generators.",
      "authors": [
        "Yuchang Zhu",
        "Huazhen Zhong",
        "Qunshu Lin",
        "Haotong Wei",
        "Xiaolong Sun",
        "Zixuan Yu",
        "Minghao Liu",
        "Zibin Zheng",
        "Liang Chen"
      ],
      "last_revised_date": "2025/06/25",
      "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
      "links": {
        "Abstract": "https://arxiv.org/abs/2506.19262",
        "HTML": "https://arxiv.org/html/2506.19262",
        "PDF": "https://arxiv.org/pdf/2506.19262"
      },
      "subjects": [
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)"
      ],
      "submission_historys": [
        {
          "date": "Tue, 24 Jun 2025 02:44:58 GMT",
          "size": "625kb",
          "version": "v1"
        },
        {
          "date": "Wed, 25 Jun 2025 03:25:04 GMT",
          "size": "625kb",
          "version": "v2"
        }
      ],
      "submitted_date": "2025/06/25",
      "title": "What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning",
      "relevance": {
        "keyword": "train_data",
        "level": "strong",
        "reason": "This paper examines the diversity and its impact on model fine-tuning, specifically addressing the use of LLM-generated data as a training strategy to enhance downstream model performance. It highlights methods for using synthetic data from LLM outputs, which is directly related to LLM data processing."
      }
    }
  ],
  "subjects": [
    "Computer Vision and Pattern Recognition (cs.CV)",
    "Multimedia (cs.MM)",
    "Computation and Language (cs.CL)",
    "Sound (cs.SD)",
    "Cryptography and Security (cs.CR)",
    "Information Retrieval (cs.IR)",
    "Artificial Intelligence (cs.AI)",
    "Audio and Speech Processing (eess.AS)",
    "Machine Learning (stat.ML)",
    "Software Engineering (cs.SE)",
    "Machine Learning (cs.LG)"
  ],
  "prompt": {
    "train_data": "\nYou are a computer science expert specializing in training data processing and data engineering for large language models (LLMs). You are skilled at identifying technical content in research papers that is related to **LLM training data**. I will provide you with a list of research papers from the arXiv (cs.\\*) domain.\n\n---\n\n### **Task Objective**\n\nFor each paper, determine whether it is directly related to the **processing of training data for LLMs**. Focus on identifying contributions in the following two areas:\n\n1. **Data Engineering Stage**:\n\n   * Includes tasks such as data collection, construction, cleaning, noise reduction, deduplication, filtering, format transformation, and data quality enhancement.\n\n2. **Training-Stage Data Processing**:\n\n   * Includes data preparation and processing for pre-training and post-training stages (e.g., fine-tuning, supervised fine-tuning (SFT), instruction tuning, etc.).\n\n---\n\n### **Relevance Level Classification Criteria**\n\n* `\"strong\"`: The paper's primary contribution involves the design, construction, or processing of LLM training data\u2014for example, proposing a novel data pipeline, creating large-scale training data, or contributing new methods for improving data quality.\n* `\"weak\"`: The paper mentions data sources or preprocessing briefly in the background or experiments section, uses public datasets or existing tools, and does not propose new data-related methods.\n* `\"none\"`: The paper does not address any aspect of LLM training data collection, construction, or processing.\n\n---\n\n### **Output Format (strictly follow this JSON schema)**\n\n```json\n{\n  \"result\": [\n    {\n      \"id\": \"<paper id>\",\n      \"level\": \"strong | weak | none\",\n      \"reason\": \"A 1-2 sentence explanation citing key parts of the abstract or methodology that justify the classification\"\n    }\n    // More papers...\n  ]\n}\n"
  },
  "description": "Data source: https://arxiv.org/list/cs/new"
}